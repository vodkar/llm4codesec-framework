{
  "metadata": {
    "name": "CVEFixes-with-Context-Benchmark",
    "task_type": "binary",
    "total_samples": 200,
    "cwe_distribution": {
      "CWE-212": 1,
      "CWE-20": 9,
      "CWE-79": 23,
      "CWE-369": 2,
      "CWE-78": 6,
      "CWE-787": 1,
      "CWE-209": 2,
      "CWE-94": 4,
      "CWE-287": 7,
      "CWE-200": 5,
      "CWE-59": 4,
      "CWE-362": 2,
      "CWE-918": 6,
      "CWE-532": 1,
      "CWE-681": 1,
      "CWE-22": 5,
      "CWE-284": 2,
      "CWE-367": 2,
      "CWE-617": 2,
      "CWE-290": 2,
      "CWE-312": 2,
      "CWE-254": 3,
      "CWE-354": 2,
      "CWE-662": 1,
      "CWE-601": 7,
      "CWE-668": 1,
      "CWE-835": 1,
      "CWE-125": 5,
      "CWE-843": 2,
      "CWE-476": 4,
      "CWE-295": 1,
      "CWE-400": 2,
      "CWE-74": 2,
      "CWE-352": 5,
      "CWE-29": 3,
      "CWE-134": 1,
      "CWE-444": 2,
      "CWE-310": 1,
      "CWE-131": 1,
      "CWE-502": 2,
      "CWE-755": 2,
      "CWE-255": 2,
      "CWE-264": 3,
      "CWE-707": 1,
      "CWE-770": 2,
      "CWE-203": 2,
      "CWE-613": 1,
      "CWE-119": 1,
      "CWE-611": 3,
      "CWE-77": 2,
      "CWE-89": 1,
      "CWE-116": 1,
      "CWE-75": 1,
      "CWE-1333": 1,
      "CWE-269": 1,
      "CWE-416": 1
    }
  },
  "samples": [
    {
      "id": "ContextAssembler-1",
      "code": "def _rpsl_db_query_to_graphql_out(query: RPSLDatabaseQuery, info: GraphQLResolveInfo):\n    \"\"\"\n    Given an RPSL database query, execute it and clean up the output\n    to be suitable to return to GraphQL.\n    Main changes are:\n    - Enum handling\n    - Adding the asn and prefix fields if applicable\n    - Ensuring the right fields are returned as a list of strings or a string\n    \"\"\"\n    database_handler = info.context['request'].app.state.database_handler\n    if info.context.get('sql_trace'):\n        if 'sql_queries' not in info.context:\n            info.context['sql_queries'] = [repr(query)]\n        else:\n            info.context['sql_queries'].append(repr(query))\n    for row in database_handler.execute_query(query, refresh_on_error=True):\n        graphql_result = {snake_to_camel_case(k): v for k, v in row.items() if k != 'parsed_data'}\n        if 'object_text' in row:\n            graphql_result['objectText'] = remove_auth_hashes(row['object_text'])\n        if 'rpki_status' in row:\n            graphql_result['rpkiStatus'] = row['rpki_status']\n        if row.get('ip_first') is not None and row.get('prefix_length'):\n            graphql_result['prefix'] = row['ip_first'] + '/' + str(row['prefix_length'])\n        if row.get('asn_first') is not None and row.get('asn_first') == row.get('asn_last'):\n            graphql_result['asn'] = row['asn_first']\n        object_type = resolve_rpsl_object_type(row)\n        for key, value in row.get('parsed_data', dict()).items():\n            if key == 'auth':\n                value = remove_auth_hashes(value)\n            graphql_type = schema.graphql_types[object_type][key]\n            if graphql_type == 'String' and isinstance(value, list):\n                value = '\\n'.join(value)\n            graphql_result[snake_to_camel_case(key)] = value\n        yield graphql_result\ndef remove_auth_hashes(input: Optional[str]):\n    if not input:\n        return input\n    if not any([pw_hash in input for pw_hash in PASSWORD_HASHERS_ALL.keys()]):\n        return input\n    return re_remove_passwords.sub(r'\\1 %s",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-24798",
        "description": "[{'lang': 'en', 'value': 'Internet Routing Registry daemon version 4 is an IRR database server, processing IRR objects in the RPSL format. IRRd did not always filter password hashes in query responses relating to `mntner` objects and database exports. This may have allowed adversaries to retrieve some of these hashes, perform a brute-force search for the clear-text passphrase, and use these to make unauthorised changes to affected IRR objects. This issue only affected instances that process password hashes, which means it is limited to IRRd instances that serve authoritative databases. IRRd instances operating solely as mirrors of other IRR databases are not affected. This has been fixed in IRRd 4.2.3 and the main branch. Versions in the 4.1.x series never were affected. Users of the 4.2.x series are strongly recommended to upgrade. There are no known workarounds for this issue.'}]",
        "cwe_number": 212
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-2",
      "code": "    def __call__(self, value, system):\n        if 'request' in system:\n            request = system['request']\n            mime, encoding = mimetypes.guess_type(value['filename'])\n            request.response_content_type = mime\n            if encoding:\n                request.response_encoding = encoding\n            f = os.path.join(self.repository_root,\n                             value['filename'][0].lower(),\n                             value['filename'])\n            if not os.path.exists(f):\n                dir_ = os.path.join(self.repository_root,\n                             value['filename'][0].lower())\n                if not os.path.exists(dir_):\n                    os.makedirs(dir_, 0750)\n                resp = requests.get(value['url'])\n                with open(f, 'wb') as rf:\n                    rf.write(resp.content)\n                return resp.content\n            else:\n                data = ''\n                with open(f, 'rb') as rf:\n                    data = ''\n                    while True:\n                        content = rf.read(2<<16)\n                        if not content:\n                            break\n                        data += content\n                return data\ndef get_release_file(root, request):\n    session = DBSession()\n    f = ReleaseFile.by_id(session, int(request.matchdict['file_id']))\n    rv = {'id': f.id,\n          'url': f.url,\n          'filename': f.filename,\n          }\n    f.downloads += 1\n    f.release.downloads += 1\n    f.release.package.downloads += 1\n    session.add(f.release.package)\n    session.add(f.release)\n    session.add(f)\n    return rv",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2013-1630",
        "description": "[{'lang': 'en', 'value': 'pyshop before 0.7.1 uses HTTP to retrieve packages from the PyPI repository, and does not perform integrity checks on package contents, which allows man-in-the-middle attackers to execute arbitrary code via a crafted response to a download operation.'}]",
        "cwe_number": 20
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-3",
      "code": "def render_description(text, content_type):\n    \"\"\"Render Description field to HTML\"\"\"\n    if re.match(r'^.+(\\n {8}.*)+\\n?$', text):\n        text = re.sub(r'^ {8}', '', text, flags=re.MULTILINE)\n    if content_type == 'text/x-rst':\n        html = publish_parts(\n            text, writer_name='html',\n            settings_overrides={'syntax_highlight': 'short'})['html_body']\n    elif content_type == 'text/markdown':\n        html = markdown(text, extensions=[GithubFlavoredMarkdownExtension()])\n    else:\n        html = format_html('<pre>{}</pre>', text)\n    return mark_safe(html)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2018-25056",
        "description": "[{'lang': 'en', 'value': 'A vulnerability, which was classified as problematic, was found in yolapi. Affected is the function render_description of the file yolapi/pypi/metadata.py. The manipulation of the argument text leads to cross site scripting. It is possible to launch the attack remotely. The name of the patch is a0fe129055a99f429133a5c40cb13b44611ff796. It is recommended to apply a patch to fix this issue. VDB-216966 is the identifier assigned to this vulnerability.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-4",
      "code": "  def testSimpleParallelGPU(self):\n    with ops.Graph().as_default():\n      with test_util.device(use_gpu=True):\n        for shape in (2,), (3,), (2, 3), (3, 2), (4, 3, 2), (100, 24, 24, 3):\n          with self.subTest(shape=shape):\n            data = self.randn(shape, np.float32)\n            xs = list(map(constant_op.constant, data))\n            c = array_ops.parallel_stack(xs)\n            self.assertAllEqual(c, data)\n  def testConst(self):\n    np.random.seed(7)\n    with test_util.use_gpu():\n      a = constant_op.constant([1, 2, 3, 4, 5, 6])\n      b = array_ops.reshape(a, array_ops.stack([2, 3]))\n      self.assertAllEqual(b.get_shape(), [2, 3])\n      for shape in (2,), (3,), (2, 3), (3, 2), (4, 3, 2), (8, 2, 10):\n        for dtype in [np.bool_, np.float32, np.int16, np.int32, np.int64]:\n          with self.subTest(shape=shape, dtype=dtype):\n            data = self.randn(shape, dtype)\n            c = array_ops.stack(data)\n            if not context.executing_eagerly():\n              self.assertEqual(c.op.type, \"Const\")\n            self.assertAllEqual(c, data)\n            if len(shape) == 1:\n              data_list = list(data)\n              cl = array_ops.stack(data_list)\n              if not context.executing_eagerly():\n                self.assertEqual(cl.op.type, \"Const\")\n              self.assertAllEqual(cl, data)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-41209",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. In affected versions the implementations for convolution operators trigger a division by 0 if passed empty filter tensor arguments. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.'}]",
        "cwe_number": 369
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-5",
      "code": "    async def get_resolved_ref(self):\n        if hasattr(self, 'resolved_ref'):\n            return self.resolved_ref\n        try:\n            self.sha1_validate(self.unresolved_ref)\n        except ValueError:\n            command = [\"git\", \"ls-remote\", self.repo, self.unresolved_ref]\n            result = subprocess.run(command, universal_newlines=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            if result.returncode:\n                raise RuntimeError(\"Unable to run git ls-remote to get the `resolved_ref`: {}\".format(result.stderr))\n            if not result.stdout:\n                return None\n            resolved_ref = result.stdout.split(None, 1)[0]\n            self.sha1_validate(resolved_ref)\n            self.resolved_ref = resolved_ref\n        else:\n            self.resolved_ref = self.unresolved_ref\n        return self.resolved_ref",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-39159",
        "description": "[{'lang': 'en', 'value': 'BinderHub is a kubernetes-based cloud service that allows users to share reproducible interactive computing environments from code repositories. In affected versions a remote code execution vulnerability has been identified in BinderHub, where providing BinderHub with maliciously crafted input could execute code in the BinderHub context, with the potential to egress credentials of the BinderHub deployment, including JupyterHub API tokens, kubernetes service accounts, and docker registry credentials. This may provide the ability to manipulate images and other user created pods in the deployment, with the potential to escalate to the host depending on the underlying kubernetes configuration. Users are advised to update to version 0.2.0-n653. If users are unable to update they may disable the git repo provider by specifying the `BinderHub.repo_providers` as a workaround.'}]",
        "cwe_number": 78
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-6",
      "code": "def parse_line(s):\n    s = s.rstrip()\n    r = re.sub(REG_LINE_GPERF, '', s)\n    if r != s: return r\n    r = re.sub(REG_HASH_FUNC, 'hash(OnigCodePoint codes[])', s)\n    if r != s: return r\n    r = re.sub(REG_STR_AT, 'onig_codes_byte_at(codes, \\\\1)', s)\n    if r != s: return r\n    r = re.sub(REG_UNFOLD_KEY, 'unicode_unfold_key(OnigCodePoint code)', s)\n    if r != s: return r\n    r = re.sub(REG_ENTRY, '{\\\\1, \\\\2, \\\\3}', s)\n    if r != s: return r\n    r = re.sub(REG_EMPTY_ENTRY, '{0xffffffff, \\\\1, \\\\2}', s)\n    if r != s: return r\n    r = re.sub(REG_IF_LEN, 'if (0 == 0)', s)\n    if r != s: return r\n    r = re.sub(REG_GET_HASH, 'int key = hash(&code);', s)\n    if r != s: return r\n    r = re.sub(REG_GET_CODE, 'OnigCodePoint gcode = wordlist[key].code;', s)\n    if r != s: return r\n    r = re.sub(REG_CODE_CHECK, 'if (code == gcode)', s)\n    if r != s: return r\n    return s",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2017-9225",
        "description": "[{'lang': 'en', 'value': 'An issue was discovered in Oniguruma 6.2.0, as used in Oniguruma-mod in Ruby through 2.4.1 and mbstring in PHP through 7.1.5. A stack out-of-bounds write in onigenc_unicode_get_case_fold_codes_by_str() occurs during regular expression compilation. Code point 0xFFFFFFFF is not properly handled in unicode_unfold_key(). A malformed regular expression could result in 4 bytes being written off the end of a stack buffer of expand_case_fold_string() during the call to onigenc_unicode_get_case_fold_codes_by_str(), a typical stack buffer overflow.'}]",
        "cwe_number": 787
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-7",
      "code": "    async def get(self, configuration: str | None = None) -> None:\n        \"\"\"Get the content of a file.\"\"\"\n        loop = asyncio.get_running_loop()\n        filename = settings.rel_path(configuration)\n        content = await loop.run_in_executor(\n            None, self._read_file, filename, configuration\n        )\n        if content is not None:\n            self.write(content)\n    async def post(self, configuration: str | None = None) -> None:\n        \"\"\"Write the content of a file.\"\"\"\n        loop = asyncio.get_running_loop()\n        config_file = settings.rel_path(configuration)\n        await loop.run_in_executor(\n            None, self._write_file, config_file, self.request.body\n        )\n        DASHBOARD.entries.async_schedule_storage_json_update(filename)\n        self.set_status(200)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-27081",
        "description": "[{'lang': 'en', 'value': 'ESPHome is a system to control your ESP8266/ESP32. A security misconfiguration in the edit configuration file API in the dashboard component of ESPHome version 2023.12.9 (command line installation) allows authenticated remote attackers to read and write arbitrary files under the configuration directory rendering remote code execution possible.  This vulnerability is patched in 2024.2.1.\\n'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-8",
      "code": "    def add(self, item: Model, raise_exception: bool = False) -> bool:\n        try:\n            self.session.add(item)\n            self.session.commit()\n            self.message = (as_unicode(self.add_row_message), \"success\")\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.add_integrity_error_message), \"warning\")\n            log.warning(LOGMSG_WAR_DBI_ADD_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            if raise_exception:\n                raise e\n            return False\n        except Exception as e:\n            self.message = (\n                as_unicode(self.general_error_message + \" \" + str(sys.exc_info()[0])),\n                \"danger\",\n            )\n            log.exception(LOGMSG_ERR_DBI_ADD_GENERIC.format(str(e)))\n            self.session.rollback()\n            if raise_exception:\n                raise e\n            return False\n    def edit(self, item: Model, raise_exception: bool = False) -> bool:\n        try:\n            self.session.merge(item)\n            self.session.commit()\n            self.message = (as_unicode(self.edit_row_message), \"success\")\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.edit_integrity_error_message), \"warning\")\n            log.warning(LOGMSG_WAR_DBI_EDIT_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            if raise_exception:\n                raise e\n            return False\n        except Exception as e:\n            self.message = (\n                as_unicode(self.general_error_message + \" \" + str(sys.exc_info()[0])),\n                \"danger\",\n            )\n            log.exception(LOGMSG_ERR_DBI_EDIT_GENERIC.format(str(e)))\n            self.session.rollback()\n            if raise_exception:\n                raise e\n            return False\n    def delete(self, item: Model, raise_exception: bool = False) -> bool:\n        try:\n            self._delete_files(item)\n            self.session.delete(item)\n            self.session.commit()\n            self.message = (as_unicode(self.delete_row_message), \"success\")\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.delete_integrity_error_message), \"warning\")\n            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            if raise_exception:\n                raise e\n            return False\n        except Exception as e:\n            self.message = (\n                as_unicode(self.general_error_message + \" \" + str(sys.exc_info()[0])),\n                \"danger\",\n            )\n            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))\n            self.session.rollback()\n            if raise_exception:\n                raise e\n            return False\n    def delete_all(self, items: List[Model]) -> bool:\n        try:\n            for item in items:\n                self._delete_files(item)\n                self.session.delete(item)\n            self.session.commit()\n            self.message = (as_unicode(self.delete_row_message), \"success\")\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.delete_integrity_error_message), \"warning\")\n            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (\n                as_unicode(self.general_error_message + \" \" + str(sys.exc_info()[0])),\n                \"danger\",\n            )\n            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False\n    def _get_values(self, lst, list_columns):\n        \"\"\"\n            Get Values: formats values for list template.\n            returns [{'col_name':'col_value',....},{'col_name':'col_value',....}]\n            :param lst:\n                The list of item objects from query\n            :param list_columns:\n                The list of columns to include\n        \"\"\"\n        retlst = []\n        for item in lst:\n            retdict = {}\n            for col in list_columns:\n                retdict[col] = self._get_attr_value(item, col)\n            retlst.append(retdict)\n        return retlst\n    def get_values(self, lst, list_columns):\n        \"\"\"\n            Get Values: formats values for list template.\n            returns [{'col_name':'col_value',....},{'col_name':'col_value',....}]\n            :param lst:\n                The list of item objects from query\n            :param list_columns:\n                The list of columns to include\n        \"\"\"\n        for item in lst:\n            retdict = {}\n            for col in list_columns:\n                retdict[col] = self._get_attr_value(item, col)\n            yield retdict\n    def get_values_json(self, lst, list_columns):\n        \"\"\"\n            Converts list of objects from query to JSON\n        \"\"\"\n        result = []\n        for item in self.get_values(lst, list_columns):\n            for key, value in list(item.items()):\n                if isinstance(value, datetime.datetime) or isinstance(\n                    value, datetime.date\n                ):\n                    value = value.isoformat()\n                    item[key] = value\n                if isinstance(value, list):\n                    item[key] = [str(v) for v in value]\n            result.append(item)\n        return result\n    def add(self, item):\n        \"\"\"\n            Adds object\n        \"\"\"\n        raise NotImplementedError\n    def edit(self, item):\n        \"\"\"\n            Edit (change) object\n        \"\"\"\n        raise NotImplementedError\n    def delete(self, item):\n        \"\"\"\n            Deletes object\n        \"\"\"\n        raise NotImplementedError\n    def get_keys(self, lst):\n        \"\"\"\n            return a list of pk values from object list\n        \"\"\"\n        pk_name = self.get_pk_name()\n        if self.is_pk_composite():\n            return [[getattr(item, pk) for pk in pk_name] for item in lst]\n        else:\n            return [getattr(item, pk_name) for item in lst]\n    def get_pk_name(self):\n        \"\"\"\n            Returns the primary key name\n        \"\"\"\n        raise NotImplementedError\n    def get(self, pk, filter=None):\n        \"\"\"\n            return the record from key, you can optionally pass filters\n            if pk exits on the db but filters exclude it it will return none.\n        \"\"\"\n        pass\n    def get_related_interface(self, col_name):\n        \"\"\"\n            Returns a BaseInterface for the related model\n            of column name.\n            :param col_name: Column name with relation\n            :return: BaseInterface\n        \"\"\"\n        raise NotImplementedError\n    def get_columns_list(self):\n        \"\"\"\n            Returns a list of all the columns names\n        \"\"\"\n        return []\n    def get_user_columns_list(self):\n        \"\"\"\n            Returns a list of user viewable columns names\n        \"\"\"\n        return self.get_columns_list()\n    def get_search_columns_list(self):\n        \"\"\"\n            Returns a list of searchable columns names\n        \"\"\"\n        return []\n    def get_order_columns_list(self, list_columns=None):\n        \"\"\"\n            Returns a list of order columns names\n        \"\"\"\n        return []",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-34110",
        "description": "[{'lang': 'en', 'value': 'Flask-AppBuilder is an application development framework, built on top of Flask. Prior to version 4.3.2, an authenticated malicious actor with Admin privileges, could by adding a special character on the add, edit User forms trigger a database error, this error is surfaced back to this actor on the UI. On certain database engines this error can include the entire user row including the pbkdf2:sha256 hashed password. This vulnerability has been fixed in version 4.3.2.\\n'}]",
        "cwe_number": 209
      },
      "cwe_types": [],
      "severity": "low"
    },
    {
      "id": "ContextAssembler-9",
      "code": "def parse_env_variables(cls: Type[\"pl.Trainer\"], template: str = \"PL_%(cls_name)s_%(cls_argument)s\") -> Namespace:\n    \"\"\"Parse environment arguments if they are defined.\n    Examples:\n        >>> from pytorch_lightning import Trainer\n        >>> parse_env_variables(Trainer)\n        Namespace()\n        >>> import os\n        >>> os.environ[\"PL_TRAINER_GPUS\"] = '42'\n        >>> os.environ[\"PL_TRAINER_BLABLABLA\"] = '1.23'\n        >>> parse_env_variables(Trainer)\n        Namespace(gpus=42)\n        >>> del os.environ[\"PL_TRAINER_GPUS\"]\n    \"\"\"\n    cls_arg_defaults = get_init_arguments_and_types(cls)\n    env_args = {}\n    for arg_name, _, _ in cls_arg_defaults:\n        env = template % {\"cls_name\": cls.__name__.upper(), \"cls_argument\": arg_name.upper()}\n        val = os.environ.get(env)\n        if not (val is None or val == \"\"):\n            with suppress(Exception):\n                val = eval(val)\n            env_args[arg_name] = val\n    return Namespace(**env_args)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-0845",
        "description": "[{'lang': 'en', 'value': 'Code Injection in GitHub repository pytorchlightning/pytorch-lightning prior to 1.6.0.'}]",
        "cwe_number": 94
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-10",
      "code": "    def login(self):\n        \"\"\"Login endpoint for the API returns a JWT and optionally a refresh token\n        ---\n        post:\n          description: >-\n            Authenticate and get a JWT access and refresh token\n          requestBody:\n            required: true\n            content:\n              application/json:\n                schema:\n                  type: object\n                  properties:\n                    username:\n                      description: The username for authentication\n                      example: admin\n                      type: string\n                    password:\n                      description: The password for authentication\n                      example: complex-password\n                      type: string\n                    provider:\n                      description: Choose an authentication provider\n                      example: db\n                      type: string\n                      enum:\n                      - db\n                      - ldap\n                    refresh:\n                      description: If true a refresh token is provided also\n                      example: true\n                      type: boolean\n          responses:\n            200:\n              description: Authentication Successful\n              content:\n                application/json:\n                  schema:\n                    type: object\n                    properties:\n                      access_token:\n                        type: string\n                      refresh_token:\n                        type: string\n            400:\n              $ref: '\n            401:\n              $ref: '\n            500:\n              $ref: '\n        \"\"\"\n        if not request.is_json:\n            return self.response_400(message=\"Request payload is not JSON\")\n        username = request.json.get(API_SECURITY_USERNAME_KEY, None)\n        password = request.json.get(API_SECURITY_PASSWORD_KEY, None)\n        provider = request.json.get(API_SECURITY_PROVIDER_KEY, None)\n        refresh = request.json.get(API_SECURITY_REFRESH_KEY, False)\n        if not username or not password or not provider:\n            return self.response_400(message=\"Missing required parameter\")\n        if provider == API_SECURITY_PROVIDER_DB:\n            user = self.appbuilder.sm.auth_user_db(username, password)\n        elif provider == API_SECURITY_PROVIDER_LDAP:\n            user = self.appbuilder.sm.auth_user_ldap(username, password)\n        else:\n            return self.response_400(\n                message=\"Provider {} not supported\".format(provider)\n            )\n        if not user:\n            return self.response_401()\n        resp = dict()\n        resp[API_SECURITY_ACCESS_TOKEN_KEY] = create_access_token(\n            identity=user.id, fresh=True\n        )\n        if refresh:\n            resp[API_SECURITY_REFRESH_TOKEN_KEY] = create_refresh_token(\n                identity=user.id\n            )\n        return self.response(200, **resp)\n    def refresh(self):\n        \"\"\"\n            Security endpoint for the refresh token, so we can obtain a new\n            token without forcing the user to login again\n        ---\n        post:\n          description: >-\n            Use the refresh token to get a new JWT access token\n          responses:\n            200:\n              description: Refresh Successful\n              content:\n                application/json:\n                  schema:\n                    type: object\n                    properties:\n                      access_token:\n                        description: A new refreshed access token\n                        type: string\n            401:\n              $ref: '\n            500:\n              $ref: '\n          security:\n            - jwt_refresh: []\n        \"\"\"\n        resp = {\n            API_SECURITY_ACCESS_TOKEN_KEY: create_access_token(\n                identity=get_jwt_identity(), fresh=False\n            )\n        }\n        return self.response(200, **resp)\n    def builtin_roles(self):\n        return self._builtin_roles\n    def auth_type(self):\n        return self.appbuilder.get_app.config[\"AUTH_TYPE\"]\n    def auth_username_ci(self):\n        return self.appbuilder.get_app.config.get(\"AUTH_USERNAME_CI\", True)\n    def auth_role_admin(self):\n        return self.appbuilder.get_app.config[\"AUTH_ROLE_ADMIN\"]\n    def auth_role_public(self):\n        return self.appbuilder.get_app.config[\"AUTH_ROLE_PUBLIC\"]\n    def auth_ldap_server(self):\n        return self.appbuilder.get_app.config[\"AUTH_LDAP_SERVER\"]\n    def auth_ldap_use_tls(self):\n        return self.appbuilder.get_app.config[\"AUTH_LDAP_USE_TLS\"]\n    def auth_user_registration(self):\n        return self.appbuilder.get_app.config[\"AUTH_USER_REGISTRATION\"]\n    def auth_user_registration_role(self):\n        return self.appbuilder.get_app.config[\"AUTH_USER_REGISTRATION_ROLE\"]",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-41265",
        "description": "[{'lang': 'en', 'value': 'Flask-AppBuilder is a development framework built on top of Flask. Verions prior to 3.3.4 contain an improper authentication vulnerability in the REST API. The issue allows for a malicious actor with a carefully crafted request to successfully authenticate and gain access to existing protected REST API endpoints. This only affects non database authentication types and new REST API endpoints. Users should upgrade to Flask-AppBuilder 3.3.4 to receive a patch.'}]",
        "cwe_number": 287
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-11",
      "code": "    def pipeline_files_upload(self, base_path, file_handle, case_customer, case_name, is_update):\n        \"\"\"\n        Handle the files for a specific\n        :return:\n        \"\"\"\n        if base_path and Path(base_path).is_dir:\n            file_handle.save(Path(base_path, file_handle.filename))\n            return InterfaceStatus.I2Success(\"Successfully saved file {} to {}\".format(file_handle.filename, base_path))\n        else:\n            return InterfaceStatus.I2Error(\"Directory {} not found. Can't save file\".format(base_path))\n    def task_files_import(self, task_args):\n        try:\n            configuration = self.get_configuration_dict()\n            if self._evidence_storage:\n                if configuration.is_success():\n                    importer = ImportDispatcher(task_self=self,\n                                                task_args=task_args,\n                                                evidence_storage=self._evidence_storage,\n                                                configuration=configuration.get_data(),\n                                                log=self.log\n                                                )\n                    ret = importer.import_files()\n                    if not ret:\n                        return InterfaceStatus.I2Error(logs=list(self.message_queue))\n                    return InterfaceStatus.I2Success(logs=list(self.message_queue))\n                else:\n                    self.log.error(logs=[configuration.get_message()])\n                    logs = [configuration.get_message()]\n            else:\n                self.log.error('Evidence storage not available')\n                logs = ['Evidence storage not available']\n            return InterfaceStatus.I2Error(logs=logs)\n        except Exception as e:\n            traceback.print_exc()\n            return InterfaceStatus.I2Error(logs=[traceback.print_exc()])",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-34060",
        "description": "[{'lang': 'en', 'value': 'IrisEVTXModule is an interface module for Evtx2Splunk and Iris in order to ingest Microsoft EVTX log files. The `iris-evtx-module` is a pipeline plugin of `iris-web` that processes EVTX files through IRIS web application. During the upload of an EVTX through this pipeline, the filename is not safely handled and may cause an Arbitrary File Write. This can lead to a remote code execution (RCE) when combined with a Server Side Template Injection (SSTI). This vulnerability has been patched in version 1.0.0.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-12",
      "code": "def filefind(filename: str, path_dirs: Sequence[str] | str | None = None) -> str:\n    \"\"\"Find a file by looking through a sequence of paths.\n    This iterates through a sequence of paths looking for a file and returns\n    the full, absolute path of the first occurrence of the file.  If no set of\n    path dirs is given, the filename is tested as is, after running through\n    :func:`expandvars` and :func:`expanduser`.  Thus a simple call::\n        filefind(\"myfile.txt\")\n    will find the file in the current working dir, but::\n        filefind(\"~/myfile.txt\")\n    Will find the file in the users home directory.  This function does not\n    automatically try any paths, such as the cwd or the user's home directory.\n    Parameters\n    ----------\n    filename : str\n        The filename to look for.\n    path_dirs : str, None or sequence of str\n        The sequence of paths to look for the file in.  If None, the filename\n        need to be absolute or be in the cwd.  If a string, the string is\n        put into a sequence and the searched.  If a sequence, walk through\n        each element and join with ``filename``, calling :func:`expandvars`\n        and :func:`expanduser` before testing for existence.\n    Returns\n    -------\n    Raises :exc:`IOError` or returns absolute path to file.\n    \"\"\"\n    filename = filename.strip('\"').strip(\"'\")\n    if os.path.isabs(filename) and os.path.isfile(filename):\n        return filename\n    if path_dirs is None:\n        path_dirs = (\"\",)\n    elif isinstance(path_dirs, str):\n        path_dirs = (path_dirs,)\n    for path in path_dirs:\n        if path == \".\":\n            path = os.getcwd()\n        testname = expand_path(os.path.join(path, filename))\n        if os.path.isfile(testname):\n            return os.path.abspath(testname)\n    msg = f\"File {filename!r} does not exist in any of the search paths: {path_dirs!r}\"\n    raise OSError(msg)\ndef expand_path(s: str) -> str:\n    \"\"\"Expand $VARS and ~names in a string, like a shell\n    :Examples:\n       In [2]: os.environ['FOO']='test'\n       In [3]: expand_path('variable FOO is $FOO')\n       Out[3]: 'variable FOO is test'\n    \"\"\"\n    if os.name == \"nt\":\n        s = s.replace(\"$\\\\\", \"IPYTHON_TEMP\")\n    s = os.path.expandvars(os.path.expanduser(s))\n    if os.name == \"nt\":\n        s = s.replace(\"IPYTHON_TEMP\", \"$\\\\\")\n    return s",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-35178",
        "description": "[{'lang': 'en', 'value': 'The Jupyter Server provides the backend for Jupyter web applications. Jupyter Server on Windows has a vulnerability that lets unauthenticated attackers leak the NTLMv2 password hash of the Windows user running the Jupyter server. An attacker can crack this password to gain access to the Windows machine hosting the Jupyter server, or access other network-accessible machines or 3rd party services using that credential. Or an attacker perform an NTLM relay attack without cracking the credential to gain access to other network-accessible machines. This vulnerability is fixed in 2.14.1.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-13",
      "code": "    def final_work(self):\n        self._finish_logging()\n        if not self.opts.build:\n            if not self.opts.quiet:\n                print(_(\"Creating compressed archive...\"))\n            try:\n                final_filename = self.archive.finalize(\n                    self.opts.compression_type)\n            except (OSError, IOError) as e:\n                if e.errno in fatal_fs_errors:\n                    self.ui_log.error(\"\")\n                    self.ui_log.error(\" %s while finalizing archive\"\n                                      % e.strerror)\n                    self.ui_log.error(\"\")\n                    self._exit(1)\n            except:\n                if self.opts.debug:\n                    raise\n                else:\n                    return False\n        else:\n            final_filename = self.archive.get_archive_path()\n        self.policy.display_results(final_filename, build=self.opts.build)\n        self.tempfile_util.clean()\n        return True\n    def verify_plugins(self):\n        if not self.loaded_plugins:\n            self.soslog.error(_(\"no valid plugins were enabled\"))\n            return False\n        return True",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2015-3171",
        "description": "[{'lang': 'en', 'value': 'sosreport 3.2 uses weak permissions for generated sosreport archives, which allows local users with access to /var/tmp/ to obtain sensitive information by reading the contents of the archive.'}]",
        "cwe_number": 200
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-14",
      "code": "    def __init__(self, op):\n        super(FileReceiver, self).__init__()\n        self.save_path = prefs.get_save_path()\n        self.save_path_file = Gio.File.new_for_path(self.save_path)\n        self.op = op\n        self.preserve_perms = prefs.preserve_permissions() and util.save_folder_is_native_fs()\n        self.preserve_timestamp = prefs.preserve_timestamp() and util.save_folder_is_native_fs()\n        self.current_path = None\n        self.current_gfile = None\n        self.current_type = None\n        self.current_stream = None\n        self.current_mode = 0\n        self.current_mtime = 0\n        self.current_mtime_usec = 0\n        for name in op.top_dir_basenames:\n            try:\n                path = os.path.join(self.save_path, name)\n                if os.path.isdir(path):\n                    shutil.rmtree(path)\n                else:\n                    os.remove(path)\n            except FileNotFoundError:\n                pass\n            except Exception as e:\n                logging.warning(\"Problem removing existing files.  Transfer may not succeed: %s\" % e)\n        self.folder_permission_change_list = []\n    def receive_data(self, s):\n        save_path = prefs.get_save_path()\n        path = os.path.join(save_path, s.relative_path)\n        if path != self.current_path:\n            self.close_current_file()\n            self.current_path = path\n            self.current_mode = s.file_mode\n            self.current_type = s.file_type\n            self.current_mtime = s.time.mtime\n            self.current_mtime_usec = s.time.mtime_usec\n        if not self.current_gfile:\n            self.current_gfile = Gio.File.new_for_path(path)\n            if self.save_path_file.get_relative_path(self.current_gfile) is None:\n                raise Exception(_(\"Resolved path is not valid: %s -> %s\") % (path, self.current_gfile.get_path()))\n        if s.file_type == FileType.DIRECTORY:\n            os.makedirs(path, exist_ok=True)\n        elif s.file_type == FileType.SYMBOLIC_LINK:\n            make_symbolic_link(self.op, path, s.symlink_target)\n        else:\n            if self.current_stream == None:\n                self.current_stream = self.current_gfile.create(Gio.FileCreateFlags.NONE, None)\n            if not s.chunk:\n                return\n            self.current_stream.write_bytes(GLib.Bytes(s.chunk), None)\n            self.op.progress_tracker.update_progress(len(s.chunk))",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-42725",
        "description": "[{'lang': 'en', 'value': 'Warpinator through 1.2.14 allows access outside of an intended directory, as demonstrated by symbolic directory links.'}]",
        "cwe_number": 59
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-15",
      "code": "def choose_tls_port_and_get_bind_sock(config, options):\n    if \"tlsport\" in options:\n        ports_to_try = [int(options[\"tlsport\"])]\n    else:\n        lower_bound, upper_bound = get_tls_port_range(config)\n        ports_to_try = list(range(lower_bound, upper_bound))\n        random.shuffle(ports_to_try)\n    if \"netns\" not in options:\n        tls_port_sock = find_tls_port_in_range_and_get_bind_sock(ports_to_try)\n    else:\n        with NetNS(nspath=options[\"netns\"]):\n            tls_port_sock = find_tls_port_in_range_and_get_bind_sock(ports_to_try)\n    if tls_port_sock:\n        return tls_port_sock\n    if \"tlsport\" in options:\n        fatal_error(\n            \"Specified port [%s] is unavailable. Try selecting a different port.\"\n            % options[\"tlsport\"]\n        )\n    else:\n        fatal_error(\n            \"Failed to locate an available port in the range [%d, %d], try specifying a different port range in %s\"\n            % (lower_bound, upper_bound, CONFIG_FILE)\n        )\ndef find_tls_port_in_range_and_get_bind_sock(ports_to_try):\n    sock = socket.socket()\n    for tls_port in ports_to_try:\n        try:\n            logging.info(\"binding %s\", tls_port)\n            sock.bind((\"localhost\", tls_port))\n            return sock\n        except socket.error as e:\n            logging.info(e)\n            continue\n    sock.close()\n    return None\ndef bootstrap_tls(\n    config,\n    init_system,\n    dns_name,\n    fs_id,\n    mountpoint,\n    options,\n    state_file_dir=STATE_FILE_DIR,\n    fallback_ip_address=None,\n):\n    tls_port_sock = choose_tls_port_and_get_bind_sock(config, options)\n    tls_port = get_tls_port_from_sock(tls_port_sock)\n    try:\n        options[\"tlsport\"] = tls_port\n        use_iam = \"iam\" in options\n        ap_id = options.get(\"accesspoint\")\n        cert_details = {}\n        security_credentials = None\n        client_info = get_client_info(config)\n        region = get_target_region(config)\n        if use_iam:\n            aws_creds_uri = options.get(\"awscredsuri\")\n            if aws_creds_uri:\n                kwargs = {\"aws_creds_uri\": aws_creds_uri}\n            else:\n                kwargs = {\"awsprofile\": get_aws_profile(options, use_iam)}\n            security_credentials, credentials_source = get_aws_security_credentials(\n                config, use_iam, region, **kwargs\n            )\n            if credentials_source:\n                cert_details[\"awsCredentialsMethod\"] = credentials_source\n        if ap_id:\n            cert_details[\"accessPoint\"] = ap_id\n        cert_details[\"mountStateDir\"] = (\n            get_mount_specific_filename(fs_id, mountpoint, tls_port) + \"+\"\n        )\n        cert_details[\"commonName\"] = socket.gethostname()[0:64]\n        region = get_target_region(config)\n        cert_details[\"region\"] = region\n        cert_details[\"certificateCreationTime\"] = create_certificate(\n            config,\n            cert_details[\"mountStateDir\"],\n            cert_details[\"commonName\"],\n            cert_details[\"region\"],\n            fs_id,\n            security_credentials,\n            ap_id,\n            client_info,\n            base_path=state_file_dir,\n        )\n        cert_details[\"certificate\"] = os.path.join(\n            state_file_dir, cert_details[\"mountStateDir\"], \"certificate.pem\"\n        )\n        cert_details[\"privateKey\"] = get_private_key_path()\n        cert_details[\"fsId\"] = fs_id\n        start_watchdog(init_system)\n        if not os.path.exists(state_file_dir):\n            create_required_directory(config, state_file_dir)\n        verify_level = int(options.get(\"verify\", DEFAULT_STUNNEL_VERIFY_LEVEL))\n        ocsp_enabled = is_ocsp_enabled(config, options)\n        stunnel_config_file = write_stunnel_config_file(\n            config,\n            state_file_dir,\n            fs_id,\n            mountpoint,\n            tls_port,\n            dns_name,\n            verify_level,\n            ocsp_enabled,\n            options,\n            region,\n            cert_details=cert_details,\n            fallback_ip_address=fallback_ip_address,\n        )\n        tunnel_args = [_stunnel_bin(), stunnel_config_file]\n        if \"netns\" in options:\n            tunnel_args = [\"nsenter\", \"--net=\" + options[\"netns\"]] + tunnel_args\n    finally:\n        logging.debug(\"Closing socket used to choose TLS port %s.\", tls_port)\n        tls_port_sock.close()\n    logging.info('Starting TLS tunnel: \"%s\"', \" \".join(tunnel_args))\n    tunnel_proc = subprocess.Popen(\n        tunnel_args,\n        stdout=subprocess.DEVNULL,\n        stderr=subprocess.PIPE,\n        preexec_fn=os.setsid,\n        close_fds=True,\n    )\n    logging.info(\"Started TLS tunnel, pid: %d\", tunnel_proc.pid)\n    temp_tls_state_file = write_tls_tunnel_state_file(\n        fs_id,\n        mountpoint,\n        tls_port,\n        tunnel_proc.pid,\n        tunnel_args,\n        [stunnel_config_file],\n        state_file_dir,\n        cert_details=cert_details,\n    )\n    if \"netns\" not in options:\n        test_tlsport(options[\"tlsport\"])\n    else:\n        with NetNS(nspath=options[\"netns\"]):\n            test_tlsport(options[\"tlsport\"])\n    try:\n        yield tunnel_proc\n    finally:\n        os.rename(\n            os.path.join(state_file_dir, temp_tls_state_file),\n            os.path.join(state_file_dir, temp_tls_state_file[1:]),\n        )",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-46174",
        "description": "[{'lang': 'en', 'value': 'efs-utils is a set of Utilities for Amazon Elastic File System (EFS). A potential race condition issue exists within the Amazon EFS mount helper in efs-utils versions v1.34.3 and below. When using TLS to mount file systems, the mount helper allocates a local port for stunnel to receive NFS connections prior to applying the TLS tunnel. In affected versions, concurrent mount operations can allocate the same local port, leading to either failed mount operations or an inappropriate mapping from an EFS customer\u2019s local mount points to that customer\u2019s EFS file systems. This issue is patched in version v1.34.4. There is no recommended work around. We recommend affected users update the installed version of efs-utils to v1.34.4 or later.'}]",
        "cwe_number": 362
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-16",
      "code": "def _start_program(args: List[str], local_env: Dict[str, str]) -> subprocess.Popen:\n    \"\"\"\n    Start the program where the path is the first item of the ``args`` array argument.\n    Parameters\n    ----------\n    args : List[str]\n        List of arguments to be passed to the program. The first list's item shall\n        be the program path.\n    local_env : Dict[str,str]\n        Environment variables to be passed to the program.\n    Returns\n    -------\n    subprocess.Popen\n        The subprocess object.\n    \"\"\"\n    return subprocess.Popen(\n        args,\n        shell=os.name != \"nt\",\n        stdin=subprocess.DEVNULL,\n        stdout=subprocess.DEVNULL,\n        stderr=subprocess.DEVNULL,\n        env=local_env,\n    )",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-29189",
        "description": "[{'lang': 'en', 'value': 'PyAnsys Geometry is a Python client library for the Ansys Geometry service and other CAD Ansys products. On file src/ansys/geometry/core/connection/product_instance.py, upon calling this method _start_program directly, users could exploit its usage to perform malicious operations on the current machine where the script is ran. This vulnerability is fixed in 0.3.3 and 0.4.12.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-17",
      "code": "  def testFlushFunction(self):\n    logdir = self.get_temp_dir()\n    with context.eager_mode():\n      writer = summary_ops.create_file_writer_v2(\n          logdir, max_queue=999999, flush_millis=999999)\n      with writer.as_default():\n        get_total = lambda: len(events_from_logdir(logdir))\n        self.assertEqual(1, get_total())\n        summary_ops.write('tag', 1, step=0)\n        summary_ops.write('tag', 1, step=0)\n        self.assertEqual(1, get_total())\n        summary_ops.flush()\n        self.assertEqual(3, get_total())\n        summary_ops.write('tag', 1, step=0)\n        self.assertEqual(3, get_total())\n        summary_ops.flush(writer=writer)\n        self.assertEqual(4, get_total())\n        summary_ops.write('tag', 1, step=0)\n        self.assertEqual(4, get_total())\n        summary_ops.flush(writer=writer._resource)\n        self.assertEqual(5, get_total())",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-29207",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. Prior to versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4, multiple TensorFlow operations misbehave in eager mode when the resource handle provided to them is invalid. In graph mode, it would have been impossible to perform these API calls, but migration to TF 2.x eager mode opened up this vulnerability. If the resource handle is empty, then a reference is bound to a null pointer inside TensorFlow codebase (various codepaths). This is undefined behavior. Versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4 contain a patch for this issue.'}]",
        "cwe_number": 20
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-18",
      "code": "def _get_http_response_with_retries(\n    method,\n    url,\n    max_retries,\n    backoff_factor,\n    backoff_jitter,\n    retry_codes,\n    raise_on_status=True,\n    **kwargs,\n):\n    \"\"\"\n    Performs an HTTP request using Python's `requests` module with an automatic retry policy.\n    :param method: a string indicating the method to use, e.g. \"GET\", \"POST\", \"PUT\".\n    :param url: the target URL address for the HTTP request.\n    :param max_retries: Maximum total number of retries.\n    :param backoff_factor: a time factor for exponential backoff. e.g. value 5 means the HTTP\n      request will be retried with interval 5, 10, 20... seconds. A value of 0 turns off the\n      exponential backoff.\n    :param backoff_jitter: A random jitter to add to the backoff interval.\n    :param retry_codes: a list of HTTP response error codes that qualifies for retry.\n    :param raise_on_status: whether to raise an exception, or return a response, if status falls\n      in retry_codes range and retries have been exhausted.\n    :param kwargs: Additional keyword arguments to pass to `requests.Session.request()`\n    :return: requests.Response object.\n    \"\"\"\n    session = _get_request_session(\n        max_retries, backoff_factor, backoff_jitter, retry_codes, raise_on_status\n    )\n    return session.request(method, url, **kwargs)\ndef cloud_storage_http_request(\n    method,\n    url,\n    max_retries=5,\n    backoff_factor=2,\n    backoff_jitter=1.0,\n    retry_codes=_TRANSIENT_FAILURE_RESPONSE_CODES,\n    timeout=None,\n    **kwargs,\n):\n    \"\"\"\n    Performs an HTTP PUT/GET/PATCH request using Python's `requests` module with automatic retry.\n    :param method: string of 'PUT' or 'GET' or 'PATCH', specify to do http PUT or GET or PATCH\n    :param url: the target URL address for the HTTP request.\n    :param max_retries: maximum number of retries before throwing an exception.\n    :param backoff_factor: a time factor for exponential backoff. e.g. value 5 means the HTTP\n      request will be retried with interval 5, 10, 20... seconds. A value of 0 turns off the\n      exponential backoff.\n    :param backoff_jitter: A random jitter to add to the backoff interval.\n    :param retry_codes: a list of HTTP response error codes that qualifies for retry.\n    :param timeout: wait for timeout seconds for response from remote server for connect and\n      read request. Default to None owing to long duration operation in read / write.\n    :param kwargs: Additional keyword arguments to pass to `requests.Session.request()`\n    :return requests.Response object.\n    \"\"\"\n    if method.lower() not in (\"put\", \"get\", \"patch\", \"delete\"):\n        raise ValueError(\"Illegal http method: \" + method)\n    return _get_http_response_with_retries(\n        method,\n        url,\n        max_retries,\n        backoff_factor,\n        backoff_jitter,\n        retry_codes,\n        timeout=timeout,\n        **kwargs,\n    )",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-6974",
        "description": "[{'lang': 'en', 'value': 'A malicious user could use this issue to access internal HTTP(s) servers and in the worst case (ie: aws instance) it could be abuse to get a remote code execution on the victim machine.'}]",
        "cwe_number": 918
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-19",
      "code": "    def __init__(\n        self,\n        *tables: t.Union[t.Type[Table], TableConfig],\n        forms: t.List[FormConfig] = [],\n        auth_table: t.Type[BaseUser] = BaseUser,\n        session_table: t.Type[SessionsBase] = SessionsBase,\n        session_expiry: timedelta = timedelta(hours=1),\n        max_session_expiry: timedelta = timedelta(days=7),\n        increase_expiry: t.Optional[timedelta] = timedelta(minutes=20),\n        page_size: int = 15,\n        read_only: bool = False,\n        rate_limit_provider: t.Optional[RateLimitProvider] = None,\n        production: bool = False,\n        site_name: str = \"Piccolo Admin\",\n        default_language_code: str = \"auto\",\n        translations: t.Optional[t.List[Translation]] = None,\n        allowed_hosts: t.Sequence[str] = [],\n        debug: bool = False,\n        sidebar_links: t.Dict[str, str] = {},\n    ) -> None:\n        super().__init__(\n            title=site_name,\n            description=f\"{site_name} documentation\",\n            middleware=[\n                Middleware(CSRFMiddleware, allowed_hosts=allowed_hosts)\n            ],\n            debug=debug,\n            exception_handlers={500: log_error},\n            docs_url=None,\n            redoc_url=None,\n        )\n        table_configs: t.List[TableConfig] = []\n        for table in tables:\n            if isinstance(table, TableConfig):\n                table_configs.append(table)\n            else:\n                table_configs.append(TableConfig(table_class=table))\n        self.table_configs = sorted(\n            table_configs,\n            key=lambda table_config: table_config.table_class._meta.tablename,\n        )\n        self.table_config_map = {\n            table_config.table_class._meta.tablename: table_config\n            for table_config in self.table_configs\n        }\n        for table_config in table_configs:\n            table_class = table_config.table_class\n            for column in table_class._meta.columns:\n                if column._meta.secret and column._meta.required:\n                    message = (\n                        f\"{table_class._meta.tablename}.\"\n                        f\"{column._meta._name} is using `secret` and \"\n                        f\"`required` column args which are incompatible. \"\n                        f\"You may encounter unexpected behavior when using \"\n                        f\"this table within Piccolo Admin.\"\n                    )\n                    colored_warning(message, level=Level.high)\n        media_storage = [\n            i\n            for i in itertools.chain(\n                *[\n                    table_config.media_storage or []\n                    for table_config in table_configs\n                ]\n            )\n        ]\n        if len(media_storage) != len(set(media_storage)):\n            raise ValueError(\n                \"Media storage is misconfigured - multiple columns are saving \"\n                \"to the same location.\"\n            )\n        self.default_language_code = default_language_code\n        self.translations_map = {\n            translation.language_code.lower(): translation\n            for translation in (translations or TRANSLATIONS)\n        }\n        self.auth_table = auth_table\n        self.site_name = site_name\n        self.forms = forms\n        self.read_only = read_only\n        self.sidebar_links = sidebar_links\n        self.form_config_map = {form.slug: form for form in self.forms}\n        with open(os.path.join(ASSET_PATH, \"index.html\")) as f:\n            self.template = f.read()\n        private_app = FastAPI(\n            docs_url=None,\n            redoc_url=None,\n            debug=debug,\n            exception_handlers={500: log_error},\n        )\n        private_app.mount(\"/docs/\", swagger_ui(schema_url=\"../openapi.json\"))\n        for table_config in table_configs:\n            table_class = table_config.table_class\n            visible_column_names = table_config.get_visible_column_names()\n            visible_filter_names = table_config.get_visible_filter_names()\n            rich_text_columns_names = (\n                table_config.get_rich_text_columns_names()\n            )\n            media_columns_names = table_config.get_media_columns_names()\n            link_column_name = table_config.get_link_column()._meta.name\n            order_by = table_config.get_order_by()\n            time_resolution = table_config.get_time_resolution()\n            validators = table_config.validators\n            if table_class in (auth_table, session_table):\n                validators = validators or Validators()\n                validators.every = [superuser_validators, *validators.every]\n            FastAPIWrapper(\n                root_url=f\"/tables/{table_class._meta.tablename}/\",\n                fastapi_app=private_app,\n                piccolo_crud=PiccoloCRUD(\n                    table=table_class,\n                    read_only=read_only,\n                    page_size=page_size,\n                    schema_extra={\n                        \"visible_column_names\": visible_column_names,\n                        \"visible_filter_names\": visible_filter_names,\n                        \"rich_text_columns\": rich_text_columns_names,\n                        \"media_columns\": media_columns_names,\n                        \"link_column_name\": link_column_name,\n                        \"order_by\": tuple(i.to_dict() for i in order_by),\n                        \"time_resolution\": time_resolution,\n                    },\n                    validators=validators,\n                    hooks=table_config.hooks,\n                ),\n                fastapi_kwargs=FastAPIKwargs(\n                    all_routes={\n                        \"tags\": [f\"{table_class._meta.tablename.capitalize()}\"]\n                    },\n                ),\n            )\n        private_app.add_api_route(\n            path=\"/tables/\",\n            endpoint=self.get_table_list,\n            methods=[\"GET\"],\n            response_model=t.List[str],\n            tags=[\"Tables\"],\n        )\n        private_app.add_api_route(\n            path=\"/tables/grouped/\",\n            endpoint=self.get_table_list_grouped,\n            methods=[\"GET\"],\n            response_model=GroupedTableNamesResponseModel,\n            tags=[\"Tables\"],\n        )\n        private_app.add_api_route(\n            path=\"/links/\",\n            endpoint=self.get_sidebar_links,\n            methods=[\"GET\"],\n            tags=[\"Links\"],\n        )\n        private_app.add_api_route(\n            path=\"/forms/\",\n            endpoint=self.get_forms,\n            methods=[\"GET\"],\n            tags=[\"Forms\"],\n            response_model=t.List[FormConfigResponseModel],\n        )\n        private_app.add_api_route(\n            path=\"/forms/{form_slug:str}/\",\n            endpoint=self.get_single_form,\n            methods=[\"GET\"],\n            tags=[\"Forms\"],\n        )\n        private_app.add_api_route(\n            path=\"/forms/{form_slug:str}/schema/\",\n            endpoint=self.get_single_form_schema,\n            methods=[\"GET\"],\n            tags=[\"Forms\"],\n        )\n        private_app.add_api_route(\n            path=\"/forms/{form_slug:str}/\",\n            endpoint=self.post_single_form,\n            methods=[\"POST\"],\n            tags=[\"Forms\"],\n        )\n        private_app.add_api_route(\n            path=\"/user/\",\n            endpoint=self.get_user,\n            methods=[\"GET\"],\n            tags=[\"User\"],\n            response_model=UserResponseModel,\n        )\n        private_app.add_route(\n            path=\"/change-password/\",\n            route=change_password(\n                login_url=\"./../../public/login/\",\n                session_table=session_table,\n                read_only=read_only,\n            ),\n            methods=[\"POST\"],\n        )\n        private_app.add_api_route(\n            path=\"/media/\",\n            endpoint=self.store_file,\n            methods=[\"POST\"],\n            tags=[\"Media\"],\n            response_model=StoreFileResponseModel,\n        )\n        private_app.add_api_route(\n            path=\"/media/generate-file-url/\",\n            endpoint=self.generate_file_url,\n            methods=[\"POST\"],\n            tags=[\"Media\"],\n            response_model=GenerateFileURLResponseModel,\n        )\n        for table_config in self.table_configs:\n            if table_config.media_columns:\n                for (\n                    column,\n                    media_storage,\n                ) in table_config.media_columns.items():\n                    if isinstance(media_storage, LocalMediaStorage):\n                        private_app.mount(\n                            path=f\"/media-files/{column._meta.table._meta.tablename}/{column._meta.name}/\",\n                            app=StaticFiles(\n                                directory=media_storage.media_path\n                            ),\n                        )\n        public_app = FastAPI(\n            redoc_url=None,\n            docs_url=None,\n            debug=debug,\n            exception_handlers={500: log_error},\n        )\n        public_app.mount(\"/docs/\", swagger_ui(schema_url=\"../openapi.json\"))\n        if not rate_limit_provider:\n            rate_limit_provider = InMemoryLimitProvider(\n                limit=100, timespan=300\n            )\n        public_app.mount(\n            path=\"/login/\",\n            app=RateLimitingMiddleware(\n                app=session_login(\n                    auth_table=self.auth_table,\n                    session_table=session_table,\n                    session_expiry=session_expiry,\n                    max_session_expiry=max_session_expiry,\n                    redirect_to=None,\n                    production=production,\n                ),\n                provider=rate_limit_provider,\n            ),\n        )\n        public_app.add_route(\n            path=\"/logout/\",\n            route=session_logout(session_table=session_table),\n            methods=[\"POST\"],\n        )\n        public_app.add_api_route(\n            \"/meta/\", endpoint=self.get_meta, tags=[\"Meta\"]\n        )\n        public_app.add_api_route(\n            \"/translations/\",\n            endpoint=self.get_translation_list,\n            methods=[\"GET\"],\n            tags=[\"Translations\"],\n            response_model=TranslationListResponse,\n        )\n        public_app.add_api_route(\n            \"/translations/{language_code:str}/\",\n            endpoint=self.get_translation,\n            methods=[\"GET\"],\n            tags=[\"Translations\"],\n            response_model=Translation,\n        )\n        self.router.add_route(\n            path=\"/\", endpoint=self.get_root, methods=[\"GET\"]\n        )\n        self.mount(\n            path=\"/assets\",\n            app=StaticFiles(directory=os.path.join(ASSET_PATH, \"assets\")),\n        )\n        auth_middleware = partial(\n            AuthenticationMiddleware,\n            backend=SessionsAuthBackend(\n                auth_table=auth_table,\n                session_table=session_table,\n                admin_only=True,\n                increase_expiry=increase_expiry,\n            ),\n            on_error=handle_auth_exception,\n        )\n        self.mount(path=\"/api\", app=auth_middleware(private_app))\n        self.mount(path=\"/public\", app=public_app)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-30248",
        "description": "[{'lang': 'en', 'value': \"Piccolo Admin is an admin interface/content management system for Python, built on top of Piccolo. Piccolo's admin panel allows media files to be uploaded. As a default, SVG is an allowed file type for upload. An attacker can upload an SVG which when loaded can allow arbitrary access to the admin page. This vulnerability was patched in version 1.3.2.\"}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-20",
      "code": "def escape(s):\n    s = s.replace(\"&\", \"&amp;\")\n    s = s.replace(\"<\", \"&lt;\")\n    s = s.replace(\">\", \"&gt;\")\n    return s\ndef handleInputParameterError(f):\n    def new_f(*args, **kwargs):\n        try:\n            return f(*args, **kwargs)\n        except InputParameterError as e:\n            msgStr = str(e)\n            log.warning('%s', msgStr)\n            return HttpResponseBadRequest(escape(msgStr))\n    return new_f",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-4730",
        "description": "[{'lang': 'en', 'value': 'A vulnerability was found in Graphite Web. It has been classified as problematic. Affected is an unknown function of the component Absolute Time Range Handler. The manipulation leads to cross site scripting. It is possible to launch the attack remotely. The exploit has been disclosed to the public and may be used. The name of the patch is 2f178f490e10efc03cd1d27c72f64ecab224eb23. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-216744.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-21",
      "code": "    def constructed_device(self):\n        return self._constructed_device\n    def size(self):\n        return self.volume.size - LUKS_OVERHEAD",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2020-11932",
        "description": "[{'lang': 'en', 'value': 'It was discovered that the Subiquity installer for Ubuntu Server logged the LUKS full disk encryption password if one was entered.'}]",
        "cwe_number": 532
      },
      "cwe_types": [],
      "severity": "low"
    },
    {
      "id": "ContextAssembler-22",
      "code": "class NumericRangeFilter(Filter):\n    def filter(self, qs, value):\n        if value:\n            if value.start is not None and value.stop is not None:\n                value = (value.start, value.stop)\n            elif value.start is not None:\n                self.lookup_expr = 'startswith'\n                value = value.start\n            elif value.stop is not None:\n                self.lookup_expr = 'endswith'\n                value = value.stop\n        return super().filter(qs, value)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2020-15225",
        "description": "[{'lang': 'en', 'value': 'django-filter is a generic system for filtering Django QuerySets based on user selections. In django-filter before version 2.4.0, automatically generated `NumberFilter` instances, whose value was later converted to an integer, were subject to potential DoS from maliciously input using exponential format with sufficiently large exponents. Version 2.4.0+ applies a `MaxValueValidator` with a a default `limit_value` of 1e50 to the form field used by `NumberFilter` instances. In addition, `NumberFilter` implements the new `get_max_validator()` which should return a configured validator instance to customise the limit, or else `None` to disable the additional validation. Users may manually apply an equivalent validator if they are not able to upgrade.'}]",
        "cwe_number": 681
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-23",
      "code": "    def post(self, request, *args, **kwargs):\n        serializer = self.serializer_class(data=request.data, context={'request': request})\n        serializer.is_valid(raise_exception=True)\n        user = serializer.validated_data['user']\n        if user is None:\n            return FormattedResponse(status=HTTP_401_UNAUTHORIZED, d={'reason': 'login_failed'}, m='login_failed')\n        if not user.has_2fa():\n            return FormattedResponse(status=HTTP_401_UNAUTHORIZED, d={'reason': '2fa_not_enabled'}, m='2fa_not_enabled')\n        token = serializer.data['tfa']\n        if len(token) == 6:\n            if user.totp_device is not None and user.totp_device.validate_token(token):\n                return self.issue_token(user)\n        elif len(token) == 8:\n            for code in user.backup_codes:\n                if token == code.code:\n                    code.delete()\n                    return self.issue_token(user)\n        return self.issue_token(user)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-21329",
        "description": "[{'lang': 'en', 'value': 'RATCF is an open-source framework for hosting Cyber-Security Capture the Flag events. In affected versions of RATCF users with multi factor authentication enabled are able to log in without a valid token. This is fixed in commit cebb67b.'}]",
        "cwe_number": 287
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-24",
      "code": "def jstree_data(node, selected_node):\n    result = []\n    result.append('{')\n    result.append('\"text\": \"{}\",'.format(node.label))\n    result.append(\n        '\"state\": {{ \"opened\": true, \"selected\": {} }},'.format(\n            'true' if node == selected_node else 'false'\n        )\n    )\n    result.append(\n        '\"data\": {{ \"href\": \"{}\" }},'.format(node.get_absolute_url())\n    )\n    children = node.get_children().order_by('label',)\n    if children:\n        result.append('\"children\" : [')\n        for child in children:\n            result.extend(jstree_data(node=child, selected_node=selected_node))\n        result.append(']')\n    result.append('},')\n    return result",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2018-16406",
        "description": "[{'lang': 'en', 'value': 'An issue was discovered in Mayan EDMS before 3.0.2. The Cabinets app has XSS via a crafted cabinet label.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-25",
      "code": "    def handle_get(self) -> bool:\n        if self.do_log:\n            logmsg = \"%-4s %s @%s\" % (self.mode, self.req, self.uname)\n            if \"range\" in self.headers:\n                try:\n                    rval = self.headers[\"range\"].split(\"=\", 1)[1]\n                except:\n                    rval = self.headers[\"range\"]\n                logmsg += \" [\\033[36m\" + rval + \"\\033[0m]\"\n            self.log(logmsg)\n        if self.vpath.startswith(\".cpr\"):\n            if self.vpath.startswith(\".cpr/ico/\"):\n                return self.tx_ico(self.vpath.split(\"/\")[-1], exact=True)\n            if self.vpath.startswith(\".cpr/ssdp\"):\n                return self.conn.hsrv.ssdp.reply(self)\n            if self.vpath.startswith(\".cpr/dd/\") and self.args.mpmc:\n                if self.args.mpmc == \".\":\n                    raise Pebkac(404)\n                loc = self.args.mpmc.rstrip(\"/\") + self.vpath[self.vpath.rfind(\"/\") :]\n                h = {\"Location\": loc, \"Cache-Control\": \"max-age=39\"}\n                self.reply(b\"\", 301, headers=h)\n                return True\n            static_path = os.path.join(self.E.mod, \"web/\", self.vpath[5:])\n            return self.tx_file(static_path)\n        if \"cf_challenge\" in self.uparam:\n            self.reply(self.j2s(\"cf\").encode(\"utf-8\", \"replace\"))\n            return True\n        if not self.can_read and not self.can_write and not self.can_get:\n            t = \"@{} has no access to [{}]\"\n            self.log(t.format(self.uname, self.vpath))\n            if \"on403\" in self.vn.flags:\n                ret = self.on40x(self.vn.flags[\"on403\"], self.vn, self.rem)\n                if ret == \"true\":\n                    return True\n                elif ret == \"false\":\n                    return False\n                elif ret == \"allow\":\n                    self.log(\"plugin override; access permitted\")\n                    self.can_read = self.can_write = self.can_move = True\n                    self.can_delete = self.can_get = self.can_upget = True\n                    self.can_admin = True\n                else:\n                    return self.tx_404(True)\n            else:\n                if self.vpath:\n                    return self.tx_404(True)\n                self.uparam[\"h\"] = \"\"\n        if \"tree\" in self.uparam:\n            return self.tx_tree()\n        if \"scan\" in self.uparam:\n            return self.scanvol()\n        if self.args.getmod:\n            if \"delete\" in self.uparam:\n                return self.handle_rm([])\n            if \"move\" in self.uparam:\n                return self.handle_mv()\n        if not self.vpath:\n            if \"reload\" in self.uparam:\n                return self.handle_reload()\n            if \"stack\" in self.uparam:\n                return self.tx_stack()\n            if \"ups\" in self.uparam:\n                return self.tx_ups()\n            if \"k304\" in self.uparam:\n                return self.set_k304()\n            if \"setck\" in self.uparam:\n                return self.setck()\n            if \"reset\" in self.uparam:\n                return self.set_cfg_reset()\n            if \"hc\" in self.uparam:\n                return self.tx_svcs()\n        if \"h\" in self.uparam:\n            return self.tx_mounts()\n        if self.vpath == \"\" and not self.ouparam:\n            nread = len(self.rvol)\n            nwrite = len(self.wvol)\n            if nread + nwrite == 1 or (self.rvol == self.wvol and nread == 1):\n                if nread == 1:\n                    vpath = self.rvol[0]\n                else:\n                    vpath = self.wvol[0]\n                if self.vpath != vpath:\n                    self.redirect(vpath, flavor=\"redirecting to\", use302=True)\n                    return True\n        return self.tx_browser()\n    def handle_propfind(self) -> bool:\n        if self.do_log:\n            self.log(\"PFIND %s @%s\" % (self.req, self.uname))\n        if self.args.no_dav:\n            raise Pebkac(405, \"WebDAV is disabled in server config\")\n        vn, rem = self.asrv.vfs.get(self.vpath, self.uname, False, False, err=401)\n        tap = vn.canonical(rem)\n        if \"davauth\" in vn.flags and self.uname == \"*\":\n            self.can_read = self.can_write = self.can_get = False\n        if not self.can_read and not self.can_write and not self.can_get:\n            self.log(\"inaccessible: [{}]\".format(self.vpath))\n            raise Pebkac(401, \"authenticate\")\n        from .dxml import parse_xml\n        enc = \"utf-8\"\n        uenc = enc.upper()\n        clen = int(self.headers.get(\"content-length\", 0))\n        if clen:\n            buf = b\"\"\n            for rbuf in self.get_body_reader()[0]:\n                buf += rbuf\n                if not rbuf or len(buf) >= 32768:\n                    break\n            xroot = parse_xml(buf.decode(enc, \"replace\"))\n            xtag = next(x for x in xroot if x.tag.split(\"}\")[-1] == \"prop\")\n            props_lst = [y.tag.split(\"}\")[-1] for y in xtag]\n        else:\n            props_lst = [\n                \"contentclass\",\n                \"creationdate\",\n                \"defaultdocument\",\n                \"displayname\",\n                \"getcontentlanguage\",\n                \"getcontentlength\",\n                \"getcontenttype\",\n                \"getlastmodified\",\n                \"href\",\n                \"iscollection\",\n                \"ishidden\",\n                \"isreadonly\",\n                \"isroot\",\n                \"isstructureddocument\",\n                \"lastaccessed\",\n                \"name\",\n                \"parentname\",\n                \"resourcetype\",\n                \"supportedlock\",\n            ]\n        props = set(props_lst)\n        depth = self.headers.get(\"depth\", \"infinity\").lower()\n        try:\n            topdir = {\"vp\": \"\", \"st\": bos.stat(tap)}\n        except OSError as ex:\n            if ex.errno not in (errno.ENOENT, errno.ENOTDIR):\n                raise\n            raise Pebkac(404)\n        if depth == \"0\" or not self.can_read or not stat.S_ISDIR(topdir[\"st\"].st_mode):\n            fgen = []\n        elif depth == \"infinity\":\n            if not self.args.dav_inf:\n                self.log(\"client wants --dav-inf\", 3)\n                zb = b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<D:error xmlns:D=\"DAV:\"><D:propfind-finite-depth/></D:error>'\n                self.reply(zb, 403, \"application/xml; charset=utf-8\")\n                return True\n            fgen = vn.zipgen(\n                rem,\n                rem,\n                set(),\n                self.uname,\n                self.args.ed,\n                True,\n                not self.args.no_scandir,\n                wrap=False,\n            )\n        elif depth == \"1\":\n            _, vfs_ls, vfs_virt = vn.ls(\n                rem,\n                self.uname,\n                not self.args.no_scandir,\n                [[True, False]],\n                lstat=\"davrt\" not in vn.flags,\n            )\n            if not self.args.ed:\n                names = set(exclude_dotfiles([x[0] for x in vfs_ls]))\n                vfs_ls = [x for x in vfs_ls if x[0] in names]\n            zi = int(time.time())\n            zsr = os.stat_result((16877, -1, -1, 1, 1000, 1000, 8, zi, zi, zi))\n            ls = [{\"vp\": vp, \"st\": st} for vp, st in vfs_ls]\n            ls += [{\"vp\": v, \"st\": zsr} for v in vfs_virt]\n            fgen = ls\n        else:\n            t = \"invalid depth value '{}' (must be either '0' or '1'{})\"\n            t2 = \" or 'infinity'\" if self.args.dav_inf else \"\"\n            raise Pebkac(412, t.format(depth, t2))\n        fgen = itertools.chain([topdir], fgen)\n        vtop = vjoin(self.args.R, vjoin(vn.vpath, rem))\n        chunksz = 0x7FF8\n        self.send_headers(\n            None, 207, \"text/xml; charset=\" + enc, {\"Transfer-Encoding\": \"chunked\"}\n        )\n        ret = '<?xml version=\"1.0\" encoding=\"{}\"?>\\n<D:multistatus xmlns:D=\"DAV:\">'\n        ret = ret.format(uenc)\n        for x in fgen:\n            rp = vjoin(vtop, x[\"vp\"])\n            st: os.stat_result = x[\"st\"]\n            mtime = st.st_mtime\n            if stat.S_ISLNK(st.st_mode):\n                try:\n                    st = bos.stat(os.path.join(tap, x[\"vp\"]))\n                except:\n                    continue\n            isdir = stat.S_ISDIR(st.st_mode)\n            ret += \"<D:response><D:href>/%s%s</D:href><D:propstat><D:prop>\" % (\n                quotep(rp),\n                \"/\" if isdir and rp else \"\",\n            )\n            pvs: dict[str, str] = {\n                \"displayname\": html_escape(rp.split(\"/\")[-1]),\n                \"getlastmodified\": formatdate(mtime, usegmt=True),\n                \"resourcetype\": '<D:collection xmlns:D=\"DAV:\"/>' if isdir else \"\",\n                \"supportedlock\": '<D:lockentry xmlns:D=\"DAV:\"><D:lockscope><D:exclusive/></D:lockscope><D:locktype><D:write/></D:locktype></D:lockentry>',\n            }\n            if not isdir:\n                pvs[\"getcontenttype\"] = html_escape(guess_mime(rp))\n                pvs[\"getcontentlength\"] = str(st.st_size)\n            for k, v in pvs.items():\n                if k not in props:\n                    continue\n                elif v:\n                    ret += \"<D:%s>%s</D:%s>\" % (k, v, k)\n                else:\n                    ret += \"<D:%s/>\" % (k,)\n            ret += \"</D:prop><D:status>HTTP/1.1 200 OK</D:status></D:propstat>\"\n            missing = [\"<D:%s/>\" % (x,) for x in props if x not in pvs]\n            if missing and clen:\n                t = \"<D:propstat><D:prop>{}</D:prop><D:status>HTTP/1.1 404 Not Found</D:status></D:propstat>\"\n                ret += t.format(\"\".join(missing))\n            ret += \"</D:response>\"\n            while len(ret) >= chunksz:\n                ret = self.send_chunk(ret, enc, chunksz)\n        ret += \"</D:multistatus>\"\n        while ret:\n            ret = self.send_chunk(ret, enc, chunksz)\n        self.send_chunk(\"\", enc, chunksz)\n        return True",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-37474",
        "description": "[{'lang': 'en', 'value': 'Copyparty is a portable file server. Versions prior to 1.8.2 are subject to a path traversal vulnerability detected in the `.cpr` subfolder. The Path Traversal attack technique allows an attacker access to files, directories, and commands that reside outside the web document root directory. This issue has been addressed in commit `043e3c7d` which has been included in release 1.8.2. Users are advised to upgrade. There are no known workarounds for this vulnerability.'}]",
        "cwe_number": 22
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-26",
      "code": "    def add_apispec_components(self, api_spec):\n        super(SecurityApi, self).add_apispec_components(api_spec)\n        jwt_scheme = {\"type\": \"http\", \"scheme\": \"bearer\", \"bearerFormat\": \"JWT\"}\n        api_spec.components.security_scheme(\"jwt\", jwt_scheme)\n        api_spec.components.security_scheme(\"jwt_refresh\", jwt_scheme)\n    def login(self):\n        \"\"\"Login endpoint for the API returns a JWT and optionally a refresh token\n        ---\n        post:\n          description: >-\n            Authenticate and get a JWT access and refresh token\n          requestBody:\n            required: true\n            content:\n              application/json:\n                schema:\n                  type: object\n                  properties:\n                    username:\n                      description: The username for authentication\n                      example: admin\n                      type: string\n                    password:\n                      description: The password for authentication\n                      example: complex-password\n                      type: string\n                    provider:\n                      description: Choose an authentication provider\n                      example: db\n                      type: string\n                      enum:\n                      - db\n                      - ldap\n                    refresh:\n                      description: If true a refresh token is provided also\n                      example: true\n                      type: boolean\n          responses:\n            200:\n              description: Authentication Successful\n              content:\n                application/json:\n                  schema:\n                    type: object\n                    properties:\n                      access_token:\n                        type: string\n                      refresh_token:\n                        type: string\n            400:\n              $ref: '\n            401:\n              $ref: '\n            500:\n              $ref: '\n        \"\"\"\n        if not request.is_json:\n            return self.response_400(message=\"Request payload is not JSON\")\n        username = request.json.get(API_SECURITY_USERNAME_KEY, None)\n        password = request.json.get(API_SECURITY_PASSWORD_KEY, None)\n        provider = request.json.get(API_SECURITY_PROVIDER_KEY, None)\n        refresh = request.json.get(API_SECURITY_REFRESH_KEY, False)\n        if not username or not password or not provider:\n            return self.response_400(message=\"Missing required parameter\")\n        if provider == API_SECURITY_PROVIDER_DB:\n            user = self.appbuilder.sm.auth_user_db(username, password)\n        elif provider == API_SECURITY_PROVIDER_LDAP:\n            user = self.appbuilder.sm.auth_user_ldap(username, password)\n        else:\n            return self.response_400(\n                message=\"Provider {} not supported\".format(provider)\n            )\n        if not user:\n            return self.response_401()\n        resp = dict()\n        resp[API_SECURITY_ACCESS_TOKEN_KEY] = create_access_token(\n            identity=user.id, fresh=True\n        )\n        if refresh:\n            resp[API_SECURITY_REFRESH_TOKEN_KEY] = create_refresh_token(\n                identity=user.id\n            )\n        return self.response(200, **resp)\n    def refresh(self):\n        \"\"\"\n            Security endpoint for the refresh token, so we can obtain a new\n            token without forcing the user to login again\n        ---\n        post:\n          description: >-\n            Use the refresh token to get a new JWT access token\n          responses:\n            200:\n              description: Refresh Successful\n              content:\n                application/json:\n                  schema:\n                    type: object\n                    properties:\n                      access_token:\n                        description: A new refreshed access token\n                        type: string\n            401:\n              $ref: '\n            500:\n              $ref: '\n          security:\n            - jwt_refresh: []\n        \"\"\"\n        resp = {\n            API_SECURITY_ACCESS_TOKEN_KEY: create_access_token(\n                identity=get_jwt_identity(), fresh=False\n            )\n        }\n        return self.response(200, **resp)\n    def __init__(self, appbuilder):\n        super(BaseSecurityManager, self).__init__(appbuilder)\n        app = self.appbuilder.get_app\n        app.config.setdefault(\"AUTH_ROLE_ADMIN\", \"Admin\")\n        app.config.setdefault(\"AUTH_ROLE_PUBLIC\", \"Public\")\n        app.config.setdefault(\"AUTH_TYPE\", AUTH_DB)\n        app.config.setdefault(\"AUTH_USER_REGISTRATION\", False)\n        app.config.setdefault(\"AUTH_USER_REGISTRATION_ROLE\", self.auth_role_public)\n        app.config.setdefault(\"AUTH_USER_REGISTRATION_ROLE_JMESPATH\", None)\n        app.config.setdefault(\"AUTH_ROLES_MAPPING\", {})\n        app.config.setdefault(\"AUTH_ROLES_SYNC_AT_LOGIN\", False)\n        if self.auth_type == AUTH_LDAP:\n            if \"AUTH_LDAP_SERVER\" not in app.config:\n                raise Exception(\n                    \"No AUTH_LDAP_SERVER defined on config\"\n                    \" with AUTH_LDAP authentication type.\"\n                )\n            app.config.setdefault(\"AUTH_LDAP_SEARCH\", \"\")\n            app.config.setdefault(\"AUTH_LDAP_SEARCH_FILTER\", \"\")\n            app.config.setdefault(\"AUTH_LDAP_APPEND_DOMAIN\", \"\")\n            app.config.setdefault(\"AUTH_LDAP_USERNAME_FORMAT\", \"\")\n            app.config.setdefault(\"AUTH_LDAP_BIND_USER\", \"\")\n            app.config.setdefault(\"AUTH_LDAP_BIND_PASSWORD\", \"\")\n            app.config.setdefault(\"AUTH_LDAP_USE_TLS\", False)\n            app.config.setdefault(\"AUTH_LDAP_ALLOW_SELF_SIGNED\", False)\n            app.config.setdefault(\"AUTH_LDAP_TLS_DEMAND\", False)\n            app.config.setdefault(\"AUTH_LDAP_TLS_CACERTDIR\", \"\")\n            app.config.setdefault(\"AUTH_LDAP_TLS_CACERTFILE\", \"\")\n            app.config.setdefault(\"AUTH_LDAP_TLS_CERTFILE\", \"\")\n            app.config.setdefault(\"AUTH_LDAP_TLS_KEYFILE\", \"\")\n            app.config.setdefault(\"AUTH_LDAP_UID_FIELD\", \"uid\")\n            app.config.setdefault(\"AUTH_LDAP_GROUP_FIELD\", \"memberOf\")\n            app.config.setdefault(\"AUTH_LDAP_FIRSTNAME_FIELD\", \"givenName\")\n            app.config.setdefault(\"AUTH_LDAP_LASTNAME_FIELD\", \"sn\")\n            app.config.setdefault(\"AUTH_LDAP_EMAIL_FIELD\", \"mail\")\n        if self.auth_type == AUTH_OID:\n            from flask_openid import OpenID\n            self.oid = OpenID(app)\n        if self.auth_type == AUTH_OAUTH:\n            from authlib.integrations.flask_client import OAuth\n            self.oauth = OAuth(app)\n            self.oauth_remotes = dict()\n            for _provider in self.oauth_providers:\n                provider_name = _provider[\"name\"]\n                log.debug(\"OAuth providers init {0}\".format(provider_name))\n                obj_provider = self.oauth.register(\n                    provider_name, **_provider[\"remote_app\"]\n                )\n                obj_provider._tokengetter = self.oauth_tokengetter\n                if not self.oauth_user_info:\n                    self.oauth_user_info = self.get_oauth_user_info\n                if \"whitelist\" in _provider:\n                    self.oauth_whitelists[provider_name] = _provider[\"whitelist\"]\n                self.oauth_remotes[provider_name] = obj_provider\n        self._builtin_roles = self.create_builtin_roles()\n        self.lm = self.create_login_manager(app)\n        self.jwt_manager = self.create_jwt_manager(app)\n    def get_url_for_registeruser(self):\n        return url_for(\n            \"%s.%s\"\n            % (self.registeruser_view.endpoint, self.registeruser_view.default_view)\n        )",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-41265",
        "description": "[{'lang': 'en', 'value': 'Flask-AppBuilder is a development framework built on top of Flask. Verions prior to 3.3.4 contain an improper authentication vulnerability in the REST API. The issue allows for a malicious actor with a carefully crafted request to successfully authenticate and gain access to existing protected REST API endpoints. This only affects non database authentication types and new REST API endpoints. Users should upgrade to Flask-AppBuilder 3.3.4 to receive a patch.'}]",
        "cwe_number": 287
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-27",
      "code": "    def _find_working_git(self):\n        test_cmd = 'version'\n        if app.GIT_PATH:\n            main_git = '\"' + app.GIT_PATH + '\"'\n        else:\n            main_git = 'git'\n        log.debug(u'Checking if we can use git commands: {0} {1}', main_git, test_cmd)\n        _, _, exit_status = self._run_git(main_git, test_cmd)\n        if exit_status == 0:\n            log.debug(u'Using: {0}', main_git)\n            return main_git\n        else:\n            log.debug(u'Not using: {0}', main_git)\n        alternative_git = []\n        if platform.system().lower() == 'darwin':\n            alternative_git.append('/usr/local/git/bin/git')\n        if platform.system().lower() == 'windows':\n            if main_git != main_git.lower():\n                alternative_git.append(main_git.lower())\n        if alternative_git:\n            log.debug(u'Trying known alternative git locations')\n            for cur_git in alternative_git:\n                log.debug(u'Checking if we can use git commands: {0} {1}', cur_git, test_cmd)\n                _, _, exit_status = self._run_git(cur_git, test_cmd)\n                if exit_status == 0:\n                    log.debug(u'Using: {0}', cur_git)\n                    return cur_git\n                else:\n                    log.debug(u'Not using: {0}', cur_git)\n    def _run_git(self, git_path, args):\n        output = err = exit_status = None\n        if not git_path:\n            git_path = self._find_working_git()\n            if git_path:\n                self._git_path = git_path\n            else:\n                if app.VERSION_NOTIFY:\n                    log.warning(u\"No git specified, can't use git commands\")\n                    app.NEWEST_VERSION_STRING = ERROR_MESSAGE\n                exit_status = 1\n                return output, err, exit_status\n        app.NEWEST_VERSION_STRING = None\n        cmd = git_path + ' ' + args\n        try:\n            log.debug(u'Executing {cmd} with your shell in {dir}', {'cmd': cmd, 'dir': app.PROG_DIR})\n            p = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n                                 shell=True, cwd=app.PROG_DIR)\n            output, err = p.communicate()\n            exit_status = p.returncode\n            if isinstance(output, (bytes, bytearray)):\n                output = output.decode('utf-8')\n            if output:\n                output = output.strip()\n        except OSError:\n            log.info(u\"Command {cmd} didn't work\", {'cmd': cmd})\n            exit_status = 1\n        if exit_status == 0:\n            log.debug(u'{cmd} : returned successful', {'cmd': cmd})\n        elif exit_status == 1:\n            if output:\n                if 'stash' in output:\n                    log.warning(u\"Enable 'git reset' in settings or stash your changes in local files\")\n                else:\n                    log.warning(u'{cmd} returned : {output}', {'cmd': cmd, 'output': output})\n            else:\n                log.warning(u'{cmd} returned no data', {'cmd': cmd})\n        elif exit_status == 128:\n            log.warning('{cmd} returned ({status}) : {output}',\n                        {'cmd': cmd, 'status': exit_status, 'output': output})\n        elif exit_status == 129:\n            if 'unknown option' in output and 'set-upstream-to' in output:\n                log.info(\"Can't set upstream to origin/{0} because you're running an old version of git.\"\n                         '\\nPlease upgrade your git installation to its latest version.', app.BRANCH)\n            else:\n                log.warning('{cmd} returned ({status}) : {output}',\n                            {'cmd': cmd, 'status': exit_status, 'output': output})\n        else:\n            log.warning(u'{cmd} returned : {output}. Treat as error for now', {'cmd': cmd, 'output': output})\n            exit_status = 1\n        return output, err, exit_status\n    def update_commit_hash(self):\n        \"\"\"Attempt to set the hash of the currently installed version of the application.\n        Uses git to get commit version.\n        \"\"\"\n        output, _, exit_status = self._run_git(self._git_path, 'rev-parse HEAD')\n        if exit_status == 0 and output:\n            cur_commit_hash = output.strip()\n            if not re.match('^[a-z0-9]+$', cur_commit_hash):\n                log.warning(u\"Output doesn't look like a hash, not using it\")\n                return False\n            self._cur_commit_hash = cur_commit_hash\n            app.CUR_COMMIT_HASH = cur_commit_hash\n            return True\n        return False",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-28627",
        "description": "[{'lang': 'en', 'value': 'pymedusa is an automatic video library manager for TV Shows. In versions prior 1.0.12 an attacker with access to the web interface can update the git executable path in /config/general/ > advanced settings with arbitrary OS commands. An attacker may exploit this vulnerability to take execute arbitrary OS commands as the user running the pymedusa program. Users are advised to upgrade. There are no known workarounds for this vulnerability.'}]",
        "cwe_number": 78
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-28",
      "code": "    def add_authorizedkey(self, key, comment=None):\n        \"\"\"\n        Add the given key to the user. Adding the key to his `authorized_keys`\n        file if it exists and adding it to database.\n        \"\"\"\n        assert key\n        key = authorizedkeys.check_publickey(key)\n        key = authorizedkeys.AuthorizedKey(\n            options=None, keytype=key.keytype, key=key.key, comment=comment or key.comment\n        )\n        filename = os.path.join(self.user_root, '.ssh', 'authorized_keys')\n        if os.path.isfile(filename):\n            with open(filename, mode=\"r+\", encoding='utf-8') as fh:\n                if authorizedkeys.exists(fh, key):\n                    raise DuplicateSSHKeyError(_(\"SSH key already exists\"))\n                logger.info(\"add key [%s] to [%s] authorized_keys\", key, self.username)\n                authorizedkeys.add(fh, key)\n        else:\n            logger.info(\"add key [%s] to [%s] database\", key, self.username)\n            try:\n                SshKey(userid=self.userid, fingerprint=key.fingerprint, key=key.getvalue()).add().flush()\n            except IntegrityError:\n                raise DuplicateSSHKeyError(\n                    _(\"Duplicate key. This key already exists or is associated to another user.\")\n                )\n        cherrypy.engine.publish('user_attr_changed', self, {'authorizedkeys': True})\n        cherrypy.engine.publish('authorizedkey_added', self, fingerprint=key.fingerprint, comment=comment)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-4724",
        "description": "[{'lang': 'en', 'value': 'Improper Access Control in GitHub repository ikus060/rdiffweb prior to 2.5.5.'}]",
        "cwe_number": 284
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-29",
      "code": "    def __render_string(self, value):\n        orig = last = value\n        max_recursion = self.__dict__.get('jinja_max_recursion', 5)\n        for _ in range(max_recursion):\n            template = jinja2.Template(value, keep_trailing_newline=True)\n            value = _to_native(template.render(self.__dict__))\n            if value == last:\n                return value\n            last = value\n        raise ValueError(\"too deep jinja re-evaluation on '{}'\".format(orig))",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-6395",
        "description": "[{'lang': 'en', 'value': 'The Mock software contains a vulnerability wherein an attacker could potentially exploit privilege escalation, enabling the execution of arbitrary code with root user privileges. This weakness stems from the absence of proper sandboxing during the expansion and execution of Jinja2 templates, which may be included in certain configuration parameters. While the Mock documentation advises treating users added to the mock group as privileged, certain build systems invoking mock on behalf of users might inadvertently permit less privileged users to define configuration tags. These tags could then be passed as parameters to mock during execution, potentially leading to the utilization of Jinja2 templates for remote privilege escalation and the execution of arbitrary code as the root user on the build server.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-30",
      "code": "    def _create_database(self, last_upgrade_to_run):\n        \"\"\"\n        Make sure that the database is created and sets the file permissions.\n        This should be done before storing any sensitive data in it.\n        \"\"\"\n        conn = self._connect()\n        try:\n            with conn:\n                self._create_tables(conn, last_upgrade_to_run)\n        finally:\n            conn.close()\n        os.chmod(self.filename, stat.S_IRUSR | stat.S_IWUSR)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-23651",
        "description": "[{'lang': 'en', 'value': 'b2-sdk-python is a python library to access cloud storage provided by backblaze. Linux and Mac releases of the SDK version 1.14.0 and below contain a key disclosure vulnerability that, in certain conditions, can be exploited by local attackers through a time-of-check-time-of-use (TOCTOU) race condition. SDK users of the SqliteAccountInfo format are vulnerable while users of the InMemoryAccountInfo format are safe. The SqliteAccountInfo saves API keys (and bucket name-to-id mapping) in a local database file ($XDG_CONFIG_HOME/b2/account_info, ~/.b2_account_info or a user-defined path). When first created, the file is world readable and is (typically a few milliseconds) later altered to be private to the user. If the directory containing the file is readable by a local attacker then during the brief period between file creation and permission modification, a local attacker can race to open the file and maintain a handle to it. This allows the local attacker to read the contents after the file after the sensitive information has been saved to it. Consumers of this SDK who rely on it to save data using SqliteAccountInfo class should upgrade to the latest version of the SDK. Those who believe a local user might have opened a handle using this race condition, should remove the affected database files and regenerate all application keys. Users should upgrade to b2-sdk-python 1.14.1 or later.'}]",
        "cwe_number": 367
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-31",
      "code": "  def _testDrawBoundingBoxColorCycling(self, img, colors=None):\n    \"\"\"Tests if cycling works appropriately.\n    Args:\n      img: 3-D numpy image on which to draw.\n    \"\"\"\n    color_table = colors\n    if colors is None:\n      color_table = np.asarray([[1, 1, 0, 1], [0, 0, 1, 1], [1, 0, 0, 1],\n                                [0, 1, 0, 1], [0.5, 0, 0.5,\n                                               1], [0.5, 0.5, 0, 1],\n                                [0.5, 0, 0, 1], [0, 0, 0.5, 1], [0, 1, 1, 1],\n                                [1, 0, 1, 1]])\n    assert len(img.shape) == 3\n    depth = img.shape[2]\n    assert depth <= color_table.shape[1]\n    assert depth == 1 or depth == 3 or depth == 4\n    if depth == 1:\n      color_table[:, 0] = 1\n    num_colors = color_table.shape[0]\n    for num_boxes in range(1, num_colors + 2):\n      image = np.copy(img)\n      color = color_table[(num_boxes - 1) % num_colors, 0:depth]\n      test_drawn_image = self._fillBorder(image, color)\n      bboxes = np.asarray([0, 0, 1, 1])\n      bboxes = np.vstack([bboxes for _ in range(num_boxes)])\n      bboxes = math_ops.cast(bboxes, dtypes.float32)\n      bboxes = array_ops.expand_dims(bboxes, 0)\n      image = ops.convert_to_tensor(image)\n      image = image_ops_impl.convert_image_dtype(image, dtypes.float32)\n      image = array_ops.expand_dims(image, 0)\n      image = image_ops.draw_bounding_boxes(image, bboxes, colors=colors)\n      with self.cached_session(use_gpu=False) as sess:\n        op_drawn_image = np.squeeze(sess.run(image), 0)\n        self.assertAllEqual(test_drawn_image, op_drawn_image)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-36001",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. When `DrawBoundingBoxes` receives an input `boxes` that is not of dtype `float`, it gives a `CHECK` fail that can trigger a denial of service attack. We have patched the issue in GitHub commit da0d65cdc1270038e72157ba35bf74b85d9bda11. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue.'}]",
        "cwe_number": 617
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-32",
      "code": "    def do_GET(self):\n        \"\"\"This method services the GET request typically from either the Tenant or the Cloud Verifier.\n        Only tenant and cloudverifier uri's are supported. Both requests require a nonce parameter.\n        The Cloud verifier requires an additional mask paramter.  If the uri or parameters are incorrect, a 400 response is returned.\n        \"\"\"\n        logger.info('GET invoked from %s with uri: %s', self.client_address, self.path)\n        rest_params = web_util.get_restful_params(self.path)\n        if rest_params is None:\n            web_util.echo_json_response(\n                self, 405, \"Not Implemented: Use /version, /keys/ or /quotes/ interfaces\")\n            return\n        if \"version\" in rest_params:\n            version_info = {\n                \"supported_version\": keylime_api_version.current_version()\n            }\n            web_util.echo_json_response(self, 200, version_info)\n            return\n        if not rest_params[\"api_version\"]:\n            web_util.echo_json_response(self, 400, \"API Version not supported\")\n            return\n        if \"keys\" in rest_params and rest_params['keys'] == 'verify':\n            if self.server.K is None:\n                logger.info('GET key challenge returning 400 response. bootstrap key not available')\n                web_util.echo_json_response(\n                    self, 400, \"Bootstrap key not yet available.\")\n                return\n            if \"challenge\" not in rest_params:\n                logger.info('GET key challenge returning 400 response. No challenge provided')\n                web_util.echo_json_response(\n                    self, 400, \"No challenge provided.\")\n                return\n            challenge = rest_params['challenge']\n            response = {}\n            response['hmac'] = crypto.do_hmac(self.server.K, challenge)\n            web_util.echo_json_response(self, 200, \"Success\", response)\n            logger.info('GET key challenge returning 200 response.')\n        elif \"keys\" in rest_params and rest_params[\"keys\"] == \"pubkey\":\n            response = {}\n            response['pubkey'] = self.server.rsapublickey_exportable\n            web_util.echo_json_response(self, 200, \"Success\", response)\n            logger.info('GET pubkey returning 200 response.')\n            return\n        elif \"quotes\" in rest_params:\n            nonce = rest_params.get('nonce', None)\n            pcrmask = rest_params.get('mask', None)\n            ima_ml_entry = rest_params.get('ima_ml_entry', '0')\n            if nonce is None:\n                logger.warning('GET quote returning 400 response. nonce not provided as an HTTP parameter in request')\n                web_util.echo_json_response(\n                    self, 400, \"nonce not provided as an HTTP parameter in request\")\n                return\n            if not (nonce.isalnum() and\n                    (pcrmask is None or config.valid_hex(pcrmask)) and\n                    ima_ml_entry.isalnum()):\n                logger.warning('GET quote returning 400 response. parameters should be strictly alphanumeric')\n                web_util.echo_json_response(\n                    self, 400, \"parameters should be strictly alphanumeric\")\n                return\n            if len(nonce) > tpm_instance.MAX_NONCE_SIZE:\n                logger.warning('GET quote returning 400 response. Nonce is too long (max size %i): %i',\n                               tpm_instance.MAX_NONCE_SIZE, len(nonce))\n                web_util.echo_json_response(\n                    self, 400, f'Nonce is too long (max size {tpm_instance.MAX_NONCE_SIZE}): {len(nonce)}')\n                return\n            hash_alg = tpm_instance.defaults['hash']\n            if not tpm_instance.is_vtpm() or rest_params[\"quotes\"] == 'identity':\n                quote = tpm_instance.create_quote(\n                    nonce, self.server.rsapublickey_exportable, pcrmask, hash_alg)\n                imaMask = pcrmask\n            enc_alg = tpm_instance.defaults['encrypt']\n            sign_alg = tpm_instance.defaults['sign']\n            if \"partial\" in rest_params and (rest_params[\"partial\"] is None or rest_params[\"partial\"] == \"1\"):\n                response = {\n                    'quote': quote,\n                    'hash_alg': hash_alg,\n                    'enc_alg': enc_alg,\n                    'sign_alg': sign_alg,\n                }\n            else:\n                response = {\n                    'quote': quote,\n                    'hash_alg': hash_alg,\n                    'enc_alg': enc_alg,\n                    'sign_alg': sign_alg,\n                    'pubkey': self.server.rsapublickey_exportable,\n                }\n            response['boottime'] = self.server.boottime\n            if TPM_Utilities.check_mask(imaMask, config.IMA_PCR):\n                ima_ml_entry = int(ima_ml_entry)\n                if ima_ml_entry > self.server.next_ima_ml_entry:\n                    ima_ml_entry = 0\n                ml, nth_entry, num_entries = ima.read_measurement_list(config.IMA_ML, ima_ml_entry)\n                if num_entries > 0:\n                    response['ima_measurement_list'] = ml\n                    response['ima_measurement_list_entry'] = nth_entry\n                    self.server.next_ima_ml_entry = num_entries\n            if TPM_Utilities.check_mask(imaMask, config.MEASUREDBOOT_PCRS[0]):\n                if not os.path.exists(config.MEASUREDBOOT_ML):\n                    logger.warning(\"TPM2 event log not available: %s\", config.MEASUREDBOOT_ML)\n                else:\n                    with open(config.MEASUREDBOOT_ML, 'rb') as f:\n                        el = base64.b64encode(f.read())\n                    response['mb_measurement_list'] = el\n            web_util.echo_json_response(self, 200, \"Success\", response)\n            logger.info('GET %s quote returning 200 response.', rest_params[\"quotes\"])\n            return\n        else:\n            logger.warning('GET returning 400 response. uri not supported: %s', self.path)\n            web_util.echo_json_response(self, 400, \"uri not supported\")\n            return\ndef _process_measurement_list(agentAttestState, lines, hash_alg, lists=None, m2w=None, pcrval=None, ima_keyrings=None,\n                              boot_aggregates=None):\n    failure = Failure(Component.IMA)\n    running_hash = agentAttestState.get_pcr_state(config.IMA_PCR, hash_alg)\n    found_pcr = (pcrval is None)\n    errors = {}\n    pcrval_bytes = b''\n    if pcrval is not None:\n        pcrval_bytes = codecs.decode(pcrval.encode('utf-8'), 'hex')\n    if lists is not None:\n        if isinstance(lists, str):\n            lists = json.loads(lists)\n        allow_list = lists['allowlist']\n        exclude_list = lists['exclude']\n    else:\n        allow_list = None\n        exclude_list = None\n    ima_log_hash_alg = Hash.SHA1\n    if allow_list is not None:\n        try:\n            ima_log_hash_alg = Hash(allow_list[\"ima\"][\"log_hash_alg\"])\n        except ValueError:\n            logger.warning(\"Specified IMA log hash algorithm %s is not a valid algorithm! Defaulting to SHA1.\",\n                           allow_list[\"ima\"][\"log_hash_alg\"])\n    if boot_aggregates and allow_list:\n        if 'boot_aggregate' not in allow_list['hashes'] :\n            allow_list['hashes']['boot_aggregate'] = []\n        for alg in boot_aggregates.keys() :\n            for val in boot_aggregates[alg] :\n                if val not in allow_list['hashes']['boot_aggregate'] :\n                    allow_list['hashes']['boot_aggregate'].append(val)\n    is_valid, compiled_regex, err_msg = config.valid_exclude_list(exclude_list)\n    if not is_valid:\n        err_msg += \" Exclude list will be ignored.\"\n        logger.error(err_msg)\n    ima_validator = ima_ast.Validator(\n        {ima_ast.ImaSig: functools.partial(_validate_ima_sig, compiled_regex, ima_keyrings, allow_list),\n         ima_ast.ImaNg: functools.partial(_validate_ima_ng, compiled_regex, allow_list),\n         ima_ast.Ima: functools.partial(_validate_ima_ng, compiled_regex, allow_list),\n         ima_ast.ImaBuf: functools.partial(_validate_ima_buf, compiled_regex, allow_list, ima_keyrings),\n         }\n    )\n    if not found_pcr and len(lines) <= 2:\n        found_pcr = (running_hash == pcrval_bytes)\n    for linenum, line in enumerate(lines):\n        line = line.strip()\n        if line == '':\n            continue\n        try:\n            entry = ima_ast.Entry(line, ima_validator, ima_hash_alg=ima_log_hash_alg, pcr_hash_alg=hash_alg)\n            running_hash = hash_alg.hash(running_hash + entry.pcr_template_hash)\n            validation_failure = entry.invalid()\n            if validation_failure:\n                failure.merge(validation_failure)\n                errors[type(entry.mode)] = errors.get(type(entry.mode), 0) + 1\n            if not found_pcr:\n                found_pcr = (running_hash == pcrval_bytes)\n                if found_pcr:\n                    logger.debug('Found match at linenum %s' % (linenum + 1))\n                    agentAttestState.update_ima_attestation(int(entry.pcr), running_hash, linenum + 1)\n            if m2w is not None and (type(entry.mode) in [ima_ast.Ima, ima_ast.ImaNg, ima_ast.ImaSig]):\n                hash_value = codecs.encode(entry.mode.digest.bytes, \"hex\")\n                path = entry.mode.path.name\n                m2w.write(f\"{hash_value} {path}\\n\")\n        except ima_ast.ParserError:\n            failure.add_event(\"entry\", f\"Line was not parsable into a valid IMA entry: {line}\", True, [\"parser\"])\n            logger.error(f\"Line was not parsable into a valid IMA entry: {line}\")\n    if not found_pcr:\n        logger.error(f\"IMA measurement list does not match TPM PCR {pcrval}\")\n        failure.add_event(\"pcr_mismatch\", f\"IMA measurement list does not match TPM PCR {pcrval}\", True)\n    if sum(errors.values()) > 0:\n        error_msg = \"IMA ERRORS: Some entries couldn't be validated. Number of failures in modes: \"\n        error_msg += \", \".join([f'{k.__name__ } {v}' for k, v in errors.items()])\n        logger.error(error_msg + \".\")\n    return codecs.encode(running_hash, 'hex').decode('utf-8'), failure\ndef process_measurement_list(agentAttestState, lines, lists=None, m2w=None, pcrval=None, ima_keyrings=None,\n                             boot_aggregates=None, hash_alg=Hash.SHA1):\n    failure = Failure(Component.IMA)\n    try:\n        running_hash, failure = _process_measurement_list(agentAttestState, lines, hash_alg, lists=lists, m2w=m2w,\n                                                          pcrval=pcrval, ima_keyrings=ima_keyrings,\n                                                          boot_aggregates=boot_aggregates)\n    except:\n        raise\n    finally:\n        if failure:\n            agentAttestState.reset_ima_attestation()\n    return running_hash, failure\ndef validate_agent_data(agent_data):\n    if agent_data is None:\n        return False, None\n    lists = json.loads(agent_data['allowlist'])\n    is_valid, _, err_msg = config.valid_exclude_list(lists.get('exclude'))\n    if not is_valid:\n        err_msg += \" Exclude list regex is misformatted. Please correct the issue and try again.\"\n    return is_valid, err_msg",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-23949",
        "description": "[{'lang': 'en', 'value': 'In Keylime before 6.3.0, unsanitized UUIDs can be passed by a rogue agent and can lead to log spoofing on the verifier and registrar.'}]",
        "cwe_number": 290
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-33",
      "code": "    def save(self, request, login_code_url='login_code', domain_override=None, extra_context=None):\n        login_code = models.LoginCode.create_code_for_user(\n            user=self.cleaned_data['user'],\n            next=self.cleaned_data['next'],\n        )\n        if not domain_override:\n            current_site = get_current_site(request)\n            site_name = current_site.name\n            domain = current_site.domain\n        else:\n            site_name = domain = domain_override\n        url = '{}://{}{}?code={}'.format(\n            'https' if request.is_secure() else 'http',\n            domain,\n            resolve_url(login_code_url),\n            login_code.code,\n        )\n        context = {\n            'domain': domain,\n            'site_name': site_name,\n            'code': login_code.code,\n            'url': url,\n        }\n        if extra_context:\n            context.update(extra_context)\n        self.send_login_code(login_code, context)\n        return login_code\n    def clean_code(self):\n        code = self.cleaned_data['code']\n        username = code.user.get_username()\n        user = authenticate(self.request, **{\n            get_user_model().USERNAME_FIELD: username,\n            'code': code.code,\n        })\n        if not user:\n            raise forms.ValidationError(\n                self.error_messages['invalid_code'],\n                code='invalid_code',\n            )\n        self.cleaned_data['user'] = user\n        return code\n    def save(self):\n        self.cleaned_data['code'].delete()\n    def authenticate(self, request, username=None, code=None, **kwargs):\n        if username is None:\n            username = kwargs.get(get_user_model().USERNAME_FIELD)\n        if not username or not code:\n            return\n        try:\n            user = get_user_model()._default_manager.get_by_natural_key(username)\n            if not self.user_can_authenticate(user):\n                return\n            timeout = getattr(settings, 'NOPASSWORD_LOGIN_CODE_TIMEOUT', 900)\n            timestamp = timezone.now() - timedelta(seconds=timeout)\n            user.login_code = LoginCode.objects.get(user=user, code=code, timestamp__gt=timestamp)\n            return user\n        except (get_user_model().DoesNotExist, LoginCode.DoesNotExist):\n            return\n    def create_code_for_user(cls, user, next=None):\n        if not user.is_active:\n            return None\n        code = cls.generate_code()\n        login_code = LoginCode(user=user, code=code)\n        if next is not None:\n            login_code.next = next\n        login_code.save()\n        return login_code\n    def generate_code(cls):\n        hash_algorithm = getattr(settings, 'NOPASSWORD_HASH_ALGORITHM', 'sha256')\n        m = getattr(hashlib, hash_algorithm)()\n        m.update(getattr(settings, 'SECRET_KEY', None).encode('utf-8'))\n        m.update(os.urandom(16))\n        if getattr(settings, 'NOPASSWORD_NUMERIC_CODES', False):\n            hashed = str(int(m.hexdigest(), 16))\n        else:\n            hashed = m.hexdigest()\n        return hashed\n    def get_response(self):\n        token_serializer = self.token_serializer_class(\n            instance=self.token,\n            context=self.get_serializer_context(),\n        )\n        data = token_serializer.data\n        data['next'] = self.serializer.validated_data['code'].next\n        return Response(data, status=status.HTTP_200_OK)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2019-10682",
        "description": "[{'lang': 'en', 'value': 'django-nopassword before 5.0.0 stores cleartext secrets in the database.'}]",
        "cwe_number": 312
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-34",
      "code": "def login_record(success=True):\n    denied_hosts = read_hosts_deny()\n    val = (0, 0)\n    if success and request.client in denied_hosts:\n        del denied_hosts[request.client]\n    elif not success and not request.is_local:\n        val = denied_hosts.get(request.client, (0, 0))\n        if time.time() - val[1] < expiration_failed_logins \\\n            and val[0] >= allowed_number_of_attempts:\n            return val[0]\n        time.sleep(2 ** val[0])\n        val = (val[0] + 1, int(time.time()))\n        denied_hosts[request.client] = val\n    write_hosts_deny(denied_hosts)\n    return val[0]",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2016-10321",
        "description": "[{'lang': 'en', 'value': 'web2py before 2.14.6 does not properly check if a host is denied before verifying passwords, allowing a remote attacker to perform brute-force attacks.'}]",
        "cwe_number": 254
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-35",
      "code": "def is_safe_url(url, host=None):\n    \"\"\"\n    Return ``True`` if the url is a safe redirection (i.e. it doesn't point to\n    a different host and uses a safe scheme).\n    Always returns ``False`` on an empty url.\n    \"\"\"\n    if url is not None:\n        url = url.strip()\n    if not url:\n        return False\n    url = url.replace('\\\\', '/')\n    if url.startswith('///'):\n        return False\n    url_info = urlparse(url)\n    if not url_info.netloc and url_info.scheme:\n        return False\n    if unicodedata.category(url[0])[0] == 'C':\n        return False\n    return ((not url_info.netloc or url_info.netloc == host) and\n            (not url_info.scheme or url_info.scheme in ['http', 'https']))",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2016-2512",
        "description": "[{'lang': 'en', 'value': 'The utils.http.is_safe_url function in Django before 1.8.10 and 1.9.x before 1.9.3 allows remote attackers to redirect users to arbitrary web sites and conduct phishing attacks or possibly conduct cross-site scripting (XSS) attacks via a URL containing basic authentication, as demonstrated by http://mysite.example.com\\\\@attacker.com.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-36",
      "code": "    def authenticate(self, username, password):\n        child = None\n        try:\n            child = pexpect.spawn('/bin/sh', ['-c', '/bin/su -c \"/bin/echo SUCCESS\" - %s' % username], timeout=5)\n            child.expect('.*:')\n            child.sendline(password)\n            result = child.expect(['su: .*', 'SUCCESS'])\n        except Exception as err:\n            if child and child.isalive():\n                child.close()\n            logging.error('Error checking password: %s', err)\n            return False\n        if result == 0:\n            return False\n        else:\n            return True",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2019-25066",
        "description": "[{'lang': 'en', 'value': 'A vulnerability has been found in ajenti 2.1.31 and classified as critical. This vulnerability affects unknown code of the component API. The manipulation leads to privilege escalation. The attack can be initiated remotely. The exploit has been disclosed to the public and may be used. Upgrading to version 2.1.32 is able to address this issue. The name of the patch is 7aa146b724e0e20cfee2c71ca78fafbf53a8767c. It is recommended to upgrade the affected component.'}]",
        "cwe_number": 78
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-37",
      "code": "def create_app(name=\"\"):\n    if \"mscolab.server\" in name:\n        from mslib.mscolab.app import APP\n    else:\n        from mslib.mswms.app import APP\n    @APP.route('/xstatic/<name>/', defaults=dict(filename=''))\n    @APP.route('/xstatic/<name>/<path:filename>')\n    def files(name, filename):\n        base_path = _xstatic(name)\n        if base_path is None:\n            abort(404)\n        if not filename:\n            abort(404)\n        return send_from_directory(base_path, filename)\n    @APP.route('/mss_theme/<name>/', defaults=dict(filename=''))\n    @APP.route('/mss_theme/<name>/<path:filename>')\n    def mss_theme(name, filename):\n        if name != 'img':\n            abort(404)\n        base_path = os.path.join(DOCS_SERVER_PATH, 'static', 'img')\n        return send_from_directory(base_path, filename)\n    def get_topmenu():\n        if \"mscolab\" in \" \".join(sys.argv):\n            menu = [\n                (url_for('index'), 'Mission Support System',\n                 ((url_for('about'), 'About'),\n                  (url_for('install'), 'Install'),\n                  (url_for('help'), 'Help'),\n                  )),\n            ]\n        else:\n            menu = [\n                (url_for('index'), 'Mission Support System',\n                 ((url_for('about'), 'About'),\n                  (url_for('install'), 'Install'),\n                  (url_for(\"plots\"), 'Gallery'),\n                  (url_for('help'), 'Help'),\n                  )),\n            ]\n        return menu\n    APP.jinja_env.globals.update(get_topmenu=get_topmenu)\n    def get_content(filename, overrides=None):\n        markdown = Markdown(extensions=[\"fenced_code\"])\n        content = \"\"\n        if os.path.isfile(filename):\n            with codecs.open(filename, 'r', 'utf-8') as f:\n                md_data = f.read()\n            md_data = md_data.replace(':ref:', '')\n            if overrides is not None:\n                v1, v2 = overrides\n                md_data = md_data.replace(v1, v2)\n            content = markdown.convert(md_data)\n        return content\n    @APP.route(\"/index\")\n    def index():\n        return render_template(\"/index.html\")\n    @APP.route(\"/mss/about\")\n    @APP.route(\"/mss\")\n    def about():\n        _file = os.path.join(DOCS_SERVER_PATH, 'static', 'docs', 'about.md')\n        img_url = url_for('overview')\n        overrides = ['![image](/mss/overview.png)', f'![image]({img_url})']\n        content = get_content(_file,\n                              overrides=overrides)\n        return render_template(\"/content.html\", act=\"about\", content=content)\n    @APP.route(\"/mss/install\")\n    def install():\n        _file = os.path.join(DOCS_SERVER_PATH, 'static', 'docs', 'installation.md')\n        content = get_content(_file)\n        return render_template(\"/content.html\", act=\"install\", content=content)\n    @APP.route(\"/mss/plots\")\n    def plots():\n        if STATIC_LOCATION != \"\" and os.path.exists(os.path.join(STATIC_LOCATION, 'plots.html')):\n            _file = os.path.join(STATIC_LOCATION, 'plots.html')\n            content = get_content(_file)\n        else:\n            content = \"Gallery was not generated for this server.<br>\" \\\n                      \"For further info on how to generate it, run the \" \\\n                      \"<b>gallery --help</b> command line parameter of mswms.<br>\" \\\n                      \"An example of the gallery can be seen \" \\\n                      \"<a href=\\\"https://mss.readthedocs.io/en/stable/gallery/index.html\\\">here</a>\"\n        return render_template(\"/content.html\", act=\"plots\", content=content)\n    @APP.route(\"/mss/code/<path:filename>\")\n    def code(filename):\n        download = request.args.get(\"download\", False)\n        _file = os.path.join(STATIC_LOCATION, 'code', filename)\n        content = get_content(_file)\n        if not download:\n            return render_template(\"/content.html\", act=\"code\", content=content)\n        else:\n            with open(_file) as f:\n                text = f.read()\n            return Response(\"\".join([s.replace(\"\\t\", \"\", 1) for s in text.split(\"```python\")[-1]\n                                    .splitlines(keepends=True)][1:-2]),\n                            mimetype=\"text/plain\",\n                            headers={\"Content-disposition\": f\"attachment; filename={filename.split('-')[0]}.py\"})\n    @APP.route(\"/mss/help\")\n    def help():\n        _file = os.path.join(DOCS_SERVER_PATH, 'static', 'docs', 'help.md')\n        content = get_content(_file)\n        return render_template(\"/content.html\", act=\"help\", content=content)\n    @APP.route(\"/mss/imprint\")\n    def imprint():\n        _file = os.path.join(DOCS_SERVER_PATH, 'static', 'docs', 'imprint.md')\n        content = get_content(_file)\n        return render_template(\"/content.html\", act=\"imprint\", content=content)\n    @APP.route('/mss/favicon.ico')\n    def favicons():\n        base_path = icons(\"16x16\", \"favicon.ico\")\n        return send_file(base_path)\n    @APP.route('/mss/logo.png')\n    def logo():\n        base_path = icons(\"64x64\", \"mss-logo.png\")\n        return send_file(base_path)\n    @APP.route('/mss/overview.png')\n    def overview():\n        base_path = os.path.join(DOCS_SERVER_PATH, 'static', 'img', 'wise12_overview.png')\n        return send_file(base_path)\n    return APP\ndef uploads(name=None, filename=None):\n    base_path = mscolab_settings.UPLOAD_FOLDER\n    if name is None:\n        abort(404)\n    if filename is None:\n        abort(404)\n    return send_from_directory(fs.path.join(base_path, name), filename)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-25123",
        "description": "[{'lang': 'en', 'value': 'MSS (Mission Support System) is an open source package designed for planning atmospheric research flights. In file: `index.py`, there is a method that is vulnerable to path manipulation attack. By modifying file paths, an attacker can acquire sensitive information from different resources. The `filename` variable is joined with other variables to form a file path in `_file`. However, `filename` is a route parameter that can capture path type values i.e. values including slashes (\\\\). So it is possible for an attacker to manipulate the file being read by assigning a value containing ../ to `filename` and so the attacker may be able to gain access to other files on the host filesystem. This issue has been addressed in MSS version 8.3.3. Users are advised to upgrade. There are no known workarounds for this vulnerability.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-38",
      "code": "def GetMaxPoolFwdTest(input_size, filter_size, strides, padding):\n  def Test(self):\n    if not test.is_gpu_available(cuda_only=True):\n      return\n    self._CompareMaxPoolingFwd(input_size, filter_size, strides, padding)\n  return Test\ndef GetMaxPoolGradTest(input_size, filter_size, output_size, strides, padding):\n  def Test(self):\n    if not test.is_gpu_available(cuda_only=True):\n      return\n    self._CompareMaxPoolingBk(input_size, output_size, filter_size, strides,\n                              padding)\n  return Test\ndef GetMaxPoolGradGradTest(input_size, filter_size, output_size, strides,\n                           padding):\n  def Test(self):\n    if not test.is_gpu_available(cuda_only=True):\n      return\n    self._CompareMaxPoolingGradBk(input_size, output_size, filter_size, strides,\n                                  padding)\n  return Test\nif __name__ == \"__main__\":\n  for (name_, input_size_, filter_size_, output_size_, stride_,\n       padding_) in GetShrunkInceptionMaxPoolShapes():\n    setattr(PoolingTest, \"testMaxPoolFwd_\" + name_,\n            GetMaxPoolFwdTest(input_size_, filter_size_, stride_, padding_))\n    setattr(PoolingTest, \"testMaxPoolGrad_\" + name_,\n            GetMaxPoolGradTest(input_size_, filter_size_, output_size_, stride_,\n                               padding_))\n    setattr(PoolingTest, \"testMaxPoolGradGrad_\" + name_,\n            GetMaxPoolGradGradTest(input_size_, filter_size_, output_size_,\n                                   stride_, padding_))\n  test.main()",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-41206",
        "description": "[{'lang': 'en', 'value': \"TensorFlow is an open source platform for machine learning. In affected versions several TensorFlow operations are missing validation for the shapes of the tensor arguments involved in the call. Depending on the API, this can result in undefined behavior and segfault or `CHECK`-fail related crashes but in some scenarios writes and reads from heap populated arrays are also possible. We have discovered these issues internally via tooling while working on improving/testing GPU op determinism. As such, we don't have reproducers and there will be multiple fixes for these issues. These fixes will be included in TensorFlow 2.7.0. We will also cherrypick these commits on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.\"}]",
        "cwe_number": 354
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-39",
      "code": "  def __init__(self,\n               python_function,\n               name,\n               input_signature=None,\n               attributes=None,\n               autograph=True,\n               autograph_options=None,\n               experimental_relax_shapes=False,\n               capture_by_value=None,\n               jit_compile=None,\n               experimental_follow_type_hints=False):\n    \"\"\"Initializes a `Function`.\n    Args:\n      python_function: the function to be wrapped.\n      name: the name given to it.\n      input_signature: a possibly nested sequence of `TensorSpec` objects\n        specifying the input signature of this function. If `None`, a separate\n        function is instantiated for each inferred input signature.\n      attributes: dict, extra keyword arguments that will be added as attribute\n        of the function.\n      autograph: whether to use autograph to compile\n        `python_function`. See https://www.tensorflow.org/guide/autograph for\n        more information.\n      autograph_options: Experimental knobs to control behavior\n        `when autograph=True`. See https://www.tensorflow.org/guide/autograph\n        for more information.\n      experimental_relax_shapes: When true, argument shapes may be relaxed to\n        avoid unnecessary retracing.\n      capture_by_value: Experimental. Whether to capture resource variables by\n        value or reference. If None, will inherit from a parent context or\n        default to False.\n      jit_compile: Force-compile the function with XLA, cf.\n        def_function.Function doc on jit_compile.\n      experimental_follow_type_hints: See the documentation for `tf.function`.\n    Raises:\n      ValueError: if `input_signature` is not None and the `python_function`'s\n        argspec has keyword arguments.\n    \"\"\"\n    self._python_function = python_function\n    pure_function = attributes and IMPLEMENTS_ATTRIBUTE_NAME in attributes\n    self._function_spec = FunctionSpec.from_function_and_signature(\n        python_function,\n        input_signature,\n        is_pure=pure_function,\n        experimental_follow_type_hints=experimental_follow_type_hints)\n    self._name = name\n    self._autograph = autograph\n    self._autograph_options = autograph_options\n    self._experimental_relax_shapes = experimental_relax_shapes\n    self._function_cache = FunctionCache()\n    self._function_attributes = attributes or {}\n    self._capture_by_value = capture_by_value\n    self.tracing_count = 0\n    if self.input_signature is not None:\n      self._hashable_input_signature = hash(self.flat_input_signature)\n    self._lock = threading.Lock()\n    self._descriptor_cache = weakref.WeakKeyDictionary()\n    self._jit_compile = jit_compile\n    self._experimental_follow_type_hints = experimental_follow_type_hints\n  def __init__(self,\n               python_function,\n               name,\n               input_signature=None,\n               autograph=True,\n               jit_compile=None,\n               experimental_implements=None,\n               experimental_autograph_options=None,\n               experimental_relax_shapes=False,\n               experimental_follow_type_hints=None):\n    \"\"\"Initializes a `Function`.\n    Args:\n      python_function: the function to be wrapped.\n      name: the name given to it.\n      input_signature: See the documentation for `tf.function`.\n      autograph: See the documentation for `tf.function`.\n      jit_compile: See the documentation for `tf.function`.\n      experimental_implements: See the documentation for `tf.function`.\n      experimental_autograph_options: See the documentation for `tf.function`.\n      experimental_relax_shapes: See the documentation for `tf.function`.\n      experimental_follow_type_hints: See the documentation for `tf.function`.\n    Raises:\n      ValueError: if `input_signature` is not None and the `python_function`'s\n        argspec has keyword arguments.\n    \"\"\"\n    self._lock = threading.Lock()\n    self._python_function = python_function\n    self._function_spec = function_lib.FunctionSpec.from_function_and_signature(\n        python_function,\n        input_signature,\n        jit_compile=jit_compile,\n        experimental_follow_type_hints=experimental_follow_type_hints,\n    )\n    self._implements = experimental_implements\n    self._shared_rendezvous = None\n    self._autograph = autograph\n    self._experimental_autograph_options = experimental_autograph_options\n    self._experimental_relax_shapes = experimental_relax_shapes\n    self._jit_compile = jit_compile\n    if experimental_follow_type_hints is None:\n      experimental_follow_type_hints = False\n    self._experimental_follow_type_hints = experimental_follow_type_hints\n    self._created_variables = None\n    self._stateful_fn = None\n    self._stateless_fn = None\n    self._descriptor_cache = weakref.WeakKeyDictionary()\n    self._name = name\n    self._input_signature = input_signature\n    self._key_for_call_stats = self._get_key_for_call_stats()\n    self._omit_frequent_tracing_warning = False\n    ops._tf_function_api_guage.get_cell().set(True)\n  def __setstate__(self, state):\n    \"\"\"Restore from pickled state.\"\"\"\n    self.__dict__ = state\n    self._lock = threading.Lock()\n    self._descriptor_cache = weakref.WeakKeyDictionary()\n    self._key_for_call_stats = self._get_key_for_call_stats()",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-41213",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. In affected versions the code behind `tf.function` API can be made to deadlock when two `tf.function` decorated Python functions are mutually recursive. This occurs due to using a non-reentrant `Lock` Python object. Loading any model which contains mutually recursive functions is vulnerable. An attacker can cause denial of service by causing users to load such models and calling a recursive `tf.function`, although this is not a frequent scenario. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.'}]",
        "cwe_number": 662
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-40",
      "code": "def spider_list(request, client_id, project_name):\n    \"\"\"\n    get spider list from one client\n    :param request: request Object\n    :param client_id: client id\n    :param project_name: project name\n    :return: json\n    \"\"\"\n    if request.method == 'GET':\n        client = Client.objects.get(id=client_id)\n        scrapyd = get_scrapyd(client)\n        spiders = scrapyd.list_spiders(project_name)\n        spiders = [{'name': spider, 'id': index + 1} for index, spider in enumerate(spiders)]\n        return JsonResponse(spiders)\ndef spider_start(request, client_id, project_name, spider_name):\n    \"\"\"\n    start a spider\n    :param request: request object\n    :param client_id: client id\n    :param project_name: project name\n    :param spider_name: spider name\n    :return: json\n    \"\"\"\n    if request.method == 'GET':\n        client = Client.objects.get(id=client_id)\n        scrapyd = get_scrapyd(client)\n        job = scrapyd.schedule(project_name, spider_name)\n        return JsonResponse({'job': job})\ndef project_configure(request, project_name):\n    \"\"\"\n    get configuration\n    :param request: request object\n    :param project_name: project name\n    :return: json\n    \"\"\"\n    if request.method == 'GET':\n        project = Project.objects.get(name=project_name)\n        project = model_to_dict(project)\n        project['configuration'] = json.loads(project['configuration']) if project['configuration'] else None\n        return JsonResponse(project)\n    elif request.method == 'POST':\n        project = Project.objects.filter(name=project_name)\n        data = json.loads(request.body)\n        configuration = json.dumps(data.get('configuration'), ensure_ascii=False)\n        project.update(**{'configuration': configuration})\n        project_name = re.sub('[\\!\\@\\\n        cmd = ' '.join(['gerapy', 'generate', project_name])\n        p = Popen(cmd, shell=True, stdin=PIPE, stdout=PIPE, stderr=PIPE)\n        stdout, stderr = bytes2str(p.stdout.read()), bytes2str(p.stderr.read())\n        if not stderr:\n            return JsonResponse({'status': '1'})\n        else:\n            return JsonResponse({'status': '0', 'message': stderr})\ndef project_tree(request, project_name):\n    \"\"\"\n    get file tree of project\n    :param request: request object\n    :param project_name: project name\n    :return: json of tree\n    \"\"\"\n    if request.method == 'GET':\n        path = os.path.abspath(join(os.getcwd(), PROJECTS_FOLDER))\n        tree = get_tree(join(path, project_name))\n        return JsonResponse(tree)\ndef project_create(request):\n    \"\"\"\n    create a configurable project\n    :param request: request object\n    :return: json\n    \"\"\"\n    if request.method == 'POST':\n        data = json.loads(request.body)\n        data['configurable'] = 1\n        project, result = Project.objects.update_or_create(**data)\n        path = join(os.path.abspath(join(os.getcwd(), PROJECTS_FOLDER)), data['name'])\n        os.mkdir(path)\n        return JsonResponse(model_to_dict(project))\ndef project_upload(request):\n    \"\"\"\n    upload project\n    :param request: request object\n    :return: json\n    \"\"\"\n    if request.method == 'POST':\n        file = request.FILES['file']\n        file_name = file.name\n        fs = FileSystemStorage(PROJECTS_FOLDER)\n        zip_file_name = fs.save(file_name, file)\n        logger.debug('zip file name %s', zip_file_name)\n        with zipfile.ZipFile(join(PROJECTS_FOLDER, zip_file_name), 'r') as zip_ref:\n            zip_ref.extractall(PROJECTS_FOLDER)\n        logger.debug('extracted files to %s', PROJECTS_FOLDER)\n        return JsonResponse({'status': True})\ndef project_clone(request):\n    \"\"\"\n    clone project from github\n    :param request: request object\n    :return: json\n    \"\"\"\n    if request.method == 'POST':\n        data = json.loads(request.body)\n        address = data.get('address')\n        if not address.startswith('http'):\n            return JsonResponse({'status': False})\n        address = address + '.git' if not address.endswith('.git') else address\n        cmd = 'git clone {address} {target}'.format(address=address, target=join(PROJECTS_FOLDER, Path(address).stem))\n        logger.debug('clone cmd %s', cmd)\n        p = Popen(cmd, shell=True, stdin=PIPE, stdout=PIPE, stderr=PIPE)\n        stdout, stderr = bytes2str(p.stdout.read()), bytes2str(p.stderr.read())\n        logger.debug('clone run result %s', stdout)\n        if stderr: logger.error(stderr)\n        return JsonResponse({'status': True}) if not stderr else JsonResponse({'status': False})\ndef project_remove(request, project_name):\n    \"\"\"\n    remove project from disk and db\n    :param request: request object\n    :param project_name: project name\n    :return: result of remove\n    \"\"\"\n    if request.method == 'POST':\n        project = Project.objects.get(name=project_name)\n        Deploy.objects.filter(project=project).delete()\n        result = Project.objects.filter(name=project_name).delete()\n        path = join(os.path.abspath(os.getcwd()), PROJECTS_FOLDER)\n        project_path = join(path, project_name)\n        if exists(project_path):\n            rmtree(project_path)\n        return JsonResponse({'result': result})\ndef project_version(request, client_id, project_name):\n    \"\"\"\n    get project deploy version\n    :param request: request object\n    :param client_id: client id\n    :param project_name: project name\n    :return: deploy version of project\n    \"\"\"\n    if request.method == 'GET':\n        client = Client.objects.get(id=client_id)\n        project = Project.objects.get(name=project_name)\n        scrapyd = get_scrapyd(client)\n        if Deploy.objects.filter(client=client, project=project):\n            deploy = Deploy.objects.get(client=client, project=project)\n        else:\n            try:\n                versions = scrapyd.list_versions(project_name)\n            except ConnectionError:\n                return JsonResponse({'message': 'Connect Error'}, status=500)\n            if len(versions) > 0:\n                version = versions[-1]\n                deployed_at = timezone.datetime.fromtimestamp(int(version), tz=pytz.timezone(TIME_ZONE))\n            else:\n                deployed_at = None\n            deploy, result = Deploy.objects.update_or_create(client=client, project=project, deployed_at=deployed_at)\n        return JsonResponse(model_to_dict(deploy))\ndef project_deploy(request, client_id, project_name):\n    \"\"\"\n    deploy project operation\n    :param request: request object\n    :param client_id: client id\n    :param project_name: project name\n    :return: json of deploy result\n    \"\"\"\n    if request.method == 'POST':\n        path = os.path.abspath(join(os.getcwd(), PROJECTS_FOLDER))\n        project_path = join(path, project_name)\n        egg = find_egg(project_path)\n        if not egg:\n            return JsonResponse({'message': 'egg not found'}, status=500)\n        egg_file = open(join(project_path, egg), 'rb')\n        client = Client.objects.get(id=client_id)\n        project = Project.objects.get(name=project_name)\n        scrapyd = get_scrapyd(client)\n        scrapyd.add_version(project_name, int(time.time()), egg_file.read())\n        deployed_at = timezone.now()\n        Deploy.objects.filter(client=client, project=project).delete()\n        deploy, result = Deploy.objects.update_or_create(client=client, project=project, deployed_at=deployed_at,\n                                                         description=project.description)\n        return JsonResponse(model_to_dict(deploy))\ndef project_build(request, project_name):\n    \"\"\"\n    get build info or execute build operation\n    :param request: request object\n    :param project_name: project name\n    :return: json\n    \"\"\"\n    path = os.path.abspath(join(os.getcwd(), PROJECTS_FOLDER))\n    project_path = join(path, project_name)\n    if request.method == 'GET':\n        egg = find_egg(project_path)\n        if egg:\n            built_at = timezone.datetime.fromtimestamp(os.path.getmtime(join(project_path, egg)),\n                                                       tz=pytz.timezone(TIME_ZONE))\n            if not Project.objects.filter(name=project_name):\n                Project(name=project_name, built_at=built_at, egg=egg).save()\n                model = Project.objects.get(name=project_name)\n            else:\n                model = Project.objects.get(name=project_name)\n                model.built_at = built_at\n                model.egg = egg\n                model.save()\n        else:\n            if not Project.objects.filter(name=project_name):\n                Project(name=project_name).save()\n            model = Project.objects.get(name=project_name)\n        data = model_to_dict(model)\n        return JsonResponse(data)\n    elif request.method == 'POST':\n        data = json.loads(request.body)\n        description = data['description']\n        build_project(project_name)\n        egg = find_egg(project_path)\n        if not egg:\n            return JsonResponse({'message': 'egg not found'}, status=500)\n        built_at = timezone.now()\n        if not Project.objects.filter(name=project_name):\n            Project(name=project_name, description=description, built_at=built_at, egg=egg).save()\n            model = Project.objects.get(name=project_name)\n        else:\n            model = Project.objects.get(name=project_name)\n            model.built_at = built_at\n            model.egg = egg\n            model.description = description\n            model.save()\n        data = model_to_dict(model)\n        return JsonResponse(data)\ndef project_parse(request, project_name):\n    \"\"\"\n    parse project\n    :param request: request object\n    :param project_name: project name\n    :return: requests, items, response\n    \"\"\"\n    if request.method == 'POST':\n        project_path = join(PROJECTS_FOLDER, project_name)\n        data = json.loads(request.body)\n        logger.debug('post data %s', data)\n        spider_name = data.get('spider')\n        args = {\n            'start': data.get('start', False),\n            'method': data.get('method', 'GET'),\n            'url': data.get('url'),\n            'callback': data.get('callback'),\n            'cookies': \"'\" + json.dumps(data.get('cookies', {}), ensure_ascii=False) + \"'\",\n            'headers': \"'\" + json.dumps(data.get('headers', {}), ensure_ascii=False) + \"'\",\n            'meta': \"'\" + json.dumps(data.get('meta', {}), ensure_ascii=False) + \"'\",\n            'dont_filter': data.get('dont_filter', False),\n            'priority': data.get('priority', 0),\n        }\n        body = data.get('body', '')\n        if args.get('method').lower() != 'get':\n            args['body'] = \"'\" + json.dumps(body, ensure_ascii=False) + \"'\"\n        args_cmd = ' '.join(\n            ['--{arg} {value}'.format(arg=arg, value=value) for arg, value in args.items()])\n        logger.debug('args cmd %s', args_cmd)\n        cmd = 'gerapy parse {args_cmd} {project_path} {spider_name}'.format(\n            args_cmd=args_cmd,\n            project_path=project_path,\n            spider_name=spider_name\n        )\n        logger.debug('parse cmd %s', cmd)\n        p = Popen(cmd, shell=True, stdin=PIPE, stdout=PIPE, stderr=PIPE, close_fds=True)\n        stdout, stderr = bytes2str(p.stdout.read()), bytes2str(p.stderr.read())\n        logger.debug('stdout %s, stderr %s', stdout, stderr)\n        if not stderr:\n            return JsonResponse({'status': True, 'result': json.loads(stdout)})\n        else:\n            return JsonResponse({'status': False, 'message': stderr})\ndef project_file_read(request):\n    \"\"\"\n    get content of project file\n    :param request: request object\n    :return: file content\n    \"\"\"\n    if request.method == 'POST':\n        data = json.loads(request.body)\n        path = join(data['path'], data['label'])\n        with open(path, 'rb') as f:\n            return HttpResponse(f.read().decode('utf-8'))\ndef job_log(request, client_id, project_name, spider_name, job_id):\n    \"\"\"\n    get log of jog\n    :param request: request object\n    :param client_id: client id\n    :param project_name: project name\n    :param spider_name: spider name\n    :param job_id: job id\n    :return: log of job\n    \"\"\"\n    if request.method == 'GET':\n        client = Client.objects.get(id=client_id)\n        url = log_url(client.ip, client.port, project_name, spider_name, job_id)\n        response = requests.get(url, timeout=5, headers={\n            'Range': 'bytes=-1000'\n        }, auth=(client.username, client.password) if client.auth else None)\n        encoding = response.apparent_encoding\n        if response.status_code == 404:\n            return JsonResponse({'message': 'Log Not Found'}, status=404)\n        text = response.content.decode(encoding, errors='replace')\n        return HttpResponse(text)\ndef job_cancel(request, client_id, project_name, job_id):\n    \"\"\"\n    cancel a job\n    :param request: request object\n    :param client_id: client id\n    :param project_name: project name\n    :param job_id: job id\n    :return: json of cancel\n    \"\"\"\n    if request.method == 'GET':\n        client = Client.objects.get(id=client_id)\n        scrapyd = get_scrapyd(client)\n        result = scrapyd.cancel(project_name, job_id)\n        return JsonResponse(result)\ndef task_create(request):\n    \"\"\"\n    add task\n    :param request: request object\n    :return: Bool\n    \"\"\"\n    if request.method == 'POST':\n        data = json.loads(request.body)\n        task = Task.objects.create(clients=json.dumps(data.get('clients'), ensure_ascii=False),\n                                   project=data.get('project'),\n                                   name=data.get('name'),\n                                   spider=data.get('spider'),\n                                   trigger=data.get('trigger'),\n                                   configuration=json.dumps(data.get('configuration'), ensure_ascii=False),\n                                   modified=1)\n        return JsonResponse({'result': '1', 'data': model_to_dict(task)})\ndef task_update(request, task_id):\n    \"\"\"\n    update task info\n    :param request: request object\n    :param task_id: task id\n    :return: json\n    \"\"\"\n    if request.method == 'POST':\n        task = Task.objects.filter(id=task_id)\n        data = json.loads(request.body)\n        data['clients'] = json.dumps(data.get('clients'), ensure_ascii=False)\n        data['configuration'] = json.dumps(data.get('configuration'), ensure_ascii=False)\n        data['modified'] = 1\n        task.update(**data)\n        return JsonResponse(model_to_dict(Task.objects.get(id=task_id)))\ndef task_remove(request, task_id):\n    \"\"\"\n    remove task by task_id\n    :param request:\n    :return:\n    \"\"\"\n    if request.method == 'POST':\n        task = Task.objects.get(id=task_id)\n        clients = clients_of_task(task)\n        for client in clients:\n            job_id = get_job_id(client, task)\n            DjangoJob.objects.filter(name=job_id).delete()\n        Task.objects.filter(id=task_id).delete()\n        return JsonResponse({'result': '1'})\ndef task_info(request, task_id):\n    \"\"\"\n    get task info\n    :param request: request object\n    :param task_id: task id\n    :return: json\n    \"\"\"\n    if request.method == 'GET':\n        task = Task.objects.get(id=task_id)\n        data = model_to_dict(task)\n        data['clients'] = json.loads(data.get('clients'))\n        data['configuration'] = json.loads(data.get('configuration'))\n        return JsonResponse({'data': data})\ndef task_status(request, task_id):\n    \"\"\"\n    get task status info\n    :param request: request object\n    :param task_id: task id\n    :return:\n    \"\"\"\n    if request.method == 'GET':\n        result = []\n        task = Task.objects.get(id=task_id)\n        clients = clients_of_task(task)\n        for client in clients:\n            job_id = get_job_id(client, task)\n            jobs = DjangoJob.objects.filter(name=job_id)\n            logger.debug('jobs from djangojob %s', jobs)\n            if not jobs: continue\n            job = DjangoJob.objects.get(name=job_id)\n            executions = serialize('json', DjangoJobExecution.objects.filter(job=job))\n            result.append({\n                'client': model_to_dict(client),\n                'next': job.next_run_time,\n                'executions': json.loads(executions)\n            })\n        return JsonResponse({'data': result})\ndef render_html(request):\n    \"\"\"\n    render html with url\n    :param request:\n    :return:\n    \"\"\"\n    if request.method == 'GET':\n        url = request.GET.get('url')\n        url = unquote(base64.b64decode(url).decode('utf-8'))\n        js = request.GET.get('js', 0)\n        script = request.GET.get('script')\n        response = requests.get(url, timeout=5)\n        response.encoding = response.apparent_encoding\n        html = process_html(response.text)\n        return HttpResponse(html)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-43857",
        "description": "[{'lang': 'en', 'value': 'Gerapy is a distributed crawler management framework. Gerapy prior to version 0.9.8 is vulnerable to remote code execution, and this issue is patched in version 0.9.8.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-41",
      "code": "    def secret_set(\n        self,\n        id: str,\n        *,\n        content: Optional[Dict[str, str]] = None,\n        label: Optional[str] = None,\n        description: Optional[str] = None,\n        expire: Optional[datetime.datetime] = None,\n        rotate: Optional[SecretRotate] = None,\n    ):\n        args = [id]\n        if label is not None:\n            args.extend(['--label', label])\n        if description is not None:\n            args.extend(['--description', description])\n        if expire is not None:\n            args.extend(['--expire', expire.isoformat()])\n        if rotate is not None:\n            args += ['--rotate', rotate.value]\n        if content is not None:\n            for k, v in content.items():\n                args.append(f'{k}={v}')\n        self._run_for_secret('secret-set', *args)\n    def secret_add(\n        self,\n        content: Dict[str, str],\n        *,\n        label: Optional[str] = None,\n        description: Optional[str] = None,\n        expire: Optional[datetime.datetime] = None,\n        rotate: Optional[SecretRotate] = None,\n        owner: Optional[str] = None,\n    ) -> str:\n        args: List[str] = []\n        if label is not None:\n            args.extend(['--label', label])\n        if description is not None:\n            args.extend(['--description', description])\n        if expire is not None:\n            args.extend(['--expire', expire.isoformat()])\n        if rotate is not None:\n            args += ['--rotate', rotate.value]\n        if owner is not None:\n            args += ['--owner', owner]\n        for k, v in content.items():\n            args.append(f'{k}={v}')\n        result = self._run('secret-add', *args, return_output=True)\n        secret_id = typing.cast(str, result)\n        return secret_id.strip()\n    def secret_grant(self, id: str, relation_id: int, *, unit: Optional[str] = None):\n        args = [id, '--relation', str(relation_id)]\n        if unit is not None:\n            args += ['--unit', str(unit)]\n        self._run_for_secret('secret-grant', *args)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-41129",
        "description": "[{'lang': 'en', 'value': 'The ops library is a Python framework for developing and testing Kubernetes and machine charms. The issue here is that ops passes the secret content as one of the args via CLI. This issue may affect any of the charms that are using: Juju (>=3.0), Juju secrets and not correctly capturing and processing `subprocess.CalledProcessError`. This vulnerability is fixed in 2.15.0.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-42",
      "code": "def login():\n    form = forms.UserForm()\n    if form.validate_on_submit():\n        db = get_db()\n        user = db.search(\n            (Query().username == form.username.data) & (Query().type == \"user\")\n        )\n        if user and check_password_hash(user[0][\"hashed_password\"], form.password.data):\n            user = User.from_db(user[0])\n            login_user(user, remember=True)\n            flash(\"Login successful!\", \"success\")\n            next_url = request.args.get(\"next\")\n            return redirect(next_url or \"/\")\n        flash(\"Invalid credentials\", \"error\")\n        return redirect(\"/login\")\n    return render_template(\"users/login.html\", form=form, title=\"Login\")",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-0697",
        "description": "[{'lang': 'en', 'value': 'Open Redirect in GitHub repository archivy/archivy prior to 1.7.0.'}]",
        "cwe_number": 601
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-43",
      "code": "def whitelist(allow_guest=False, xss_safe=False, methods=None):\n\t\"\"\"\n\tDecorator for whitelisting a function and making it accessible via HTTP.\n\tStandard request will be `/api/method/[path.to.method]`\n\t:param allow_guest: Allow non logged-in user to access this method.\n\t:param methods: Allowed http method to access the method.\n\tUse as:\n\t\t@frappe.whitelist()\n\t\tdef myfunc(param1, param2):\n\t\t\tpass\n\t\"\"\"\n\tif not methods:\n\t\tmethods = ['GET', 'POST', 'PUT', 'DELETE']\n\tdef innerfn(fn):\n\t\tglobal whitelisted, guest_methods, xss_safe_methods, allowed_http_methods_for_whitelisted_func\n\t\twhitelisted.append(fn)\n\t\tallowed_http_methods_for_whitelisted_func[fn] = methods\n\t\tif allow_guest:\n\t\t\tguest_methods.append(fn)\n\t\t\tif xss_safe:\n\t\t\t\txss_safe_methods.append(fn)\n\t\treturn fn\n\treturn innerfn\ndef execute_cmd(cmd, from_async=False):\n\t\"\"\"execute a request as python module\"\"\"\n\tfor hook in frappe.get_hooks(\"override_whitelisted_methods\", {}).get(cmd, []):\n\t\tcmd = hook\n\t\tbreak\n\tif run_server_script_api(cmd):\n\t\treturn None\n\ttry:\n\t\tmethod = get_attr(cmd)\n\texcept Exception as e:\n\t\tif frappe.local.conf.developer_mode:\n\t\t\traise e\n\t\telse:\n\t\t\tfrappe.respond_as_web_page(title='Invalid Method', html='Method not found',\n\t\t\tindicator_color='red', http_status_code=404)\n\t\treturn\n\tif from_async:\n\t\tmethod = method.queue\n\tis_whitelisted(method)\n\tis_valid_http_method(method)\n\treturn frappe.call(method, **frappe.form_dict)\ndef is_whitelisted(method):\n\tif frappe.session['user'] == 'Guest':\n\t\tif (method not in frappe.guest_methods):\n\t\t\tfrappe.throw(_(\"Not permitted\"), frappe.PermissionError)\n\t\tif method not in frappe.xss_safe_methods:\n\t\t\tfor key, value in frappe.form_dict.items():\n\t\t\t\tif isinstance(value, string_types):\n\t\t\t\t\tfrappe.form_dict[key] = frappe.utils.sanitize_html(value)\n\telse:\n\t\tif not method in frappe.whitelisted:\n\t\t\tfrappe.throw(_(\"Not permitted\"), frappe.PermissionError)\ndef runserverobj(method, docs=None, dt=None, dn=None, arg=None, args=None):\n\tfrappe.desk.form.run_method.runserverobj(method, docs=docs, dt=dt, dn=dn, arg=arg, args=args)\ndef run_custom_method(doctype, name, custom_method):\n\t\"\"\"cmd=run_custom_method&doctype={doctype}&name={name}&custom_method={custom_method}\"\"\"\n\tdoc = frappe.get_doc(doctype, name)\n\tif getattr(doc, custom_method, frappe._dict()).is_whitelisted:\n\t\tfrappe.call(getattr(doc, custom_method), **frappe.local.form_dict)\n\telse:\n\t\tfrappe.throw(_(\"Not permitted\"), frappe.PermissionError)\ndef validate_and_sanitize_search_inputs(fn, instance, args, kwargs):\n\tkwargs.update(dict(zip(fn.__code__.co_varnames, args)))\n\tsanitize_searchfield(kwargs['searchfield'])\n\tkwargs['start'] = cint(kwargs['start'])\n\tkwargs['page_len'] = cint(kwargs['page_len'])\n\tif kwargs['doctype'] and not frappe.db.exists('DocType', kwargs['doctype']):\n\t\treturn []\n\treturn fn(**kwargs)\n\tdef whitelist(f):\n\t\t\"\"\"Decorator: Whitelist method to be called remotely via REST API.\"\"\"\n\t\tf.whitelisted = True\n\t\treturn f\n\tdef is_whitelisted(self, method):\n\t\tfn = getattr(self, method, None)\n\t\tif not fn:\n\t\t\traise NotFound(\"Method {0} not found\".format(method))\n\t\telif not getattr(fn, \"whitelisted\", False):\n\t\t\traise Forbidden(\"Method {0} not whitelisted\".format(method))",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-23058",
        "description": "[{'lang': 'en', 'value': 'ERPNext in versions v12.0.9-v13.0.3 are affected by a stored XSS vulnerability that allows low privileged users to store malicious scripts in the \u2018username\u2019 field in \u2018my settings\u2019 which can lead to full account takeover.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-44",
      "code": "def show_roxy_log(\n\t\tserv, rows='10', waf='0', grep=None, hour='00',\n\t\tminut='00', hour1='24', minut1='00', service='haproxy', **kwargs\n) -> str:\n\texgrep = form.getvalue('exgrep')\n\tlog_file = form.getvalue('file')\n\tdate = checkAjaxInput(hour) + ':' + checkAjaxInput(minut)\n\tdate1 = checkAjaxInput(hour1) + ':' + checkAjaxInput(minut1)\n\trows = checkAjaxInput(rows)\n\twaf = checkAjaxInput(waf)\n\tcmd = ''\n\tawk_column = 3\n\tif grep is not None:\n\t\tgrep_act = '|egrep \"%s\"' % checkAjaxInput(grep)\n\telse:\n\t\tgrep_act = ''\n\tif exgrep is not None:\n\t\texgrep_act = '|egrep -v \"%s\"' % checkAjaxInput(exgrep)\n\telse:\n\t\texgrep_act = ''\n\tlog_file = checkAjaxInput(log_file) if log_file is not None else log_file\n\tif service in ('nginx', 'haproxy', 'apache', 'keepalived'):\n\t\tsyslog_server_enable = sql.get_setting('syslog_server_enable')\n\t\tif syslog_server_enable is None or syslog_server_enable == 0:\n\t\t\tif service == 'nginx':\n\t\t\t\tlocal_path_logs = sql.get_setting('nginx_path_logs')\n\t\t\t\tcommands = [\"sudo cat %s/%s |tail -%s %s %s\" % (local_path_logs, log_file, rows, grep_act, exgrep_act)]\n\t\t\telif service == 'apache':\n\t\t\t\tlocal_path_logs = sql.get_setting('apache_path_logs')\n\t\t\t\tcommands = [\n\t\t\t\t\t\"sudo cat %s/%s| awk -F\\\"/|:\\\" '$3>\\\"%s:00\\\" && $3<\\\"%s:00\\\"' |tail -%s %s %s\" % (local_path_logs, log_file, date, date1, rows, grep_act, exgrep_act)\n\t\t\t\t]\n\t\t\telif service == 'keepalived':\n\t\t\t\tlocal_path_logs = sql.get_setting('keepalived_path_logs')\n\t\t\t\tcommands = [\n\t\t\t\t\t\"sudo cat %s/%s| awk '$3>\\\"%s:00\\\" && $3<\\\"%s:00\\\"' |tail -%s %s %s\" % (\n\t\t\t\t\t\tlocal_path_logs, log_file, date, date1, rows, grep_act, exgrep_act)\n\t\t\t\t]\n\t\t\telse:\n\t\t\t\tlocal_path_logs = sql.get_setting('haproxy_path_logs')\n\t\t\t\tcommands = [\"sudo cat %s/%s| awk '$3>\\\"%s:00\\\" && $3<\\\"%s:00\\\"' |tail -%s %s %s\" % (local_path_logs, log_file, date, date1, rows, grep_act, exgrep_act)]\n\t\t\tsyslog_server = serv\n\t\telse:\n\t\t\tcommands = [\"sudo cat /var/log/%s/syslog.log | sed '/ %s:00/,/ %s:00/! d' |tail -%s %s %s %s\" % (serv, date, date1, rows, grep_act, grep, exgrep_act)]\n\t\t\tsyslog_server = sql.get_setting('syslog_server')\n\t\tif waf == \"1\":\n\t\t\tlocal_path_logs = '/var/log/waf.log'\n\t\t\tcommands = [\"sudo cat %s |tail -%s %s %s\" % (local_path_logs, rows, grep_act, exgrep_act)]\n\t\tif kwargs.get('html') == 0:\n\t\t\ta = server_mod.ssh_command(syslog_server, commands)\n\t\t\treturn show_log(a, html=0, grep=grep)\n\t\telse:\n\t\t\treturn server_mod.ssh_command(syslog_server, commands, show_log='1', grep=grep, timeout=10)\n\telif service == 'apache_internal':\n\t\tapache_log_path = sql.get_setting('apache_log_path')\n\t\tif serv == 'roxy-wi.access.log':\n\t\t\tcmd = 'sudo cat {}| awk -F\"/|:\" \\'$3>\"{}:00\" && $3<\"{}:00\"\\' |tail -{} {} {}'.format(apache_log_path + \"/\" + serv, date, date1, rows, grep_act, exgrep_act)\n\t\telif serv == 'roxy-wi.error.log':\n\t\t\tcmd = \"sudo cat {}| awk '$4>\\\"{}:00\\\" && $4<\\\"{}:00\\\"' |tail -{} {} {}\".format(apache_log_path + \"/\" + serv, date, date1, rows, grep_act, exgrep_act)\n\t\telif serv == 'fail2ban.log':\n\t\t\tcmd = 'sudo cat {}| awk -F\"/|:\" \\'$3>\"{}:00\" && $3<\"{}:00\\' |tail -{} {} {}'.format(\"/var/log/\" + serv, date, date1, rows, grep_act, exgrep_act)\n\t\toutput, stderr = server_mod.subprocess_execute(cmd)\n\t\treturn show_log(output, grep=grep)\n\telif service == 'internal':\n\t\tlog_path = get_config_var.get_config_var('main', 'log_path')\n\t\tlogs_files = roxywi_common.get_files(log_path, \"log\")\n\t\tuser_group = roxywi_common.get_user_group()\n\t\tuser_grep = ''\n\t\tif user_group != '' and user_group != 'Default':\n\t\t\tuser_grep = f\"|grep 'group: {user_group}'\"\n\t\tfor key, value in logs_files:\n\t\t\tif int(serv) == key:\n\t\t\t\tserv = value\n\t\t\t\tbreak\n\t\telse:\n\t\t\treturn 'Haha'\n\t\tif serv == 'backup.log':\n\t\t\tawk_column = 2\n\t\tcmd = f\"cat {log_path}/{serv}| awk '${awk_column}>\\\"{date}:00\\\" && ${awk_column}<\\\"{date1}:00\\\"' {user_grep} {grep_act} {exgrep_act} |tail -{rows}\"\n\t\toutput, stderr = server_mod.subprocess_execute(cmd)\n\t\treturn show_log(output, grep=grep)\ndef create_ssh_cred() -> None:\n\tfrom jinja2 import Environment, FileSystemLoader\n\tuser_group = roxywi_common.get_user_group()\n\tname = common.checkAjaxInput(form.getvalue('new_ssh'))\n\tname = f'{name}_{user_group}'\n\tenable = common.checkAjaxInput(form.getvalue('ssh_enable'))\n\tgroup = common.checkAjaxInput(form.getvalue('new_group'))\n\tusername = common.checkAjaxInput(form.getvalue('ssh_user'))\n\tpassword = common.checkAjaxInput(form.getvalue('ssh_pass'))\n\tpage = common.checkAjaxInput(form.getvalue('page'))\n\tpage = page.split(\"\n\tlang = roxywi_common.get_user_lang()\n\tif username is None or name is None:\n\t\tprint(error_mess)\n\telse:\n\t\tif sql.insert_new_ssh(name, enable, group, username, password):\n\t\t\tenv = Environment(loader=FileSystemLoader('templates/'), autoescape=True)\n\t\t\ttemplate = env.get_template('ajax/new_ssh.html')\n\t\t\toutput_from_parsed_template = template.render(groups=sql.select_groups(), sshs=sql.select_ssh(name=name), page=page, lang=lang)\n\t\t\tprint(output_from_parsed_template)\n\t\t\troxywi_common.logging('Roxy-WI server', f'New SSH credentials {name} has been created', roxywi=1, login=1)\ndef upload_ssh_key(name: str, user_group: str, key: str) -> bool:\n\ttry:\n\t\tkey = paramiko.pkey.load_private_key(key)\n\texcept Exception as e:\n\t\tprint(f'error: Cannot save SSH key file: {e}')\n\t\treturn False\n\tlib_path = get_config.get_config_var('main', 'lib_path')\n\tfull_dir = f'{lib_path}/keys/'\n\tssh_keys = f'{name}.pem'\n\ttry:\n\t\t_check_split = name.split('_')[1]\n\t\tsplit_name = True\n\texcept Exception:\n\t\tsplit_name = False\n\tif not os.path.isfile(ssh_keys) and not split_name:\n\t\tname = f'{name}_{user_group}'\n\tif not os.path.exists(full_dir):\n\t\tos.makedirs(full_dir)\n\tssh_keys = f'{full_dir}{name}.pem'\n\ttry:\n\t\tkey.write_private_key_file(ssh_keys)\n\texcept Exception as e:\n\t\tprint(f'error: Cannot save SSH key file: {e}')\n\t\treturn False\n\telse:\n\t\tprint(f'success: SSH key has been saved into: {ssh_keys}')\n\ttry:\n\t\tos.chmod(ssh_keys, 0o600)\n\texcept IOError as e:\n\t\troxywi_common.logging('Roxy-WI server', e.args[0], roxywi=1)\n\t\treturn False\n\troxywi_common.logging(\"Roxy-WI server\", f\"A new SSH cert has been uploaded {ssh_keys}\", roxywi=1, login=1)\n\treturn True\ndef update_ssh_key() -> None:\n\tssh_id = common.checkAjaxInput(form.getvalue('id'))\n\tname = common.checkAjaxInput(form.getvalue('name'))\n\tenable = common.checkAjaxInput(form.getvalue('ssh_enable'))\n\tgroup = common.checkAjaxInput(form.getvalue('group'))\n\tusername = common.checkAjaxInput(form.getvalue('ssh_user'))\n\tpassword = common.checkAjaxInput(form.getvalue('ssh_pass'))\n\tnew_ssh_key_name = ''\n\tif username is None:\n\t\tprint(error_mess)\n\telse:\n\t\tlib_path = get_config.get_config_var('main', 'lib_path')\n\t\tfor sshs in sql.select_ssh(id=ssh_id):\n\t\t\tssh_enable = sshs.enable\n\t\t\tssh_key_name = f'{lib_path}/keys/{sshs.name}.pem'\n\t\t\tnew_ssh_key_name = f'{lib_path}/keys/{name}.pem'\n\t\tif ssh_enable == 1:\n\t\t\tos.rename(ssh_key_name, new_ssh_key_name)\n\t\t\tos.chmod(new_ssh_key_name, 0o600)\n\t\tsql.update_ssh(ssh_id, name, enable, group, username, password)\n\t\troxywi_common.logging('Roxy-WI server', f'The SSH credentials {name} has been updated ', roxywi=1, login=1)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-25802",
        "description": "[{'lang': 'en', 'value': \"Roxy-WI is a Web interface for managing Haproxy, Nginx, Apache, and Keepalived servers. Versions prior to 6.3.6.0 don't correctly neutralize `dir/../filename` sequences, such as `/etc/nginx/../passwd`, allowing an actor to gain information about a server. Version 6.3.6.0 has a patch for this issue.\"}]",
        "cwe_number": 668
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-45",
      "code": "def _export_annotations(\n    db_instance: models.Project | models.Task | models.Job,\n    rq_id: str,\n    request: HttpRequest,\n    format_name: str,\n    action: str,\n    callback: Callable[[int, Optional[str], Optional[str]], str],\n    filename: Optional[str],\n    location_conf: Dict[str, Any]\n):\n    if action not in {\"\", \"download\"}:\n        raise serializers.ValidationError(\n            \"Unexpected action specified for the request\")\n    format_desc = {f.DISPLAY_NAME: f\n        for f in dm.views.get_export_formats()}.get(format_name)\n    if format_desc is None:\n        raise serializers.ValidationError(\n            \"Unknown format specified for the request\")\n    elif not format_desc.ENABLED:\n        return Response(status=status.HTTP_405_METHOD_NOT_ALLOWED)\n    queue = django_rq.get_queue(settings.CVAT_QUEUES.EXPORT_DATA.value)\n    rq_job = queue.fetch_job(rq_id)\n    location = location_conf.get('location')\n    if location not in Location.list():\n        raise serializers.ValidationError(\n            f\"Unexpected location {location} specified for the request\"\n        )\n    cache_ttl = dm.views.get_export_cache_ttl(db_instance)\n    instance_update_time = timezone.localtime(db_instance.updated_date)\n    if isinstance(db_instance, Project):\n        tasks_update = list(map(lambda db_task: timezone.localtime(db_task.updated_date), db_instance.tasks.all()))\n        instance_update_time = max(tasks_update + [instance_update_time])\n    instance_timestamp = datetime.strftime(instance_update_time, \"%Y_%m_%d_%H_%M_%S\")\n    is_annotation_file = rq_id.startswith('export:annotations')\n    if rq_job:\n        rq_request = rq_job.meta.get('request', None)\n        request_time = rq_request.get('timestamp', None) if rq_request else None\n        if request_time is None or request_time < instance_update_time:\n            rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)\n            rq_job.delete()\n        else:\n            if rq_job.is_finished:\n                if location == Location.CLOUD_STORAGE:\n                    rq_job.delete()\n                    return Response(status=status.HTTP_200_OK)\n                elif location == Location.LOCAL:\n                    file_path = rq_job.return_value()\n                    if not file_path:\n                        return Response(\n                            'A result for exporting job was not found for finished RQ job',\n                            status=status.HTTP_500_INTERNAL_SERVER_ERROR\n                        )\n                    with dm.util.get_export_cache_lock(\n                        file_path, ttl=60,\n                    ):\n                        if action == \"download\":\n                            if not osp.exists(file_path):\n                                return Response(\n                                    \"The exported file has expired, please retry exporting\",\n                                    status=status.HTTP_404_NOT_FOUND\n                                )\n                            filename = filename or \\\n                                build_annotations_file_name(\n                                    class_name=db_instance.__class__.__name__,\n                                    identifier=db_instance.name if isinstance(db_instance, (Task, Project)) else db_instance.id,\n                                    timestamp=instance_timestamp,\n                                    format_name=format_name,\n                                    is_annotation_file=is_annotation_file,\n                                    extension=osp.splitext(file_path)[1]\n                                )\n                            rq_job.delete()\n                            return sendfile(request, file_path, attachment=True, attachment_filename=filename)\n                        else:\n                            if osp.exists(file_path):\n                                os.utime(file_path, None)\n                                return Response(status=status.HTTP_201_CREATED)\n                            else:\n                                rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)\n                                rq_job.delete()\n                else:\n                    raise NotImplementedError(f\"Export to {location} location is not implemented yet\")\n            elif rq_job.is_failed:\n                exc_info = rq_job.meta.get('formatted_exception', str(rq_job.exc_info))\n                rq_job.delete()\n                return Response(exc_info, status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n            elif rq_job.is_deferred and rq_id not in queue.deferred_job_registry.get_job_ids():\n                rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)\n                rq_job.delete()\n            else:\n                return Response(status=status.HTTP_202_ACCEPTED)\n    try:\n        if request.scheme:\n            server_address = request.scheme + '://'\n        server_address += request.get_host()\n    except Exception:\n        server_address = None\n    user_id = request.user.id\n    func = callback if location == Location.LOCAL else export_resource_to_cloud_storage\n    func_args = (db_instance.id, format_name, server_address)\n    if location == Location.CLOUD_STORAGE:\n        try:\n            storage_id = location_conf['storage_id']\n        except KeyError:\n            raise serializers.ValidationError(\n                'Cloud storage location was selected as the destination,'\n                ' but cloud storage id was not specified')\n        db_storage = get_cloud_storage_for_import_or_export(\n            storage_id=storage_id, request=request,\n            is_default=location_conf['is_default'])\n        filename_pattern = build_annotations_file_name(\n            class_name=db_instance.__class__.__name__,\n            identifier=db_instance.name if isinstance(db_instance, (Task, Project)) else db_instance.id,\n            timestamp=instance_timestamp,\n            format_name=format_name,\n            is_annotation_file=is_annotation_file,\n        )\n        func_args = (db_storage, filename, filename_pattern, callback) + func_args\n    else:\n        db_storage = None\n    with get_rq_lock_by_user(queue, user_id):\n        queue.enqueue_call(\n            func=func,\n            args=func_args,\n            job_id=rq_id,\n            meta=get_rq_job_meta(request=request, db_obj=db_instance),\n            depends_on=define_dependent_job(queue, user_id, rq_id=rq_id),\n            result_ttl=cache_ttl.total_seconds(),\n            failure_ttl=cache_ttl.total_seconds(),\n        )\n    handle_dataset_export(db_instance,\n        format_name=format_name, cloud_storage=db_storage, save_images=not is_annotation_file)\n    return Response(status=status.HTTP_202_ACCEPTED)\ndef _import_project_dataset(request, rq_id_template, rq_func, db_obj, format_name, filename=None, conv_mask_to_poly=True, location_conf=None):\n    format_desc = {f.DISPLAY_NAME: f\n        for f in dm.views.get_import_formats()}.get(format_name)\n    if format_desc is None:\n        raise serializers.ValidationError(\n            \"Unknown input format '{}'\".format(format_name))\n    elif not format_desc.ENABLED:\n        return Response(status=status.HTTP_405_METHOD_NOT_ALLOWED)\n    rq_id = rq_id_template.format(db_obj.pk, request.user)\n    queue = django_rq.get_queue(settings.CVAT_QUEUES.IMPORT_DATA.value)\n    rq_job = queue.fetch_job(rq_id)\n    if not rq_job or rq_job.is_finished or rq_job.is_failed:\n        if rq_job and (rq_job.is_finished or rq_job.is_failed):\n            rq_job.delete()\n        location = location_conf.get('location') if location_conf else None\n        db_storage = None\n        if not filename and location != Location.CLOUD_STORAGE:\n            serializer = DatasetFileSerializer(data=request.data)\n            if serializer.is_valid(raise_exception=True):\n                dataset_file = serializer.validated_data['dataset_file']\n                with NamedTemporaryFile(\n                    prefix='cvat_{}'.format(db_obj.pk),\n                    dir=settings.TMP_FILES_ROOT,\n                    delete=False) as tf:\n                    filename = tf.name\n                    for chunk in dataset_file.chunks():\n                        tf.write(chunk)\n        elif location == Location.CLOUD_STORAGE:\n            assert filename, 'The filename was not specified'\n            try:\n                storage_id = location_conf['storage_id']\n            except KeyError:\n                raise serializers.ValidationError(\n                    'Cloud storage location was selected as the source,'\n                    ' but cloud storage id was not specified')\n            db_storage = get_cloud_storage_for_import_or_export(\n                storage_id=storage_id, request=request,\n                is_default=location_conf['is_default'])\n            key = filename\n            with NamedTemporaryFile(\n                prefix='cvat_{}'.format(db_obj.pk),\n                dir=settings.TMP_FILES_ROOT,\n                delete=False) as tf:\n                filename = tf.name\n        func = import_resource_with_clean_up_after\n        func_args = (rq_func, filename, db_obj.pk, format_name, conv_mask_to_poly)\n        if location == Location.CLOUD_STORAGE:\n            func_args = (db_storage, key, func) + func_args\n            func = import_resource_from_cloud_storage\n        user_id = request.user.id\n        with get_rq_lock_by_user(queue, user_id):\n            rq_job = queue.enqueue_call(\n                func=func,\n                args=func_args,\n                job_id=rq_id,\n                meta={\n                    'tmp_file': filename,\n                    **get_rq_job_meta(request=request, db_obj=db_obj),\n                },\n                depends_on=define_dependent_job(queue, user_id, rq_id=rq_id),\n                result_ttl=settings.IMPORT_CACHE_SUCCESS_TTL.total_seconds(),\n                failure_ttl=settings.IMPORT_CACHE_FAILED_TTL.total_seconds()\n            )\n        handle_dataset_import(db_obj, format_name=format_name, cloud_storage=db_storage)\n    else:\n        return Response(status=status.HTTP_409_CONFLICT, data='Import job already exists')\n    serializer = RqIdSerializer(data={'rq_id': rq_id})\n    serializer.is_valid(raise_exception=True)\n    return Response(serializer.data, status=status.HTTP_202_ACCEPTED)\ndef export(db_instance, request, queue_name):\n    action = request.query_params.get('action', None)\n    filename = request.query_params.get('filename', None)\n    if action not in (None, 'download'):\n        raise serializers.ValidationError(\n            \"Unexpected action specified for the request\")\n    if isinstance(db_instance, Task):\n        obj_type = 'task'\n        logger = slogger.task[db_instance.pk]\n        Exporter = TaskExporter\n        cache_ttl = TASK_CACHE_TTL\n        use_target_storage_conf = request.query_params.get('use_default_location', True)\n    elif isinstance(db_instance, Project):\n        obj_type = 'project'\n        logger = slogger.project[db_instance.pk]\n        Exporter = ProjectExporter\n        cache_ttl = PROJECT_CACHE_TTL\n        use_target_storage_conf = request.query_params.get('use_default_location', True)\n    else:\n        raise Exception(\n            \"Unexpected type of db_instance: {}\".format(type(db_instance)))\n    use_settings = to_bool(use_target_storage_conf)\n    obj = db_instance if use_settings else request.query_params\n    location_conf = get_location_configuration(\n        obj=obj,\n        use_settings=use_settings,\n        field_name=StorageType.TARGET\n    )\n    queue = django_rq.get_queue(queue_name)\n    rq_id = f\"export:{obj_type}.id{db_instance.pk}-by-{request.user}\"\n    rq_job = queue.fetch_job(rq_id)\n    last_instance_update_time = timezone.localtime(db_instance.updated_date)\n    timestamp = datetime.strftime(last_instance_update_time, \"%Y_%m_%d_%H_%M_%S\")\n    location = location_conf.get('location')\n    if rq_job:\n        rq_request = rq_job.meta.get('request', None)\n        request_time = rq_request.get(\"timestamp\", None) if rq_request else None\n        if request_time is None or request_time < last_instance_update_time:\n            rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)\n            rq_job.delete()\n        else:\n            if rq_job.is_finished:\n                if location == Location.LOCAL:\n                    file_path = rq_job.return_value()\n                    if not file_path:\n                        return Response('A result for exporting job was not found for finished RQ job', status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n                    elif not os.path.exists(file_path):\n                        return Response('The result file does not exist in export cache', status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n                    filename = filename or build_backup_file_name(\n                        class_name=obj_type,\n                        identifier=db_instance.name,\n                        timestamp=timestamp,\n                        extension=os.path.splitext(file_path)[1]\n                    )\n                    if action == \"download\":\n                        rq_job.delete()\n                        return sendfile(request, file_path, attachment=True,\n                            attachment_filename=filename)\n                    return Response(status=status.HTTP_201_CREATED)\n                elif location == Location.CLOUD_STORAGE:\n                    rq_job.delete()\n                    return Response(status=status.HTTP_200_OK)\n                else:\n                    raise NotImplementedError()\n            elif rq_job.is_failed:\n                exc_info = rq_job.meta.get('formatted_exception', str(rq_job.exc_info))\n                rq_job.delete()\n                return Response(exc_info,\n                    status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n            else:\n                return Response(status=status.HTTP_202_ACCEPTED)\n    ttl = dm.views.PROJECT_CACHE_TTL.total_seconds()\n    user_id = request.user.id\n    func = _create_backup if location == Location.LOCAL else export_resource_to_cloud_storage\n    func_args = (db_instance, Exporter, '{}_backup.zip'.format(obj_type), logger, cache_ttl)\n    if location == Location.CLOUD_STORAGE:\n        try:\n            storage_id = location_conf['storage_id']\n        except KeyError:\n            raise serializers.ValidationError(\n                'Cloud storage location was selected as the destination,'\n                ' but cloud storage id was not specified')\n        db_storage = get_cloud_storage_for_import_or_export(\n            storage_id=storage_id, request=request,\n            is_default=location_conf['is_default'])\n        filename_pattern = build_backup_file_name(\n            class_name=obj_type,\n            identifier=db_instance.name,\n            timestamp=timestamp,\n        )\n        func_args = (db_storage, filename, filename_pattern, _create_backup) + func_args\n    with get_rq_lock_by_user(queue, user_id):\n        queue.enqueue_call(\n            func=func,\n            args=func_args,\n            job_id=rq_id,\n            meta=get_rq_job_meta(request=request, db_obj=db_instance),\n            depends_on=define_dependent_job(queue, user_id, rq_id=rq_id),\n            result_ttl=ttl,\n            failure_ttl=ttl,\n        )\n    return Response(status=status.HTTP_202_ACCEPTED)\ndef _import(importer, request, queue, rq_id, Serializer, file_field_name, location_conf, filename=None):\n    rq_job = queue.fetch_job(rq_id)\n    if (user_id_from_meta := getattr(rq_job, 'meta', {}).get('user', {}).get('id')) and user_id_from_meta != request.user.id:\n        return Response(status=status.HTTP_403_FORBIDDEN)\n    if not rq_job:\n        org_id = getattr(request.iam_context['organization'], 'id', None)\n        location = location_conf.get('location')\n        if location == Location.LOCAL:\n            if not filename:\n                serializer = Serializer(data=request.data)\n                serializer.is_valid(raise_exception=True)\n                payload_file = serializer.validated_data[file_field_name]\n                with NamedTemporaryFile(\n                    prefix='cvat_',\n                    dir=settings.TMP_FILES_ROOT,\n                    delete=False) as tf:\n                    filename = tf.name\n                    for chunk in payload_file.chunks():\n                        tf.write(chunk)\n        else:\n            file_name = request.query_params.get('filename')\n            assert file_name, \"The filename wasn't specified\"\n            try:\n                storage_id = location_conf['storage_id']\n            except KeyError:\n                raise serializers.ValidationError(\n                    'Cloud storage location was selected as the source,'\n                    ' but cloud storage id was not specified')\n            db_storage = get_cloud_storage_for_import_or_export(\n                storage_id=storage_id, request=request,\n                is_default=location_conf['is_default'])\n            key = filename\n            with NamedTemporaryFile(prefix='cvat_', dir=settings.TMP_FILES_ROOT, delete=False) as tf:\n                filename = tf.name\n        func = import_resource_with_clean_up_after\n        func_args = (importer, filename, request.user.id, org_id)\n        if location == Location.CLOUD_STORAGE:\n            func_args = (db_storage, key, func) + func_args\n            func = import_resource_from_cloud_storage\n        user_id = request.user.id\n        with get_rq_lock_by_user(queue, user_id):\n            rq_job = queue.enqueue_call(\n                func=func,\n                args=func_args,\n                job_id=rq_id,\n                meta={\n                    'tmp_file': filename,\n                    **get_rq_job_meta(request=request, db_obj=None)\n                },\n                depends_on=define_dependent_job(queue, user_id),\n                result_ttl=settings.IMPORT_CACHE_SUCCESS_TTL.total_seconds(),\n                failure_ttl=settings.IMPORT_CACHE_FAILED_TTL.total_seconds()\n            )\n    else:\n        if rq_job.is_finished:\n            project_id = rq_job.return_value()\n            rq_job.delete()\n            return Response({'id': project_id}, status=status.HTTP_201_CREATED)\n        elif rq_job.is_failed:\n            exc_info = process_failed_job(rq_job)\n            import_error_prefix = '{}.{}'.format(\n                CvatImportError.__module__, CvatImportError.__name__)\n            if exc_info.startswith(import_error_prefix):\n                exc_info = exc_info.replace(import_error_prefix + ': ', '')\n                return Response(data=exc_info,\n                    status=status.HTTP_400_BAD_REQUEST)\n            else:\n                return Response(data=exc_info,\n                    status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n    serializer = RqIdSerializer(data={'rq_id': rq_id})\n    serializer.is_valid(raise_exception=True)\n    return Response(serializer.data, status=status.HTTP_202_ACCEPTED)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-37306",
        "description": "[{'lang': 'en', 'value': 'Computer Vision Annotation Tool (CVAT) is an interactive video and image annotation tool for computer vision. Starting in version 2.2.0 and prior to version 2.14.3, if an attacker can trick a logged-in CVAT user into visiting a malicious URL, they can initiate a dataset export or a backup from a project, task or job that the victim user has permission to export into a cloud storage that the victim user has access to. The name of the resulting file can be chosen by the attacker. This implies that the attacker can overwrite arbitrary files in any cloud storage that the victim can access and, if the attacker has read access to the cloud storage used in the attack, they can obtain media files, annotations, settings and other information from any projects, tasks or jobs that the victim has permission to export. Version 2.14.3 contains a fix for the issue. No known workarounds are available.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-46",
      "code": "    def data_received(self, data):\n        self._buffer += data\n        while self._buffer:\n            try:\n                buf = Buffer(self._buffer)\n                try:\n                    header = header_from_binary(buf)\n                except NotEnoughData:\n                    logger.debug('Not enough data while parsing header from client, waiting for more')\n                    return\n                if len(buf) < header.body_size:\n                    logger.debug('We did not receive enough data from client. Need %s got %s', header.body_size,\n                                 len(buf))\n                    return\n                self.messages.put_nowait((header, buf))\n                self._buffer = self._buffer[(header.header_size + header.body_size):]\n            except Exception:\n                logger.exception('Exception raised while parsing message from client')\n                return\n    async def _process_received_message_loop(self):\n        \"\"\"\n        Take message from the queue and try to process it.\n        \"\"\"\n        while True:\n            header, buf = await self.messages.get()\n            if header is None and buf is None:\n                break\n            try:\n                await self._process_one_msg(header, buf)\n            except Exception:\n                logger.exception('Exception raised while processing message from client')",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-26151",
        "description": "[{'lang': 'en', 'value': 'Versions of the package asyncua before 0.9.96 are vulnerable to Denial of Service (DoS) such that an attacker can send a malformed packet and as a result, the server will enter into an infinite loop and consume excessive memory.'}]",
        "cwe_number": 835
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-47",
      "code": "def kick(bot, trigger):\n    \"\"\"Kick a user from the channel.\"\"\"\n    chanops = get_chanops(str(trigger.sender), bot.memory['channelmgnt']['jdcache'])\n    dodeop = False\n    if chanops:\n        if bot.channels[trigger.sender].privileges[bot.nick] < OP and trigger.account in chanops:\n            bot.say('Please wait...')\n            bot.say('op ' + trigger.sender, 'ChanServ')\n            time.sleep(1)\n            dodeop = True\n        text = trigger.group().split()\n        argc = len(text)\n        if argc < 2:\n            return\n        nick = Identifier(text[1])\n        reason = ' '.join(text[2:])\n        if nick != bot.config.core.nick and trigger.account in chanops:\n            bot.write(['KICK', trigger.sender, nick, ':' + reason])\n            if dodeop:\n                deopbot(trigger.sender, bot)\n        else:\n            bot.reply('Access Denied. If in error, please contact the channel founder.')\n    else:\n        bot.reply(f'No ChanOps Found. Please ask for assistance in {bot.settings.channelmgnt.support_channel}')\ndef parse_host_mask(text):\n    \"\"\"Identify hostmask.\"\"\"\n    argc = len(text)\n    if argc >= 2:\n        opt = Identifier(text[1])\n        mask = opt\n        if not opt.is_nick() and argc < 3:\n            return None\n        if not opt.is_nick():\n            mask = text[2]\n        if re.match('^[^.@!/]+$', mask) is not None:\n            return f'{mask}!*@*'\n        if re.match('^[^@!]+$', mask) is not None:\n            return f'*!*@{mask}'\n        m = re.match('^([^!@]+)@$', mask)\n        if m is not None:\n            return f'*!{m.group(1)}@*'\n        m = re.match('^([^!@]+)@([^@!]+)$', mask)\n        if m is not None:\n            return f'*!{m.group(1)}@{m.group(2)}'\n        m = re.match('^([^!@]+)!(^[!@]+)@?$', mask)\n        if m is not None:\n            return f'{m.group(1)}!{m.group(2)}@*'\n        return ''\n    return None\ndef unban(bot, trigger):\n    \"\"\"Unban a user from the channel. The bot must be a channel operator for this command to work.\"\"\"\n    makemodechange(bot, trigger, '-b', isbqmode=True)\ndef quiet(bot, trigger):\n    \"\"\"Quiet a user. The bot must be a channel operator for this command to work.\"\"\"\n    makemodechange(bot, trigger, '+q', isbqmode=True)\ndef kickban(bot, trigger):\n    \"\"\"Kick and ban a user from the channel. The bot must be a channel operator for this command to work.\"\"\"\n    chanops = get_chanops(str(trigger.sender), bot.memory['channelmgnt']['jdcache'])\n    dodeop = False\n    if chanops:\n        if bot.channels[trigger.sender].privileges[bot.nick] < OP and trigger.account in chanops:\n            bot.say('Please wait...')\n            bot.say('op ' + trigger.sender, 'ChanServ')\n            time.sleep(1)\n            dodeop = True\n        text = trigger.group().split()\n        argc = len(text)\n        if argc < 3:\n            bot.reply('Syntax is: .kickban <nick> <reason>')\n            if dodeop:\n                deopbot(trigger.sender, bot)\n            return\n        nick = Identifier(text[1])\n        mask = text[2] if any(s in text[2] for s in '!@*') else ''\n        reasonidx = 3 if mask != '' else 2\n        reason = ' '.join(text[reasonidx:])\n        mask = parse_host_mask(trigger.group().split())\n        if mask == '':\n            mask = nick + '!*@*'\n        if trigger.account in chanops:\n            bot.write(['MODE', trigger.sender, '+b', mask])\n            bot.write(['KICK', trigger.sender, nick, ':' + reason])\n            if dodeop:\n                deopbot(trigger.sender, bot)\n        else:\n            bot.reply('Access Denied. If in error, please contact the channel founder.')\n    else:\n        bot.reply(f'No ChanOps Found. Please ask for assistance in {bot.settings.channelmgnt.support_channel}')\ndef get_mask(bot, channel, default):\n    \"\"\"Get mask for given channel.\"\"\"\n    return (bot.db.get_channel_value(channel, 'topic_mask') or default).replace('%s', '{}')\ndef topic(bot, trigger):\n    \"\"\"Change the channel topic. The bot must be a channel operator for this command to work.\"\"\"\n    chanops = get_chanops(str(trigger.sender), bot.memory['channelmgnt']['jdcache'])\n    dodeop = False\n    if chanops:\n        if bot.channels[trigger.sender].privileges[bot.nick] < OP and trigger.account in chanops:\n            bot.say('Please wait...')\n            bot.say('op ' + trigger.sender, 'ChanServ')\n            time.sleep(1)\n            dodeop = True\n        if not trigger.group(2):\n            return None\n        channel = trigger.sender.lower()\n        mask = get_mask(bot, channel, default_mask(trigger))\n        narg = len(re.findall('{}', mask))\n        top = trigger.group(2)\n        args = []\n        args = top.split('~', narg)\n        if len(args) != narg:\n            message = f'Not enough arguments. You gave {args}, it requires {narg}.'\n            if dodeop:\n                deopbot(trigger.sender, bot)\n            return bot.say(message)\n        topictext = mask.format(*args)\n        if trigger.account in chanops:\n            bot.write(('TOPIC', channel + ' :' + topictext))\n            if dodeop:\n                deopbot(trigger.sender, bot)\n        else:\n            return bot.reply('Access Denied. If in error, please contact the channel founder.')\n    return bot.reply(f'No ChanOps Found. Please ask for assistance in {bot.settings.channelmgnt.support_channel}')",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-21431",
        "description": "[{'lang': 'en', 'value': 'sopel-channelmgnt is a channelmgnt plugin for sopel. In versions prior to 2.0.1, on some IRC servers, restrictions around the removal of the bot using the kick/kickban command could be bypassed when kicking multiple users at once. We also believe it may have been possible to remove users from other channels but due to the wonder that is IRC and following RfCs, We have no POC for that. Freenode is not affected. This is fixed in version 2.0.1. As a workaround, do not use this plugin on networks where TARGMAX > 1.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-48",
      "code": "def execute_cmd(cmd, from_async=False):\n\t\"\"\"execute a request as python module\"\"\"\n\tfor hook in frappe.get_hooks(\"override_whitelisted_methods\", {}).get(cmd, []):\n\t\tcmd = hook\n\t\tbreak\n\tif run_server_script_api(cmd):\n\t\treturn None\n\ttry:\n\t\tmethod = get_attr(cmd)\n\texcept Exception as e:\n\t\tif frappe.local.conf.developer_mode:\n\t\t\traise e\n\t\telse:\n\t\t\tfrappe.respond_as_web_page(title='Invalid Method', html='Method not found',\n\t\t\tindicator_color='red', http_status_code=404)\n\t\treturn\n\tif from_async:\n\t\tmethod = method.queue\n\tis_whitelisted(method)\n\tis_valid_http_method(method)\n\treturn frappe.call(method, **frappe.form_dict)\ndef is_valid_http_method(method):\n\thttp_method = frappe.local.request.method\n\tif http_method not in frappe.allowed_http_methods_for_whitelisted_func[method]:\n\t\tfrappe.throw(_(\"Not permitted\"), frappe.PermissionError)\ndef upload_file():\n\tuser = None\n\tif frappe.session.user == 'Guest':\n\t\tif frappe.get_system_settings('allow_guests_to_upload_files'):\n\t\t\tignore_permissions = True\n\t\telse:\n\t\t\treturn\n\telse:\n\t\tuser = frappe.get_doc(\"User\", frappe.session.user)\n\t\tignore_permissions = False\n\tfiles = frappe.request.files\n\tis_private = frappe.form_dict.is_private\n\tdoctype = frappe.form_dict.doctype\n\tdocname = frappe.form_dict.docname\n\tfieldname = frappe.form_dict.fieldname\n\tfile_url = frappe.form_dict.file_url\n\tfolder = frappe.form_dict.folder or 'Home'\n\tmethod = frappe.form_dict.method\n\tcontent = None\n\tfilename = None\n\tif 'file' in files:\n\t\tfile = files['file']\n\t\tcontent = file.stream.read()\n\t\tfilename = file.filename\n\tfrappe.local.uploaded_file = content\n\tfrappe.local.uploaded_filename = filename\n\tif frappe.session.user == 'Guest' or (user and not user.has_desk_access()):\n\t\timport mimetypes\n\t\tfiletype = mimetypes.guess_type(filename)[0]\n\t\tif filetype not in ALLOWED_MIMETYPES:\n\t\t\tfrappe.throw(_(\"You can only upload JPG, PNG, PDF, or Microsoft documents.\"))\n\tif method:\n\t\tmethod = frappe.get_attr(method)\n\t\tis_whitelisted(method)\n\t\treturn method()\n\telse:\n\t\tret = frappe.get_doc({\n\t\t\t\"doctype\": \"File\",\n\t\t\t\"attached_to_doctype\": doctype,\n\t\t\t\"attached_to_name\": docname,\n\t\t\t\"attached_to_field\": fieldname,\n\t\t\t\"folder\": folder,\n\t\t\t\"file_name\": filename,\n\t\t\t\"file_url\": file_url,\n\t\t\t\"is_private\": cint(is_private),\n\t\t\t\"content\": content\n\t\t})\n\t\tret.save(ignore_permissions=ignore_permissions)\n\t\treturn ret\ndef get_attr(cmd):\n\t\"\"\"get method object from cmd\"\"\"\n\tif '.' in cmd:\n\t\tmethod = frappe.get_attr(cmd)\n\telse:\n\t\tmethod = globals()[cmd]\n\tfrappe.log(\"method:\" + cmd)\n\treturn method\ndef ping():\n\treturn \"pong\"\ndef whitelist(allow_guest=False, xss_safe=False, methods=None):\n\t\"\"\"\n\tDecorator for whitelisting a function and making it accessible via HTTP.\n\tStandard request will be `/api/method/[path.to.method]`\n\t:param allow_guest: Allow non logged-in user to access this method.\n\t:param methods: Allowed http method to access the method.\n\tUse as:\n\t\t@frappe.whitelist()\n\t\tdef myfunc(param1, param2):\n\t\t\tpass\n\t\"\"\"\n\tif not methods:\n\t\tmethods = ['GET', 'POST', 'PUT', 'DELETE']\n\tdef innerfn(fn):\n\t\tglobal whitelisted, guest_methods, xss_safe_methods, allowed_http_methods_for_whitelisted_func\n\t\twhitelisted.append(fn)\n\t\tallowed_http_methods_for_whitelisted_func[fn] = methods\n\t\tif allow_guest:\n\t\t\tguest_methods.append(fn)\n\t\t\tif xss_safe:\n\t\t\t\txss_safe_methods.append(fn)\n\t\treturn fn\n\treturn innerfn\ndef read_only():\n\tdef innfn(fn):\n\t\tdef wrapper_fn(*args, **kwargs):\n\t\t\tif conf.read_from_replica:\n\t\t\t\tconnect_replica()\n\t\t\ttry:\n\t\t\t\tretval = fn(*args, **get_newargs(fn, kwargs))\n\t\t\texcept:\n\t\t\t\traise\n\t\t\tfinally:\n\t\t\t\tif local and hasattr(local, 'primary_db'):\n\t\t\t\t\tlocal.db.close()\n\t\t\t\t\tlocal.db = local.primary_db\n\t\t\treturn retval\n\t\treturn wrapper_fn\n\treturn innfn\ndef validate_and_sanitize_search_inputs(fn, instance, args, kwargs):\n\tkwargs.update(dict(zip(fn.__code__.co_varnames, args)))\n\tsanitize_searchfield(kwargs['searchfield'])\n\tkwargs['start'] = cint(kwargs['start'])\n\tkwargs['page_len'] = cint(kwargs['page_len'])\n\tif kwargs['doctype'] and not frappe.db.exists('DocType', kwargs['doctype']):\n\t\treturn []\n\treturn fn(**kwargs)\n\tdef whitelist(f):\n\t\t\"\"\"Decorator: Whitelist method to be called remotely via REST API.\"\"\"\n\t\tf.whitelisted = True\n\t\treturn f\n\tdef is_whitelisted(self, method):\n\t\tfn = getattr(self, method, None)\n\t\tif not fn:\n\t\t\traise NotFound(\"Method {0} not found\".format(method))\n\t\telif not getattr(fn, \"whitelisted\", False):\n\t\t\traise Forbidden(\"Method {0} not whitelisted\".format(method))",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-23057",
        "description": "[{'lang': 'en', 'value': 'In ERPNext, versions v12.0.9--v13.0.3 are vulnerable to Stored Cross-Site-Scripting (XSS), due to user input not being validated properly. A low privileged attacker could inject arbitrary code into input fields when editing his profile.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-49",
      "code": "def call_with_ns(f, ns, arg=1):\n    td = Rtd()\n    this = ns.get('context', ns.get('here'))\n    td.this = this\n    request = ns.get('request', {})\n    td._push(request)\n    td._push(InstanceDict(td.this, td))\n    td._push(ns)\n    try:\n        if arg==2:\n            return f(None, td)\n        else:\n            return f(td)\n    finally:\n        td._pop(3)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2009-5145",
        "description": "[{'lang': 'en', 'value': 'Cross-site scripting (XSS) vulnerability in ZMI pages that use the manage_tabs_message in Zope 2.11.4, 2.11.2, 2.10.9, 2.10.7, 2.10.6, 2.10.5, 2.10.4, 2.10.2, 2.10.1, 2.12.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-50",
      "code": "    def rebuild_proxies(self, prepared_request, proxies):\n        \"\"\"This method re-evaluates the proxy configuration by considering the\n        environment variables. If we are redirected to a URL covered by\n        NO_PROXY, we strip the proxy configuration. Otherwise, we set missing\n        proxy keys for this URL (in case they were stripped by a previous\n        redirect).\n        This method also replaces the Proxy-Authorization header where\n        necessary.\n        :rtype: dict\n        \"\"\"\n        headers = prepared_request.headers\n        scheme = urlparse(prepared_request.url).scheme\n        new_proxies = resolve_proxies(prepared_request, proxies, self.trust_env)\n        if \"Proxy-Authorization\" in headers:\n            del headers[\"Proxy-Authorization\"]\n        try:\n            username, password = get_auth_from_url(new_proxies[scheme])\n        except KeyError:\n            username, password = None, None\n        if username and password:\n            headers[\"Proxy-Authorization\"] = _basic_auth_str(username, password)\n        return new_proxies",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-32681",
        "description": "[{'lang': 'en', 'value': 'Requests is a HTTP library. Since Requests 2.3.0, Requests has been leaking Proxy-Authorization headers to destination servers when redirected to an HTTPS endpoint. This is a product of how we use `rebuild_proxies` to reattach the `Proxy-Authorization` header to requests. For HTTP connections sent through the tunnel, the proxy will identify the header in the request itself and remove it prior to forwarding to the destination server. However when sent over HTTPS, the `Proxy-Authorization` header must be sent in the CONNECT request as the proxy has no visibility into the tunneled request. This results in Requests forwarding proxy credentials to the destination server unintentionally, allowing a malicious actor to potentially exfiltrate sensitive information. This issue has been patched in version 2.31.0.\\n\\n'}]",
        "cwe_number": 200
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-51",
      "code": "    def baseline(self, upper, diag, lower, vec):\n      diag_part = array_ops.expand_dims(diag, -1) * vec\n      lower_part = array_ops.pad(\n          array_ops.expand_dims(lower[:, 1:], -1) * vec[:, :-1, :],\n          [[0, 0], [1, 0], [0, 0]])\n      upper_part = array_ops.pad(\n          array_ops.expand_dims(upper[:, :-1], -1) * vec[:, 1:, :],\n          [[0, 0], [0, 1], [0, 0]])\n      return lower_part + diag_part + upper_part\n    def _generateData(self, batch_size, m, n, seed=42):\n      np.random.seed(seed)\n      data = np.random.normal(size=(batch_size, m, 3 + n))\n      return (variables.Variable(data[:, :, 0], dtype=dtypes.float64),\n              variables.Variable(data[:, :, 1], dtype=dtypes.float64),\n              variables.Variable(data[:, :, 2], dtype=dtypes.float64),\n              variables.Variable(data[:, :, 3:], dtype=dtypes.float64))\n    def benchmarkTridiagonalMulOp(self):\n      devices = [('/cpu:0', 'cpu')]\n      if test.is_gpu_available(cuda_only=True):\n        devices += [('/gpu:0', 'gpu')]\n      for device_option, size_option in itertools.product(devices, self.sizes):\n        device_id, device_name = device_option\n        m, batch_size, n = size_option\n        with ops.Graph().as_default(), \\\n            session.Session(config=benchmark.benchmark_config()) as sess, \\\n            ops.device(device_id):\n          upper, diag, lower, vec = self._generateData(batch_size, m, n)\n          x1 = self.baseline(upper, diag, lower, vec)\n          x2 = linalg_impl.tridiagonal_matmul((upper, diag, lower),\n                                              vec,\n                                              diagonals_format='sequence')\n          self.evaluate(variables.global_variables_initializer())\n          self.run_op_benchmark(\n              sess,\n              control_flow_ops.group(x1),\n              min_iters=10,\n              store_memory_usage=False,\n              name=('tridiagonal_matmul_baseline_%s'\n                    '_batch_size_%d_m_%d_n_%d' %\n                    (device_name, batch_size, m, n)))\n          self.run_op_benchmark(\n              sess,\n              control_flow_ops.group(x2),\n              min_iters=10,\n              store_memory_usage=False,\n              name=('tridiagonal_matmul_%s_batch_size_%d_m_%d_n_%d' %\n                    (device_name, batch_size, m, n)))",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-41206",
        "description": "[{'lang': 'en', 'value': \"TensorFlow is an open source platform for machine learning. In affected versions several TensorFlow operations are missing validation for the shapes of the tensor arguments involved in the call. Depending on the API, this can result in undefined behavior and segfault or `CHECK`-fail related crashes but in some scenarios writes and reads from heap populated arrays are also possible. We have discovered these issues internally via tooling while working on improving/testing GPU op determinism. As such, we don't have reproducers and there will be multiple fixes for these issues. These fixes will be included in TensorFlow 2.7.0. We will also cherrypick these commits on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.\"}]",
        "cwe_number": 354
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-52",
      "code": "if __name__ == \"__main__\":\n  test.main()",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-41880",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. When the `BaseCandidateSamplerOp` function receives a value in `true_classes` larger than `range_max`, a heap oob read occurs. We have patched the issue in GitHub commit b389f5c944cadfdfe599b3f1e4026e036f30d2d4. The fix will be included in TensorFlow 2.11. We will also cherrypick this commit on TensorFlow 2.10.1, 2.9.3, and TensorFlow 2.8.4, as these are also affected and still in supported range.'}]",
        "cwe_number": 125
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-53",
      "code": "  def testSimple(self):\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.float32)\n        pi = array_ops.placeholder(dtypes.int64)\n        gi = array_ops.placeholder(dtypes.int64)\n        v = 2. * (array_ops.zeros([128, 128]) + x)\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea([dtypes.float32])\n        stage = stager.put(pi, [v], [0])\n        k, y = stager.get(gi)\n        y = math_ops.reduce_max(math_ops.matmul(y, y))\n    G.finalize()\n    with self.session(graph=G) as sess:\n      sess.run(stage, feed_dict={x: -1, pi: 0})\n      for i in range(10):\n        _, yval = sess.run([stage, y], feed_dict={x: i, pi: i + 1, gi: i})\n        self.assertAllClose(4 * (i - 1) * (i - 1) * 128, yval, rtol=1e-4)\n  def testMultiple(self):\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.float32)\n        pi = array_ops.placeholder(dtypes.int64)\n        gi = array_ops.placeholder(dtypes.int64)\n        v = 2. * (array_ops.zeros([128, 128]) + x)\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea([dtypes.float32, dtypes.float32])\n        stage = stager.put(pi, [x, v], [0, 1])\n        k, (z, y) = stager.get(gi)\n        y = math_ops.reduce_max(z * math_ops.matmul(y, y))\n    G.finalize()\n    with self.session(graph=G) as sess:\n      sess.run(stage, feed_dict={x: -1, pi: 0})\n      for i in range(10):\n        _, yval = sess.run([stage, y], feed_dict={x: i, pi: i + 1, gi: i})\n        self.assertAllClose(\n            4 * (i - 1) * (i - 1) * (i - 1) * 128, yval, rtol=1e-4)\n  def testDictionary(self):\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.float32)\n        pi = array_ops.placeholder(dtypes.int64)\n        gi = array_ops.placeholder(dtypes.int64)\n        v = 2. * (array_ops.zeros([128, 128]) + x)\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea(\n            [dtypes.float32, dtypes.float32],\n            shapes=[[], [128, 128]],\n            names=['x', 'v'])\n        stage = stager.put(pi, {'x': x, 'v': v})\n        key, ret = stager.get(gi)\n        z = ret['x']\n        y = ret['v']\n        y = math_ops.reduce_max(z * math_ops.matmul(y, y))\n    G.finalize()\n    with self.session(graph=G) as sess:\n      sess.run(stage, feed_dict={x: -1, pi: 0})\n      for i in range(10):\n        _, yval = sess.run([stage, y], feed_dict={x: i, pi: i + 1, gi: i})\n        self.assertAllClose(\n            4 * (i - 1) * (i - 1) * (i - 1) * 128, yval, rtol=1e-4)\n  def testColocation(self):\n    gpu_dev = test.gpu_device_name()\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.float32)\n        v = 2. * (array_ops.zeros([128, 128]) + x)\n      with ops.device(gpu_dev):\n        stager = data_flow_ops.MapStagingArea([dtypes.float32])\n        y = stager.put(1, [v], [0])\n        expected_name = gpu_dev if 'gpu' not in gpu_dev else '/device:GPU:0'\n        self.assertEqual(y.device, expected_name)\n      with ops.device('/cpu:0'):\n        _, x = stager.get(1)\n        y = stager.peek(1)[0]\n        _, z = stager.get()\n        self.assertEqual(x[0].device, '/device:CPU:0')\n        self.assertEqual(y.device, '/device:CPU:0')\n        self.assertEqual(z[0].device, '/device:CPU:0')\n    G.finalize()\n  def testPeek(self):\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.int32, name='x')\n        pi = array_ops.placeholder(dtypes.int64)\n        gi = array_ops.placeholder(dtypes.int64)\n        p = array_ops.placeholder(dtypes.int32, name='p')\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea(\n            [\n                dtypes.int32,\n            ], shapes=[[]])\n        stage = stager.put(pi, [x], [0])\n        peek = stager.peek(gi)\n        size = stager.size()\n    G.finalize()\n    n = 10\n    with self.session(graph=G) as sess:\n      for i in range(n):\n        sess.run(stage, feed_dict={x: i, pi: i})\n      for i in range(n):\n        self.assertTrue(sess.run(peek, feed_dict={gi: i})[0] == i)\n      self.assertTrue(sess.run(size) == 10)\n  def testSizeAndClear(self):\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.float32, name='x')\n        pi = array_ops.placeholder(dtypes.int64)\n        gi = array_ops.placeholder(dtypes.int64)\n        v = 2. * (array_ops.zeros([128, 128]) + x)\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea(\n            [dtypes.float32, dtypes.float32],\n            shapes=[[], [128, 128]],\n            names=['x', 'v'])\n        stage = stager.put(pi, {'x': x, 'v': v})\n        size = stager.size()\n        clear = stager.clear()\n    G.finalize()\n    with self.session(graph=G) as sess:\n      sess.run(stage, feed_dict={x: -1, pi: 3})\n      self.assertEqual(sess.run(size), 1)\n      sess.run(stage, feed_dict={x: -1, pi: 1})\n      self.assertEqual(sess.run(size), 2)\n      sess.run(clear)\n      self.assertEqual(sess.run(size), 0)\n  def testCapacity(self):\n    capacity = 3\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.int32, name='x')\n        pi = array_ops.placeholder(dtypes.int64, name='pi')\n        gi = array_ops.placeholder(dtypes.int64, name='gi')\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea(\n            [\n                dtypes.int32,\n            ], capacity=capacity, shapes=[[]])\n      stage = stager.put(pi, [x], [0])\n      get = stager.get()\n      size = stager.size()\n    G.finalize()\n    from six.moves import queue as Queue\n    import threading\n    queue = Queue.Queue()\n    n = 8\n    with self.session(graph=G) as sess:\n      def thread_run():\n        for i in range(n):\n          sess.run(stage, feed_dict={x: i, pi: i})\n          queue.put(0)\n      t = threading.Thread(target=thread_run)\n      t.daemon = True\n      t.start()\n      try:\n        for i in range(n):\n          queue.get(timeout=TIMEOUT)\n      except Queue.Empty:\n        pass\n      if not i == capacity:\n        self.fail(\"Expected to timeout on iteration '{}' \"\n                  \"but instead timed out on iteration '{}' \"\n                  \"Staging Area size is '{}' and configured \"\n                  \"capacity is '{}'.\".format(capacity, i, sess.run(size),\n                                             capacity))\n      self.assertTrue(sess.run(size) == capacity)\n      for i in range(n):\n        sess.run(get)\n      self.assertTrue(sess.run(size) == 0)\n  def testMemoryLimit(self):\n    memory_limit = 512 * 1024\n    chunk = 200 * 1024\n    capacity = memory_limit // chunk\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.uint8, name='x')\n        pi = array_ops.placeholder(dtypes.int64, name='pi')\n        gi = array_ops.placeholder(dtypes.int64, name='gi')\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea(\n            [dtypes.uint8], memory_limit=memory_limit, shapes=[[]])\n        stage = stager.put(pi, [x], [0])\n        get = stager.get()\n        size = stager.size()\n    G.finalize()\n    from six.moves import queue as Queue\n    import threading\n    import numpy as np\n    queue = Queue.Queue()\n    n = 8\n    with self.session(graph=G) as sess:\n      def thread_run():\n        for i in range(n):\n          data = np.full(chunk, i, dtype=np.uint8)\n          sess.run(stage, feed_dict={x: data, pi: i})\n          queue.put(0)\n      t = threading.Thread(target=thread_run)\n      t.daemon = True\n      t.start()\n      try:\n        for i in range(n):\n          queue.get(timeout=TIMEOUT)\n      except Queue.Empty:\n        pass\n      if not i == capacity:\n        self.fail(\"Expected to timeout on iteration '{}' \"\n                  \"but instead timed out on iteration '{}' \"\n                  \"Staging Area size is '{}' and configured \"\n                  \"capacity is '{}'.\".format(capacity, i, sess.run(size),\n                                             capacity))\n      self.assertTrue(sess.run(size) == capacity)\n      for i in range(n):\n        sess.run(get)\n      self.assertTrue(sess.run(size) == 0)\n  def testOrdering(self):\n    import six\n    import random\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.int32, name='x')\n        pi = array_ops.placeholder(dtypes.int64, name='pi')\n        gi = array_ops.placeholder(dtypes.int64, name='gi')\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea(\n            [\n                dtypes.int32,\n            ], shapes=[[]], ordered=True)\n        stage = stager.put(pi, [x], [0])\n        get = stager.get()\n        size = stager.size()\n    G.finalize()\n    n = 10\n    with self.session(graph=G) as sess:\n      keys = list(reversed(six.moves.range(n)))\n      for i in keys:\n        sess.run(stage, feed_dict={pi: i, x: i})\n      self.assertTrue(sess.run(size) == n)\n      for i, k in enumerate(reversed(keys)):\n        get_key, values = sess.run(get)\n        self.assertTrue(i == k == get_key == values)\n      self.assertTrue(sess.run(size) == 0)\n  def testPartialDictInsert(self):\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.float32)\n        f = array_ops.placeholder(dtypes.float32)\n        v = array_ops.placeholder(dtypes.float32)\n        pi = array_ops.placeholder(dtypes.int64)\n        gi = array_ops.placeholder(dtypes.int64)\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea(\n            [dtypes.float32, dtypes.float32, dtypes.float32],\n            names=['x', 'v', 'f'])\n        stage_xf = stager.put(pi, {'x': x, 'f': f})\n        stage_v = stager.put(pi, {'v': v})\n        key, ret = stager.get(gi)\n        size = stager.size()\n        isize = stager.incomplete_size()\n    G.finalize()\n    with self.session(graph=G) as sess:\n      self.assertTrue(sess.run([size, isize]) == [0, 0])\n      sess.run(stage_xf, feed_dict={pi: 0, x: 1, f: 2})\n      self.assertTrue(sess.run([size, isize]) == [0, 1])\n      sess.run(stage_xf, feed_dict={pi: 1, x: 1, f: 2})\n      self.assertTrue(sess.run([size, isize]) == [0, 2])\n      sess.run(stage_v, feed_dict={pi: 0, v: 1})\n      self.assertTrue(sess.run([size, isize]) == [1, 1])\n      self.assertTrue(\n          sess.run([key, ret], feed_dict={\n              gi: 0\n          }) == [0, {\n              'x': 1,\n              'f': 2,\n              'v': 1\n          }])\n      self.assertTrue(sess.run([size, isize]) == [0, 1])\n      sess.run(stage_v, feed_dict={pi: 1, v: 3})\n      self.assertTrue(\n          sess.run([key, ret], feed_dict={\n              gi: 1\n          }) == [1, {\n              'x': 1,\n              'f': 2,\n              'v': 3\n          }])\n  def testPartialIndexInsert(self):\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.float32)\n        f = array_ops.placeholder(dtypes.float32)\n        v = array_ops.placeholder(dtypes.float32)\n        pi = array_ops.placeholder(dtypes.int64)\n        gi = array_ops.placeholder(dtypes.int64)\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea(\n            [dtypes.float32, dtypes.float32, dtypes.float32])\n        stage_xf = stager.put(pi, [x, f], [0, 2])\n        stage_v = stager.put(pi, [v], [1])\n        key, ret = stager.get(gi)\n        size = stager.size()\n        isize = stager.incomplete_size()\n    G.finalize()\n    with self.session(graph=G) as sess:\n      self.assertTrue(sess.run([size, isize]) == [0, 0])\n      sess.run(stage_xf, feed_dict={pi: 0, x: 1, f: 2})\n      self.assertTrue(sess.run([size, isize]) == [0, 1])\n      sess.run(stage_xf, feed_dict={pi: 1, x: 1, f: 2})\n      self.assertTrue(sess.run([size, isize]) == [0, 2])\n      sess.run(stage_v, feed_dict={pi: 0, v: 1})\n      self.assertTrue(sess.run([size, isize]) == [1, 1])\n      self.assertTrue(sess.run([key, ret], feed_dict={gi: 0}) == [0, [1, 1, 2]])\n      self.assertTrue(sess.run([size, isize]) == [0, 1])\n      sess.run(stage_v, feed_dict={pi: 1, v: 3})\n      self.assertTrue(sess.run([key, ret], feed_dict={gi: 1}) == [1, [1, 3, 2]])\n  def testPartialDictGetsAndPeeks(self):\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.float32)\n        f = array_ops.placeholder(dtypes.float32)\n        v = array_ops.placeholder(dtypes.float32)\n        pi = array_ops.placeholder(dtypes.int64)\n        pei = array_ops.placeholder(dtypes.int64)\n        gi = array_ops.placeholder(dtypes.int64)\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea(\n            [dtypes.float32, dtypes.float32, dtypes.float32],\n            names=['x', 'v', 'f'])\n        stage_xf = stager.put(pi, {'x': x, 'f': f})\n        stage_v = stager.put(pi, {'v': v})\n        peek_xf = stager.peek(pei, ['x', 'f'])\n        peek_v = stager.peek(pei, ['v'])\n        key_xf, get_xf = stager.get(gi, ['x', 'f'])\n        key_v, get_v = stager.get(gi, ['v'])\n        pop_key_xf, pop_xf = stager.get(indices=['x', 'f'])\n        pop_key_v, pop_v = stager.get(pi, ['v'])\n        size = stager.size()\n        isize = stager.incomplete_size()\n    G.finalize()\n    with self.session(graph=G) as sess:\n      self.assertTrue(sess.run([size, isize]) == [0, 0])\n      sess.run(stage_xf, feed_dict={pi: 0, x: 1, f: 2})\n      self.assertTrue(sess.run([size, isize]) == [0, 1])\n      sess.run(stage_xf, feed_dict={pi: 1, x: 1, f: 2})\n      self.assertTrue(sess.run([size, isize]) == [0, 2])\n      sess.run(stage_v, feed_dict={pi: 0, v: 1})\n      self.assertTrue(sess.run([size, isize]) == [1, 1])\n      self.assertTrue(sess.run(peek_xf, feed_dict={pei: 0}) == {'x': 1, 'f': 2})\n      self.assertTrue(sess.run(peek_v, feed_dict={pei: 0}) == {'v': 1})\n      self.assertTrue(sess.run([size, isize]) == [1, 1])\n      self.assertTrue(\n          sess.run([key_xf, get_xf], feed_dict={\n              gi: 0\n          }) == [0, {\n              'x': 1,\n              'f': 2\n          }])\n      self.assertTrue(sess.run([size, isize]) == [1, 1])\n      with self.assertRaises(errors.InvalidArgumentError) as cm:\n        sess.run([key_xf, get_xf], feed_dict={gi: 0})\n      exc_str = (\"Tensor at index '0' for key '0' \" 'has already been removed.')\n      self.assertTrue(exc_str in cm.exception.message)\n      self.assertTrue(\n          sess.run([key_v, get_v], feed_dict={\n              gi: 0\n          }) == [0, {\n              'v': 1\n          }])\n      self.assertTrue(sess.run([size, isize]) == [0, 1])\n      sess.run(stage_v, feed_dict={pi: 1, v: 1})\n      self.assertTrue(sess.run([size, isize]) == [1, 0])\n      self.assertTrue(sess.run([pop_key_xf, pop_xf]) == [1, {'x': 1, 'f': 2}])\n      self.assertTrue(sess.run([size, isize]) == [1, 0])\n      self.assertTrue(\n          sess.run([pop_key_v, pop_v], feed_dict={\n              pi: 1\n          }) == [1, {\n              'v': 1\n          }])\n      self.assertTrue(sess.run([size, isize]) == [0, 0])\n  def testPartialIndexGets(self):\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.float32)\n        f = array_ops.placeholder(dtypes.float32)\n        v = array_ops.placeholder(dtypes.float32)\n        pi = array_ops.placeholder(dtypes.int64)\n        pei = array_ops.placeholder(dtypes.int64)\n        gi = array_ops.placeholder(dtypes.int64)\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea(\n            [dtypes.float32, dtypes.float32, dtypes.float32])\n        stage_xvf = stager.put(pi, [x, v, f], [0, 1, 2])\n        key_xf, get_xf = stager.get(gi, [0, 2])\n        key_v, get_v = stager.get(gi, [1])\n        size = stager.size()\n        isize = stager.incomplete_size()\n    G.finalize()\n    with self.session(graph=G) as sess:\n      sess.run(stage_xvf, feed_dict={pi: 0, x: 1, f: 2, v: 3})\n      self.assertTrue(sess.run([size, isize]) == [1, 0])\n      self.assertTrue(\n          sess.run([key_xf, get_xf], feed_dict={\n              gi: 0\n          }) == [0, [1, 2]])\n      self.assertTrue(sess.run([size, isize]) == [1, 0])\n      self.assertTrue(sess.run([key_v, get_v], feed_dict={gi: 0}) == [0, [3]])\n      self.assertTrue(sess.run([size, isize]) == [0, 0])",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-21734",
        "description": "[{'lang': 'en', 'value': 'Tensorflow is an Open Source Machine Learning Framework. The implementation of `MapStage` is vulnerable a `CHECK`-fail if the key tensor is not a scalar. The fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.'}]",
        "cwe_number": 843
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-54",
      "code": "  def testInt64(self):\n    @def_function.function\n    def g():\n      x = random_ops.random_normal(shape=[int(1e10)])\n      y = array_ops.ones(shape=[int(1e10)])\n      return array_ops.searchsorted(x, y, out_type=dtypes.int64)\n    _ = g.get_concrete_function()",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-35965",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. If `LowerBound` or `UpperBound` is given an empty`sorted_inputs` input, it results in a `nullptr` dereference, leading to a segfault that can be used to trigger a denial of service attack. We have patched the issue in GitHub commit bce3717eaef4f769019fd18e990464ca4a2efeea. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue.'}]",
        "cwe_number": 476
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-55",
      "code": "    def __read_chunk_length(self, rfile):\n        buf = BytesIO()\n        while 1:\n            char = rfile.read(1)\n            if not char:\n                self._chunked_input_error = True\n                raise _InvalidClientInput(\"EOF before chunk end reached\")\n            if char == b'\\r':\n                break\n            if char == b';':\n                break\n            if char not in _HEX:\n                self._chunked_input_error = True\n                raise _InvalidClientInput(\"Non-hex data\", char)\n            buf.write(char)\n            if buf.tell() > 16:\n                self._chunked_input_error = True\n                raise _InvalidClientInput(\"Chunk-size too large.\")\n        if char == b';':\n            i = 0\n            while i < MAX_REQUEST_LINE:\n                char = rfile.read(1)\n                if char == b'\\r':\n                    break\n                i += 1\n            else:\n                self._chunked_input_error = True\n                raise _InvalidClientInput(\"Too large chunk extension\")\n        if char == b'\\r':\n            char = rfile.read(1)\n            if char != b'\\n':\n                self._chunked_input_error = True\n                raise _InvalidClientInput(\"Line didn't end in CRLF\")\n            return int(buf.getvalue(), 16)\n    def _chunked_read(self, length=None, use_readline=False):\n        rfile = self.rfile\n        self._send_100_continue()\n        if length == 0:\n            return b\"\"\n        if use_readline:\n            reader = self.rfile.readline\n        else:\n            reader = self.rfile.read\n        response = []\n        while self.chunk_length != 0:\n            maxreadlen = self.chunk_length - self.position\n            if length is not None and length < maxreadlen:\n                maxreadlen = length\n            if maxreadlen > 0:\n                data = reader(maxreadlen)\n                if not data:\n                    self.chunk_length = 0\n                    self._chunked_input_error = True\n                    raise IOError(\"unexpected end of file while parsing chunked data\")\n                datalen = len(data)\n                response.append(data)\n                self.position += datalen\n                if self.chunk_length == self.position:\n                    rfile.readline()\n                if length is not None:\n                    length -= datalen\n                    if length == 0:\n                        break\n                if use_readline and data[-1] == b\"\\n\"[0]:\n                    break\n            else:\n                self.chunk_length = self.__read_chunk_length(rfile)\n                self.position = 0\n                if self.chunk_length == 0:\n                    rfile.readline()\n        return b''.join(response)\n    def readlines(self, hint=None):\n        return list(self)\n    def __iter__(self):\n        return self\n    def next(self):\n        line = self.readline()\n        if not line:\n            raise StopIteration\n        return line\n    __next__ = next\ntry:\n    import mimetools\n    headers_factory = mimetools.Message\nexcept ImportError:\n    from http import client\n    class OldMessage(client.HTTPMessage):\n        def __init__(self, **kwargs):\n            super(client.HTTPMessage, self).__init__(**kwargs)\n            self.status = ''\n        def getheader(self, name, default=None):\n            return self.get(name, default)\n        @property\n        def headers(self):\n            for key, value in self._headers:\n                yield '%s: %s\\r\\n' % (key, value)\n        @property\n        def typeheader(self):\n            return self.get('content-type')\n    def headers_factory(fp, *args):\n        try:\n            ret = client.parse_headers(fp, _class=OldMessage)\n        except client.LineTooLong:\n            ret = OldMessage()\n            ret.status = 'Line too long'\n        return ret\nclass WSGIHandler(object):\n    protocol_version = 'HTTP/1.1'\n    def MessageClass(self, *args):\n        return headers_factory(*args)\n    status = None\n    _orig_status = None\n    response_headers = None\n    code = None\n    provided_date = None\n    provided_content_length = None\n    close_connection = False\n    time_start = 0\n    time_finish = 0\n    headers_sent = False\n    response_use_chunked = False\n    connection_upgraded = False\n    environ = None\n    application = None\n    requestline = None\n    def read_requestline(self):\n        \"\"\"\n        Read and return the HTTP request line.\n        Under both Python 2 and 3, this should return the native\n        ``str`` type; under Python 3, this probably means the bytes read\n        from the network need to be decoded (using the ISO-8859-1 charset, aka\n        latin-1).\n        \"\"\"\n        line = self.rfile.readline(MAX_REQUEST_LINE)\n        line = line.decode('latin-1')\n        return line\n    def handle_one_request(self):\n        \"\"\"\n        Handles one HTTP request using ``self.socket`` and ``self.rfile``.\n        Each invocation of this method will do several things, including (but not limited to):\n        - Read the request line using :meth:`read_requestline`;\n        - Read the rest of the request, including headers, with :meth:`read_request`;\n        - Construct a new WSGI environment in ``self.environ`` using :meth:`get_environ`;\n        - Store the application in ``self.application``, retrieving it from the server;\n        - Handle the remainder of the request, including invoking the application,\n          with :meth:`handle_one_response`\n        There are several possible return values to indicate the state\n        of the client connection:\n        - ``None``\n            The client connection is already closed or should\n            be closed because the WSGI application or client set the\n            ``Connection: close`` header. The request handling\n            loop should terminate and perform cleanup steps.\n        - (status, body)\n            An HTTP status and body tuple. The request was in error,\n            as detailed by the status and body. The request handling\n            loop should terminate, close the connection, and perform\n            cleanup steps. Note that the ``body`` is the complete contents\n            to send to the client, including all headers and the initial\n            status line.\n        - ``True``\n            The literal ``True`` value. The request was successfully handled\n            and the response sent to the client by :meth:`handle_one_response`.\n            The connection remains open to process more requests and the connection\n            handling loop should call this method again. This is the typical return\n            value.\n        .. seealso:: :meth:`handle`\n        .. versionchanged:: 1.1b6\n           Funnel exceptions having to do with invalid HTTP requests through\n           :meth:`_handle_client_error` to allow subclasses to customize. Note that\n           this is experimental and may change in the future.\n        \"\"\"\n        if self.rfile.closed:\n            return\n        try:\n            self.requestline = self.read_requestline()\n            if isinstance(self.requestline, bytes):\n                self.requestline = self.requestline.decode('latin-1')\n        except socket.error:\n            return\n        if not self.requestline:\n            return\n        self.response_length = 0\n        if len(self.requestline) >= MAX_REQUEST_LINE:\n            return ('414', _REQUEST_TOO_LONG_RESPONSE)\n        try:\n            if not self.read_request(self.requestline):\n                return ('400', _BAD_REQUEST_RESPONSE)\n        except Exception as ex:\n            return self._handle_client_error(ex)\n        self.environ = self.get_environ()\n        self.application = self.server.application\n        self.handle_one_response()\n        if self.close_connection:\n            return\n        if self.rfile.closed:\n            return\n        return True\n    def get_environ(self):\n        \"\"\"\n        Construct and return a new WSGI environment dictionary for a specific request.\n        This should begin with asking the server for the base environment\n        using :meth:`WSGIServer.get_environ`, and then proceed to add the\n        request specific values.\n        By the time this method is invoked the request line and request shall have\n        been parsed and ``self.headers`` shall be populated.\n        \"\"\"\n        env = self.server.get_environ()\n        env['REQUEST_METHOD'] = self.command\n        env['SCRIPT_NAME'] = ''\n        path, query = self.path.split('?', 1) if '?' in self.path else (self.path, '')\n        env['PATH_INFO'] = unquote_latin1(path)\n        env['QUERY_STRING'] = query\n        if self.headers.typeheader is not None:\n            env['CONTENT_TYPE'] = self.headers.typeheader\n        length = self.headers.getheader('content-length')\n        if length:\n            env['CONTENT_LENGTH'] = length\n        env['SERVER_PROTOCOL'] = self.request_version\n        client_address = self.client_address\n        if isinstance(client_address, tuple):\n            env['REMOTE_ADDR'] = str(client_address[0])\n            env['REMOTE_PORT'] = str(client_address[1])\n        for key, value in self._headers():\n            if key in env:\n                if 'COOKIE' in key:\n                    env[key] += '; ' + value\n                else:\n                    env[key] += ',' + value\n            else:\n                env[key] = value\n        sock = self.socket if env.get('HTTP_EXPECT') == '100-continue' else None\n        chunked = env.get('HTTP_TRANSFER_ENCODING', '').lower() == 'chunked'\n        handling_reads = not self._connection_upgrade_requested()\n        self.wsgi_input = Input(self.rfile, self.content_length, socket=sock, chunked_input=chunked)\n        env['wsgi.input'] = self.wsgi_input if handling_reads else self.rfile\n        env['wsgi.input_terminated'] = handling_reads\n        return env\n    def flush(self):\n        pass\n    def writelines(self, *args, **kwargs):\n        pass\nclass LoggingLogAdapter(object):\ndef check_output(*popenargs, **kwargs):\n    r\"\"\"\n    check_output(args, *, input=None, stdin=None, stderr=None, shell=False, universal_newlines=False, timeout=None) -> output\n    Run command with arguments and return its output.\n    If the exit code was non-zero it raises a :exc:`CalledProcessError`.  The\n    ``CalledProcessError`` object will have the return code in the returncode\n    attribute and output in the output attribute.\n    The arguments are the same as for the Popen constructor.  Example::\n        >>> check_output([\"ls\", \"-1\", \"/dev/null\"])\n        '/dev/null\\n'\n    The ``stdout`` argument is not allowed as it is used internally.\n    To capture standard error in the result, use ``stderr=STDOUT``::\n        >>> print(check_output([\"/bin/sh\", \"-c\",\n        ...               \"ls -l non_existent_file ; exit 0\"],\n        ...              stderr=STDOUT).decode('ascii').strip())\n        ls: non_existent_file: No such file or directory\n    There is an additional optional argument, \"input\", allowing you to\n    pass a string to the subprocess's stdin.  If you use this argument\n    you may not also use the Popen constructor's \"stdin\" argument, as\n    it too will be used internally.  Example::\n        >>> check_output([\"sed\", \"-e\", \"s/foo/bar/\"],\n        ...              input=b\"when in the course of fooman events\\n\")\n        'when in the course of barman events\\n'\n    If ``universal_newlines=True`` is passed, the return value will be a\n    string rather than bytes.\n    .. versionchanged:: 1.2a1\n       The ``timeout`` keyword argument is now accepted on all supported\n       versions of Python (not just Python 3) and if it expires will raise a\n       :exc:`TimeoutExpired` exception (under Python 2 this is a subclass of :exc:`~.Timeout`).\n    .. versionchanged:: 1.2a1\n       The ``input`` keyword argument is now accepted on all supported\n       versions of Python, not just Python 3\n    .. versionchanged:: 22.08.0\n       Passing the ``check`` keyword argument is forbidden, just as in Python 3.11.\n    \"\"\"\n    timeout = kwargs.pop('timeout', None)\n    if 'stdout' in kwargs:\n        raise ValueError('stdout argument not allowed, it will be overridden.')\n    if 'check' in kwargs:\n        raise ValueError('check argument not allowed, it will be overridden.')\n    if 'input' in kwargs:\n        if 'stdin' in kwargs:\n            raise ValueError('stdin and input arguments may not both be used.')\n        inputdata = kwargs['input']\n        del kwargs['input']\n        kwargs['stdin'] = PIPE\n    else:\n        inputdata = None\n    with Popen(*popenargs, stdout=PIPE, **kwargs) as process:\n        try:\n            output, unused_err = process.communicate(inputdata, timeout=timeout)\n        except TimeoutExpired:\n            process.kill()\n            output, unused_err = process.communicate()\n            raise TimeoutExpired(process.args, timeout, output=output)\n        except:\n            process.kill()\n            process.wait()\n            raise\n        retcode = process.poll()\n        if retcode:\n            raise CalledProcessError(retcode, process.args, output=output)\n    return output\n    def __new__(cls, classname, bases, classDict):\n        timeout = classDict.get('__timeout__', 'NONE')\n        if timeout == 'NONE':\n            timeout = getattr(bases[0], '__timeout__', None)\n            if sysinfo.RUN_LEAKCHECKS and timeout is not None:\n                timeout *= 6\n        check_totalrefcount = _get_class_attr(classDict, bases, 'check_totalrefcount', True)\n        error_fatal = _get_class_attr(classDict, bases, 'error_fatal', True)\n        uses_handle_error = _get_class_attr(classDict, bases, 'uses_handle_error', True)\n        for key, value in list(classDict.items()):\n            if key.startswith('test') and callable(value):\n                classDict.pop(key)\n                value = _wrap_timeout(timeout, value)\n                error_fatal = getattr(value, 'error_fatal', error_fatal)\n                if error_fatal:\n                    value = errorhandler.wrap_error_fatal(value)\n                if uses_handle_error:\n                    value = errorhandler.wrap_restore_handle_error(value)\n                if check_totalrefcount and sysinfo.RUN_LEAKCHECKS:\n                    value = leakcheck.wrap_refcount(value)\n                classDict[key] = value\n        return type.__new__(cls, classname, bases, classDict)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-41419",
        "description": "[{'lang': 'en', 'value': 'An issue in Gevent before version 23.9.0 allows a remote attacker to escalate privileges via a crafted script to the WSGIServer component.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-56",
      "code": "def manage_action_containers(\n    request, action, o_type=None, o_id=None, conn=None, **kwargs\n):\n    \"\"\"\n    Handles many different actions on various objects.\n    @param action:      \"addnewcontainer\", (creates a new Project, Dataset,\n                        Screen), \"editname\", \"savename\", \"editdescription\",\n                        \"savedescription\",  (used as GET and POST for in-line\n                        editing),\n                        \"removefromshare\", (tree P/D/I moving etc)\n                        \"delete\", \"deletemany\"      (delete objects)\n                        \"remove\" (remove tag/comment from object)\n    @param o_type:      \"dataset\", \"project\", \"image\", \"screen\", \"plate\",\n                        \"acquisition\", \"well\",\"comment\", \"file\", \"tag\",\n                        \"tagset\",\"share\", \"sharecomment\"\n    \"\"\"\n    template = None\n    manager = None\n    if o_type in (\n        \"dataset\",\n        \"project\",\n        \"image\",\n        \"screen\",\n        \"plate\",\n        \"acquisition\",\n        \"well\",\n        \"comment\",\n        \"file\",\n        \"tag\",\n        \"tagset\",\n    ):\n        kw = {}\n        if o_type is not None and int(o_id) > 0:\n            o_id = int(o_id)\n            kw[str(o_type)] = o_id\n        try:\n            manager = BaseContainer(conn, **kw)\n        except AttributeError as x:\n            return handlerInternalError(request, x)\n    elif o_type in (\"share\", \"sharecomment\", \"chat\"):\n        manager = BaseShare(conn, o_id)\n    else:\n        manager = BaseContainer(conn)\n    form = None\n    if action == \"addnewcontainer\":\n        if not request.method == \"POST\":\n            return JsonResponse(\n                {\"Error\": \"Must use POST to create container\"}, status=405\n            )\n        form = ContainerForm(data=request.POST.copy())\n        if form.is_valid():\n            logger.debug(\"Create new in %s: %s\" % (o_type, str(form.cleaned_data)))\n            name = form.cleaned_data[\"name\"]\n            description = form.cleaned_data[\"description\"]\n            owner = form.cleaned_data[\"owner\"]\n            if o_type == \"project\" and hasattr(manager, o_type) and o_id > 0:\n                oid = manager.createDataset(name, description, owner=owner)\n            elif o_type == \"tagset\" and o_id > 0:\n                oid = manager.createTag(name, description, owner=owner)\n            elif request.POST.get(\"folder_type\") in (\n                \"project\",\n                \"screen\",\n                \"dataset\",\n                \"tag\",\n                \"tagset\",\n            ):\n                folder_type = request.POST.get(\"folder_type\")\n                if folder_type == \"dataset\":\n                    oid = manager.createDataset(\n                        name,\n                        description,\n                        owner=owner,\n                        img_ids=request.POST.getlist(\"image\", None),\n                    )\n                else:\n                    oid = conn.createContainer(\n                        folder_type, name, description, owner=owner\n                    )\n            else:\n                return HttpResponseServerError(\"Object does not exist\")\n            rdict = {\"bad\": \"false\", \"id\": oid}\n            return JsonResponse(rdict)\n        else:\n            d = dict()\n            for e in form.errors.items():\n                d.update({e[0]: unicode(e[1])})\n            rdict = {\"bad\": \"true\", \"errs\": d}\n            return JsonResponse(rdict)\n    elif action == \"add\":\n        template = \"webclient/public/share_form.html\"\n        experimenters = list(conn.getExperimenters())\n        experimenters.sort(key=lambda x: x.getOmeName().lower())\n        if o_type == \"share\":\n            img_ids = request.GET.getlist(\"image\", request.POST.getlist(\"image\"))\n            if request.method == \"GET\" and len(img_ids) == 0:\n                return HttpResponse(\"No images specified\")\n            images_to_share = list(conn.getObjects(\"Image\", img_ids))\n            if request.method == \"POST\":\n                form = BasketShareForm(\n                    initial={\"experimenters\": experimenters, \"images\": images_to_share},\n                    data=request.POST.copy(),\n                )\n                if form.is_valid():\n                    images = form.cleaned_data[\"image\"]\n                    message = form.cleaned_data[\"message\"]\n                    expiration = form.cleaned_data[\"expiration\"]\n                    members = form.cleaned_data[\"members\"]\n                    enable = form.cleaned_data[\"enable\"]\n                    host = \"%s?server=%i\" % (\n                        request.build_absolute_uri(\n                            reverse(\"load_template\", args=[\"public\"])\n                        ),\n                        int(conn.server_id),\n                    )\n                    shareId = manager.createShare(\n                        host, images, message, members, enable, expiration\n                    )\n                    return HttpResponse(\"shareId:%s\" % shareId)\n            else:\n                initial = {\n                    \"experimenters\": experimenters,\n                    \"images\": images_to_share,\n                    \"enable\": True,\n                    \"selected\": request.GET.getlist(\"image\"),\n                }\n                form = BasketShareForm(initial=initial)\n        template = \"webclient/public/share_form.html\"\n        context = {\"manager\": manager, \"form\": form}\n    elif action == \"edit\":\n        if o_id is None:\n            raise Http404(\"No share ID\")\n        if o_type == \"share\" and int(o_id) > 0:\n            template = \"webclient/public/share_form.html\"\n            manager.getMembers(o_id)\n            manager.getComments(o_id)\n            experimenters = list(conn.getExperimenters())\n            experimenters.sort(key=lambda x: x.getOmeName().lower())\n            initial = {\n                \"message\": manager.share.message,\n                \"expiration\": \"\",\n                \"shareMembers\": manager.membersInShare,\n                \"enable\": manager.share.active,\n                \"experimenters\": experimenters,\n            }\n            if manager.share.getExpireDate() is not None:\n                initial[\"expiration\"] = manager.share.getExpireDate().strftime(\n                    \"%Y-%m-%d\"\n                )\n            form = ShareForm(initial=initial)\n            context = {\"manager\": manager, \"form\": form}\n    elif action == \"save\":\n        if not request.method == \"POST\":\n            return HttpResponseRedirect(\n                reverse(\"manage_action_containers\", args=[\"edit\", o_type, o_id])\n            )\n        if o_type == \"share\":\n            experimenters = list(conn.getExperimenters())\n            experimenters.sort(key=lambda x: x.getOmeName().lower())\n            form = ShareForm(\n                initial={\"experimenters\": experimenters}, data=request.POST.copy()\n            )\n            if form.is_valid():\n                logger.debug(\"Update share: %s\" % (str(form.cleaned_data)))\n                message = form.cleaned_data[\"message\"]\n                expiration = form.cleaned_data[\"expiration\"]\n                members = form.cleaned_data[\"members\"]\n                enable = form.cleaned_data[\"enable\"]\n                host = \"%s?server=%i\" % (\n                    request.build_absolute_uri(\n                        reverse(\"load_template\", args=[\"public\"])\n                    ),\n                    int(conn.server_id),\n                )\n                manager.updateShareOrDiscussion(\n                    host, message, members, enable, expiration\n                )\n                r = \"enable\" if enable else \"disable\"\n                return HttpResponse(r)\n            else:\n                template = \"webclient/public/share_form.html\"\n                context = {\"share\": manager, \"form\": form}\n        else:\n            return HttpResponseServerError(\"Object does not exist\")\n    elif action == \"editname\":\n        if hasattr(manager, o_type) and o_id > 0:\n            obj = getattr(manager, o_type)\n            template = \"webclient/ajax_form/container_form_ajax.html\"\n            if o_type == \"tag\":\n                txtValue = obj.textValue\n            else:\n                txtValue = obj.getName()\n            form = ContainerNameForm(initial={\"name\": txtValue})\n            context = {\"manager\": manager, \"form\": form}\n        else:\n            return HttpResponseServerError(\"Object does not exist\")\n    elif action == \"savename\":\n        if not request.method == \"POST\":\n            return HttpResponseRedirect(\n                reverse(\"manage_action_containers\", args=[\"edit\", o_type, o_id])\n            )\n        if hasattr(manager, o_type) and o_id > 0:\n            form = ContainerNameForm(data=request.POST.copy())\n            if form.is_valid():\n                logger.debug(\"Update name form:\" + str(form.cleaned_data))\n                name = form.cleaned_data[\"name\"]\n                rdict = {\"bad\": \"false\", \"o_type\": o_type}\n                manager.updateName(o_type, name)\n                return JsonResponse(rdict)\n            else:\n                d = dict()\n                for e in form.errors.items():\n                    d.update({e[0]: unicode(e[1])})\n                rdict = {\"bad\": \"true\", \"errs\": d}\n                return JsonResponse(rdict)\n        else:\n            return HttpResponseServerError(\"Object does not exist\")\n    elif action == \"editdescription\":\n        if hasattr(manager, o_type) and o_id > 0:\n            obj = getattr(manager, o_type)\n            template = \"webclient/ajax_form/container_form_ajax.html\"\n            form = ContainerDescriptionForm(initial={\"description\": obj.description})\n            context = {\"manager\": manager, \"form\": form}\n        else:\n            return HttpResponseServerError(\"Object does not exist\")\n    elif action == \"savedescription\":\n        if not request.method == \"POST\":\n            return HttpResponseServerError(\n                \"Action '%s' on the '%s' id:%s cannot be complited\"\n                % (action, o_type, o_id)\n            )\n        if hasattr(manager, o_type) and o_id > 0:\n            form = ContainerDescriptionForm(data=request.POST.copy())\n            if form.is_valid():\n                logger.debug(\"Update name form:\" + str(form.cleaned_data))\n                description = form.cleaned_data[\"description\"]\n                manager.updateDescription(o_type, description)\n                rdict = {\"bad\": \"false\"}\n                return JsonResponse(rdict)\n            else:\n                d = dict()\n                for e in form.errors.items():\n                    d.update({e[0]: unicode(e[1])})\n                rdict = {\"bad\": \"true\", \"errs\": d}\n                return JsonResponse(rdict)\n        else:\n            return HttpResponseServerError(\"Object does not exist\")\n    elif action == \"remove\":\n        parents = request.POST[\"parent\"]\n        try:\n            manager.remove(parents.split(\"|\"))\n        except Exception as x:\n            logger.error(traceback.format_exc())\n            rdict = {\"bad\": \"true\", \"errs\": str(x)}\n            return JsonResponse(rdict)\n        rdict = {\"bad\": \"false\"}\n        return JsonResponse(rdict)\n    elif action == \"removefromshare\":\n        image_id = request.POST.get(\"source\")\n        try:\n            manager.removeImage(image_id)\n        except Exception as x:\n            logger.error(traceback.format_exc())\n            rdict = {\"bad\": \"true\", \"errs\": str(x)}\n            return JsonResponse(rdict)\n        rdict = {\"bad\": \"false\"}\n        return JsonResponse(rdict)\n    elif action == \"delete\":\n        child = toBoolean(request.POST.get(\"child\"))\n        anns = toBoolean(request.POST.get(\"anns\"))\n        try:\n            handle = manager.deleteItem(child, anns)\n            request.session[\"callback\"][str(handle)] = {\n                \"job_type\": \"delete\",\n                \"delmany\": False,\n                \"did\": o_id,\n                \"dtype\": o_type,\n                \"status\": \"in progress\",\n                \"error\": 0,\n                \"dreport\": _formatReport(handle),\n                \"start_time\": datetime.datetime.now(),\n            }\n            request.session.modified = True\n        except Exception as x:\n            logger.error(\n                \"Failed to delete: %r\" % {\"did\": o_id, \"dtype\": o_type}, exc_info=True\n            )\n            rdict = {\"bad\": \"true\", \"errs\": str(x)}\n        else:\n            rdict = {\"bad\": \"false\"}\n        return JsonResponse(rdict)\n    elif action == \"deletemany\":\n        object_ids = {\n            \"Image\": request.POST.getlist(\"image\"),\n            \"Dataset\": request.POST.getlist(\"dataset\"),\n            \"Project\": request.POST.getlist(\"project\"),\n            \"Annotation\": request.POST.getlist(\"tag\"),\n            \"Screen\": request.POST.getlist(\"screen\"),\n            \"Plate\": request.POST.getlist(\"plate\"),\n            \"Well\": request.POST.getlist(\"well\"),\n            \"PlateAcquisition\": request.POST.getlist(\"acquisition\"),\n        }\n        child = toBoolean(request.POST.get(\"child\"))\n        anns = toBoolean(request.POST.get(\"anns\"))\n        logger.debug(\n            \"Delete many: child? %s anns? %s object_ids %s\" % (child, anns, object_ids)\n        )\n        try:\n            for key, ids in object_ids.items():\n                if ids is not None and len(ids) > 0:\n                    handle = manager.deleteObjects(key, ids, child, anns)\n                    if key == \"PlateAcquisition\":\n                        key = \"Plate Run\"\n                    dMap = {\n                        \"job_type\": \"delete\",\n                        \"start_time\": datetime.datetime.now(),\n                        \"status\": \"in progress\",\n                        \"error\": 0,\n                        \"dreport\": _formatReport(handle),\n                        \"dtype\": key,\n                    }\n                    if len(ids) > 1:\n                        dMap[\"delmany\"] = len(ids)\n                        dMap[\"did\"] = ids\n                    else:\n                        dMap[\"delmany\"] = False\n                        dMap[\"did\"] = ids[0]\n                    request.session[\"callback\"][str(handle)] = dMap\n            request.session.modified = True\n        except Exception:\n            logger.error(\n                \"Failed to delete: %r\" % {\"did\": ids, \"dtype\": key}, exc_info=True\n            )\n            raise\n        else:\n            rdict = {\"bad\": \"false\"}\n        return JsonResponse(rdict)\n    context[\"template\"] = template\n    return context\ndef _table_query(request, fileid, conn=None, query=None, lazy=False, **kwargs):\n    \"\"\"\n    Query a table specified by fileid\n    Returns a dictionary with query result if successful, error information\n    otherwise\n    @param request:     http request; querystring must contain key 'query'\n                        with query to be executed, or '*' to retrieve all rows.\n                        If query is in the format word-number, e.g. \"Well-7\",\n                        if will be run as (word==number), e.g. \"(Well==7)\".\n                        This is supported to allow more readable query strings.\n    @param fileid:      Numeric identifier of file containing the table\n    @param query:       The table query. If None, use request.GET.get('query')\n                        E.g. '*' to return all rows.\n                        If in the form 'colname-1', query will be (colname==1)\n    @param lazy:        If True, instead of returning a 'rows' list,\n                        'lazy_rows' will be a generator.\n                        Each gen.next() will return a list of row data\n                        AND 'table' returned MUST be closed.\n    @param conn:        L{omero.gateway.BlitzGateway}\n    @param **kwargs:    offset, limit\n    @return:            A dictionary with key 'error' with an error message\n                        or with key 'data' containing a dictionary with keys\n                        'columns' (an array of column names) and 'rows'\n                        (an array of rows, each an array of values)\n    \"\"\"\n    if query is None:\n        query = request.GET.get(\"query\")\n    if not query:\n        return dict(error=\"Must specify query parameter, use * to retrieve all\")\n    col_names = request.GET.getlist(\"col_names\")\n    ctx = conn.createServiceOptsDict()\n    ctx.setOmeroGroup(\"-1\")\n    r = conn.getSharedResources()\n    t = r.openTable(omero.model.OriginalFileI(fileid), ctx)\n    if not t:\n        return dict(error=\"Table %s not found\" % fileid)\n    try:\n        cols = t.getHeaders()\n        col_indices = range(len(cols))\n        if col_names:\n            enumerated_columns = (\n                [(i, j) for (i, j) in enumerate(cols) if j.name in col_names]\n                if col_names\n                else [(i, j) for (i, j) in enumerate(cols)]\n            )\n            cols = []\n            col_indices = []\n            for col_name in col_names:\n                for (i, j) in enumerated_columns:\n                    if col_name == j.name:\n                        col_indices.append(i)\n                        cols.append(j)\n                        break\n        rows = t.getNumberOfRows()\n        offset = kwargs.get(\"offset\", 0)\n        limit = kwargs.get(\"limit\", None)\n        if not offset:\n            offset = int(request.GET.get(\"offset\", 0))\n        if not limit:\n            limit = (\n                int(request.GET.get(\"limit\"))\n                if request.GET.get(\"limit\") is not None\n                else None\n            )\n        range_start = offset\n        range_size = kwargs.get(\"limit\", rows)\n        range_end = min(rows, range_start + range_size)\n        if query == \"*\":\n            hits = range(range_start, range_end)\n            totalCount = rows\n        else:\n            match = re.match(r\"^(\\w+)-(\\d+)\", query)\n            if match:\n                query = \"(%s==%s)\" % (match.group(1), match.group(2))\n            try:\n                logger.info(query)\n                hits = t.getWhereList(query, None, 0, rows, 1)\n                totalCount = len(hits)\n                hits = hits[range_start:range_end]\n            except Exception:\n                return dict(error=\"Error executing query: %s\" % query)\n        def row_generator(table, h):\n            idx = 0\n            batch = 1000\n            while idx < len(h):\n                batch = min(batch, len(h) - idx)\n                res = table.slice(col_indices, h[idx : idx + batch])\n                idx += batch\n                yield [\n                    [col.values[row] for col in res.columns]\n                    for row in range(0, len(res.rowNumbers))\n                ]\n        row_gen = row_generator(t, hits)\n        rsp_data = {\n            \"data\": {\n                \"column_types\": [col.__class__.__name__ for col in cols],\n                \"columns\": [col.name for col in cols],\n            },\n            \"meta\": {\n                \"rowCount\": rows,\n                \"totalCount\": totalCount,\n                \"limit\": limit,\n                \"offset\": offset,\n            },\n        }\n        if not lazy:\n            row_data = []\n            for rows in list(row_gen):\n                row_data.extend(rows)\n            rsp_data[\"data\"][\"rows\"] = row_data\n        else:\n            rsp_data[\"data\"][\"lazy_rows\"] = row_gen\n            rsp_data[\"table\"] = t\n        return rsp_data\n    finally:\n        if not lazy:\n            t.close()\n    def prepare_context(self, request, context, *args, **kwargs):\n        \"\"\"\n        This allows templates to access the current eventContext and user from\n        the L{omero.gateway.BlitzGateway}.\n        E.g. <h1>{{ ome.user.getFullName }}</h1>\n        If these are not required by the template, then they will not need to\n        be loaded by the Blitz Gateway.\n        The results are cached by Blitz Gateway, so repeated calls have no\n        additional cost.\n        We also process some values from settings and add these to the\n        context.\n        \"\"\"\n        if \"conn\" not in kwargs:\n            return\n        conn = kwargs[\"conn\"]\n        context[\"omero\"] = {\n            \"constants\": {\n                \"NSCOMPANIONFILE\": constants.namespaces.NSCOMPANIONFILE,\n                \"ORIGINALMETADATA\": constants.annotation.file.ORIGINALMETADATA,\n                \"NSCLIENTMAPANNOTATION\": constants.metadata.NSCLIENTMAPANNOTATION,\n            }\n        }\n        context.setdefault(\"ome\", {})\n        public_user = omeroweb.decorators.is_public_user(request)\n        if public_user is not None:\n            context[\"ome\"][\"is_public_user\"] = public_user\n        context[\"ome\"][\"eventContext\"] = eventContextMarshal(conn.getEventContext())\n        context[\"ome\"][\"user\"] = conn.getUser\n        context[\"ome\"][\"user_id\"] = request.session.get(\"user_id\", conn.getUserId())\n        context[\"ome\"][\"group_id\"] = request.session.get(\"group_id\", None)\n        context[\"ome\"][\"active_group\"] = request.session.get(\n            \"active_group\", conn.getEventContext().groupId\n        )\n        context[\"global_search_form\"] = GlobalSearchForm()\n        context[\"ome\"][\"can_create\"] = request.session.get(\"can_create\", True)\n        if request.session.get(\"server_settings\"):\n            context[\"ome\"][\"email\"] = request.session.get(\"server_settings\").get(\n                \"email\", False\n            )\n            if request.session.get(\"server_settings\").get(\"ui\"):\n                context.setdefault(\"ui\", {\"tree\": {}})\n                context[\"ui\"][\"orphans\"] = (\n                    request.session.get(\"server_settings\")\n                    .get(\"ui\", {})\n                    .get(\"tree\", {})\n                    .get(\"orphans\")\n                )\n                context[\"ui\"][\"dropdown_menu\"] = (\n                    request.session.get(\"server_settings\")\n                    .get(\"ui\", {})\n                    .get(\"menu\", {})\n                    .get(\"dropdown\")\n                )\n                context[\"ui\"][\"tree\"][\"type_order\"] = (\n                    request.session.get(\"server_settings\")\n                    .get(\"ui\", {})\n                    .get(\"tree\", {})\n                    .get(\"type_order\")\n                )\n        self.load_settings(request, context, conn)\n    def __init__(self, *args, **kwargs):\n        super(BasketShareForm, self).__init__(*args, **kwargs)\n        try:\n            self.fields[\"image\"] = GroupModelMultipleChoiceField(\n                queryset=kwargs[\"initial\"][\"images\"],\n                initial=kwargs[\"initial\"][\"selected\"],\n                widget=forms.SelectMultiple(attrs={\"size\": 10}),\n            )\n        except Exception:\n            self.fields[\"image\"] = GroupModelMultipleChoiceField(\n                queryset=kwargs[\"initial\"][\"images\"],\n                widget=forms.SelectMultiple(attrs={\"size\": 10}),\n            )\n    def prepare_context(self, request, context, *args, **kwargs):\n        \"\"\" Hook for adding additional data to the context dict \"\"\"\n        pass",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-21376",
        "description": "[{'lang': 'en', 'value': 'OMERO.web is open source Django-based software for managing microscopy imaging. OMERO.web before version 5.9.0 loads various information about the current user such as their id, name and the groups they are in, and these are available on the main webclient pages. This represents an information exposure vulnerability. Some additional information being loaded is not used by the webclient and is being removed in this release. This is fixed in version 5.9.0.'}]",
        "cwe_number": 200
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-57",
      "code": "    def handle(self, request, data):\n        endpoint = data.get('region', None) or settings.OPENSTACK_KEYSTONE_URL\n        region_name = dict(self.fields['region'].choices)[endpoint]\n        request.session['region_endpoint'] = endpoint\n        request.session['region_name'] = region_name\n        redirect_to = request.REQUEST.get(REDIRECT_FIELD_NAME, \"\")\n        if data.get('tenant', None):\n            try:\n                token = api.token_create(request,\n                                         data.get('tenant'),\n                                         data['username'],\n                                         data['password'])\n                tenants = api.tenant_list_for_token(request, token.id)\n            except:\n                msg = _('Unable to authenticate for that project.')\n                exceptions.handle(request,\n                                  message=msg,\n                                  escalate=True)\n            _set_session_data(request, token)\n            user = users.get_user_from_request(request)\n            redirect = redirect_to or base.Horizon.get_user_home(user)\n            return shortcuts.redirect(redirect)\n        elif data.get('username', None):\n            try:\n                unscoped_token = api.token_create(request,\n                                                  '',\n                                                  data['username'],\n                                                  data['password'])\n            except keystone_exceptions.Unauthorized:\n                exceptions.handle(request,\n                                  _('Invalid user name or password.'))\n            except:\n                request.session.clear()\n                exceptions.handle(request,\n                                  message=_(\"An error occurred authenticating.\"\n                                            \" Please try again later.\"),\n                                  escalate=True)\n            request.session['unscoped_token'] = unscoped_token.id\n            request.user.username = data['username']\n            try:\n                tenants = api.tenant_list_for_token(request, unscoped_token.id)\n            except:\n                exceptions.handle(request)\n                tenants = []\n            if not tenants:\n                messages.error(request,\n                               _('You are not authorized for any projects.') %\n                                {\"user\": data['username']},\n                               extra_tags=\"login\")\n                return\n            while tenants:\n                tenant = tenants.pop()\n                try:\n                    token = api.token_create_scoped(request,\n                                                    tenant.id,\n                                                    unscoped_token.id)\n                    break\n                except:\n                    exceptions.handle(request, ignore=True)\n                    token = None\n            if token is None:\n                raise exceptions.NotAuthorized(\n                    _(\"You are not authorized for any available projects.\"))\n            _set_session_data(request, token)\n            user = users.get_user_from_request(request)\n        redirect = redirect_to or base.Horizon.get_user_home(user)\n        return shortcuts.redirect(redirect)\ndef handle(request, message=None, redirect=None, ignore=False, escalate=False):\n    \"\"\" Centralized error handling for Horizon.\n    Because Horizon consumes so many different APIs with completely\n    different ``Exception`` types, it's necessary to have a centralized\n    place for handling exceptions which may be raised.\n    Exceptions are roughly divided into 3 types:\n       problems. These result in being logged out and sent to the login screen.\n       located via the API. These generally result in a user-facing error\n       message, but are otherwise returned to the normal code flow. Optionally\n       a redirect value may be passed to the error handler so users are\n       returned to a different view than the one requested in addition to the\n       error message.\n       but drop directly back to the regular code flow.\n    All other exceptions bubble the stack as normal unless the ``ignore``\n    argument is passed in as ``True``, in which case only unrecognized\n    errors are bubbled.\n    If the exception is not re-raised, an appropriate wrapper exception\n    class indicating the type of exception that was encountered will be\n    returned.\n    \"\"\"\n    exc_type, exc_value, exc_traceback = sys.exc_info()\n    handled = issubclass(exc_type, HandledException)\n    wrap = False\n    if handled:\n        exc_type, exc_value, exc_traceback = exc_value.wrapped\n        wrap = True\n    if issubclass(exc_type, HorizonException):\n        message = exc_value\n    elif message and \"%(exc)s\" in message:\n        message = message % {\"exc\": exc_value}\n    if issubclass(exc_type, UNAUTHORIZED):\n        if ignore:\n            return NotAuthorized\n        request.session.clear()\n        if not handled:\n            LOG.debug(\"Unauthorized: %s\" % exc_value)\n            fallback = _(\"Unauthorized. Please try logging in again.\")\n            messages.error(request, message or fallback, extra_tags=\"login\")\n        raise NotAuthorized\n    if issubclass(exc_type, NOT_FOUND):\n        wrap = True\n        if not ignore and not handled:\n            LOG.debug(\"Not Found: %s\" % exc_value)\n            messages.error(request, message or exc_value)\n        if redirect:\n            raise Http302(redirect)\n        if not escalate:\n            return NotFound\n    if issubclass(exc_type, RECOVERABLE):\n        wrap = True\n        if not ignore and not handled:\n            LOG.debug(\"Recoverable error: %s\" % exc_value)\n            messages.error(request, message or exc_value)\n        if redirect:\n            raise Http302(redirect)\n        if not escalate:\n            return RecoverableError\n    if wrap:\n        raise HandledException([exc_type, exc_value, exc_traceback])\n    raise exc_type, exc_value, exc_traceback\ndef get_user_from_request(request):\n    \"\"\" Checks the current session and returns a :class:`~horizon.users.User`.\n    If the session contains user data the User will be treated as\n    authenticated and the :class:`~horizon.users.User` will have all\n    its attributes set.\n    If not, the :class:`~horizon.users.User` will have no attributes set.\n    If the session contains invalid data,\n    :exc:`~horizon.exceptions.NotAuthorized` will be raised.\n    \"\"\"\n    if 'user_id' not in request.session:\n        return User()\n    try:\n        return User(id=request.session['user_id'],\n                    token=request.session['token'],\n                    user=request.session['user_name'],\n                    tenant_id=request.session['tenant_id'],\n                    tenant_name=request.session['tenant'],\n                    service_catalog=request.session['serviceCatalog'],\n                    roles=request.session['roles'],\n                    request=request)\n    except KeyError:\n        LOG.exception(\"Error while creating User from session.\")\n        request.session.clear()\n        raise exceptions.NotAuthorized(_(\"Your session has expired. \"\n                                         \"Please log in again.\"))\ndef logout(request):\n    \"\"\" Clears the session and logs the current user out. \"\"\"\n    request.session.clear()\n    return shortcuts.redirect('splash')",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2012-2144",
        "description": "[{'lang': 'en', 'value': 'Session fixation vulnerability in OpenStack Dashboard (Horizon) folsom-1 and 2012.1 allows remote attackers to hijack web sessions via the sessionid cookie.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-58",
      "code": "    def __init__(self, wsgidav_app, next_app, config):\n        super().__init__(wsgidav_app, next_app, config)\n        self.dir_config = util.get_dict_value(config, \"dir_browser\", as_dict=True)\n        self.mount_path = config.get(\"mount_path\") or \"\"\n        htdocs_path = self.dir_config.get(\"htdocs_path\")\n        if htdocs_path:\n            self.htdocs_path = os.path.realpath(htdocs_path)\n        else:\n            self.htdocs_path = os.path.join(os.path.dirname(__file__), \"htdocs\")\n        if not os.path.isdir(self.htdocs_path):\n            raise ValueError(\n                \"Invalid dir_browser htdocs_path {!r}\".format(self.htdocs_path)\n            )\n        self.wsgidav_app.add_provider(ASSET_SHARE, self.htdocs_path, readonly=True)\n        config.get(\"simple_dc\", {}).get(\"user_mapping\", {}).setdefault(\n            ASSET_SHARE, True\n        )\n        templateLoader = FileSystemLoader(searchpath=self.htdocs_path)\n        templateEnv = Environment(loader=templateLoader)\n        self.template = templateEnv.get_template(\"template.html\")",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-41905",
        "description": "[{'lang': 'en', 'value': 'WsgiDAV is a generic and extendable WebDAV server based on WSGI. Implementations using this library with directory browsing enabled may be susceptible to Cross Site Scripting (XSS) attacks. This issue has been patched, users can upgrade to version 4.1.0. As a workaround, set `dir_browser.enable = False` in the configuration.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-59",
      "code": "def download(url, location):\n    \"\"\"\n    Download files to the specified location.\n    :param url: The file URL.\n    :type url: str\n    :param location: The absolute path to where the downloaded\n        file is to be stored.\n    :type location: str\n    \"\"\"\n    request = urllib2.urlopen(url)\n    try:\n        content = request.read()\n        fp = open(location, 'w+')\n        try:\n            fp.write(content)\n        finally:\n            fp.close()\n    finally:\n        request.close()\ndef update_server_key(conf):\n    \"\"\"\n    Download the server's RSA key and store in the location\n    specified in the configuration.\n    :param conf: The consumer configuration object.\n    :type conf: dict\n    \"\"\"\n    host = conf['server']['host']\n    location = conf['server']['rsa_pub']\n    url = 'https://%s/pulp/static/rsa_pub.key' % host\n    try:\n        os.makedirs(os.path.dirname(location))\n    except OSError, e:\n        if e.errno != errno.EEXIST:\n            raise\n    download(url, location)\n    def register(self, **kwargs):\n        consumer_id = kwargs['consumer-id']\n        existing_consumer = load_consumer_id(self.context)\n        if existing_consumer:\n            m = _('This system has already been registered as a consumer. Please '\n                  'use the unregister command to remove the consumer before attempting '\n                  'to re-register.')\n            self.prompt.render_failure_message(m)\n            return\n        name = kwargs.get('display-name', consumer_id)\n        description = kwargs.get('description')\n        notes = kwargs.get('note')\n        if notes:\n            notes = args_to_notes_dict(notes, include_none=False)\n        id_cert_dir = self.context.config['filesystem']['id_cert_dir']\n        if not os.access(id_cert_dir, os.W_OK):\n            msg = _(\"Write permission is required for %(d)s to perform this operation.\")\n            self.prompt.render_failure_message(msg % {'d': id_cert_dir})\n            return exceptions.CODE_PERMISSIONS_EXCEPTION\n        path = self.context.config['authentication']['rsa_key']\n        key = RSA.gen_key(2048, 65535, no_passphrase_callback)\n        key.save_key(path, None)\n        path = self.context.config['authentication']['rsa_pub']\n        key.save_pub_key(path)\n        fp = open(path)\n        try:\n            rsa_pub = fp.read()\n        finally:\n            fp.close()\n        reply = self.context.server.consumer.register(\n            consumer_id,\n            name=name,\n            description=description,\n            notes=notes,\n            rsa_pub=rsa_pub)\n        certificate = reply.response_body['certificate']\n        id_cert_name = self.context.config['filesystem']['id_cert_filename']\n        cert_filename = os.path.join(id_cert_dir, id_cert_name)\n        fp = open(cert_filename, 'w')\n        try:\n            fp.write(certificate)\n        finally:\n            fp.close()\n        try:\n            update_server_key(self.context.config)\n        except Exception, e:\n            msg = _('Download server RSA key failed [%(e)s]' % {'e': e})\n            self.prompt.render_failure_message(msg)\n        self.prompt.render_success_message('Consumer [%s] successfully registered' % consumer_id)\n    def update(self, **kwargs):\n        consumer_id = load_consumer_id(self.context)\n        if not consumer_id:\n            self.prompt.render_failure_message(\"This consumer is not registered to the Pulp server.\")\n            return\n        delta = dict([(k, v) for k, v in kwargs.items() if v is not None])\n        if 'note' in delta.keys():\n            if delta['note']:\n                delta['notes'] = args_to_notes_dict(kwargs['note'], include_none=False)\n            delta.pop('note')\n        key = 'display-name'\n        if key in delta:\n            v = delta.pop(key)\n            key = key.replace('-', '_')\n            delta[key] = v\n        if kwargs.get(OPTION_EXCHANGE_KEYS.keyword):\n            path = self.context.config['authentication']['rsa_pub']\n            fp = open(path)\n            try:\n                delta['rsa_pub'] = fp.read()\n            finally:\n                fp.close()\n        try:\n            self.context.server.consumer.update(consumer_id, delta)\n            self.prompt.render_success_message('Consumer [%s] successfully updated' % consumer_id)\n            if not kwargs.get(OPTION_EXCHANGE_KEYS.keyword):\n                return\n            try:\n                update_server_key(self.context.config)\n            except Exception, e:\n                msg = _('Download server RSA key failed [%(e)s]' % {'e': e})\n                self.prompt.render_failure_message(msg)\n        except NotFoundException:\n            self.prompt.write('Consumer [%s] does not exist on the server' % consumer_id, tag='not-found')\n    def DELETE(self, path, body=None, log_request_body=True):\n        return self._request('DELETE', path, body=body, log_request_body=log_request_body)\n    def GET(self, path, queries=()):\n        return self._request('GET', path, queries)\n    def HEAD(self, path):\n        return self._request('HEAD', path)\n    def POST(self, path, body=None, ensure_encoding=True, log_request_body=True):\n        return self._request('POST', path, body=body, ensure_encoding=ensure_encoding,\n                             log_request_body=log_request_body)\n    def PUT(self, path, body, ensure_encoding=True, log_request_body=True):\n        return self._request('PUT', path, body=body, ensure_encoding=ensure_encoding,\n                             log_request_body=log_request_body)\n    def _request(self, method, path, queries=(), body=None, ensure_encoding=True,\n                 log_request_body=True):\n        \"\"\"\n        make a HTTP request to the pulp server and return the response\n        :param method:  name of an HTTP method such as GET, POST, PUT, HEAD\n                        or DELETE\n        :type  method:  basestring\n        :param path:    URL for this request\n        :type  path:    basestring\n        :param queries: mapping object or a sequence of 2-element tuples,\n                        in either case representing key-value pairs to be used\n                        as query parameters on the URL.\n        :type  queries: mapping object or sequence of 2-element tuples\n        :param body:    Data structure that will be JSON serialized and send as\n                        the request's body.\n        :type  body:    Anything that is JSON-serializable.\n        :param ensure_encoding: toggle proper string encoding for the body\n        :type ensure_encoding: bool\n        :param log_request_body: Toggle logging of the request body, defaults to true\n        :type log_request_body: bool\n        :return:    Response object\n        :rtype:     pulp.bindings.responses.Response\n        :raises:    ConnectionException or one of the RequestExceptions\n                    (depending on response codes) in case of unsuccessful\n                    request\n        \"\"\"\n        url = self._build_url(path, queries)\n        if ensure_encoding:\n            body = self._process_body(body)\n        if not isinstance(body, (NoneType, basestring)):\n            body = json.dumps(body)\n        self.log.debug('sending %s request to %s' % (method, url))\n        response_code, response_body = self.server_wrapper.request(method, url, body)\n        if self.api_responses_logger:\n            if log_request_body:\n                self.api_responses_logger.info(\n                    '%s request to %s with parameters %s' % (method, url, body))\n            else:\n                self.api_responses_logger.info(\n                    '%s request to %s' % (method, url))\n            self.api_responses_logger.info(\"Response status : %s \\n\" % response_code)\n            self.api_responses_logger.info(\n                \"Response body :\\n %s\\n\" % json.dumps(response_body, indent=2))\n        if response_code >= 300:\n            self._handle_exceptions(response_code, response_body)\n        elif response_code == 200 or response_code == 201:\n            body = response_body\n        elif response_code == 202:\n            if isinstance(response_body, list):\n                body = [Task(t) for t in response_body]\n            else:\n                body = Task(response_body)\n        return Response(response_code, body)\n    def _build_url(self, path, queries=()):\n        \"\"\"\n        Takes a relative path and query parameters, combines them with the\n        base path, and returns the result. Handles utf-8 encoding as necessary.\n        :param path:    relative path for this request, relative to\n                        self.base_prefix. NOTE: if this parameter starts with a\n                        leading '/', this method will strip it and treat it as\n                        relative. That is not a standards-compliant way to\n                        combine path segments, so be aware.\n        :type  path:    basestring\n        :param queries: mapping object or a sequence of 2-element tuples,\n                        in either case representing key-value pairs to be used\n                        as query parameters on the URL.\n        :type  queries: mapping object or sequence of 2-element tuples\n        :return:    path that is a composite of self.path_prefix, path, and\n                    queries. May be relative or absolute depending on the nature\n                    of self.path_prefix\n        \"\"\"\n        if not path.startswith(self.path_prefix):\n            if path.startswith('/'):\n                path = path[1:]\n            path = '/'.join((self.path_prefix, path))\n        try:\n            path = urllib.quote(str(path))\n        except UnicodeEncodeError:\n            path = urllib.quote(path.encode('utf-8'))\n        except UnicodeDecodeError:\n            path = urllib.quote(path.decode('utf-8'))\n        queries = urllib.urlencode(queries)\n        if queries:\n            path = '?'.join((path, queries))\n        return path",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2015-5263",
        "description": "[{'lang': 'en', 'value': \"pulp-consumer-client 2.4.0 through 2.6.3 does not check the server's TLS certificate signatures when retrieving the server's public key upon registration.\"}]",
        "cwe_number": 295
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-60",
      "code": "    def debug_decisions(self, text):\n        \"\"\"\n        Classifies candidate periods as sentence breaks, yielding a dict for\n        each that may be used to understand why the decision was made.\n        See format_debug_decision() to help make this output readable.\n        \"\"\"\n        for match in self._lang_vars.period_context_re().finditer(text):\n            decision_text = match.group() + match.group(\"after_tok\")\n            tokens = self._tokenize_words(decision_text)\n            tokens = list(self._annotate_first_pass(tokens))\n            while tokens and not tokens[0].tok.endswith(self._lang_vars.sent_end_chars):\n                tokens.pop(0)\n            yield {\n                \"period_index\": match.end() - 1,\n                \"text\": decision_text,\n                \"type1\": tokens[0].type,\n                \"type2\": tokens[1].type,\n                \"type1_in_abbrs\": bool(tokens[0].abbr),\n                \"type1_is_initial\": bool(tokens[0].is_initial),\n                \"type2_is_sent_starter\": tokens[1].type_no_sentperiod\n                in self._params.sent_starters,\n                \"type2_ortho_heuristic\": self._ortho_heuristic(tokens[1]),\n                \"type2_ortho_contexts\": set(\n                    self._params._debug_ortho_context(tokens[1].type_no_sentperiod)\n                ),\n                \"collocation\": (\n                    tokens[0].type_no_sentperiod,\n                    tokens[1].type_no_sentperiod,\n                )\n                in self._params.collocations,\n                \"reason\": self._second_pass_annotation(tokens[0], tokens[1])\n                or REASON_DEFAULT_DECISION,\n                \"break_decision\": tokens[0].sentbreak,\n            }\n    def _slices_from_text(self, text):\n        last_break = 0\n        for match in self._lang_vars.period_context_re().finditer(text):\n            context = match.group() + match.group(\"after_tok\")\n            if self.text_contains_sentbreak(context):\n                yield slice(last_break, match.end())\n                if match.group(\"next_tok\"):\n                    last_break = match.start(\"next_tok\")\n                else:\n                    last_break = match.end()\n        yield slice(last_break, len(text.rstrip()))",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-43854",
        "description": "[{'lang': 'en', 'value': 'NLTK (Natural Language Toolkit) is a suite of open source Python modules, data sets, and tutorials supporting research and development in Natural Language Processing. Versions prior to 3.6.5 are vulnerable to regular expression denial of service (ReDoS) attacks. The vulnerability is present in PunktSentenceTokenizer, sent_tokenize and word_tokenize. Any users of this class, or these two functions, are vulnerable to the ReDoS attack. In short, a specifically crafted long input to any of these vulnerable functions will cause them to take a significant amount of execution time. If your program relies on any of the vulnerable functions for tokenizing unpredictable user input, then we would strongly recommend upgrading to a version of NLTK without the vulnerability. For users unable to upgrade the execution time can be bounded by limiting the maximum length of an input to any of the vulnerable functions. Our recommendation is to implement such a limit.'}]",
        "cwe_number": 400
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-61",
      "code": "    def emit(self, s, depth, reflow=True):\n        if reflow:\n            lines = reflow_lines(s, depth)\n        else:\n            lines = [s]\n        for line in lines:\n            line = (\" \" * TABSIZE * depth) + line + \"\\n\"\n            self.file.write(line)\n    def visitField(self, field, name, sum=None, prod=None, depth=0):\n        ctype = get_c_type(field.type)\n        if field.opt:\n            check = \"exists_not_none(obj, &PyId_%s)\" % (field.name,)\n        else:\n            check = \"_PyObject_HasAttrId(obj, &PyId_%s)\" % (field.name,)\n        self.emit(\"if (%s) {\" % (check,), depth, reflow=False)\n        self.emit(\"int res;\", depth+1)\n        if field.seq:\n            self.emit(\"Py_ssize_t len;\", depth+1)\n            self.emit(\"Py_ssize_t i;\", depth+1)\n        self.emit(\"tmp = _PyObject_GetAttrId(obj, &PyId_%s);\" % field.name, depth+1)\n        self.emit(\"if (tmp == NULL) goto failed;\", depth+1)\n        if field.seq:\n            self.emit(\"if (!PyList_Check(tmp)) {\", depth+1)\n            self.emit(\"PyErr_Format(PyExc_TypeError, \\\"%s field \\\\\\\"%s\\\\\\\" must \"\n                      \"be a list, not a %%.200s\\\", tmp->ob_type->tp_name);\" %\n                      (name, field.name),\n                      depth+2, reflow=False)\n            self.emit(\"goto failed;\", depth+2)\n            self.emit(\"}\", depth+1)\n            self.emit(\"len = PyList_GET_SIZE(tmp);\", depth+1)\n            if self.isSimpleType(field):\n                self.emit(\"%s = _Ta3_asdl_int_seq_new(len, arena);\" % field.name, depth+1)\n            else:\n                self.emit(\"%s = _Ta3_asdl_seq_new(len, arena);\" % field.name, depth+1)\n            self.emit(\"if (%s == NULL) goto failed;\" % field.name, depth+1)\n            self.emit(\"for (i = 0; i < len; i++) {\", depth+1)\n            self.emit(\"%s value;\" % ctype, depth+2)\n            self.emit(\"res = obj2ast_%s(PyList_GET_ITEM(tmp, i), &value, arena);\" %\n                      field.type, depth+2, reflow=False)\n            self.emit(\"if (res != 0) goto failed;\", depth+2)\n            self.emit(\"if (len != PyList_GET_SIZE(tmp)) {\", depth+2)\n            self.emit(\"PyErr_SetString(PyExc_RuntimeError, \\\"%s field \\\\\\\"%s\\\\\\\" \"\n                      \"changed size during iteration\\\");\" %\n                      (name, field.name),\n                      depth+3, reflow=False)\n            self.emit(\"goto failed;\", depth+3)\n            self.emit(\"}\", depth+2)\n            self.emit(\"asdl_seq_SET(%s, i, value);\" % field.name, depth+2)\n            self.emit(\"}\", depth+1)\n        else:\n            self.emit(\"res = obj2ast_%s(tmp, &%s, arena);\" %\n                      (field.type, field.name), depth+1)\n            self.emit(\"if (res != 0) goto failed;\", depth+1)\n        self.emit(\"Py_CLEAR(tmp);\", depth+1)\n        self.emit(\"} else {\", depth)\n        if not field.opt:\n            message = \"required field \\\\\\\"%s\\\\\\\" missing from %s\" % (field.name, name)\n            format = \"PyErr_SetString(PyExc_TypeError, \\\"%s\\\");\"\n            self.emit(format % message, depth+1, reflow=False)\n            self.emit(\"return 1;\", depth+1)\n        else:\n            if self.isNumeric(field):\n                self.emit(\"%s = 0;\" % field.name, depth+1)\n            elif not self.isSimpleType(field):\n                self.emit(\"%s = NULL;\" % field.name, depth+1)\n            else:\n                raise TypeError(\"could not determine the default value for %s\" % field.name)\n        self.emit(\"}\", depth)\n    def visitModule(self, mod):\n        self.emit(\"\"\"\ntypedef struct {\n    PyObject_HEAD\n    PyObject *dict;\n} AST_object;\nstatic void\nast_dealloc(AST_object *self)\n{\n    Py_CLEAR(self->dict);\n    Py_TYPE(self)->tp_free(self);\n}\nstatic int\nast_traverse(AST_object *self, visitproc visit, void *arg)\n{\n    Py_VISIT(self->dict);\n    return 0;\n}\nstatic void\nast_clear(AST_object *self)\n{\n    Py_CLEAR(self->dict);\n}\nstatic int\nast_type_init(PyObject *self, PyObject *args, PyObject *kw)\n{\n    _Py_IDENTIFIER(_fields);\n    Py_ssize_t i, numfields = 0;\n    int res = -1;\n    PyObject *key, *value, *fields;\n    fields = _PyObject_GetAttrId((PyObject*)Py_TYPE(self), &PyId__fields);\n    if (!fields)\n        PyErr_Clear();\n    if (fields) {\n        numfields = PySequence_Size(fields);\n        if (numfields == -1)\n            goto cleanup;\n    }\n    res = 0; /* if no error occurs, this stays 0 to the end */\n    if (PyTuple_GET_SIZE(args) > 0) {\n        if (numfields != PyTuple_GET_SIZE(args)) {\n            PyErr_Format(PyExc_TypeError, \"%.400s constructor takes %s\"\n                         \"%zd positional argument%s\",\n                         Py_TYPE(self)->tp_name,\n                         numfields == 0 ? \"\" : \"either 0 or \",\n                         numfields, numfields == 1 ? \"\" : \"s\");\n            res = -1;\n            goto cleanup;\n        }\n        for (i = 0; i < PyTuple_GET_SIZE(args); i++) {\n            /* cannot be reached when fields is NULL */\n            PyObject *name = PySequence_GetItem(fields, i);\n            if (!name) {\n                res = -1;\n                goto cleanup;\n            }\n            res = PyObject_SetAttr(self, name, PyTuple_GET_ITEM(args, i));\n            Py_DECREF(name);\n            if (res < 0)\n                goto cleanup;\n        }\n    }\n    if (kw) {\n        i = 0;  /* needed by PyDict_Next */\n        while (PyDict_Next(kw, &i, &key, &value)) {\n            res = PyObject_SetAttr(self, key, value);\n            if (res < 0)\n                goto cleanup;\n        }\n    }\n  cleanup:\n    Py_XDECREF(fields);\n    return res;\n}\n/* Pickling support */\nstatic PyObject *\nast_type_reduce(PyObject *self, PyObject *unused)\n{\n    PyObject *res;\n    _Py_IDENTIFIER(__dict__);\n    PyObject *dict = _PyObject_GetAttrId(self, &PyId___dict__);\n    if (dict == NULL) {\n        if (PyErr_ExceptionMatches(PyExc_AttributeError))\n            PyErr_Clear();\n        else\n            return NULL;\n    }\n    if (dict) {\n        res = Py_BuildValue(\"O()O\", Py_TYPE(self), dict);\n        Py_DECREF(dict);\n        return res;\n    }\n    return Py_BuildValue(\"O()\", Py_TYPE(self));\n}\nstatic PyMethodDef ast_type_methods[] = {\n    {\"__reduce__\", ast_type_reduce, METH_NOARGS, NULL},\n    {NULL}\n};\nstatic PyGetSetDef ast_type_getsets[] = {\n    {\"__dict__\", PyObject_GenericGetDict, PyObject_GenericSetDict},\n    {NULL}\n};\nstatic PyTypeObject AST_type = {\n    PyVarObject_HEAD_INIT(NULL, 0)\n    \"_ast3.AST\",\n    sizeof(AST_object),\n    0,\n    (destructor)ast_dealloc, /* tp_dealloc */\n    0,                       /* tp_print */\n    0,                       /* tp_getattr */\n    0,                       /* tp_setattr */\n    0,                       /* tp_reserved */\n    0,                       /* tp_repr */\n    0,                       /* tp_as_number */\n    0,                       /* tp_as_sequence */\n    0,                       /* tp_as_mapping */\n    0,                       /* tp_hash */\n    0,                       /* tp_call */\n    0,                       /* tp_str */\n    PyObject_GenericGetAttr, /* tp_getattro */\n    PyObject_GenericSetAttr, /* tp_setattro */\n    0,                       /* tp_as_buffer */\n    Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE | Py_TPFLAGS_HAVE_GC, /* tp_flags */\n    0,                       /* tp_doc */\n    (traverseproc)ast_traverse, /* tp_traverse */\n    (inquiry)ast_clear,      /* tp_clear */\n    0,                       /* tp_richcompare */\n    0,                       /* tp_weaklistoffset */\n    0,                       /* tp_iter */\n    0,                       /* tp_iternext */\n    ast_type_methods,        /* tp_methods */\n    0,                       /* tp_members */\n    ast_type_getsets,        /* tp_getset */\n    0,                       /* tp_base */\n    0,                       /* tp_dict */\n    0,                       /* tp_descr_get */\n    0,                       /* tp_descr_set */\n    offsetof(AST_object, dict),/* tp_dictoffset */\n    (initproc)ast_type_init, /* tp_init */\n    PyType_GenericAlloc,     /* tp_alloc */\n    PyType_GenericNew,       /* tp_new */\n    PyObject_GC_Del,         /* tp_free */\n};\nstatic PyTypeObject* make_type(char *type, PyTypeObject* base, char**fields, int num_fields)\n{\n    PyObject *fnames, *result;\n    int i;\n    fnames = PyTuple_New(num_fields);\n    if (!fnames) return NULL;\n    for (i = 0; i < num_fields; i++) {\n        PyObject *field = PyUnicode_FromString(fields[i]);\n        if (!field) {\n            Py_DECREF(fnames);\n            return NULL;\n        }\n        PyTuple_SET_ITEM(fnames, i, field);\n    }\n    result = PyObject_CallFunction((PyObject*)&PyType_Type, \"s(O){sOss}\",\n                    type, base, \"_fields\", fnames, \"__module__\", \"_ast3\");\n    Py_DECREF(fnames);\n    return (PyTypeObject*)result;\n}\nstatic int add_attributes(PyTypeObject* type, char**attrs, int num_fields)\n{\n    int i, result;\n    _Py_IDENTIFIER(_attributes);\n    PyObject *s, *l = PyTuple_New(num_fields);\n    if (!l)\n        return 0;\n    for (i = 0; i < num_fields; i++) {\n        s = PyUnicode_FromString(attrs[i]);\n        if (!s) {\n            Py_DECREF(l);\n            return 0;\n        }\n        PyTuple_SET_ITEM(l, i, s);\n    }\n    result = _PyObject_SetAttrId((PyObject*)type, &PyId__attributes, l) >= 0;\n    Py_DECREF(l);\n    return result;\n}\n/* Conversion AST -> Python */\nstatic PyObject* ast2obj_list(asdl_seq *seq, PyObject* (*func)(void*))\n{\n    Py_ssize_t i, n = asdl_seq_LEN(seq);\n    PyObject *result = PyList_New(n);\n    PyObject *value;\n    if (!result)\n        return NULL;\n    for (i = 0; i < n; i++) {\n        value = func(asdl_seq_GET(seq, i));\n        if (!value) {\n            Py_DECREF(result);\n            return NULL;\n        }\n        PyList_SET_ITEM(result, i, value);\n    }\n    return result;\n}\nstatic PyObject* ast2obj_object(void *o)\n{\n    if (!o)\n        o = Py_None;\n    Py_INCREF((PyObject*)o);\n    return (PyObject*)o;\n}\nstatic PyObject* ast2obj_int(long b)\n{\n    return PyLong_FromLong(b);\n}\n/* Conversion Python -> AST */\nstatic int obj2ast_singleton(PyObject *obj, PyObject** out, PyArena* arena)\n{\n    if (obj != Py_None && obj != Py_True && obj != Py_False) {\n        PyErr_SetString(PyExc_ValueError,\n                        \"AST singleton must be True, False, or None\");\n        return 1;\n    }\n    *out = obj;\n    return 0;\n}\nstatic int obj2ast_object(PyObject* obj, PyObject** out, PyArena* arena)\n{\n    if (obj == Py_None)\n        obj = NULL;\n    if (obj) {\n        if (PyArena_AddPyObject(arena, obj) < 0) {\n            *out = NULL;\n            return -1;\n        }\n        Py_INCREF(obj);\n    }\n    *out = obj;\n    return 0;\n}\nstatic int obj2ast_constant(PyObject* obj, PyObject** out, PyArena* arena)\n{\n    if (obj) {\n        if (PyArena_AddPyObject(arena, obj) < 0) {\n            *out = NULL;\n            return -1;\n        }\n        Py_INCREF(obj);\n    }\n    *out = obj;\n    return 0;\n}\nstatic int obj2ast_identifier(PyObject* obj, PyObject** out, PyArena* arena)\n{\n    if (!PyUnicode_CheckExact(obj) && obj != Py_None) {\n        PyErr_SetString(PyExc_TypeError, \"AST identifier must be of type str\");\n        return 1;\n    }\n    return obj2ast_object(obj, out, arena);\n}\nstatic int obj2ast_string(PyObject* obj, PyObject** out, PyArena* arena)\n{\n    if (!PyUnicode_CheckExact(obj) && !PyBytes_CheckExact(obj)) {\n        PyErr_SetString(PyExc_TypeError, \"AST string must be of type str\");\n        return 1;\n    }\n    return obj2ast_object(obj, out, arena);\n}\nstatic int obj2ast_bytes(PyObject* obj, PyObject** out, PyArena* arena)\n{\n    if (!PyBytes_CheckExact(obj)) {\n        PyErr_SetString(PyExc_TypeError, \"AST bytes must be of type bytes\");\n        return 1;\n    }\n    return obj2ast_object(obj, out, arena);\n}\nstatic int obj2ast_int(PyObject* obj, int* out, PyArena* arena)\n{\n    int i;\n    if (!PyLong_Check(obj)) {\n        PyErr_Format(PyExc_ValueError, \"invalid integer value: %R\", obj);\n        return 1;\n    }\n    i = _PyLong_AsInt(obj);\n    if (i == -1 && PyErr_Occurred())\n        return 1;\n    *out = i;\n    return 0;\n}\nstatic int add_ast_fields(void)\n{\n    PyObject *empty_tuple, *d;\n    if (PyType_Ready(&AST_type) < 0)\n        return -1;\n    d = AST_type.tp_dict;\n    empty_tuple = PyTuple_New(0);\n    if (!empty_tuple ||\n        PyDict_SetItemString(d, \"_fields\", empty_tuple) < 0 ||\n        PyDict_SetItemString(d, \"_attributes\", empty_tuple) < 0) {\n        Py_XDECREF(empty_tuple);\n        return -1;\n    }\n    Py_DECREF(empty_tuple);\n    return 0;\n}\nstatic int exists_not_none(PyObject *obj, _Py_Identifier *id)\n{\n    int isnone;\n    PyObject *attr = _PyObject_GetAttrId(obj, id);\n    if (!attr) {\n        PyErr_Clear();\n        return 0;\n    }\n    isnone = attr == Py_None;\n    Py_DECREF(attr);\n    return !isnone;\n}\n\"\"\", 0, reflow=False)\n        self.emit(\"static int init_types(void)\",0)\n        self.emit(\"{\", 0)\n        self.emit(\"static int initialized;\", 1)\n        self.emit(\"if (initialized) return 1;\", 1)\n        self.emit(\"if (add_ast_fields() < 0) return 0;\", 1)\n        for dfn in mod.dfns:\n            self.visit(dfn)\n        self.emit(\"initialized = 1;\", 1)\n        self.emit(\"return 1;\", 1);\n        self.emit(\"}\", 0)\n    def visitProduct(self, prod, name):\n        if prod.fields:\n            fields = name+\"_fields\"\n        else:\n            fields = \"NULL\"\n        self.emit('%s_type = make_type(\"%s\", &AST_type, %s, %d);' %\n                        (name, name, fields, len(prod.fields)), 1)\n        self.emit(\"if (!%s_type) return 0;\" % name, 1)\n        if prod.attributes:\n            self.emit(\"if (!add_attributes(%s_type, %s_attributes, %d)) return 0;\" %\n                            (name, name, len(prod.attributes)), 1)\n        else:\n            self.emit(\"if (!add_attributes(%s_type, NULL, 0)) return 0;\" % name, 1)\n    def func_begin(self, name):\n        ctype = get_c_type(name)\n        self.emit(\"PyObject*\", 0)\n        self.emit(\"ast2obj_%s(void* _o)\" % (name), 0)\n        self.emit(\"{\", 0)\n        self.emit(\"%s o = (%s)_o;\" % (ctype, ctype), 1)\n        self.emit(\"PyObject *result = NULL, *value = NULL;\", 1)\n        self.emit('if (!o) {', 1)\n        self.emit(\"Py_INCREF(Py_None);\", 2)\n        self.emit('return Py_None;', 2)\n        self.emit(\"}\", 1)\n        self.emit('', 0)\n    def func_end(self):\n        self.emit(\"return result;\", 1)\n        self.emit(\"failed:\", 0)\n        self.emit(\"Py_XDECREF(value);\", 1)\n        self.emit(\"Py_XDECREF(result);\", 1)\n        self.emit(\"return NULL;\", 1)\n        self.emit(\"}\", 0)\n        self.emit(\"\", 0)\ndef main(srcfile, dump_module=False):\n    argv0 = sys.argv[0]\n    components = argv0.split(os.sep)\n    argv0 = os.sep.join(components[-2:])\n    auto_gen_msg = common_msg % argv0\n    mod = asdl.parse(srcfile)\n    if dump_module:\n        print('Parsed Module:')\n        print(mod)\n    if not asdl.check(mod):\n        sys.exit(1)\n    if INC_DIR:\n        p = \"%s/%s-ast.h\" % (INC_DIR, mod.name)\n        f = open(p, \"w\")\n        f.write(auto_gen_msg)\n        f.write('\n        c = ChainOfVisitors(TypeDefVisitor(f),\n                            StructVisitor(f),\n                            PrototypeVisitor(f),\n                            )\n        c.visit(mod)\n        f.write(\"PyObject* Ta3AST_mod2obj(mod_ty t);\\n\")\n        f.write(\"mod_ty Ta3AST_obj2mod(PyObject* ast, PyArena* arena, int mode);\\n\")\n        f.write(\"int Ta3AST_Check(PyObject* obj);\\n\")\n        f.close()\n    if SRC_DIR:\n        p = os.path.join(SRC_DIR, str(mod.name) + \"-ast.c\")\n        f = open(p, \"w\")\n        f.write(auto_gen_msg)\n        f.write('\n        f.write('\\n')\n        f.write('\n        f.write('\n        f.write('\\n')\n        f.write(\"static PyTypeObject AST_type;\\n\")\n        v = ChainOfVisitors(\n            PyTypesDeclareVisitor(f),\n            PyTypesVisitor(f),\n            Obj2ModPrototypeVisitor(f),\n            FunctionVisitor(f),\n            ObjVisitor(f),\n            Obj2ModVisitor(f),\n            ASTModuleVisitor(f),\n            PartingShots(f),\n            )\n        v.visit(mod)\n        f.close()\ndef parse(source, filename='<unknown>', mode='exec', feature_version=LATEST_MINOR_VERSION):\n    \"\"\"\n    Parse the source into an AST node including type comments.\n    Similar to compile(source, filename, mode, PyCF_ONLY_AST).\n    Set feature_version to limit the syntax parsed to that minor version of\n    Python 3.  For example, feature_version=5 will prevent new syntax features\n    from Python 3.6+ from being used, such as fstrings.  Currently only\n    fully supported for Python 3.5+ with partial support for Python 3.4.\n    So, feature_version=3 or less are all equivalent to feature_version=4.\n    When feature_version=4, the parser will forbid the use of the async/await\n    keywords and the '@' operator, but will not forbid the use of PEP 448\n    additional unpacking generalizations, which were also added in Python 3.5.\n    \"\"\"\n    return _ast3._parse(source, filename, mode, feature_version)\n_NUM_TYPES = (int, float, complex)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2019-19275",
        "description": "[{'lang': 'en', 'value': 'typed_ast 1.3.0 and 1.3.1 has an ast_for_arguments out-of-bounds read. An attacker with the ability to cause a Python interpreter to parse Python source (but not necessarily execute it) may be able to crash the interpreter process. This could be a concern, for example, in a web-based service that parses (but does not execute) Python code. (This issue also affected certain Python 3.8.0-alpha prereleases.)'}]",
        "cwe_number": 125
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-62",
      "code": "def add_public_key(session, user, public_key_str):\n    \"\"\"Add a public key for a particular user.\n    Args:\n        session: db session\n        user: User model of user in question\n        public_key_str: public key to add\n    Throws:\n        DuplicateKey if key is already in use\n        PublicKeyParseError if key can't be parsed\n        BadPublicKey if a plugin rejects the key\n    Returns:\n        PublicKey model object representing the key\n    \"\"\"\n    pubkey = sshpubkeys.SSHKey(public_key_str, strict=True)\n    try:\n        pubkey.parse()\n    except sshpubkeys.InvalidKeyException as e:\n        raise PublicKeyParseError(str(e))\n    try:\n        get_plugin_proxy().will_add_public_key(pubkey)\n    except PluginRejectedPublicKey as e:\n        raise BadPublicKey(str(e))\n    db_pubkey = PublicKey(\n        user=user,\n        public_key=pubkey.keydata.strip(),\n        fingerprint=pubkey.hash_md5().replace(\"MD5:\", \"\"),\n        fingerprint_sha256=pubkey.hash_sha256().replace(\"SHA256:\", \"\"),\n        key_size=pubkey.bits,\n        key_type=pubkey.key_type,\n        comment=pubkey.comment,\n    )\n    try:\n        db_pubkey.add(session)\n        Counter.incr(session, \"updates\")\n    except IntegrityError:\n        session.rollback()\n        raise DuplicateKey()\n    session.commit()\n    return db_pubkey\ndef delete_public_key(session, user_id, key_id):\n    \"\"\"Delete a particular user's public key.\n    Args:\n        session(models.base.session.Session): database session\n        user_id(int): id of user in question\n        key_id(int): id of the user's key we want to delete\n    Throws:\n        KeyNotFound if specified key wasn't found\n    \"\"\"\n    pkey = get_public_key(session, user_id, key_id)\n    pkey.delete(session)\n    Counter.incr(session, \"updates\")\n    session.commit()",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-4768",
        "description": "[{'lang': 'en', 'value': 'A vulnerability was found in Dropbox merou. It has been classified as critical. Affected is the function add_public_key of the file grouper/public_key.py of the component SSH Public Key Handler. The manipulation of the argument public_key_str leads to injection. It is possible to launch the attack remotely. The name of the patch is d93087973afa26bc0a2d0a5eb5c0fde748bdd107. It is recommended to apply a patch to fix this issue. VDB-216906 is the identifier assigned to this vulnerability.'}]",
        "cwe_number": 74
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-63",
      "code": "    def _floatToString(self, x):\n        if -1e-10 < x < 1e-10:\n            return '0'\n        elif -1e-10 < int(x) - x < 1e-10:\n            return str(int(x))\n        else:\n            return str(x)\n    def _complexToString(self, x):\n        realS = self._floatToString(x.real)\n        imagS = self._floatToString(x.imag)\n        if imagS == '0':\n            return realS\n        elif imagS == '1':\n            imagS = '+i'\n        elif imagS == '-1':\n            imagS = '-i'\n        elif x.imag < 0:\n            imagS = '%si' % imagS\n        else:\n            imagS = '+%si' % imagS\n        if realS == '0' and imagS == '0':\n            return '0'\n        elif realS == '0':\n            return imagS.lstrip('+')\n        elif imagS == '0':\n            return realS\n        else:\n            return '%s%s' % (realS, imagS)\n    _calc_match_forbidden_chars = re.compile('[_\\[\\]]')\n    _calc_remover = utils.str.MultipleRemover('_[] \\t')\n    def calc(self, irc, msg, args, text):\n        \"\"\"<math expression>\n        Returns the value of the evaluated <math expression>.  The syntax is\n        Python syntax; the type of arithmetic is floating point.  Floating\n        point arithmetic is used in order to prevent a user from being able to\n        crash to the bot with something like '10**10**10**10'.  One consequence\n        is that large values such as '10**24' might not be exact.\n        \"\"\"\n        try:\n            text = str(text)\n        except UnicodeEncodeError:\n            irc.error(_(\"There's no reason you should have fancy non-ASCII \"\n                            \"characters in your mathematical expression. \"\n                            \"Please remove them.\"))\n            return\n        if self._calc_match_forbidden_chars.match(text):\n            irc.error(_('There\\'s really no reason why you should have '\n                           'underscores or brackets in your mathematical '\n                           'expression.  Please remove them.'))\n            return\n        text = self._calc_remover(text)\n        if 'lambda' in text:\n            irc.error(_('You can\\'t use lambda in this command.'))\n            return\n        text = text.lower()\n        def handleMatch(m):\n            s = m.group(1)\n            if s.startswith('0x'):\n                i = int(s, 16)\n            elif s.startswith('0') and '.' not in s:\n                try:\n                    i = int(s, 8)\n                except ValueError:\n                    i = int(s)\n            else:\n                i = float(s)\n            x = complex(i)\n            if x.imag == 0:\n                x = x.real\n                return '%.16f' % x\n            return str(x)\n        text = self._mathRe.sub(handleMatch, text)\n        try:\n            self.log.info('evaluating %q from %s', text, msg.prefix)\n            x = complex(eval(text, self._mathSafeEnv, self._mathSafeEnv))\n            irc.reply(self._complexToString(x))\n        except OverflowError:\n            maxFloat = math.ldexp(0.9999999999999999, 1024)\n            irc.error(_('The answer exceeded %s or so.') % maxFloat)\n        except TypeError:\n            irc.error(_('Something in there wasn\\'t a valid number.'))\n        except NameError as e:\n            irc.error(_('%s is not a defined function.') % str(e).split()[1])\n        except Exception as e:\n            irc.error(str(e))",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2019-19010",
        "description": "[{'lang': 'en', 'value': 'Eval injection in the Math plugin of Limnoria (before 2019.11.09) and Supybot (through 2018-05-09) allows remote unprivileged attackers to disclose information or possibly have unspecified other impact via the calc and icalc IRC commands.'}]",
        "cwe_number": 94
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-64",
      "code": "    def createRequest(self, op, req, outgoing_data):\n        \"\"\"Return a new QNetworkReply object.\n        Args:\n             op: Operation op\n             req: const QNetworkRequest & req\n             outgoing_data: QIODevice * outgoingData\n        Return:\n            A QNetworkReply.\n        \"\"\"\n        proxy_factory = objreg.get('proxy-factory', None)\n        if proxy_factory is not None:\n            proxy_error = proxy_factory.get_error()\n            if proxy_error is not None:\n                return networkreply.ErrorNetworkReply(\n                    req, proxy_error, QNetworkReply.UnknownProxyError,\n                    self)\n        scheme = req.url().scheme()\n        if scheme in self._scheme_handlers:\n            result = self._scheme_handlers[scheme](req)\n            if result is not None:\n                result.setParent(self)\n                return result\n        for header, value in shared.custom_headers(url=req.url()):\n            req.setRawHeader(header, value)\n        host_blocker = objreg.get('host-blocker')\n        if host_blocker.is_blocked(req.url()):\n            log.webview.info(\"Request to {} blocked by host blocker.\".format(\n                req.url().host()))\n            return networkreply.ErrorNetworkReply(\n                req, HOSTBLOCK_ERROR_STRING, QNetworkReply.ContentAccessDenied,\n                self)\n        current_url = QUrl()\n        if self._tab_id is not None:\n            assert self._win_id is not None\n            try:\n                tab = objreg.get('tab', scope='tab', window=self._win_id,\n                                 tab=self._tab_id)\n                current_url = tab.url()\n            except (KeyError, RuntimeError):\n                current_url = QUrl()\n        if 'log-requests' in self._args.debug_flags:\n            operation = debug.qenum_key(QNetworkAccessManager, op)\n            operation = operation.replace('Operation', '').upper()\n            log.webview.debug(\"{} {}, first-party {}\".format(\n                operation,\n                req.url().toDisplayString(),\n                current_url.toDisplayString()))\n        self.set_referer(req, current_url)\n        return super().createRequest(op, req, outgoing_data)\n    def requestStarted(self, job):\n        \"\"\"Handle a request for a qute: scheme.\n        This method must be reimplemented by all custom URL scheme handlers.\n        The request is asynchronous and does not need to be handled right away.\n        Args:\n            job: QWebEngineUrlRequestJob\n        \"\"\"\n        url = job.requestUrl()\n        if url.scheme() in ['chrome-error', 'chrome-extension']:\n            job.fail(QWebEngineUrlRequestJob.UrlInvalid)\n            return\n        assert job.requestMethod() == b'GET'\n        assert url.scheme() == 'qute'\n        log.misc.debug(\"Got request for {}\".format(url.toDisplayString()))\n        try:\n            mimetype, data = qutescheme.data_for_url(url)\n        except qutescheme.NoHandlerFound:\n            log.misc.debug(\"No handler found for {}\".format(\n                url.toDisplayString()))\n            job.fail(QWebEngineUrlRequestJob.UrlNotFound)\n        except qutescheme.QuteSchemeOSError:\n            log.misc.exception(\"OSError while handling qute://* URL\")\n            job.fail(QWebEngineUrlRequestJob.UrlNotFound)\n        except qutescheme.QuteSchemeError:\n            log.misc.exception(\"Error while handling qute://* URL\")\n            job.fail(QWebEngineUrlRequestJob.RequestFailed)\n        except qutescheme.Redirect as e:\n            qtutils.ensure_valid(e.url)\n            job.redirect(e.url)\n        else:\n            log.misc.debug(\"Returning {} data\".format(mimetype))\n            buf = QBuffer(parent=self)\n            buf.open(QIODevice.WriteOnly)\n            buf.write(data)\n            buf.seek(0)\n            buf.close()\n            job.reply(mimetype.encode('ascii'), buf)\n    def interceptRequest(self, info):\n        \"\"\"Handle the given request.\n        Reimplementing this virtual function and setting the interceptor on a\n        profile makes it possible to intercept URL requests. This function is\n        executed on the IO thread, and therefore running long tasks here will\n        block networking.\n        info contains the information about the URL request and will track\n        internally whether its members have been altered.\n        Args:\n            info: QWebEngineUrlRequestInfo &info\n        \"\"\"\n        if 'log-requests' in self._args.debug_flags:\n            resource_type = debug.qenum_key(QWebEngineUrlRequestInfo,\n                                            info.resourceType())\n            navigation_type = debug.qenum_key(QWebEngineUrlRequestInfo,\n                                              info.navigationType())\n            log.webview.debug(\"{} {}, first-party {}, resource {}, \"\n                              \"navigation {}\".format(\n                                  bytes(info.requestMethod()).decode('ascii'),\n                                  info.requestUrl().toDisplayString(),\n                                  info.firstPartyUrl().toDisplayString(),\n                                  resource_type, navigation_type))\n        url = info.requestUrl()\n        if self._host_blocker.is_blocked(url):\n            log.webview.info(\"Request to {} blocked by host blocker.\".format(\n                url.host()))\n            info.block(True)\n        for header, value in shared.custom_headers(url=url):\n            info.setHttpHeader(header, value)\n        user_agent = config.instance.get('content.headers.user_agent', url=url)\n        if user_agent is not None:\n            info.setHttpHeader(b'User-Agent', user_agent.encode('ascii'))\ndef handler(request):\n    \"\"\"Scheme handler for qute:// URLs.\n    Args:\n        request: QNetworkRequest to answer to.\n    Return:\n        A QNetworkReply.\n    \"\"\"\n    try:\n        mimetype, data = qutescheme.data_for_url(request.url())\n    except qutescheme.NoHandlerFound:\n        errorstr = \"No handler found for {}!\".format(\n            request.url().toDisplayString())\n        return networkreply.ErrorNetworkReply(\n            request, errorstr, QNetworkReply.ContentNotFoundError)\n    except qutescheme.QuteSchemeOSError as e:\n        return networkreply.ErrorNetworkReply(\n            request, str(e), QNetworkReply.ContentNotFoundError)\n    except qutescheme.QuteSchemeError as e:\n        return networkreply.ErrorNetworkReply(request, e.errorstring, e.error)\n    except qutescheme.Redirect as e:\n        qtutils.ensure_valid(e.url)\n        return networkreply.RedirectNetworkReply(e.url)\n    return networkreply.FixedDataNetworkReply(request, data, mimetype)\ndef qute_pdfjs(url):\n    \"\"\"Handler for qute://pdfjs. Return the pdf.js viewer.\"\"\"\n    try:\n        data = pdfjs.get_pdfjs_res(url.path())\n    except pdfjs.PDFJSNotFound as e:\n        log.misc.warning(\n            \"pdfjs resource requested but not found: {}\".format(e.path))\n        raise qutescheme.QuteSchemeError(\"Can't find pdfjs resource \"\n                                         \"'{}'\".format(e.path),\n                                         QNetworkReply.ContentNotFoundError)\n    else:\n        mimetype, _encoding = mimetypes.guess_type(url.fileName())\n        assert mimetype is not None, url\n        return mimetype, data\ndef handler(request):\n    \"\"\"Handler for a file:// URL.\n    Args:\n        request: QNetworkRequest to answer to.\n    Return:\n        A QNetworkReply for directories, None for files.\n    \"\"\"\n    path = request.url().toLocalFile()\n    try:\n        if os.path.isdir(path):\n            data = dirbrowser_html(path)\n            return networkreply.FixedDataNetworkReply(\n                request, data, 'text/html')\n        return None\n    except UnicodeEncodeError:\n        return None\ndef qute_bindings(_url):\n    \"\"\"Handler for qute://bindings. View keybindings.\"\"\"\n    bindings = {}\n    defaults = config.val.bindings.default\n    modes = set(defaults.keys()).union(config.val.bindings.commands)\n    modes.remove('normal')\n    modes = ['normal'] + sorted(list(modes))\n    for mode in modes:\n        bindings[mode] = config.key_instance.get_bindings_for(mode)\n    src = jinja.render('bindings.html', title='Bindings',\n                       bindings=bindings)\n    return 'text/html', src\ndef qute_back(url):\n    \"\"\"Handler for qute://back.\n    Simple page to free ram / lazy load a site, goes back on focusing the tab.\n    \"\"\"\n    src = jinja.render(\n        'back.html',\n        title='Suspended: ' + urllib.parse.unquote(url.fragment()))\n    return 'text/html', src",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2018-10895",
        "description": "[{'lang': 'en', 'value': \"qutebrowser before version 1.4.1 is vulnerable to a cross-site request forgery flaw that allows websites to access 'qute://*' URLs. A malicious website could exploit this to load a 'qute://settings/set' URL, which then sets 'editor.command' to a bash script, resulting in arbitrary code execution.\"}]",
        "cwe_number": 352
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-65",
      "code": "def is_local_uri(uri, is_tracking_or_registry_uri=True):\n    \"\"\"\n    Returns true if the specified URI is a local file path (/foo or file:/foo).\n    :param uri: The URI.\n    :param is_tracking_uri: Whether or not the specified URI is an MLflow Tracking or MLflow\n                            Model Registry URI. Examples of other URIs are MLflow artifact URIs,\n                            filesystem paths, etc.\n    \"\"\"\n    if uri == \"databricks\" and is_tracking_or_registry_uri:\n        return False\n    if is_windows() and uri.startswith(\"\\\\\\\\\"):\n        return False\n    parsed_uri = urllib.parse.urlparse(uri)\n    scheme = parsed_uri.scheme\n    if scheme == \"\":\n        return True\n    if parsed_uri.hostname and not (\n        parsed_uri.hostname == \".\"\n        or parsed_uri.hostname.startswith(\"localhost\")\n        or parsed_uri.hostname.startswith(\"127.0.0.1\")\n    ):\n        return False\n    if scheme == \"file\":\n        return True\n    if is_windows() and len(scheme) == 1 and scheme.lower() == pathlib.Path(uri).drive.lower()[0]:\n        return True\n    return False\ndef is_file_uri(uri):\n    return urllib.parse.urlparse(uri).scheme == \"file\"\ndef is_http_uri(uri):\n    scheme = urllib.parse.urlparse(uri).scheme\n    return scheme == \"http\" or scheme == \"https\"\ndef is_databricks_uri(uri):\n    \"\"\"\n    Databricks URIs look like 'databricks' (default profile) or 'databricks://profile'\n    or 'databricks://secret_scope:secret_key_prefix'.\n    \"\"\"\n    scheme = urllib.parse.urlparse(uri).scheme\n    return scheme == \"databricks\" or uri == \"databricks\"",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-3573",
        "description": "[{'lang': 'en', 'value': \"mlflow/mlflow is vulnerable to Local File Inclusion (LFI) due to improper parsing of URIs, allowing attackers to bypass checks and read arbitrary files on the system. The issue arises from the 'is_local_uri' function's failure to properly handle URIs with empty or 'file' schemes, leading to the misclassification of URIs as non-local. Attackers can exploit this by crafting malicious model versions with specially crafted 'source' parameters, enabling the reading of sensitive files within at least two directory levels from the server's root.\"}]",
        "cwe_number": 29
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-66",
      "code": "    def as_const(self, eval_ctx=None):\n        eval_ctx = get_eval_context(self, eval_ctx)\n        if eval_ctx.volatile:\n            raise Impossible()\n        obj = self.node.as_const(eval_ctx)\n        args = [x.as_const(eval_ctx) for x in self.args]\n        if isinstance(obj, _context_function_types):\n            if getattr(obj, 'contextfunction', False):\n                raise Impossible()\n            elif getattr(obj, 'evalcontextfunction', False):\n                args.insert(0, eval_ctx)\n            elif getattr(obj, 'environmentfunction', False):\n                args.insert(0, self.environment)\n        kwargs = dict(x.as_const(eval_ctx) for x in self.kwargs)\n        if self.dyn_args is not None:\n            try:\n                args.extend(self.dyn_args.as_const(eval_ctx))\n            except Exception:\n                raise Impossible()\n        if self.dyn_kwargs is not None:\n            try:\n                kwargs.update(self.dyn_kwargs.as_const(eval_ctx))\n            except Exception:\n                raise Impossible()\n        try:\n            return obj(*args, **kwargs)\n        except Exception:\n            raise Impossible()\ndef is_internal_attribute(obj, attr):\n    \"\"\"Test if the attribute given is an internal python attribute.  For\n    example this function returns `True` for the `func_code` attribute of\n    python objects.  This is useful if the environment method\n    :meth:`~SandboxedEnvironment.is_safe_attribute` is overridden.\n    >>> from jinja2.sandbox import is_internal_attribute\n    >>> is_internal_attribute(str, \"mro\")\n    True\n    >>> is_internal_attribute(str, \"upper\")\n    False\n    \"\"\"\n    if isinstance(obj, types.FunctionType):\n        if attr in UNSAFE_FUNCTION_ATTRIBUTES:\n            return True\n    elif isinstance(obj, types.MethodType):\n        if attr in UNSAFE_FUNCTION_ATTRIBUTES or \\\n           attr in UNSAFE_METHOD_ATTRIBUTES:\n            return True\n    elif isinstance(obj, type):\n        if attr == 'mro':\n            return True\n    elif isinstance(obj, (types.CodeType, types.TracebackType, types.FrameType)):\n        return True\n    elif isinstance(obj, types.GeneratorType):\n        if attr in UNSAFE_GENERATOR_ATTRIBUTES:\n            return True\n    return attr.startswith('__')\ndef modifies_known_mutable(obj, attr):\n    \"\"\"This function checks if an attribute on a builtin mutable object\n    (list, dict, set or deque) would modify it if called.  It also supports\n    the \"user\"-versions of the objects (`sets.Set`, `UserDict.*` etc.) and\n    with Python 2.6 onwards the abstract base classes `MutableSet`,\n    `MutableMapping`, and `MutableSequence`.\n    >>> modifies_known_mutable({}, \"clear\")\n    True\n    >>> modifies_known_mutable({}, \"keys\")\n    False\n    >>> modifies_known_mutable([], \"append\")\n    True\n    >>> modifies_known_mutable([], \"index\")\n    False\n    If called with an unsupported object (such as unicode) `False` is\n    returned.\n    >>> modifies_known_mutable(\"foo\", \"upper\")\n    False\n    \"\"\"\n    for typespec, unsafe in _mutable_spec:\n        if isinstance(obj, typespec):\n            return attr in unsafe\n    return False\nclass SandboxedEnvironment(Environment):\n    sandboxed = True\n    default_binop_table = {\n    default_unop_table = {",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2016-10745",
        "description": "[{'lang': 'en', 'value': 'In Pallets Jinja before 2.8.1, str.format allows a sandbox escape.'}]",
        "cwe_number": 134
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-67",
      "code": "    def _real_extract(self, url):\n        if url.startswith('//'):\n            return self.url_result(self.http_scheme() + url)\n        parsed_url = urllib.parse.urlparse(url)\n        if not parsed_url.scheme:\n            default_search = self.get_param('default_search')\n            if default_search is None:\n                default_search = 'fixup_error'\n            if default_search in ('auto', 'auto_warning', 'fixup_error'):\n                if re.match(r'^[^\\s/]+\\.[^\\s/]+/', url):\n                    self.report_warning('The url doesn\\'t specify the protocol, trying with http')\n                    return self.url_result('http://' + url)\n                elif default_search != 'fixup_error':\n                    if default_search == 'auto_warning':\n                        if re.match(r'^(?:url|URL)$', url):\n                            raise ExtractorError(\n                                'Invalid URL:  %r . Call yt-dlp like this:  yt-dlp -v \"https://www.youtube.com/watch?v=BaW_jenozKc\"  ' % url,\n                                expected=True)\n                        else:\n                            self.report_warning(\n                                'Falling back to youtube search for  %s . Set --default-search \"auto\" to suppress this warning.' % url)\n                    return self.url_result('ytsearch:' + url)\n            if default_search in ('error', 'fixup_error'):\n                raise ExtractorError(\n                    '%r is not a valid URL. '\n                    'Set --default-search \"ytsearch\" (or run  yt-dlp \"ytsearch:%s\" ) to search YouTube'\n                    % (url, url), expected=True)\n            else:\n                if ':' not in default_search:\n                    default_search += ':'\n                return self.url_result(default_search + url)\n        original_url = url\n        url, smuggled_data = unsmuggle_url(url, {})\n        force_videoid = None\n        is_intentional = smuggled_data.get('to_generic')\n        if 'force_videoid' in smuggled_data:\n            force_videoid = smuggled_data['force_videoid']\n            video_id = force_videoid\n        else:\n            video_id = self._generic_id(url)\n        full_response = self._request_webpage(url, video_id, headers={\n            'Accept-Encoding': 'identity',\n            **smuggled_data.get('http_headers', {})\n        })\n        new_url = full_response.url\n        url = urllib.parse.urlparse(url)._replace(scheme=urllib.parse.urlparse(new_url).scheme).geturl()\n        if new_url != extract_basic_auth(url)[0]:\n            self.report_following_redirect(new_url)\n            if force_videoid:\n                new_url = smuggle_url(new_url, {'force_videoid': force_videoid})\n            return self.url_result(new_url)\n        info_dict = {\n            'id': video_id,\n            'title': self._generic_title(url),\n            'timestamp': unified_timestamp(full_response.headers.get('Last-Modified'))\n        }\n        content_type = full_response.headers.get('Content-Type', '').lower()\n        m = re.match(r'^(?P<type>audio|video|application(?=/(?:ogg$|(?:vnd\\.apple\\.|x-)?mpegurl)))/(?P<format_id>[^;\\s]+)', content_type)\n        if m:\n            self.report_detected('direct video link')\n            headers = smuggled_data.get('http_headers', {})\n            format_id = str(m.group('format_id'))\n            ext = determine_ext(url, default_ext=None) or urlhandle_detect_ext(full_response)\n            subtitles = {}\n            if format_id.endswith('mpegurl') or ext == 'm3u8':\n                formats, subtitles = self._extract_m3u8_formats_and_subtitles(url, video_id, 'mp4', headers=headers)\n            elif format_id.endswith('mpd') or format_id.endswith('dash+xml') or ext == 'mpd':\n                formats, subtitles = self._extract_mpd_formats_and_subtitles(url, video_id, headers=headers)\n            elif format_id == 'f4m' or ext == 'f4m':\n                formats = self._extract_f4m_formats(url, video_id, headers=headers)\n            else:\n                formats = [{\n                    'format_id': format_id,\n                    'url': url,\n                    'ext': ext,\n                    'vcodec': 'none' if m.group('type') == 'audio' else None\n                }]\n                info_dict['direct'] = True\n            info_dict.update({\n                'formats': formats,\n                'subtitles': subtitles,\n                'http_headers': headers or None,\n            })\n            self._extra_manifest_info(info_dict, url)\n            return info_dict\n        if not self.get_param('test', False) and not is_intentional:\n            force = self.get_param('force_generic_extractor', False)\n            self.report_warning('%s generic information extractor' % ('Forcing' if force else 'Falling back on'))\n        first_bytes = full_response.read(512)\n        if first_bytes.startswith(b'\n            self.report_detected('M3U playlist')\n            info_dict['formats'], info_dict['subtitles'] = self._extract_m3u8_formats_and_subtitles(url, video_id, 'mp4')\n            self._extra_manifest_info(info_dict, url)\n            return info_dict\n        if not is_html(first_bytes):\n            self.report_warning(\n                'URL could be a direct video link, returning it as such.')\n            info_dict.update({\n                'direct': True,\n                'url': url,\n            })\n            return info_dict\n        webpage = self._webpage_read_content(\n            full_response, url, video_id, prefix=first_bytes)\n        if '<title>DPG Media Privacy Gate</title>' in webpage:\n            webpage = self._download_webpage(url, video_id)\n        self.report_extraction(video_id)\n        try:\n            try:\n                doc = compat_etree_fromstring(webpage)\n            except xml.etree.ElementTree.ParseError:\n                doc = compat_etree_fromstring(webpage.encode('utf-8'))\n            if doc.tag == 'rss':\n                self.report_detected('RSS feed')\n                return self._extract_rss(url, video_id, doc)\n            elif doc.tag == 'SmoothStreamingMedia':\n                info_dict['formats'], info_dict['subtitles'] = self._parse_ism_formats_and_subtitles(doc, url)\n                self.report_detected('ISM manifest')\n                return info_dict\n            elif re.match(r'^(?:{[^}]+})?smil$', doc.tag):\n                smil = self._parse_smil(doc, url, video_id)\n                self.report_detected('SMIL file')\n                return smil\n            elif doc.tag == '{http://xspf.org/ns/0/}playlist':\n                self.report_detected('XSPF playlist')\n                return self.playlist_result(\n                    self._parse_xspf(\n                        doc, video_id, xspf_url=url,\n                        xspf_base_url=full_response.url),\n                    video_id)\n            elif re.match(r'(?i)^(?:{[^}]+})?MPD$', doc.tag):\n                info_dict['formats'], info_dict['subtitles'] = self._parse_mpd_formats_and_subtitles(\n                    doc,\n                    mpd_base_url=full_response.url.rpartition('/')[0],\n                    mpd_url=url)\n                self._extra_manifest_info(info_dict, url)\n                self.report_detected('DASH manifest')\n                return info_dict\n            elif re.match(r'^{http://ns\\.adobe\\.com/f4m/[12]\\.0}manifest$', doc.tag):\n                info_dict['formats'] = self._parse_f4m_formats(doc, url, video_id)\n                self.report_detected('F4M manifest')\n                return info_dict\n        except xml.etree.ElementTree.ParseError:\n            pass\n        info_dict.update({\n            'title': self._generic_title('', webpage, default='video'),\n            'description': self._og_search_description(webpage, default=None),\n            'thumbnail': self._og_search_thumbnail(webpage, default=None),\n            'age_limit': self._rta_search(webpage),\n        })\n        self._downloader.write_debug('Looking for embeds')\n        embeds = list(self._extract_embeds(original_url, webpage, urlh=full_response, info_dict=info_dict))\n        if len(embeds) == 1:\n            return merge_dicts(embeds[0], info_dict)\n        elif embeds:\n            return self.playlist_result(embeds, **info_dict)\n        raise UnsupportedError(url)\n    def _extract_embeds(self, url, webpage, *, urlh=None, info_dict={}):\n        \"\"\"Returns an iterator of video entries\"\"\"\n        info_dict = types.MappingProxyType(info_dict)\n        video_id = traverse_obj(info_dict, 'display_id', 'id') or self._generic_id(url)\n        url, smuggled_data = unsmuggle_url(url, {})\n        actual_url = urlh.url if urlh else url\n        embeds = []\n        for ie in self._downloader._ies.values():\n            if ie.ie_key() in smuggled_data.get('block_ies', []):\n                continue\n            gen = ie.extract_from_webpage(self._downloader, url, webpage)\n            current_embeds = []\n            try:\n                while True:\n                    current_embeds.append(next(gen))\n            except self.StopExtraction:\n                self.report_detected(f'{ie.IE_NAME} exclusive embed', len(current_embeds),\n                                     embeds and 'discarding other embeds')\n                return current_embeds\n            except StopIteration:\n                self.report_detected(f'{ie.IE_NAME} embed', len(current_embeds))\n                embeds.extend(current_embeds)\n        if embeds:\n            return embeds\n        jwplayer_data = self._find_jwplayer_data(\n            webpage, video_id, transform_source=js_to_json)\n        if jwplayer_data:\n            if isinstance(jwplayer_data.get('playlist'), str):\n                self.report_detected('JW Player playlist')\n                return [self.url_result(jwplayer_data['playlist'], 'JWPlatform')]\n            try:\n                info = self._parse_jwplayer_data(\n                    jwplayer_data, video_id, require_title=False, base_url=url)\n                if traverse_obj(info, 'formats', ('entries', ..., 'formats')):\n                    self.report_detected('JW Player data')\n                    return [info]\n            except ExtractorError:\n                pass\n        mobj = re.search(\n            r'(?s)\\bvideojs\\s*\\(.+?([a-zA-Z0-9_$]+)\\.src\\s*\\(\\s*((?:\\[.+?\\]|{.+?}))\\s*\\)\\s*;',\n            webpage)\n        if mobj is not None:\n            varname = mobj.group(1)\n            sources = variadic(self._parse_json(\n                mobj.group(2), video_id, transform_source=js_to_json, fatal=False) or [])\n            formats, subtitles, src = [], {}, None\n            for source in sources:\n                src = source.get('src')\n                if not src or not isinstance(src, str):\n                    continue\n                src = urllib.parse.urljoin(url, src)\n                src_type = source.get('type')\n                if isinstance(src_type, str):\n                    src_type = src_type.lower()\n                ext = determine_ext(src).lower()\n                if src_type == 'video/youtube':\n                    return [self.url_result(src, YoutubeIE.ie_key())]\n                if src_type == 'application/dash+xml' or ext == 'mpd':\n                    fmts, subs = self._extract_mpd_formats_and_subtitles(\n                        src, video_id, mpd_id='dash', fatal=False)\n                    formats.extend(fmts)\n                    self._merge_subtitles(subs, target=subtitles)\n                elif src_type == 'application/x-mpegurl' or ext == 'm3u8':\n                    fmts, subs = self._extract_m3u8_formats_and_subtitles(\n                        src, video_id, 'mp4', entry_protocol='m3u8_native',\n                        m3u8_id='hls', fatal=False)\n                    formats.extend(fmts)\n                    self._merge_subtitles(subs, target=subtitles)\n                if not formats:\n                    formats.append({\n                        'url': src,\n                        'ext': (mimetype2ext(src_type)\n                                or ext if ext in KNOWN_EXTENSIONS else 'mp4'),\n                        'http_headers': {\n                            'Referer': actual_url,\n                        },\n                    })\n            for sub_match in re.finditer(rf'(?s){re.escape(varname)}' r'\\.addRemoteTextTrack\\(({.+?})\\s*,\\s*(?:true|false)\\)', webpage):\n                sub = self._parse_json(\n                    sub_match.group(1), video_id, transform_source=js_to_json, fatal=False) or {}\n                sub_src = str_or_none(sub.get('src'))\n                if not sub_src:\n                    continue\n                subtitles.setdefault(dict_get(sub, ('language', 'srclang')) or 'und', []).append({\n                    'url': urllib.parse.urljoin(url, sub_src),\n                    'name': sub.get('label'),\n                    'http_headers': {\n                        'Referer': actual_url,\n                    },\n                })\n            if formats or subtitles:\n                self.report_detected('video.js embed')\n                info_dict = {'formats': formats, 'subtitles': subtitles}\n                if formats:\n                    self._extra_manifest_info(info_dict, src)\n                return [info_dict]\n        found = self._search_regex((\n            r'<script\\b[^>]+?\\bsrc\\s*=\\s*([\"\\'])https?://(?:(?!\\1)[^?\n            r'kt_player\\s*\\(\\s*([\"\\'])(?:(?!\\1)[\\w\\W])+\\1\\s*,\\s*([\"\\'])https?://(?:(?!\\2)[^?\n        ), webpage, 'KVS player', group='ver', default=False)\n        if found:\n            self.report_detected('KVS Player')\n            if found.split('.')[0] not in ('4', '5', '6'):\n                self.report_warning(f'Untested major version ({found}) in player engine - download may fail.')\n            return [self._extract_kvs(url, webpage, video_id)]\n        json_ld = self._search_json_ld(webpage, video_id, default={})\n        if json_ld.get('url') not in (url, None):\n            self.report_detected('JSON LD')\n            is_direct = json_ld.get('ext') not in (None, *MEDIA_EXTENSIONS.manifests)\n            return [merge_dicts({\n                '_type': 'video' if is_direct else 'url_transparent',\n                'url': smuggle_url(json_ld['url'], {\n                    'force_videoid': video_id,\n                    'to_generic': True,\n                    'http_headers': {'Referer': url},\n                }),\n            }, json_ld)]\n        def check_video(vurl):\n            if YoutubeIE.suitable(vurl):\n                return True\n            if RtmpIE.suitable(vurl):\n                return True\n            vpath = urllib.parse.urlparse(vurl).path\n            vext = determine_ext(vpath, None)\n            return vext not in (None, 'swf', 'png', 'jpg', 'srt', 'sbv', 'sub', 'vtt', 'ttml', 'js', 'xml')\n        def filter_video(urls):\n            return list(filter(check_video, urls))\n        found = filter_video(re.findall(r'flashvars: [\\'\"](?:.*&)?file=(http[^\\'\"&]*)', webpage))\n        if found:\n            self.report_detected('JW Player in SFWObject')\n        else:\n            found = filter_video(re.findall(r'''(?sx)\n                (?:\n                    jw_plugins|\n                    JWPlayerOptions|\n                    jwplayer\\s*\\(\\s*[\"'][^'\"]+[\"']\\s*\\)\\s*\\.setup\n                )\n                .*?\n                ['\"]?file['\"]?\\s*:\\s*[\"\\'](.*?)[\"\\']''', webpage))\n            if found:\n                self.report_detected('JW Player embed')\n        if not found:\n            found = filter_video(re.findall(r'[^A-Za-z0-9]?(?:file|source)=(http[^\\'\"&]*)', webpage))\n            if found:\n                self.report_detected('video file')\n        if not found:\n            found = filter_video(re.findall(\n                r'[^A-Za-z0-9]?(?:file|video_url)[\"\\']?:\\s*[\"\\'](http(?![^\\'\"]+\\.[0-9]+[\\'\"])[^\\'\"]+)[\"\\']', webpage))\n            if found:\n                self.report_detected('JW Player JS loader')\n        if not found:\n            found = filter_video(re.findall(r'''(?xs)\n                flowplayer\\(\"[^\"]+\",\\s*\n                    \\{[^}]+?\\}\\s*,\n                    \\s*\\{[^}]+? [\"']?clip[\"']?\\s*:\\s*\\{\\s*\n                        [\"']?url[\"']?\\s*:\\s*[\"']([^\"']+)[\"']\n            ''', webpage))\n            if found:\n                self.report_detected('Flow Player')\n        if not found:\n            found = re.findall(\n                r\"cinerama\\.embedPlayer\\(\\s*\\'[^']+\\',\\s*'([^']+)'\", webpage)\n            if found:\n                self.report_detected('Cinerama player')\n        if not found:\n            found = filter_video(re.findall(\n                r'<meta (?:property|name)=\"twitter:player:stream\" (?:content|value)=\"(.+?)\"', webpage))\n            if found:\n                self.report_detected('Twitter card')\n        if not found:\n            m_video_type = re.findall(r'<meta.*?property=\"og:video:type\".*?content=\"video/(.*?)\"', webpage)\n            if m_video_type is not None:\n                found = filter_video(re.findall(r'<meta.*?property=\"og:(?:video|audio)\".*?content=\"(.*?)\"', webpage))\n                if found:\n                    self.report_detected('Open Graph video info')\n        if not found:\n            REDIRECT_REGEX = r'[0-9]{,2};\\s*(?:URL|url)=\\'?([^\\'\"]+)'\n            found = re.search(\n                r'(?i)<meta\\s+(?=(?:[a-z-]+=\"[^\"]+\"\\s+)*http-equiv=\"refresh\")'\n                r'(?:[a-z-]+=\"[^\"]+\"\\s+)*?content=\"%s' % REDIRECT_REGEX,\n                webpage)\n            if not found:\n                refresh_header = urlh and urlh.headers.get('Refresh')\n                if refresh_header:\n                    found = re.search(REDIRECT_REGEX, refresh_header)\n            if found:\n                new_url = urllib.parse.urljoin(url, unescapeHTML(found.group(1)))\n                if new_url != url:\n                    self.report_following_redirect(new_url)\n                    return [self.url_result(new_url)]\n                else:\n                    found = None\n        if not found:\n            embed_url = self._html_search_meta('twitter:player', webpage, default=None)\n            if embed_url and embed_url != url:\n                self.report_detected('twitter:player iframe')\n                return [self.url_result(embed_url)]\n        if not found:\n            return []\n        domain_name = self._search_regex(r'^(?:https?://)?([^/]*)/.*', url, 'video uploader', default=None)\n        entries = []\n        for video_url in orderedSet(found):\n            video_url = video_url.encode().decode('unicode-escape')\n            video_url = unescapeHTML(video_url)\n            video_url = video_url.replace('\\\\/', '/')\n            video_url = urllib.parse.urljoin(url, video_url)\n            video_id = urllib.parse.unquote(os.path.basename(video_url))\n            if YoutubeIE.suitable(video_url):\n                entries.append(self.url_result(video_url, 'Youtube'))\n                continue\n            video_id = os.path.splitext(video_id)[0]\n            headers = {\n                'referer': actual_url\n            }\n            entry_info_dict = {\n                'id': video_id,\n                'uploader': domain_name,\n                'title': info_dict['title'],\n                'age_limit': info_dict['age_limit'],\n                'http_headers': headers,\n            }\n            if RtmpIE.suitable(video_url):\n                entry_info_dict.update({\n                    '_type': 'url_transparent',\n                    'ie_key': RtmpIE.ie_key(),\n                    'url': video_url,\n                })\n                entries.append(entry_info_dict)\n                continue\n            ext = determine_ext(video_url)\n            if ext == 'smil':\n                entry_info_dict = {**self._extract_smil_info(video_url, video_id), **entry_info_dict}\n            elif ext == 'xspf':\n                return [self._extract_xspf_playlist(video_url, video_id)]\n            elif ext == 'm3u8':\n                entry_info_dict['formats'], entry_info_dict['subtitles'] = self._extract_m3u8_formats_and_subtitles(video_url, video_id, ext='mp4', headers=headers)\n                self._extra_manifest_info(entry_info_dict, video_url)\n            elif ext == 'mpd':\n                entry_info_dict['formats'], entry_info_dict['subtitles'] = self._extract_mpd_formats_and_subtitles(video_url, video_id, headers=headers)\n                self._extra_manifest_info(entry_info_dict, video_url)\n            elif ext == 'f4m':\n                entry_info_dict['formats'] = self._extract_f4m_formats(video_url, video_id, headers=headers)\n            elif re.search(r'(?i)\\.(?:ism|smil)/manifest', video_url) and video_url != url:\n                entry_info_dict = self.url_result(\n                    smuggle_url(video_url, {'to_generic': True}),\n                    GenericIE.ie_key())\n            else:\n                entry_info_dict['url'] = video_url\n            entries.append(entry_info_dict)\n        if len(entries) > 1:\n            for num, e in enumerate(entries, start=1):\n                if e.get('title') is not None:\n                    e['title'] = '%s (%d)' % (e['title'], num)\n        return entries\n    def _real_extract(self, url):\n        video_id = self._match_id(url)\n        webpage, urlh = self._download_embed_webpage_handle(\n            video_id, headers=traverse_obj(parse_qs(url), {\n                'Referer': ('embed_parent_url', -1),\n                'Origin': ('embed_container_origin', -1)}))\n        redirect_url = urlh.url\n        if 'domain_not_allowed' in redirect_url:\n            domain = traverse_obj(parse_qs(redirect_url), ('allowed_domains[]', ...), get_all=False)\n            if not domain:\n                raise ExtractorError(\n                    'This is an embed-only presentation. Try passing --referer', expected=True)\n            webpage, _ = self._download_embed_webpage_handle(video_id, headers={\n                'Referer': f'https://{domain}/',\n                'Origin': f'https://{domain}',\n            })\n        player_token = self._search_regex(r'data-player-token=\"([^\"]+)\"', webpage, 'player token')\n        player_data = self._download_webpage(\n            f'https://ben.slideslive.com/player/{video_id}', video_id,\n            note='Downloading player info', query={'player_token': player_token})\n        player_info = self._extract_custom_m3u8_info(player_data)\n        service_name = player_info['service_name'].lower()\n        assert service_name in ('url', 'yoda', 'vimeo', 'youtube')\n        service_id = player_info['service_id']\n        slide_url_template = 'https://slides.slideslive.com/%s/slides/original/%s%s'\n        slides, slides_info = {}, []\n        if player_info.get('slides_json_url'):\n            slides = self._download_json(\n                player_info['slides_json_url'], video_id, fatal=False,\n                note='Downloading slides JSON', errnote=False) or {}\n            slide_ext_default = '.png'\n            slide_quality = traverse_obj(slides, ('slide_qualities', 0))\n            if slide_quality:\n                slide_ext_default = '.jpg'\n                slide_url_template = f'https://cdn.slideslive.com/data/presentations/%s/slides/{slide_quality}/%s%s'\n            for slide_id, slide in enumerate(traverse_obj(slides, ('slides', ...), expected_type=dict), 1):\n                slides_info.append((\n                    slide_id, traverse_obj(slide, ('image', 'name')),\n                    traverse_obj(slide, ('image', 'extname'), default=slide_ext_default),\n                    int_or_none(slide.get('time'), scale=1000)))\n        if not slides and player_info.get('slides_xml_url'):\n            slides = self._download_xml(\n                player_info['slides_xml_url'], video_id, fatal=False,\n                note='Downloading slides XML', errnote='Failed to download slides info')\n            if isinstance(slides, xml.etree.ElementTree.Element):\n                slide_url_template = 'https://cdn.slideslive.com/data/presentations/%s/slides/big/%s%s'\n                for slide_id, slide in enumerate(slides.findall('./slide')):\n                    slides_info.append((\n                        slide_id, xpath_text(slide, './slideName', 'name'), '.jpg',\n                        int_or_none(xpath_text(slide, './timeSec', 'time'))))\n        chapters, thumbnails = [], []\n        if url_or_none(player_info.get('thumbnail')):\n            thumbnails.append({'id': 'cover', 'url': player_info['thumbnail']})\n        for slide_id, slide_path, slide_ext, start_time in slides_info:\n            if slide_path:\n                thumbnails.append({\n                    'id': f'{slide_id:03d}',\n                    'url': slide_url_template % (video_id, slide_path, slide_ext),\n                })\n            chapters.append({\n                'title': f'Slide {slide_id:03d}',\n                'start_time': start_time,\n            })\n        subtitles = {}\n        for sub in traverse_obj(player_info, ('subtitles', ...), expected_type=dict):\n            webvtt_url = url_or_none(sub.get('webvtt_url'))\n            if not webvtt_url:\n                continue\n            subtitles.setdefault(sub.get('language') or 'en', []).append({\n                'url': webvtt_url,\n                'ext': 'vtt',\n            })\n        info = {\n            'id': video_id,\n            'title': player_info.get('title') or self._html_search_meta('title', webpage, default=''),\n            'timestamp': unified_timestamp(player_info.get('timestamp')),\n            'is_live': player_info.get('playlist_type') != 'vod',\n            'thumbnails': thumbnails,\n            'chapters': chapters,\n            'subtitles': subtitles,\n        }\n        if service_name == 'url':\n            info['url'] = service_id\n        elif service_name == 'yoda':\n            formats, duration = self._extract_formats_and_duration(\n                player_info['video_servers'][0], service_id, video_id)\n            info.update({\n                'duration': duration,\n                'formats': formats,\n            })\n        else:\n            info.update({\n                '_type': 'url_transparent',\n                'url': service_id,\n                'ie_key': service_name.capitalize(),\n                'display_id': video_id,\n            })\n            if service_name == 'vimeo':\n                info['url'] = smuggle_url(\n                    f'https://player.vimeo.com/video/{service_id}',\n                    {'http_headers': {'Referer': url}})\n        video_slides = traverse_obj(slides, ('slides', ..., 'video', 'id'))\n        if not video_slides:\n            return info\n        def entries():\n            yield info\n            service_data = self._download_json(\n                f'https://ben.slideslive.com/player/{video_id}/slides_video_service_data',\n                video_id, fatal=False, query={\n                    'player_token': player_token,\n                    'videos': ','.join(video_slides),\n                }, note='Downloading video slides info', errnote='Failed to download video slides info') or {}\n            for slide_id, slide in enumerate(traverse_obj(slides, ('slides', ...)), 1):\n                if not traverse_obj(slide, ('video', 'service')) == 'yoda':\n                    continue\n                video_path = traverse_obj(slide, ('video', 'id'))\n                cdn_hostname = traverse_obj(service_data, (\n                    video_path, 'video_servers', ...), get_all=False)\n                if not cdn_hostname or not video_path:\n                    continue\n                formats, _ = self._extract_formats_and_duration(\n                    cdn_hostname, video_path, video_id, skip_duration=True)\n                if not formats:\n                    continue\n                yield {\n                    'id': f'{video_id}-{slide_id:03d}',\n                    'title': f'{info[\"title\"]} - Slide {slide_id:03d}',\n                    'timestamp': info['timestamp'],\n                    'duration': int_or_none(traverse_obj(slide, ('video', 'duration_ms')), scale=1000),\n                    'formats': formats,\n                }\n        return self.playlist_result(entries(), f'{video_id}-playlist', info['title'])\n    def _real_extract(self, url):\n        video_id = self._match_id(url)\n        temp = video_id.split('-')\n        series_id = temp[0]\n        season_id = temp[1]\n        episode_id = temp[2]\n        webpage_url = 'https://w.duboku.io/vodplay/%s.html' % video_id\n        webpage_html = self._download_webpage(webpage_url, video_id)\n        player_data = self._search_regex(\n            self._PLAYER_DATA_PATTERN, webpage_html, 'player_data')\n        player_data = self._parse_json(player_data, video_id, js_to_json)\n        temp = get_elements_by_class('title', webpage_html)\n        series_title = None\n        title = None\n        for html in temp:\n            mobj = re.search(r'<a\\s+.*>(.*)</a>', html)\n            if mobj:\n                href = extract_attributes(mobj.group(0)).get('href')\n                if href:\n                    mobj1 = re.search(r'/(\\d+)\\.html', href)\n                    if mobj1 and mobj1.group(1) == series_id:\n                        series_title = clean_html(mobj.group(0))\n                        series_title = re.sub(r'[\\s\\r\\n\\t]+', ' ', series_title)\n                        title = clean_html(html)\n                        title = re.sub(r'[\\s\\r\\n\\t]+', ' ', title)\n                        break\n        data_url = player_data.get('url')\n        if not data_url:\n            raise ExtractorError('Cannot find url in player_data')\n        data_from = player_data.get('from')\n        headers = {'Referer': webpage_url}\n        if data_from == 'iframe':\n            return {\n                '_type': 'url_transparent',\n                'url': smuggle_url(data_url, {'http_headers': headers}),\n                'id': video_id,\n                'title': title,\n                'series': series_title,\n                'season_number': int_or_none(season_id),\n                'season_id': season_id,\n                'episode_number': int_or_none(episode_id),\n                'episode_id': episode_id,\n            }\n        formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers)\n        return {\n            'id': video_id,\n            'title': title,\n            'series': series_title,\n            'season_number': int_or_none(season_id),\n            'season_id': season_id,\n            'episode_number': int_or_none(episode_id),\n            'episode_id': episode_id,\n            'formats': formats,\n            'http_headers': headers\n        }\n    def _parse_video(self, video):\n        title = video['title']\n        vimeo_id = self._search_regex(\n            r'https?://player\\.vimeo\\.com/external/(\\d+)',\n            video['vimeoVideoURL'], 'vimeo id')\n        uploader_id = video.get('hostID')\n        return {\n            '_type': 'url_transparent',\n            'id': vimeo_id,\n            'title': title,\n            'description': video.get('description'),\n            'url': smuggle_url(\n                'https://player.vimeo.com/video/' + vimeo_id, {\n                    'http_headers': {\n                        'Referer': 'https://storyfire.com/',\n                    }\n                }),\n            'thumbnail': video.get('storyImage'),\n            'view_count': int_or_none(video.get('views')),\n            'like_count': int_or_none(video.get('likesCount')),\n            'comment_count': int_or_none(video.get('commentsCount')),\n            'duration': int_or_none(video.get('videoDuration')),\n            'timestamp': int_or_none(video.get('publishDate')),\n            'uploader': video.get('username'),\n            'uploader_id': uploader_id,\n            'uploader_url': format_field(uploader_id, None, 'https://storyfire.com/user/%s/video'),\n            'episode_number': int_or_none(video.get('episodeNumber') or video.get('episode_number')),\n        }\n    def _real_extract(self, url):\n        activity_id, enrollment_id = self._match_valid_url(url).group('id', 'enrollment')\n        course = self._call_api('enrollment', enrollment_id)['content']\n        activity = traverse_obj(course, ('learning_modules', ..., 'activities', lambda _, v: int(activity_id) == v['id']), get_all=False)\n        if activity.get('type') not in ['Video Activity', 'Lesson Activity']:\n            raise ExtractorError('The activity is not a video', expected=True)\n        module = next((m for m in course.get('learning_modules') or []\n                      if int(activity_id) in traverse_obj(m, ('activities', ..., 'id') or [])), None)\n        vimeo_id = self._get_vimeo_id(activity_id)\n        return {\n            '_type': 'url_transparent',\n            'series': traverse_obj(course, ('content_description', 'title')),\n            'series_id': str_or_none(traverse_obj(course, ('content_description', 'id'))),\n            'id': vimeo_id,\n            'chapter': module.get('title'),\n            'chapter_id': str_or_none(module.get('id')),\n            'title': activity.get('title'),\n            'url': smuggle_url(f'https://player.vimeo.com/video/{vimeo_id}', {'http_headers': {'Referer': 'https://api.cybrary.it'}})\n        }\n    def _real_extract(self, url):\n        qs = parse_qs(url)\n        src = urllib.parse.unquote(traverse_obj(qs, ('url', 0)) or '')\n        if src and YoutubeTabIE.suitable(src):\n            return self.url_result(src, YoutubeTabIE)\n        return self.url_result(smuggle_url(\n            urllib.parse.unquote(traverse_obj(qs, ('src', 0), ('url', 0))),\n            {'http_headers': {'Referer': url}}))\n    def _smuggle_referrer(url, referrer_url):\n        return smuggle_url(url, {'http_headers': {'Referer': referrer_url}})\n    def _unsmuggle_headers(self, url):\n        \"\"\"@returns (url, smuggled_data, headers)\"\"\"\n        url, data = unsmuggle_url(url, {})\n        headers = self.get_param('http_headers').copy()\n        if 'http_headers' in data:\n            headers.update(data['http_headers'])\n        return url, data, headers",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-46121",
        "description": "[{'lang': 'en', 'value': \"yt-dlp is a youtube-dl fork with additional features and fixes. The Generic Extractor in yt-dlp is vulnerable to an attacker setting an arbitrary proxy for a request to an arbitrary url, allowing the attacker to MITM the request made from yt-dlp's HTTP session. This could lead to cookie exfiltration in some cases. Version 2023.11.14 removed the ability to smuggle `http_headers` to the Generic extractor, as well as other extractors that use the same pattern. Users are advised to upgrade. Users unable to upgrade should disable the Ggneric extractor (or only pass trusted sites with trusted content) and ake caution when using `--no-check-certificate`.\"}]",
        "cwe_number": 444
      },
      "cwe_types": [],
      "severity": "low"
    },
    {
      "id": "ContextAssembler-68",
      "code": "def is_safe_url(url, host=None):\n    \"\"\"\n    Return ``True`` if the url is a safe redirection (i.e. it doesn't point to\n    a different host).\n    Always returns ``False`` on an empty url.\n    \"\"\"\n    if not url:\n        return False\n    netloc = urllib_parse.urlparse(url)[1]\n    return not netloc or netloc == host",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2013-6044",
        "description": "[{'lang': 'en', 'value': 'The is_safe_url function in utils/http.py in Django 1.4.x before 1.4.6, 1.5.x before 1.5.2, and 1.6 before beta 2 treats a URL\\'s scheme as safe even if it is not HTTP or HTTPS, which might introduce cross-site scripting (XSS) or other vulnerabilities into Django applications that use this function, as demonstrated by \"the login view in django.contrib.auth.views\" and the javascript: scheme.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-69",
      "code": "def generate(bits, randfunc, progress_func=None):\n    \"\"\"Randomly generate a fresh, new ElGamal key.\n    :Parameters:\n        bits : int\n            Key length, or size (in bits) of the modulus *p*.\n            Recommended value is 2048.\n        randfunc : callable\n            Random number generation function; it should accept\n            a single integer N and return a string of random data\n            N bytes long.\n        progress_func : callable\n            Optional function that will be called with a short string\n            containing the key parameter currently being generated;\n            it's useful for interactive applications where a user is\n            waiting for a key to be generated.\n    :attention: You should always use a cryptographically secure random number generator,\n        such as the one defined in the ``Crypto.Random`` module; **don't** just use the\n        current time and the ``random`` module.\n    :Return: An ElGamal key object (`ElGamalobj`).\n    \"\"\"\n    obj=ElGamalobj()\n    if progress_func:\n        progress_func('p\\n')\n    obj.p=bignum(getPrime(bits, randfunc))\n    if progress_func:\n        progress_func('g\\n')\n    size=bits-1-(ord(randfunc(1)) & 63)\n    if size<1:\n        size=bits-1\n    while (1):\n        obj.g=bignum(getPrime(size, randfunc))\n        if obj.g < obj.p:\n            break\n        size=(size+1) % bits\n        if size==0:\n            size=4\n    if progress_func:\n        progress_func('x\\n')\n    while (1):\n        size=bits-1-ord(randfunc(1))\n        if size>2:\n            break\n    while (1):\n        obj.x=bignum(getPrime(size, randfunc))\n        if obj.x < obj.p:\n            break\n        size = (size+1) % bits\n        if size==0:\n            size=4\n    if progress_func:\n        progress_func('y\\n')\n    obj.y = pow(obj.g, obj.x, obj.p)\n    return obj",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2012-2417",
        "description": "[{'lang': 'en', 'value': 'PyCrypto before 2.6 does not produce appropriate prime numbers when using an ElGamal scheme to generate a key, which reduces the signature space or public key space and makes it easier for attackers to conduct brute force attacks to obtain the private key.'}]",
        "cwe_number": 310
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-70",
      "code": "def _get_object(data, position, as_class, tz_aware, uuid_subtype):\n    obj_size = struct.unpack(\"<i\", data[position:position + 4])[0]\n    encoded = data[position + 4:position + obj_size - 1]\n    object = _elements_to_dict(encoded, as_class, tz_aware, uuid_subtype)\n    position += obj_size\n    if \"$ref\" in object:\n        return (DBRef(object.pop(\"$ref\"), object.pop(\"$id\"),\n                      object.pop(\"$db\", None), object), position)\n    return object, position",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2013-2132",
        "description": "[{'lang': 'en', 'value': 'bson/_cbsonmodule.c in the mongo-python-driver (aka. pymongo) before 2.5.2, as used in MongoDB, allows context-dependent attackers to cause a denial of service (NULL pointer dereference and crash) via vectors related to decoding of an \"invalid DBRef.\"'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-71",
      "code": "if __name__ == \"__main__\":\n  for index, (input_size_, filter_size_, output_size_, stride_,\n              padding_) in enumerate(GetShrunkInceptionShapes()):\n    setattr(Conv2DTest, \"testInceptionFwd_\" + str(index),\n            test_util.run_in_graph_and_eager_modes(\n                GetInceptionFwdTest(input_size_, filter_size_, stride_,\n                                    padding_)))\n    setattr(\n        Conv2DTest, \"testInceptionFwdDilatedConv_\" + str(index),\n        test_util.run_in_graph_and_eager_modes(GetInceptionFwdDilatedConvTest(\n            input_size_, filter_size_, stride_, padding_)))\n    setattr(Conv2DTest, \"testInceptionBackInput_\" + str(index),\n            test_util.run_in_graph_and_eager_modes(\n                GetInceptionBackInputTest(input_size_, filter_size_,\n                                          output_size_, stride_, padding_)))\n    setattr(Conv2DTest, \"testInceptionBackFilter_\" + str(index),\n            test_util.run_in_graph_and_eager_modes(\n                GetInceptionBackFilterTest(input_size_, filter_size_,\n                                           output_size_, [stride_, stride_],\n                                           padding_)))\n  ishape = [1, 400, 400, 1]\n  fshape = [1, 1, 1, 256]\n  oshape = [1, 400, 400, 256]\n  setattr(Conv2DTest, \"testInceptionFwd_No_Winograd_Nonfused\",\n          test_util.run_in_graph_and_eager_modes(\n              GetInceptionFwdTest(ishape, fshape, 1, \"SAME\", gpu_only=True)))\n  setattr(Conv2DTest, \"testInceptionFwdDilatedConv_No_Winograd_Nonfused\",\n          test_util.run_in_graph_and_eager_modes(\n              GetInceptionFwdDilatedConvTest(ishape, fshape, 1, \"SAME\")))\n  setattr(Conv2DTest, \"testInceptionBackInput_No_Winograd_Nonfused\",\n          test_util.run_in_graph_and_eager_modes(\n              GetInceptionBackInputTest(ishape, fshape, oshape, 1, \"SAME\",\n                                        gpu_only=True)))\n  setattr(Conv2DTest, \"testInceptionBackFilter_No_Winograd_Nonfused\",\n          test_util.run_in_graph_and_eager_modes(\n              GetInceptionBackFilterTest(ishape, fshape, oshape, [1, 1], \"SAME\",\n                                         gpu_only=True)))\n  test.main()",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-41885",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. When `tf.raw_ops.FusedResizeAndPadConv2D` is given a large tensor shape, it overflows. We have patched the issue in GitHub commit d66e1d568275e6a2947de97dca7a102a211e01ce. The fix will be included in TensorFlow 2.11. We will also cherrypick this commit on TensorFlow 2.10.1, 2.9.3, and TensorFlow 2.8.4, as these are also affected and still in supported range.'}]",
        "cwe_number": 131
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-72",
      "code": "  def testRoundTripThroughTensorProto(self):\n    value = ragged_factory_ops.constant([[1, 2], [3], [4, 5, 6]])\n    encoded = composite_tensor_ops.composite_tensor_to_variants(value)\n    proto = parsing_ops.SerializeTensor(tensor=encoded)\n    parsed = parsing_ops.ParseTensor(serialized=proto, out_type=dtypes.variant)\n    decoded = composite_tensor_ops.composite_tensor_from_variant(\n        parsed, value._type_spec)\n    self.assertAllEqual(value, decoded)\n  def testGradient(self):\n    def func(x):\n      x2 = composite_tensor_ops.composite_tensor_to_variants(x * 2)\n      x3 = composite_tensor_ops.composite_tensor_from_variant(x2, x._type_spec)\n      return x3.with_values(x3.values * math_ops.range(6.0))\n    x = ragged_factory_ops.constant([[1.0, 2.0, 3.0], [4.0], [5.0, 6.0]])\n    if context.executing_eagerly():\n      with backprop.GradientTape() as t:\n        t.watch(x.values)\n        y = func(x)\n        g = t.gradient(y.values, x.values)\n    else:\n      y = func(x)\n      g = gradients_impl.gradients(ys=y.values, xs=x.values)[0]\n    self.assertAllClose(g, [0.0, 2.0, 4.0, 6.0, 8.0, 10.0])",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-41909",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. An input `encoded` that is not a valid `CompositeTensorVariant` tensor will trigger a segfault in `tf.raw_ops.CompositeTensorVariantToComponents`. We have patched the issue in GitHub commits bf594d08d377dc6a3354d9fdb494b32d45f91971 and 660ce5a89eb6766834bdc303d2ab3902aef99d3d. The fix will be included in TensorFlow 2.11. We will also cherrypick this commit on TensorFlow 2.10.1, 2.9.3, and TensorFlow 2.8.4, as these are also affected and still in supported range.'}]",
        "cwe_number": 476
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-73",
      "code": "    def create_option(self, name, value, label, selected, index, subindex=None, attrs=None):\n        result = super(TagFormWidget, self).create_option(\n            name=name, value=value, label=label, selected=selected,\n            index=index, subindex=subindex, attrs=attrs\n        )\n        result['attrs']['data-color'] = self.queryset.get(pk=value).color\n        return result",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2018-16407",
        "description": "[{'lang': 'en', 'value': 'An issue was discovered in Mayan EDMS before 3.0.3. The Tags app has XSS because tag label values are mishandled.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-74",
      "code": "def generate_auth_token(user_id):\n    host_list = request.host.rsplit(':')\n    if len(host_list) == 1:\n        host = ':'.join(host_list)\n    else:\n        host = ':'.join(host_list[0:-1])\n    if host.startswith('127.') or host.lower() == 'localhost' or host.startswith('[::ffff:7f'):\n        warning = _('PLease access calibre-web from non localhost to get valid api_endpoint for kobo device')\n        return render_title_template(\n            \"generate_kobo_auth_url.html\",\n            title=_(u\"Kobo Setup\"),\n            warning = warning\n        )\n    else:\n        auth_token = ub.session.query(ub.RemoteAuthToken).filter(\n            ub.RemoteAuthToken.user_id == user_id\n        ).filter(ub.RemoteAuthToken.token_type==1).first()\n        if not auth_token:\n            auth_token = ub.RemoteAuthToken()\n            auth_token.user_id = user_id\n            auth_token.expiration = datetime.max\n            auth_token.auth_token = (hexlify(urandom(16))).decode(\"utf-8\")\n            auth_token.token_type = 1\n            ub.session.add(auth_token)\n            ub.session_commit()\n        books = calibre_db.session.query(db.Books).join(db.Data).all()\n        for book in books:\n            formats = [data.format for data in book.data]\n            if not 'KEPUB' in formats and config.config_kepubifypath and 'EPUB' in formats:\n                helper.convert_book_format(book.id, config.config_calibre_dir, 'EPUB', 'KEPUB', current_user.name)\n        return render_title_template(\n            \"generate_kobo_auth_url.html\",\n            title=_(u\"Kobo Setup\"),\n            kobo_auth_url=url_for(\n                \"kobo.TopLevelEndpoint\", auth_token=auth_token.auth_token, _external=True\n            ),\n            warning = False\n        )\ndef delete_auth_token(user_id):\n    ub.session.query(ub.RemoteAuthToken).filter(ub.RemoteAuthToken.user_id == user_id)\\\n        .filter(ub.RemoteAuthToken.token_type==1).delete()\n    return ub.session_commit()\ndef _delete_user(content):\n    if ub.session.query(ub.User).filter(ub.User.role.op('&')(constants.ROLE_ADMIN) == constants.ROLE_ADMIN,\n                                        ub.User.id != content.id).count():\n        if content.name != \"Guest\":\n            ub.session.query(ub.ReadBook).filter(content.id == ub.ReadBook.user_id).delete()\n            ub.session.query(ub.Downloads).filter(content.id == ub.Downloads.user_id).delete()\n            for us in ub.session.query(ub.Shelf).filter(content.id == ub.Shelf.user_id):\n                ub.session.query(ub.BookShelf).filter(us.id == ub.BookShelf.shelf).delete()\n            ub.session.query(ub.Shelf).filter(content.id == ub.Shelf.user_id).delete()\n            ub.session.query(ub.Bookmark).filter(content.id == ub.Bookmark.user_id).delete()\n            ub.session.query(ub.User).filter(ub.User.id == content.id).delete()\n            ub.session.query(ub.ArchivedBook).filter(ub.ArchivedBook.user_id == content.id).delete()\n            ub.session.query(ub.RemoteAuthToken).filter(ub.RemoteAuthToken.user_id == content.id).delete()\n            ub.session.query(ub.User_Sessions).filter(ub.User_Sessions.user_id == content.id).delete()\n            ub.session.query(ub.KoboSyncedBooks).filter(ub.KoboSyncedBooks.user_id == content.id).delete()\n            kobo_entries = ub.session.query(ub.KoboReadingState).filter(ub.KoboReadingState.user_id == content.id).all()\n            for kobo_entry in kobo_entries:\n                ub.session.delete(kobo_entry)\n            ub.session_commit()\n            log.info(u\"User {} deleted\".format(content.name))\n            return(_(u\"User '%(nick)s' deleted\", nick=content.name))\n        else:\n            log.warning(_(u\"Can't delete Guest User\"))\n            raise Exception(_(u\"Can't delete Guest User\"))\n    else:\n        log.warning(u\"No admin user remaining, can't delete user\")\n        raise Exception(_(u\"No admin user remaining, can't delete user\"))",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-0339",
        "description": "[{'lang': 'en', 'value': 'Server-Side Request Forgery (SSRF) in Pypi calibreweb prior to 0.6.16.'}]",
        "cwe_number": 918
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-75",
      "code": "  def test_sparse_bincount_all_count(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    size = 1000\n    n_elems = 4096\n    inp_indices = np.random.randint(0, num_rows, (n_elems,))\n    inp_vals = np.random.randint(0, size, (n_elems,), dtype=dtype)\n    np_out = np.bincount(inp_vals, minlength=size)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_indices,\n                values=inp_vals,\n                dense_shape=[num_rows],\n                size=size,\n                weights=[])))\n  def test_sparse_bincount_all_count_with_weights(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    size = 1000\n    n_elems = 4096\n    inp_indices = np.random.randint(0, num_rows, (n_elems,))\n    inp_vals = np.random.randint(0, size, (n_elems,), dtype=dtype)\n    inp_weight = np.random.random((n_elems,))\n    np_out = np.bincount(inp_vals, minlength=size, weights=inp_weight)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_indices,\n                values=inp_vals,\n                dense_shape=[num_rows],\n                size=size,\n                weights=inp_weight)))\n  def test_sparse_bincount_all_binary(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    size = 10\n    n_elems = 4096\n    inp_indices = np.random.randint(0, num_rows, (n_elems,))\n    inp_vals = np.random.randint(0, size, (n_elems,), dtype=dtype)\n    np_out = np.ones((size,))\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_indices,\n                values=inp_vals,\n                dense_shape=[num_rows],\n                size=size,\n                weights=[],\n                binary_output=True)))\n  def test_sparse_bincount_all_binary_weights(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    size = 10\n    n_elems = 4096\n    inp_indices = np.random.randint(0, num_rows, (n_elems,))\n    inp_vals = np.random.randint(0, size, (n_elems,), dtype=dtype)\n    inp_weight = np.random.random((n_elems,))\n    np_out = np.ones((size,))\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_indices,\n                values=inp_vals,\n                dense_shape=[num_rows],\n                size=size,\n                weights=inp_weight,\n                binary_output=True)))\nclass RaggedBincountOpTest(test_util.TensorFlowTestCase,\n                           parameterized.TestCase):\n  def test_ragged_bincount_count(self, dtype):\n    x = ragged_factory_ops.constant([[], [], [3, 0, 1], [], [5, 0, 4, 4]])\n    expected_output = [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0,\n                                            0], [1, 1, 0, 1, 0, 0],\n                       [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 2, 1]]\n    self.assertAllEqual(\n        expected_output,\n        self.evaluate(\n            gen_math_ops.ragged_bincount(\n                splits=x.row_splits, values=x.values, weights=[], size=6)))",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-35982",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. If `SparseBincount` is given inputs for `indices`, `values`, and `dense_shape` that do not make a valid sparse tensor, it results in a segfault that can be used to trigger a denial of service attack. We have patched the issue in GitHub commit 40adbe4dd15b582b0210dfbf40c243a62f5119fa. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-76",
      "code": "    def _get_shared_models(self, args: \"DictConfig\") -> Dict[str, dict]:\n        with open(args.blueprint.model_opt_path) as f:\n            all_model_opts = yaml.load(f.read())\n        active_model_opts = {\n            model: opt\n            for model, opt in all_model_opts.items()\n            if self.conversations_needed[model] > 0\n        }\n        return TurkLikeAgent.get_bot_agents(args=args, model_opts=active_model_opts)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-39207",
        "description": "[{'lang': 'en', 'value': 'parlai is a framework for training and evaluating AI models on a variety of openly available dialogue datasets. In affected versions the package is vulnerable to YAML deserialization attack caused by unsafe loading which leads to Arbitary code execution. This security bug is patched by avoiding unsafe loader users should update to version above v1.1.0. If upgrading is not possible then users can change the Loader used to SafeLoader as a workaround. See commit 507d066ef432ea27d3e201da08009872a2f37725 for details.'}]",
        "cwe_number": 502
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-77",
      "code": "def add_security_headers(resp):\n    resp.headers['Content-Security-Policy'] = \"default-src 'self' 'unsafe-inline' 'unsafe-eval';\"\n    if request.endpoint == \"editbook.edit_book\":\n        resp.headers['Content-Security-Policy'] += \"img-src * data:\"\n    resp.headers['X-Content-Type-Options'] = 'nosniff'\n    resp.headers['X-Frame-Options'] = 'SAMEORIGIN'\n    resp.headers['X-XSS-Protection'] = '1; mode=block'\n    resp.headers['Strict-Transport-Security'] = 'max-age=31536000; includeSubDomains'\n    return resp",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-25965",
        "description": "[{'lang': 'en', 'value': 'In Calibre-web, versions 0.6.0 to 0.6.13 are vulnerable to Cross-Site Request Forgery (CSRF). By luring an authenticated user to click on a link, an attacker can create a new user role with admin privileges and attacker-controlled credentials, allowing them to take over the application.'}]",
        "cwe_number": 352
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-78",
      "code": "def uploadimage():\n    if not request.json or \"image\" not in request.json:\n        print(\"No data sent or no image provided. Aborting with 400.\")\n        abort(400)\n    im_b64 = request.json[\"image\"]\n    img_bytes = base64.b64decode(im_b64.encode(\"utf-8\"))\n    img_bytes, valid = allowed_file(img_bytes)\n    if not valid:\n        return escape({\"entry\": \"False\"})\n    img = Image.open(io.BytesIO(img_bytes))\n    file_ending = img.format\n    print(f\"File has filetype {file_ending}.\")\n    if file_ending == \"JPEG\":\n        file_ending = \".jpg\"\n    else:\n        file_ending = \".png\"\n    one_hundred_million = 100000000\n    lots_of_nine = 999999999\n    file_name = None\n    f = open(\"all_files\", \"r\")\n    all_files = ast.literal_eval(f.read())\n    f.close()\n    attempt = 0\n    while file_name is None or file_name in all_files:\n        if attempt <= 1000:\n            file_name = random.randint(one_hundred_million, lots_of_nine)\n            file_name = base64.b64encode(str(file_name).encode(\"utf-8\")).decode(\"utf-8\")\n            print(f\"Trying new file name: {file_name}\")\n        else:\n            attempt = 0\n            one_hundred_million += 100000\n            lots_of_nine += 1000000\n            while one_hundred_million >= lots_of_nine:\n                one_hundred_million -= 10000\n            one_hundred_million -= 10000\n    print(f\"Successful file name: {file_name}\")\n    title = request.json[\"title\"]\n    if title[:9] == \"[PAUSED] \":\n        title = title[9::]\n    singer = request.json[\"singer\"]\n    album = request.json[\"album\"]\n    file_db_entry = [\n        {\"title\": title, \"singer\": singer, \"album\": album},\n        file_name,\n        file_ending,\n    ]\n    print(f\"New db entry: {file_db_entry}\")\n    all_files.append(file_db_entry)\n    cache, x, y = get_config()\n    del x\n    del y\n    length = len(all_files)\n    while (\n        length > cache\n    ):\n        filename = all_files[0][1] + all_files[0][2]\n        remove(filename)\n        del all_files[0]\n        length = len(all_files)\n    f = open(\"all_files\", \"w\")\n    f.write(str(all_files))\n    f.close()\n    file_name = file_name + file_ending\n    img.save(file_name)\n    print(f\"Saved {file_name} from {file_db_entry}.\")\n    print(f\"Returning {file_db_entry}.\")\n    return escape(str({\"entry\": file_db_entry}))",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-23609",
        "description": "[{'lang': 'en', 'value': 'iTunesRPC-Remastered is a Discord Rich Presence for iTunes on Windows utility. In affected versions iTunesRPC-Remastered did not properly sanitize user input used to remove files leading to file deletion only limited by the process permissions. Users are advised to upgrade as soon as possible.'}]",
        "cwe_number": 22
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-79",
      "code": "    def skip(self, type):\n        if type == TType.STOP:\n            return\n        elif type == TType.BOOL:\n            self.readBool()\n        elif type == TType.BYTE:\n            self.readByte()\n        elif type == TType.I16:\n            self.readI16()\n        elif type == TType.I32:\n            self.readI32()\n        elif type == TType.I64:\n            self.readI64()\n        elif type == TType.DOUBLE:\n            self.readDouble()\n        elif type == TType.FLOAT:\n            self.readFloat()\n        elif type == TType.STRING:\n            self.readString()\n        elif type == TType.STRUCT:\n            name = self.readStructBegin()\n            while True:\n                (name, type, id) = self.readFieldBegin()\n                if type == TType.STOP:\n                    break\n                self.skip(type)\n                self.readFieldEnd()\n            self.readStructEnd()\n        elif type == TType.MAP:\n            (ktype, vtype, size) = self.readMapBegin()\n            for _ in range(size):\n                self.skip(ktype)\n                self.skip(vtype)\n            self.readMapEnd()\n        elif type == TType.SET:\n            (etype, size) = self.readSetBegin()\n            for _ in range(size):\n                self.skip(etype)\n            self.readSetEnd()\n        elif type == TType.LIST:\n            (etype, size) = self.readListBegin()\n            for _ in range(size):\n                self.skip(etype)\n            self.readListEnd()",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2019-3552",
        "description": "[{'lang': 'en', 'value': 'C++ Facebook Thrift servers (using cpp2) would not error upon receiving messages with containers of fields of unknown type. As a result, malicious clients could send short messages which would take a long time for the server to parse, potentially leading to denial of service. This issue affects Facebook Thrift prior to v2019.02.18.00.'}]",
        "cwe_number": 755
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-80",
      "code": "def _is_javascript_scheme(s):\n    if _is_image_dataurl(s):\n        return None\n    return _is_possibly_malicious_scheme(s)\n_substitute_whitespace = re.compile(r'[\\s\\x00-\\x08\\x0B\\x0C\\x0E-\\x19]+').sub\n_conditional_comment_re = re.compile(\n    r'\\[if[\\s\\n\\r]+.*?][\\s\\n\\r]*>', re.I|re.S)\n_find_styled_elements = etree.XPath(\n    \"descendant-or-self::*[@style]\")\n_find_external_links = etree.XPath(\n    (\"descendant-or-self::a  [normalize-space(@href) and substring(normalize-space(@href),1,1) != '\n     \"descendant-or-self::x:a[normalize-space(@href) and substring(normalize-space(@href),1,1) != '\n    namespaces={'x':XHTML_NAMESPACE})",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-43818",
        "description": "[{'lang': 'en', 'value': 'lxml is a library for processing XML and HTML in the Python language. Prior to version 4.6.5, the HTML Cleaner in lxml.html lets certain crafted script content pass through, as well as script content in SVG files embedded using data URIs. Users that employ the HTML cleaner in a security relevant context should upgrade to lxml 4.6.5 to receive a patch. There are no known workarounds available.'}]",
        "cwe_number": 74
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-81",
      "code": "    def __init__(self, fobj):\n        \"\"\"\n        fobj is a file-like object as an icns resource\n        \"\"\"\n        self.dct = dct = {}\n        self.fobj = fobj\n        sig, filesize = nextheader(fobj)\n        if sig != b'icns':\n            raise SyntaxError('not an icns file')\n        i = HEADERSIZE\n        while i < filesize:\n            sig, blocksize = nextheader(fobj)\n            i += HEADERSIZE\n            blocksize -= HEADERSIZE\n            dct[sig] = (i, blocksize)\n            fobj.seek(blocksize, 1)\n            i += blocksize\n    def itersizes(self):\n        sizes = []\n        for size, fmts in self.SIZES.items():\n            for (fmt, reader) in fmts:\n                if fmt in self.dct:\n                    sizes.append(size)\n                    break\n        return sizes",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2014-3589",
        "description": "[{'lang': 'en', 'value': 'PIL/IcnsImagePlugin.py in Python Imaging Library (PIL) and Pillow before 2.3.2 and 2.5.x before 2.5.2 allows remote attackers to cause a denial of service via a crafted block size.'}]",
        "cwe_number": 20
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-82",
      "code": "    def _redirect_safe(self, url, default=None):\n        \"\"\"Redirect if url is on our PATH\n        Full-domain redirects are allowed if they pass our CORS origin checks.\n        Otherwise use default (self.base_url if unspecified).\n        \"\"\"\n        if default is None:\n            default = self.base_url\n        parsed = urlparse(url)\n        if parsed.netloc or not (parsed.path + '/').startswith(self.base_url):\n            allow = False\n            if parsed.netloc:\n                origin = '%s://%s' % (parsed.scheme, parsed.netloc)\n                origin = origin.lower()\n                if self.allow_origin:\n                    allow = self.allow_origin == origin\n                elif self.allow_origin_pat:\n                    allow = bool(self.allow_origin_pat.match(origin))\n            if not allow:\n                self.log.warning(\"Not allowing login redirect to %r\" % url)\n                url = default\n        self.redirect(url)\n    def get(self):\n        if self.current_user:\n            next_url = self.get_argument('next', default=self.base_url)\n            self._redirect_safe(next_url)\n        else:\n            self._render()",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2019-10255",
        "description": "[{'lang': 'en', 'value': 'An Open Redirect vulnerability for all browsers in Jupyter Notebook before 5.7.7 and some browsers (Chrome, Firefox) in JupyterHub before 0.9.5 allows crafted links to the login page, which will redirect to a malicious site after successful login. Servers running on a base_url prefix are not affected.'}]",
        "cwe_number": 601
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-83",
      "code": "    def run(self):\n        os.chdir(self.config.project_target_path)\n        shutil.copyfile(DOCS_INDEX_FILE_PATH, \"index.html\")\n        port = self.args.port\n        if self.args.browser:\n            webbrowser.open_new_tab(f\"http://localhost:{port}\")\n        with socketserver.TCPServer((\"\", port), SimpleHTTPRequestHandler) as httpd:\n            click.echo(f\"Serving docs at {port}\")\n            click.echo(f\"To access from your browser, navigate to: http://localhost:{port}\")\n            click.echo(\"\\n\\n\")\n            click.echo(\"Press Ctrl+C to exit.\")\n            httpd.serve_forever()",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-36105",
        "description": "[{'lang': 'en', 'value': 'dbt enables data analysts and engineers to transform their data using the same practices that software engineers use to build applications. Prior to versions 1.6.15, 1.7.15, and 1.8.1, Binding to `INADDR_ANY (0.0.0.0)` or `IN6ADDR_ANY (::)` exposes an application on all network interfaces, increasing the risk of unauthorized access. As stated in the Python docs, a special form for address is accepted instead of a host address: `\\'\\'` represents `INADDR_ANY`, equivalent to `\"0.0.0.0\"`. On systems with IPv6, \\'\\' represents `IN6ADDR_ANY`, which is equivalent to `\"::\"`. A user who serves docs on an unsecured public network, may unknowingly be hosting an unsecured (http) web site for any remote user/system to access on the same network. The issue has has been mitigated in dbt-core v1.6.15, dbt-core v1.7.15, and dbt-core v1.8.1 by binding to localhost explicitly by default in `dbt docs serve`.\\n'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-84",
      "code": "    def visit_For(self, node):\n        if isinstance(node.iter, vy_ast.Subscript):\n            raise StructureException(\"Cannot iterate over a nested list\", node.iter)\n        if isinstance(node.iter, vy_ast.Call):\n            if node.iter.get(\"func.id\") != \"range\":\n                raise IteratorException(\n                    \"Cannot iterate over the result of a function call\", node.iter\n                )\n            range_ = node.iter\n            validate_call_args(range_, (1, 2), kwargs=[\"bound\"])\n            args = range_.args\n            kwargs = {s.arg: s.value for s in range_.keywords or []}\n            if len(args) == 1:\n                n = args[0]\n                bound = kwargs.pop(\"bound\", None)\n                validate_expected_type(n, IntegerT.any())\n                if bound is None:\n                    if not isinstance(n, vy_ast.Num):\n                        raise StateAccessViolation(\"Value must be a literal\", n)\n                    if n.value <= 0:\n                        raise StructureException(\"For loop must have at least 1 iteration\", args[0])\n                    type_list = get_possible_types_from_node(n)\n                else:\n                    if not isinstance(bound, vy_ast.Num):\n                        raise StateAccessViolation(\"bound must be a literal\", bound)\n                    if bound.value <= 0:\n                        raise StructureException(\"bound must be at least 1\", args[0])\n                    type_list = get_common_types(n, bound)\n            else:\n                if range_.keywords:\n                    raise StructureException(\n                        \"Keyword arguments are not supported for `range(N, M)` and\"\n                        \"`range(x, x + N)` expressions\",\n                        range_.keywords[0],\n                    )\n                validate_expected_type(args[0], IntegerT.any())\n                type_list = get_common_types(*args)\n                if not isinstance(args[0], vy_ast.Constant):\n                    if not isinstance(args[1], vy_ast.BinOp) or not isinstance(\n                        args[1].op, vy_ast.Add\n                    ):\n                        raise StructureException(\n                            \"Second element must be the first element plus a literal value\", args[0]\n                        )\n                    if not vy_ast.compare_nodes(args[0], args[1].left):\n                        raise StructureException(\n                            \"First and second variable must be the same\", args[1].left\n                        )\n                    if not isinstance(args[1].right, vy_ast.Int):\n                        raise InvalidLiteral(\"Literal must be an integer\", args[1].right)\n                    if args[1].right.value < 1:\n                        raise StructureException(\n                            f\"For loop has invalid number of iterations ({args[1].right.value}),\"\n                            \" the value must be greater than zero\",\n                            args[1].right,\n                        )\n                else:\n                    if not isinstance(args[1], vy_ast.Int):\n                        raise InvalidType(\"Value must be a literal integer\", args[1])\n                    validate_expected_type(args[1], IntegerT.any())\n                    if args[0].value >= args[1].value:\n                        raise StructureException(\"Second value must be > first value\", args[1])\n                if not type_list:\n                    raise TypeMismatch(\"Iterator values are of different types\", node.iter)\n        else:\n            if isinstance(node.iter, vy_ast.List) and len(node.iter.elements) == 0:\n                raise StructureException(\"For loop must have at least 1 iteration\", node.iter)\n            type_list = [\n                i.value_type\n                for i in get_possible_types_from_node(node.iter)\n                if isinstance(i, (DArrayT, SArrayT))\n            ]\n        if not type_list:\n            raise InvalidType(\"Not an iterable type\", node.iter)\n        if isinstance(node.iter, (vy_ast.Name, vy_ast.Attribute)):\n            assign = _check_iterator_modification(node.iter, node)\n            if assign:\n                raise ImmutableViolation(\"Cannot modify array during iteration\", assign)\n        iter_is_storage_var = (\n            isinstance(node.iter, vy_ast.Attribute)\n            and len(node.iter.get_descendants(vy_ast.Name, {\"id\": \"self\"})) > 0\n        )\n        if iter_is_storage_var:\n            iter_name = node.iter.attr\n            for call_node in node.get_descendants(vy_ast.Call, {\"func.value.id\": \"self\"}):\n                fn_name = call_node.func.attr\n                fn_node = self.vyper_module.get_children(vy_ast.FunctionDef, {\"name\": fn_name})[0]\n                if _check_iterator_modification(node.iter, fn_node):\n                    raise ImmutableViolation(\n                        f\"Cannot call '{fn_name}' inside for loop, it potentially \"\n                        f\"modifies iterated storage variable '{iter_name}'\",\n                        call_node,\n                    )\n                for reachable_t in (\n                    self.namespace[\"self\"].typ.members[fn_name].reachable_internal_functions\n                ):\n                    name = reachable_t.name\n                    fn_node = self.vyper_module.get_children(vy_ast.FunctionDef, {\"name\": name})[0]\n                    if _check_iterator_modification(node.iter, fn_node):\n                        raise ImmutableViolation(\n                            f\"Cannot call '{fn_name}' inside for loop, it may call to '{name}' \"\n                            f\"which potentially modifies iterated storage variable '{iter_name}'\",\n                            call_node,\n                        )\n        if not isinstance(node.target, vy_ast.Name):\n            raise StructureException(\"Invalid syntax for loop iterator\", node.target)\n        for_loop_exceptions = []\n        iter_name = node.target.id\n        for possible_target_type in type_list:\n            with self.namespace.enter_scope():\n                self.namespace[iter_name] = VarInfo(possible_target_type, is_constant=True)\n                try:\n                    with NodeMetadata.enter_typechecker_speculation():\n                        for n in node.body:\n                            self.visit(n)\n                except (TypeMismatch, InvalidOperation) as exc:\n                    for_loop_exceptions.append(exc)\n                else:\n                    self.expr_visitor.visit(node.target, possible_target_type)\n                    if isinstance(node.iter, (vy_ast.Name, vy_ast.Attribute)):\n                        iter_type = get_exact_type_from_node(node.iter)\n                        validate_expected_type(node.target, iter_type.value_type)\n                        self.expr_visitor.visit(node.iter, iter_type)\n                    if isinstance(node.iter, vy_ast.List):\n                        len_ = len(node.iter.elements)\n                        self.expr_visitor.visit(node.iter, SArrayT(possible_target_type, len_))\n                    if isinstance(node.iter, vy_ast.Call) and node.iter.func.id == \"range\":\n                        for a in node.iter.args:\n                            self.expr_visitor.visit(a, possible_target_type)\n                        for a in node.iter.keywords:\n                            if a.arg == \"bound\":\n                                self.expr_visitor.visit(a.value, possible_target_type)\n                    return\n        if len(set(str(i) for i in for_loop_exceptions)) == 1:\n            raise for_loop_exceptions[0]\n        types_str = [str(i) for i in type_list]\n        given_str = f\"{', '.join(types_str[:1])} or {types_str[-1]}\"\n        raise TypeMismatch(\n            f\"Iterator value '{iter_name}' may be cast as {given_str}, \"\n            \"but type checking fails with all possible types:\",\n            node,\n            *(\n                (f\"Casting '{iter_name}' as {typ}: {exc.message}\", exc.annotations[0])\n                for typ, exc in zip(type_list, for_loop_exceptions)\n            ),\n        )\n    def _check_valid_range_constant(self, arg_ast_node):\n        with self.context.range_scope():\n            arg_expr = Expr.parse_value_expr(arg_ast_node, self.context)\n        return arg_expr\n    def _get_range_const_value(self, arg_ast_node):\n        arg_expr = self._check_valid_range_constant(arg_ast_node)\n        return arg_expr.value\n    def _parse_For_range(self):\n        if \"type\" in self.stmt.target._metadata:\n            iter_typ = self.stmt.target._metadata[\"type\"]\n        else:\n            iter_typ = INT256_T\n        arg0 = self.stmt.iter.args[0]\n        num_of_args = len(self.stmt.iter.args)\n        kwargs = {\n            s.arg: Expr.parse_value_expr(s.value, self.context)\n            for s in self.stmt.iter.keywords or []\n        }\n        if num_of_args == 1:\n            n = Expr.parse_value_expr(arg0, self.context)\n            start = IRnode.from_list(0, typ=iter_typ)\n            rounds = n\n            rounds_bound = kwargs.get(\"bound\", rounds)\n        elif self._check_valid_range_constant(self.stmt.iter.args[1]).is_literal:\n            arg0_val = self._get_range_const_value(arg0)\n            arg1_val = self._get_range_const_value(self.stmt.iter.args[1])\n            start = IRnode.from_list(arg0_val, typ=iter_typ)\n            rounds = IRnode.from_list(arg1_val - arg0_val, typ=iter_typ)\n            rounds_bound = rounds\n        else:\n            arg1 = self.stmt.iter.args[1]\n            rounds = self._get_range_const_value(arg1.right)\n            start = Expr.parse_value_expr(arg0, self.context)\n            _, hi = start.typ.int_bounds\n            start = clamp(\"le\", start, hi + 1 - rounds)\n            rounds_bound = rounds\n        bound = rounds_bound if isinstance(rounds_bound, int) else rounds_bound.value\n        if bound < 1:\n            return\n        varname = self.stmt.target.id\n        i = IRnode.from_list(self.context.fresh_varname(\"range_ix\"), typ=UINT256_T)\n        iptr = self.context.new_variable(varname, iter_typ)\n        self.context.forvars[varname] = True\n        loop_body = [\"seq\"]\n        loop_body.append([\"mstore\", iptr, i])\n        loop_body.append(parse_body(self.stmt.body, self.context))\n        ir_node = IRnode.from_list(\n            [\"repeat\", i, start, rounds, rounds_bound, loop_body], error_msg=\"range() bounds check\"\n        )\n        del self.context.forvars[varname]\n        return ir_node\n    def __init__(self, message=\"Error Message not found.\", *items):\n        \"\"\"\n        Exception initializer.\n        Arguments\n        ---------\n        message : str\n            Error message to display with the exception.\n        *items : VyperNode | Tuple[str, VyperNode], optional\n            Vyper ast node(s), or tuple of (description, node) indicating where\n            the exception occured. Source annotations are generated in the order\n            the nodes are given.\n            A single tuple of (lineno, col_offset) is also understood to support\n            the old API, but new exceptions should not use this approach.\n        \"\"\"\n        self.message = message\n        self.lineno = None\n        self.col_offset = None\n        self.annotations = None\n        if len(items) == 1 and isinstance(items[0], tuple) and isinstance(items[0][0], int):\n            self.lineno, self.col_offset = items[0][:2]\n        else:\n            self.annotations = [k for k in items if k is not None]\n    def is_literal(self):\n        return isinstance(self.value, int) or self.value == \"multi\"\n    def is_pointer(self):\n        return self.location is not None",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-32481",
        "description": "[{'lang': 'en', 'value': 'Vyper is a pythonic Smart Contract Language for the Ethereum virtual machine. Starting in version 0.3.8 and prior to version 0.4.0b1, when looping over a `range` of the form `range(start, start + N)`, if `start` is negative, the execution will always revert. This issue is caused by an incorrect assertion inserted by the code generation of the range `stmt.parse_For_range()`. The issue arises when `start` is signed, instead of using `sle`, `le` is used and `start` is interpreted as an unsigned integer for the comparison. If it is a negative number, its 255th bit is set to `1` and is hence interpreted as a very large unsigned integer making the assertion always fail. Any contract having a `range(start, start + N)` where `start` is a signed integer with the possibility for `start` to be negative is affected. If a call goes through the loop while supplying a negative `start` the execution will revert. Version 0.4.0b1 fixes the issue.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-85",
      "code": "    def authenticate(self, context, credentials=None, ec2Credentials=None):\n        \"\"\"Validate a signed EC2 request and provide a token.\n        Other services (such as Nova) use this **admin** call to determine\n        if a request they signed received is from a valid user.\n        If it is a valid signature, an openstack token that maps\n        to the user/tenant is returned to the caller, along with\n        all the other details returned from a normal token validation\n        call.\n        The returned token is useful for making calls to other\n        OpenStack services within the context of the request.\n        :param context: standard context\n        :param credentials: dict of ec2 signature\n        :param ec2Credentials: DEPRECATED dict of ec2 signature\n        :returns: token: openstack token equivalent to access key along\n                         with the corresponding service catalog and roles\n        \"\"\"\n        if not credentials and ec2Credentials:\n            credentials = ec2Credentials\n        if not 'access' in credentials:\n            raise exception.Unauthorized(message='EC2 signature not supplied.')\n        creds_ref = self._get_credentials(context,\n                                          credentials['access'])\n        self.check_signature(creds_ref, credentials)\n        token_id = uuid.uuid4().hex\n        tenant_ref = self.identity_api.get_tenant(\n            context=context,\n            tenant_id=creds_ref['tenant_id'])\n        user_ref = self.identity_api.get_user(\n            context=context,\n            user_id=creds_ref['user_id'])\n        metadata_ref = self.identity_api.get_metadata(\n            context=context,\n            user_id=user_ref['id'],\n            tenant_id=tenant_ref['id'])\n        catalog_ref = self.catalog_api.get_catalog(\n            context=context,\n            user_id=user_ref['id'],\n            tenant_id=tenant_ref['id'],\n            metadata=metadata_ref)\n        token_ref = self.token_api.create_token(\n            context, token_id, dict(id=token_id,\n                                    user=user_ref,\n                                    tenant=tenant_ref,\n                                    metadata=metadata_ref))\n        roles_ref = []\n        for role_id in metadata_ref.get('roles', []):\n            roles_ref.append(self.identity_api.get_role(context, role_id))\n        token_controller = service.TokenController()\n        return token_controller._format_authenticate(\n            token_ref, roles_ref, catalog_ref)\n    def create_credential(self, context, user_id, tenant_id):\n        \"\"\"Create a secret/access pair for use with ec2 style auth.\n        Generates a new set of credentials that map the the user/tenant\n        pair.\n        :param context: standard context\n        :param user_id: id of user\n        :param tenant_id: id of tenant\n        :returns: credential: dict of ec2 credential\n        \"\"\"\n        if not self._is_admin(context):\n            self._assert_identity(context, user_id)\n        self._assert_valid_user_id(context, user_id)\n        self._assert_valid_tenant_id(context, tenant_id)\n        cred_ref = {'user_id': user_id,\n                    'tenant_id': tenant_id,\n                    'access': uuid.uuid4().hex,\n                    'secret': uuid.uuid4().hex}\n        self.ec2_api.create_credential(context, cred_ref['access'], cred_ref)\n        return {'credential': cred_ref}",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2012-5571",
        "description": "[{'lang': 'en', 'value': 'OpenStack Keystone Essex (2012.1) and Folsom (2012.2) does not properly handle EC2 tokens when the user role has been removed from a tenant, which allows remote authenticated users to bypass intended authorization restrictions by leveraging a token for the removed user role.'}]",
        "cwe_number": 255
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-86",
      "code": "    async def __send_notice(\n        self, guild: discord.guild, user: Union[discord.Member, discord.User], event: str, *, message_format=None\n    ) -> Optional[discord.Message]:\n        \"\"\"Sends the notice for the event.\"\"\"\n        format_str = message_format or await self.__get_random_message_format(guild, event)\n        count = await self.config.guild(guild).get_attr(event).counter()\n        plural = \"\"\n        if count and count != 1:\n            plural = \"s\"\n        channel = await self.__get_channel(guild, event)\n        role_str: str = \"\"\n        if isinstance(user, discord.Member):\n            roles = [r.name for r in user.roles if r.name != \"@everyone\"]\n            if len(roles) > 0:\n                role_str = humanize_list(roles)\n        try:\n            return await channel.send(\n                format_str.format(member=user, server=guild, bot=user, count=count or \"\", plural=plural, roles=role_str)\n            )\n        except discord.Forbidden:\n            log.error(\n                f\"Failed to send {event} message to channel ID {channel.id} (server ID {guild.id}): \"\n                \"insufficient permissions\"\n            )\n            return None\n        except discord.DiscordException:\n            log.error(f\"Failed to send {event} message to channel ID {channel.id} (server ID {guild.id})\")\n            return None\n    async def __get_random_message_format(self, guild: discord.guild, event: str) -> str:\n        \"\"\"Gets a random message for event of type event.\"\"\"\n        async with self.config.guild(guild).get_attr(event).messages() as messages:\n            return random.choice(messages)\n    async def __increment_count(self, guild: discord.Guild, event: str) -> None:\n        \"\"\"Increments the counter for <event>s today. Handles date changes.\"\"\"\n        guild_settings = self.config.guild(guild)\n        if await guild_settings.date() is None:\n            await guild_settings.date.set(Welcome.__today())\n        if Welcome.__today() > await guild_settings.date():\n            await guild_settings.date.set(Welcome.__today())\n            await guild_settings.get_attr(event).counter.set(0)\n        count: int = await guild_settings.get_attr(event).counter()\n        await guild_settings.get_attr(event).counter.set(count + 1)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-37697",
        "description": "[{'lang': 'en', 'value': 'tmerc-cogs are a collection of open source plugins for the Red Discord bot. A vulnerability has been found in the code that allows any user to access sensitive information by crafting a specific membership event message. Issue is patched in commit d63c49b4cfc30c795336e4fff08cba3795e0fcc0. As a workaround users may unload the Welcome cog.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-87",
      "code": "def get_cmd(\n    model_uri: str, port: int = None, host: int = None, timeout: int = None, nworkers: int = None\n) -> Tuple[str, Dict[str, str]]:\n    local_uri = path_to_local_file_uri(model_uri)\n    timeout = timeout or MLFLOW_SCORING_SERVER_REQUEST_TIMEOUT.get()\n    if os.name != \"nt\":\n        args = [f\"--timeout={timeout}\"]\n        if port and host:\n            args.append(f\"-b {host}:{port}\")\n        elif host:\n            args.append(f\"-b {host}\")\n        if nworkers:\n            args.append(f\"-w {nworkers}\")\n        command = (\n            f\"gunicorn {' '.join(args)} ${{GUNICORN_CMD_ARGS}}\"\n            \" -- mlflow.pyfunc.scoring_server.wsgi:app\"\n        )\n    else:\n        args = []\n        if host:\n            args.append(f\"--host={host}\")\n        if port:\n            args.append(f\"--port={port}\")\n        command = (\n            f\"waitress-serve {' '.join(args)} \"\n            \"--ident=mlflow mlflow.pyfunc.scoring_server.wsgi:app\"\n        )\n    command_env = os.environ.copy()\n    command_env[_SERVER_MODEL_PATH] = local_uri\n    return command, command_env\n    def predict(self, model_uri, input_path, output_path, content_type):\n        \"\"\"\n        Generate predictions using generic python model saved with MLflow. The expected format of\n        the input JSON is the Mlflow scoring format.\n        Return the prediction results as a JSON.\n        \"\"\"\n        local_path = _download_artifact_from_uri(model_uri)\n        local_uri = path_to_local_file_uri(local_path)\n        if self._env_manager != _EnvManager.LOCAL:\n            command = (\n                'python -c \"from mlflow.pyfunc.scoring_server import _predict; _predict('\n                \"model_uri={model_uri}, \"\n                \"input_path={input_path}, \"\n                \"output_path={output_path}, \"\n                \"content_type={content_type})\"\n                '\"'\n            ).format(\n                model_uri=repr(local_uri),\n                input_path=repr(input_path),\n                output_path=repr(output_path),\n                content_type=repr(content_type),\n            )\n            return self.prepare_env(local_path).execute(command)\n        else:\n            scoring_server._predict(local_uri, input_path, output_path, content_type)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-4033",
        "description": "[{'lang': 'en', 'value': 'OS Command Injection in GitHub repository mlflow/mlflow prior to 2.6.0.'}]",
        "cwe_number": 78
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-88",
      "code": "def check_secure(line, conf, strict=None, ssh=None):\n    \"\"\"This method is used to check the content on the typed command.\n    Its purpose is to forbid the user to user to override the lshell\n    command restrictions.\n    The forbidden characters are placed in the 'forbidden' variable.\n    Feel free to update the list. Emptying it would be quite useless..: )\n    A warning counter has been added, to kick out of lshell a user if he\n    is warned more than X time (X being the 'warning_counter' variable).\n    \"\"\"\n    oline = line\n    line = line.strip()\n    returncode = 0\n    relist = re.findall(r'[^=]\\\"(.+)\\\"', line)\n    relist2 = re.findall(r'[^=]\\'(.+)\\'', line)\n    relist = relist + relist2\n    for item in relist:\n        if os.path.exists(item):\n            ret_check_path, conf = check_path(item, conf, strict=strict)\n            returncode += ret_check_path\n    line = re.sub(r'\\\"(.+?)\\\"', '', line)\n    line = re.sub(r'\\'(.+?)\\'', '', line)\n    if re.findall('[:cntrl:].*\\n', line):\n        ret, conf = warn_count('syntax',\n                               oline,\n                               conf,\n                               strict=strict,\n                               ssh=ssh)\n        return ret, conf\n    for item in conf['forbidden']:\n        if item in ['&', '|']:\n            if re.findall(\"[^\\%s]\\%s[^\\%s]\" % (item, item, item), line):\n                ret, conf = warn_count('syntax',\n                                       oline,\n                                       conf,\n                                       strict=strict,\n                                       ssh=ssh)\n                return ret, conf\n        else:\n            if item in line:\n                ret, conf = warn_count('syntax',\n                                       oline,\n                                       conf,\n                                       strict=strict,\n                                       ssh=ssh)\n                return ret, conf\n    executions = re.findall('\\$\\([^)]+[)]', line)\n    for item in executions:\n        ret_check_path, conf = check_path(item[2:-1].strip(),\n                                          conf,\n                                          strict=strict)\n        returncode += ret_check_path\n        ret_check_secure, conf = check_secure(item[2:-1].strip(),\n                                              conf,\n                                              strict=strict)\n        returncode += ret_check_secure\n    executions = re.findall('\\`[^`]+[`]', line)\n    for item in executions:\n        ret_check_secure, conf = check_secure(item[1:-1].strip(),\n                                              conf,\n                                              strict=strict)\n        returncode += ret_check_secure\n    curly = re.findall('\\$\\{[^}]+[}]', line)\n    for item in curly:\n        if re.findall(r'=|\\+|\\?|\\-', item):\n            variable = re.split('=|\\+|\\?|\\-', item, 1)\n        else:\n            variable = item\n        ret_check_path, conf = check_path(variable[1][:-1],\n                                          conf,\n                                          strict=strict)\n        returncode += ret_check_path\n    if returncode > 0:\n        return 1, conf\n    elif line.startswith('$(') or line.startswith('`'):\n        return 0, conf\n    lines = []\n    if line[0] in [\"&\", \"|\", \";\"]:\n        start = 1\n    else:\n        start = 0\n    for i in range(1, len(line)):\n        if line[i] in [\"&\", \"|\", \";\"] and line[i - 1] != \"\\\\\":\n            if start != i:\n                lines.append(line[start:i])\n            start = i + 1\n    if start != len(line):\n        lines.append(line[start:len(line)])\n    line = re.sub('\\)$', '', line)\n    for separate_line in lines:\n        separate_line = \" \".join(separate_line.split())\n        splitcmd = separate_line.strip().split(' ')\n        command = splitcmd[0]\n        if len(splitcmd) > 1:\n            cmdargs = splitcmd\n        else:\n            cmdargs = None\n        if command == 'sudo':\n            if type(cmdargs) == list:\n                if cmdargs[1] == '-u' and cmdargs:\n                    sudocmd = cmdargs[3]\n                else:\n                    sudocmd = cmdargs[1]\n                if sudocmd not in conf['sudo_commands'] and cmdargs:\n                    ret, conf = warn_count('sudo command',\n                                           oline,\n                                           conf,\n                                           strict=strict,\n                                           ssh=ssh)\n                    return ret, conf\n        if ssh:\n            conf['allowed'] = conf['overssh']\n        if command not in conf['allowed'] and command:\n            ret, conf = warn_count('command',\n                                   command,\n                                   conf,\n                                   strict=strict,\n                                   ssh=ssh)\n            return ret, conf\n    return 0, conf",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2016-6902",
        "description": "[{'lang': 'en', 'value': 'lshell 0.9.16 allows remote authenticated users to break out of a limited shell and execute arbitrary commands.'}]",
        "cwe_number": 264
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-89",
      "code": "    def __init__(self, bus, object_path):\n        super(Engine, self).__init__(connection=bus.get_connection(),\n                                     object_path=object_path)\n        self.__context = Anthy.GContext()\n        self.__context.set_encoding(Anthy.UTF8_ENCODING)\n        self.__idle_id = 0\n        self.__prop_dict = {}\n        try:\n            self.__is_utf8 = (getpreferredencoding().lower() == 'utf-8')\n        except:\n            self.__is_utf8 = False\n        self.__ibus_version = 0.0\n        size = self.__prefs.get_value('common', 'page_size')\n        self.__lookup_table = IBus.LookupTable.new(page_size=size,\n                                                   cursor_pos=0,\n                                                   cursor_visible=True,\n                                                   round=True)\n        self.__prop_list = self.__init_props()\n        self.__init_signal()\n        self.__reset()\n        ibus_config = bus.get_config()\n        if ibus_config != None:\n            ibus_config.connect('value-changed',\n                                self.__config_value_changed_cb)\n    def __get_ibus_version(self):\n        if self.__ibus_version == 0.0:\n            self.__ibus_version = \\\n                IBus.MAJOR_VERSION + IBus.MINOR_VERSION / 1000.0 + \\\n                IBus.MICRO_VERSION / 1000000.0\n        return self.__ibus_version\n    def do_focus_out(self):\n        mode = self.__prefs.get_value('common', 'behavior_on_focus_out')\n        if mode == 0 or mode == 1:\n            self.__reset()\n            self.__invalidate()\n    def do_disable(self):\n        self.__reset()\n        self.__invalidate()\n    def do_reset(self):\n        self.__reset()\n        self.__invalidate()\n    def do_destroy(self):\n        if self.__idle_id != 0:\n            GLib.source_remove(self.__idle_id)\n            self.__idle_id = 0\n        self.__remove_dict_files()\n        super(Engine,self).destroy()\n    def __process_key_event_internal2(self, keyval, keycode, state):\n        if Engine.__typing_mode == jastring.TYPING_MODE_THUMB_SHIFT and \\\n           Engine.__input_mode not in [INPUT_MODE_LATIN, INPUT_MODE_WIDE_LATIN]:\n            return self.process_key_event_thumb(keyval, keycode, state)\n        is_press = (state & IBus.ModifierType.RELEASE_MASK) == 0\n        state = state & (IBus.ModifierType.SHIFT_MASK |\n                         IBus.ModifierType.CONTROL_MASK |\n                         IBus.ModifierType.MOD1_MASK)\n        if not is_press:\n            return False\n        if keyval in KP_Table and self.__prefs.get_value('common',\n                                                         'ten_key_mode'):\n            keyval = KP_Table[keyval]\n        key = self._mk_key(keyval, state)\n        for cmd in self.__keybind.get(key, []):\n            if config.DEBUG:\n                print 'cmd =', cmd\n            try:\n                if getattr(self, cmd)(keyval, state):\n                    return True\n            except:\n                print >> sys.stderr, 'Unknown command = %s' % cmd\n        if state & (IBus.ModifierType.CONTROL_MASK | IBus.ModifierType.MOD1_MASK):\n            return False\n        if (IBus.KEY_exclam <= keyval <= IBus.KEY_asciitilde or\n            keyval == IBus.KEY_yen):\n            if Engine.__typing_mode == jastring.TYPING_MODE_KANA:\n                if keyval == IBus.KEY_0 and state == IBus.ModifierType.SHIFT_MASK:\n                    keyval = IBus.KEY_asciitilde\n                elif keyval == IBus.KEY_backslash and keycode in [132-8, 133-8]:\n                    keyval = IBus.KEY_yen\n            ret = self.__on_key_common(keyval, state)\n            if (Engine.__input_mode != INPUT_MODE_LATIN and\n                unichr(keyval) in u',.' and\n                self.__prefs.get_value('common', 'behavior_on_period')):\n                return self.__cmd_convert(keyval, state)\n            return ret\n        else:\n            if not self.__preedit_ja_string.is_empty():\n                return True\n            return False\n    def _chk_mode(self, mode):\n        if '0' in mode and self.__preedit_ja_string.is_empty():\n            return True\n        if self.__convert_mode == CONV_MODE_OFF:\n            if '1' in mode and not self.__preedit_ja_string.is_empty():\n                return True\n        elif self.__convert_mode == CONV_MODE_ANTHY:\n            if '2' in mode and not self.__lookup_table_visible:\n                return True\n        elif self.__convert_mode == CONV_MODE_PREDICTION:\n            if '3' in mode and not self.__lookup_table_visible:\n                return True\n        else:\n            if '4' in mode:\n                return True\n        if '5' in mode and self.__lookup_table_visible:\n            return True\n        return False",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2013-4509",
        "description": "[{'lang': 'en', 'value': 'The default configuration of IBUS 1.5.4, and possibly 1.5.2 and earlier, when IBus.InputPurpose.PASSWORD is not set and used with GNOME 3, does not obscure the entered password characters, which allows physically proximate attackers to obtain a user password by reading the lockscreen.'}]",
        "cwe_number": 255
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-90",
      "code": "    def _download_file(bucket, filename, local_dir):\n        key = bucket.get_key(filename)\n        local_filename = os.path.join(local_dir, filename)\n        key.get_contents_to_filename(local_filename)\n        return local_filename",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2011-4596",
        "description": "[{'lang': 'en', 'value': 'Multiple directory traversal vulnerabilities in OpenStack Nova before 2011.3.1, when the EC2 API and the S3/RegisterImage image-registration method are enabled, allow remote authenticated users to overwrite arbitrary files via a crafted (1) tarball or (2) manifest.'}]",
        "cwe_number": 22
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-91",
      "code": "def project_configure(request, project_name):\n    \"\"\"\n    get configuration\n    :param request: request object\n    :param project_name: project name\n    :return: json\n    \"\"\"\n    if request.method == 'GET':\n        project = Project.objects.get(name=project_name)\n        project = model_to_dict(project)\n        project['configuration'] = json.loads(project['configuration']) if project['configuration'] else None\n        return JsonResponse(project)\n    elif request.method == 'POST':\n        project = Project.objects.filter(name=project_name)\n        data = json.loads(request.body)\n        configuration = json.dumps(data.get('configuration'), ensure_ascii=False)\n        project.update(**{'configuration': configuration})\n        cmd = ' '.join(['gerapy', 'generate', project_name])\n        p = Popen(cmd, shell=True, stdin=PIPE, stdout=PIPE, stderr=PIPE)\n        stdout, stderr = bytes2str(p.stdout.read()), bytes2str(p.stderr.read())\n        if not stderr:\n            return JsonResponse({'status': '1'})\n        else:\n            return JsonResponse({'status': '0', 'message': stderr})\ndef job_list(request, client_id, project_name):\n    \"\"\"\n    get job list of project from one client\n    :param request: request object\n    :param client_id: client id\n    :param project_name: project name\n    :return: list of jobs\n    \"\"\"\n    if request.method == 'GET':\n        client = Client.objects.get(id=client_id)\n        scrapyd = get_scrapyd(client)\n        try:\n            result = scrapyd.list_jobs(project_name)\n            jobs = []\n            statuses = ['pending', 'running', 'finished']\n            for status in statuses:\n                for job in result.get(status):\n                    job['status'] = status\n                    jobs.append(job)\n            return JsonResponse(jobs)\n        except ConnectionError:\n            return JsonResponse({'message': 'Connect Error'}, status=500)\ndef job_log(request, client_id, project_name, spider_name, job_id):\n    \"\"\"\n    get log of jog\n    :param request: request object\n    :param client_id: client id\n    :param project_name: project name\n    :param spider_name: spider name\n    :param job_id: job id\n    :return: log of job\n    \"\"\"\n    if request.method == 'GET':\n        client = Client.objects.get(id=client_id)\n        url = log_url(client.ip, client.port, project_name, spider_name, job_id)\n        try:\n            response = requests.get(url, timeout=5, headers={\n                'Range': 'bytes=-1000'\n            }, auth=(client.username, client.password) if client.auth else None)\n            encoding = response.apparent_encoding\n            if response.status_code == 404:\n                return JsonResponse({'message': 'Log Not Found'}, status=404)\n            text = response.content.decode(encoding, errors='replace')\n            return HttpResponse(text)\n        except requests.ConnectionError:\n            return JsonResponse({'message': 'Load Log Error'}, status=500)\ndef job_cancel(request, client_id, project_name, job_id):\n    \"\"\"\n    cancel a job\n    :param request: request object\n    :param client_id: client id\n    :param project_name: project name\n    :param job_id: job id\n    :return: json of cancel\n    \"\"\"\n    if request.method == 'GET':\n        client = Client.objects.get(id=client_id)\n        try:\n            scrapyd = get_scrapyd(client)\n            result = scrapyd.cancel(project_name, job_id)\n            return JsonResponse(result)\n        except ConnectionError:\n            return JsonResponse({'message': 'Connect Error'})\ndef del_version(request, client_id, project, version):\n    if request.method == 'GET':\n        client = Client.objects.get(id=client_id)\n        try:\n            scrapyd = get_scrapyd(client)\n            result = scrapyd.delete_version(project=project, version=version)\n            return JsonResponse(result)\n        except ConnectionError:\n            return JsonResponse({'message': 'Connect Error'})\ndef task_update(request, task_id):\n    \"\"\"\n    update task info\n    :param request: request object\n    :param task_id: task id\n    :return: json\n    \"\"\"\n    if request.method == 'POST':\n        task = Task.objects.filter(id=task_id)\n        data = json.loads(request.body)\n        data['clients'] = json.dumps(data.get('clients'), ensure_ascii=False)\n        data['configuration'] = json.dumps(data.get('configuration'), ensure_ascii=False)\n        data['modified'] = 1\n        task.update(**data)\n        return JsonResponse(model_to_dict(Task.objects.get(id=task_id)))\ndef task_remove(request, task_id):\n    \"\"\"\n    remove task by task_id\n    :param request:\n    :return:\n    \"\"\"\n    if request.method == 'POST':\n        try:\n            task = Task.objects.get(id=task_id)\n            clients = clients_of_task(task)\n            for client in clients:\n                job_id = get_job_id(client, task)\n                DjangoJob.objects.filter(name=job_id).delete()\n            Task.objects.filter(id=task_id).delete()\n            return JsonResponse({'result': '1'})\n        except:\n            return JsonResponse({'result': '0'})\ndef task_status(request, task_id):\n    \"\"\"\n    get task status info\n    :param request: request object\n    :param task_id: task id\n    :return:\n    \"\"\"\n    if request.method == 'GET':\n        result = []\n        task = Task.objects.get(id=task_id)\n        clients = clients_of_task(task)\n        for client in clients:\n            job_id = get_job_id(client, task)\n            jobs = DjangoJob.objects.filter(name=job_id)\n            logger.debug('jobs from djangojob %s', jobs)\n            if not jobs: continue\n            job = DjangoJob.objects.get(name=job_id)\n            executions = serialize('json', DjangoJobExecution.objects.filter(job=job))\n            result.append({\n                'client': model_to_dict(client),\n                'next': job.next_run_time,\n                'executions': json.loads(executions)\n            })\n        return JsonResponse({'data': result})\ndef render_html(request):\n    \"\"\"\n    render html with url\n    :param request:\n    :return:\n    \"\"\"\n    if request.method == 'GET':\n        url = request.GET.get('url')\n        url = unquote(base64.b64decode(url).decode('utf-8'))\n        js = request.GET.get('js', 0)\n        script = request.GET.get('script')\n        try:\n            response = requests.get(url, timeout=5)\n            response.encoding = response.apparent_encoding\n            html = process_html(response.text)\n            return HttpResponse(html)\n        except Exception as e:\n            return JsonResponse({'message': e.args}, status=500)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2020-7698",
        "description": "[{'lang': 'en', 'value': 'This affects the package Gerapy from 0 and before 0.9.3. The input being passed to Popen, via the project_configure endpoint, isn\u2019t being sanitized.'}]",
        "cwe_number": 78
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-92",
      "code": "    def __post_init_code(self, raw_code: str, request: HttpRequest):\n        if not raw_code:\n            LOGGER.warning(\"Missing authorization code\")\n            raise TokenError(\"invalid_grant\")\n        allowed_redirect_urls = self.provider.redirect_uris.split()\n        try:\n            if not any(fullmatch(x, self.redirect_uri) for x in allowed_redirect_urls):\n                LOGGER.warning(\n                    \"Invalid redirect uri (regex comparison)\",\n                    redirect_uri=self.redirect_uri,\n                    expected=allowed_redirect_urls,\n                )\n                Event.new(\n                    EventAction.CONFIGURATION_ERROR,\n                    message=\"Invalid redirect URI used by provider\",\n                    provider=self.provider,\n                    redirect_uri=self.redirect_uri,\n                    expected=allowed_redirect_urls,\n                ).from_http(request)\n                raise TokenError(\"invalid_client\")\n        except RegexError as exc:\n            LOGGER.info(\"Failed to parse regular expression, checking directly\", exc=exc)\n            if not any(x == self.redirect_uri for x in allowed_redirect_urls):\n                LOGGER.warning(\n                    \"Invalid redirect uri (strict comparison)\",\n                    redirect_uri=self.redirect_uri,\n                    expected=allowed_redirect_urls,\n                )\n                Event.new(\n                    EventAction.CONFIGURATION_ERROR,\n                    message=\"Invalid redirect_uri configured\",\n                    provider=self.provider,\n                ).from_http(request)\n                raise TokenError(\"invalid_client\")\n        self.authorization_code = AuthorizationCode.objects.filter(code=raw_code).first()\n        if not self.authorization_code:\n            LOGGER.warning(\"Code does not exist\", code=raw_code)\n            raise TokenError(\"invalid_grant\")\n        if self.authorization_code.is_expired:\n            LOGGER.warning(\n                \"Code is expired\",\n                token=raw_code,\n            )\n            raise TokenError(\"invalid_grant\")\n        if self.authorization_code.provider != self.provider or self.authorization_code.is_expired:\n            LOGGER.warning(\"Invalid code: invalid client or code has expired\")\n            raise TokenError(\"invalid_grant\")\n        if self.code_verifier:\n            if self.authorization_code.code_challenge_method == PKCE_METHOD_S256:\n                new_code_challenge = (\n                    urlsafe_b64encode(sha256(self.code_verifier.encode(\"ascii\")).digest())\n                    .decode(\"utf-8\")\n                    .replace(\"=\", \"\")\n                )\n            else:\n                new_code_challenge = self.code_verifier\n            if new_code_challenge != self.authorization_code.code_challenge:\n                LOGGER.warning(\"Code challenge not matching\")\n                raise TokenError(\"invalid_grant\")\n    def __post_init_refresh(self, raw_token: str, request: HttpRequest):\n        if not raw_token:\n            LOGGER.warning(\"Missing refresh token\")\n            raise TokenError(\"invalid_grant\")\n        self.refresh_token = RefreshToken.objects.filter(\n            token=raw_token, provider=self.provider\n        ).first()\n        if not self.refresh_token:\n            LOGGER.warning(\n                \"Refresh token does not exist\",\n                token=raw_token,\n            )\n            raise TokenError(\"invalid_grant\")\n        if self.refresh_token.is_expired:\n            LOGGER.warning(\n                \"Refresh token is expired\",\n                token=raw_token,\n            )\n            raise TokenError(\"invalid_grant\")\n        if not self.scope:\n            self.scope = self.refresh_token.scope\n        if self.refresh_token.revoked:\n            LOGGER.warning(\"Refresh token is revoked\", token=raw_token)\n            Event.new(\n                action=EventAction.SUSPICIOUS_REQUEST,\n                message=\"Revoked refresh token was used\",\n                token=self.refresh_token,\n                provider=self.refresh_token.provider,\n            ).from_http(request, user=self.refresh_token.user)\n            raise TokenError(\"invalid_grant\")",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-48228",
        "description": "[{'lang': 'en', 'value': 'authentik is an open-source identity provider. When initialising a oauth2 flow with a `code_challenge` and `code_method` (thus requesting PKCE), the single sign-on provider (authentik) must check if there is a matching and existing `code_verifier` during the token step. Prior to versions 2023.10.4 and 2023.8.5, authentik checks if the contents of `code_verifier` is matching only when it is provided. When it is left out completely, authentik simply accepts the token request with out it; even when the flow was started with a `code_challenge`. authentik 2023.8.5 and 2023.10.4 fix this issue.'}]",
        "cwe_number": 287
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-93",
      "code": "def dashboard(request, name=None):\n  dashboard_conf_missing = False\n  try:\n    config.check()\n  except OSError as e:\n    if e.errno == errno.ENOENT:\n      dashboard_conf_missing = True\n    else:\n      raise\n  initialError = None\n  debug = request.GET.get('debug', False)\n  theme = request.GET.get('theme', config.ui_config['theme'])\n  css_file = finders.find('css/dashboard-%s.css' % theme)\n  if css_file is None:\n    initialError = \"Invalid theme '%s'\" % theme\n    theme = config.ui_config['theme']\n  context = {\n    'schemes_json': mark_safe(json.dumps(config.schemes)),\n    'ui_config_json': mark_safe(json.dumps(config.ui_config)),\n    'jsdebug': debug or settings.JAVASCRIPT_DEBUG,\n    'debug': debug,\n    'theme': theme,\n    'initialError': initialError,\n    'querystring': mark_safe(json.dumps(dict(request.GET.items()))),\n    'dashboard_conf_missing': dashboard_conf_missing,\n    'userName': '',\n    'permissions': mark_safe(json.dumps(getPermissions(request.user))),\n    'permissionsUnauthenticated': mark_safe(json.dumps(getPermissions(None)))\n  }\n  user = request.user\n  if user:\n      context['userName'] = user.username\n  if name is not None:\n    try:\n      dashboard = Dashboard.objects.get(name=name)\n    except Dashboard.DoesNotExist:\n      context['initialError'] = \"Dashboard '%s' does not exist.\" % name\n    else:\n      context['initialState'] = dashboard.state\n  return render(request, \"dashboard.html\", context)\ndef template(request, name, val):\n  template_conf_missing = False\n  try:\n    config.check()\n  except OSError as e:\n    if e.errno == errno.ENOENT:\n      template_conf_missing = True\n    else:\n      raise\n  initialError = None\n  debug = request.GET.get('debug', False)\n  theme = request.GET.get('theme', config.ui_config['theme'])\n  css_file = finders.find('css/dashboard-%s.css' % theme)\n  if css_file is None:\n    initialError = \"Invalid theme '%s'\" % theme\n    theme = config.ui_config['theme']\n  context = {\n    'schemes_json' : json.dumps(config.schemes),\n    'ui_config_json' : json.dumps(config.ui_config),\n    'jsdebug' : debug or settings.JAVASCRIPT_DEBUG,\n    'debug' : debug,\n    'theme' : theme,\n    'initialError' : initialError,\n    'querystring' : json.dumps( dict( request.GET.items() ) ),\n    'template_conf_missing' : template_conf_missing,\n    'userName': '',\n    'permissions': json.dumps(getPermissions(request.user)),\n    'permissionsUnauthenticated': json.dumps(getPermissions(None))\n  }\n  user = request.user\n  if user:\n      context['userName'] = user.username\n  try:\n    template = Template.objects.get(name=name)\n  except Template.DoesNotExist:\n    context['initialError'] = \"Template '%s' does not exist.\" % name\n  else:\n    state = json.loads(template.loadState(val))\n    state['name'] = '%s/%s' % (name, val)\n    context['initialState'] = json.dumps(state)\n  return render(request, \"dashboard.html\", context)\ndef getPermissions(user):\n  \"\"\"Return [change, delete] based on authorisation model and user privileges/groups\"\"\"\n  if user and not isAuthenticated(user):\n    user = None\n  if not settings.DASHBOARD_REQUIRE_AUTHENTICATION:\n    return ALL_PERMISSIONS\n  if not user:\n      return []\n  permissions = ALL_PERMISSIONS\n  if settings.DASHBOARD_REQUIRE_PERMISSIONS:\n    permissions = [permission for permission in ALL_PERMISSIONS if user.has_perm('dashboard.%s_dashboard' % permission)]\n  editGroup = settings.DASHBOARD_REQUIRE_EDIT_GROUP\n  if editGroup and len(user.groups.filter(name = editGroup)) == 0:\n    permissions = []\n  return permissions\ndef save(request, name):\n  if 'change' not in getPermissions(request.user):\n    return json_response( dict(error=\"Must be logged in with appropriate permissions to save\") )\n  state = str( json.dumps( json.loads( request.POST['state'] ) ) )\n  try:\n    dashboard = Dashboard.objects.get(name=name)\n  except Dashboard.DoesNotExist:\n    dashboard = Dashboard.objects.create(name=name, state=state)\n  else:\n    dashboard.state = state\n    dashboard.save()\n  return json_response( dict(success=True) )\ndef save_template(request, name, key):\n  if 'change' not in getPermissions(request.user):\n    return json_response( dict(error=\"Must be logged in with appropriate permissions to save the template\") )\n  state = str( json.dumps( json.loads( request.POST['state'] ) ) )\n  try:\n    template = Template.objects.get(name=name)\n  except Template.DoesNotExist:\n    template = Template.objects.create(name=name)\n    template.setState(state, key)\n    template.save()\n  else:\n    template.setState(state, key)\n    template.save()\n  return json_response( dict(success=True) )\ndef load(request, name):\n  try:\n    dashboard = Dashboard.objects.get(name=name)\n  except Dashboard.DoesNotExist:\n    return json_response( dict(error=\"Dashboard '%s' does not exist. \" % name) )\n  return json_response( dict(state=json.loads(dashboard.state)) )\ndef load_template(request, name, val):\n  try:\n    template = Template.objects.get(name=name)\n  except Template.DoesNotExist:\n    return json_response( dict(error=\"Template '%s' does not exist. \" % name) )\n  state = json.loads(template.loadState(val))\n  state['name'] = '%s/%s' % (name, val)\n  return json_response( dict(state=state) )\ndef delete(request, name):\n  if 'delete' not in getPermissions(request.user):\n    return json_response( dict(error=\"Must be logged in with appropriate permissions to delete\") )\n  try:\n    dashboard = Dashboard.objects.get(name=name)\n  except Dashboard.DoesNotExist:\n    return json_response( dict(error=\"Dashboard '%s' does not exist. \" % name) )\n  else:\n    dashboard.delete()\n    return json_response( dict(success=True) )\ndef delete_template(request, name):\n  if 'delete' not in getPermissions(request.user):\n    return json_response( dict(error=\"Must be logged in with appropriate permissions to delete the template\") )\n  try:\n    template = Template.objects.get(name=name)\n  except Template.DoesNotExist:\n    return json_response( dict(error=\"Template '%s' does not exist. \" % name) )\n  else:\n    template.delete()\n    return json_response( dict(success=True) )\ndef find(request):\n  queryParams = request.GET.copy()\n  queryParams.update(request.POST)\n  query = queryParams.get('query', False)\n  query_terms = set( query.lower().split() )\n  results = []\n  for dashboard_name in Dashboard.objects.order_by('name').values_list('name', flat=True):\n    name = dashboard_name.lower()\n    if name.startswith('temporary-'):\n      continue\n    found = True\n    for term in query_terms:\n      if term in name:\n        found = True\n      else:\n        found = False\n        break\n    if found:\n      results.append( dict(name=dashboard_name) )\n  return json_response( dict(dashboards=results) )\ndef load_module(module_path, member=None):\n  module_name = splitext(basename(module_path))[0]\n  try:\n    module_file = open(module_path, 'U')\n  except ValueError:\n    module_file = open(module_path, 'rt')\n  description = ('.py', 'U', imp.PY_SOURCE)\n  module = imp.load_module(module_name, module_file, module_path, description)\n  if member:\n    return getattr(module, member)\n  else:\n    return module\ndef escape(s):\n    s = s.replace(\"&\", \"&amp;\")\n    s = s.replace(\"<\", \"&lt;\")\n    s = s.replace(\">\", \"&gt;\")\n    return s\ndef handleInputParameterError(f):\n    def new_f(*args, **kwargs):\n        try:\n            return f(*args, **kwargs)\n        except InputParameterError as e:\n            msgStr = str(e)\n            log.warning('%s', msgStr)\n            return HttpResponseBadRequest(escape(msgStr))\n    return new_f",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-4730",
        "description": "[{'lang': 'en', 'value': 'A vulnerability was found in Graphite Web. It has been classified as problematic. Affected is an unknown function of the component Absolute Time Range Handler. The manipulation leads to cross site scripting. It is possible to launch the attack remotely. The exploit has been disclosed to the public and may be used. The name of the patch is 2f178f490e10efc03cd1d27c72f64ecab224eb23. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-216744.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-94",
      "code": "    def _remove_javascript_link(self, link):\n        new = _substitute_whitespace('', link)\n        if _is_javascript_scheme(new):\n            return ''\n        return link",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2018-19787",
        "description": "[{'lang': 'en', 'value': 'An issue was discovered in lxml before 4.2.5. lxml/html/clean.py in the lxml.html.clean module does not remove javascript: URLs that use escaping, allowing a remote attacker to conduct XSS attacks, as demonstrated by \"j a v a s c r i p t:\" in Internet Explorer. This is a similar issue to CVE-2014-3146.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-95",
      "code": "    def add_field(\n        self,\n        name: str,\n        value: Any,\n        *,\n        content_type: Optional[str] = None,\n        filename: Optional[str] = None,\n        content_transfer_encoding: Optional[str] = None,\n    ) -> None:\n        if isinstance(value, io.IOBase):\n            self._is_multipart = True\n        elif isinstance(value, (bytes, bytearray, memoryview)):\n            if filename is None and content_transfer_encoding is None:\n                filename = name\n        type_options: MultiDict[str] = MultiDict({\"name\": name})\n        if filename is not None and not isinstance(filename, str):\n            raise TypeError(\n                \"filename must be an instance of str. \" \"Got: %s\" % filename\n            )\n        if filename is None and isinstance(value, io.IOBase):\n            filename = guess_filename(value, name)\n        if filename is not None:\n            type_options[\"filename\"] = filename\n            self._is_multipart = True\n        headers = {}\n        if content_type is not None:\n            if not isinstance(content_type, str):\n                raise TypeError(\n                    \"content_type must be an instance of str. \" \"Got: %s\" % content_type\n                )\n            headers[hdrs.CONTENT_TYPE] = content_type\n            self._is_multipart = True\n        if content_transfer_encoding is not None:\n            if not isinstance(content_transfer_encoding, str):\n                raise TypeError(\n                    \"content_transfer_encoding must be an instance\"\n                    \" of str. Got: %s\" % content_transfer_encoding\n                )\n            headers[hdrs.CONTENT_TRANSFER_ENCODING] = content_transfer_encoding\n            self._is_multipart = True\n        self._fields.append((type_options, headers, value))\n    def __init__(\n        self, boundary: bytes, headers: \"CIMultiDictProxy[str]\", content: StreamReader\n    ) -> None:\n        self.headers = headers\n        self._boundary = boundary\n        self._content = content\n        self._at_eof = False\n        length = self.headers.get(CONTENT_LENGTH, None)\n        self._length = int(length) if length is not None else None\n        self._read_bytes = 0\n        self._unread: Deque[bytes] = deque()\n        self._prev_chunk: Optional[bytes] = None\n        self._content_eof = 0\n        self._cache: Dict[str, Any] = {}\n    def decode(self, data: bytes) -> bytes:\n        \"\"\"Decodes data.\n        Decoding is done according the specified Content-Encoding\n        or Content-Transfer-Encoding headers value.\n        \"\"\"\n        if CONTENT_TRANSFER_ENCODING in self.headers:\n            data = self._decode_content_transfer(data)\n        if CONTENT_ENCODING in self.headers:\n            return self._decode_content(data)\n        return data\n    def get_charset(self, default: str) -> str:\n        \"\"\"Returns charset parameter from Content-Type header or default.\"\"\"\n        ctype = self.headers.get(CONTENT_TYPE, \"\")\n        mimetype = parse_mimetype(ctype)\n        return mimetype.parameters.get(\"charset\", default)\n    async def next(\n        self,\n    ) -> Optional[Union[\"MultipartReader\", BodyPartReader]]:\n        \"\"\"Emits the next multipart body part.\"\"\"\n        if self._at_eof:\n            return None\n        await self._maybe_release_last_part()\n        if self._at_bof:\n            await self._read_until_first_boundary()\n            self._at_bof = False\n        else:\n            await self._read_boundary()\n        if self._at_eof:\n            return None\n        self._last_part = await self.fetch_next_part()\n        return self._last_part\n    def _get_part_reader(\n        self,\n        headers: \"CIMultiDictProxy[str]\",\n    ) -> Union[\"MultipartReader\", BodyPartReader]:\n        \"\"\"Dispatches the response by the `Content-Type` header.\n        Returns a suitable reader instance.\n        :param dict headers: Response headers\n        \"\"\"\n        ctype = headers.get(CONTENT_TYPE, \"\")\n        mimetype = parse_mimetype(ctype)\n        if mimetype.type == \"multipart\":\n            if self.multipart_reader_cls is None:\n                return type(self)(headers, self._content)\n            return self.multipart_reader_cls(headers, self._content)\n        else:\n            return self.part_reader_cls(self._boundary, headers, self._content)\n    def _get_boundary(self) -> str:\n        mimetype = parse_mimetype(self.headers[CONTENT_TYPE])\n        assert mimetype.type == \"multipart\", \"multipart/* content type expected\"\n        if \"boundary\" not in mimetype.parameters:\n            raise ValueError(\n                \"boundary missed for Content-Type: %s\" % self.headers[CONTENT_TYPE]\n            )\n        boundary = mimetype.parameters[\"boundary\"]\n        if len(boundary) > 70:\n            raise ValueError(\"boundary %r is too long (70 chars max)\" % boundary)\n        return boundary\n    def append_payload(self, payload: Payload) -> Payload:\n        \"\"\"Adds a new body part to multipart writer.\"\"\"\n        encoding: Optional[str] = payload.headers.get(\n            CONTENT_ENCODING,\n            \"\",\n        ).lower()\n        if encoding and encoding not in (\"deflate\", \"gzip\", \"identity\"):\n            raise RuntimeError(f\"unknown content encoding: {encoding}\")\n        if encoding == \"identity\":\n            encoding = None\n        te_encoding: Optional[str] = payload.headers.get(\n            CONTENT_TRANSFER_ENCODING,\n            \"\",\n        ).lower()\n        if te_encoding not in (\"\", \"base64\", \"quoted-printable\", \"binary\"):\n            raise RuntimeError(\n                \"unknown content transfer encoding: {}\" \"\".format(te_encoding)\n            )\n        if te_encoding == \"binary\":\n            te_encoding = None\n        size = payload.size\n        if size is not None and not (encoding or te_encoding):\n            payload.headers[CONTENT_LENGTH] = str(size)\n        self._parts.append((payload, encoding, te_encoding))\n        return payload",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-30251",
        "description": "[{'lang': 'en', 'value': 'aiohttp is an asynchronous HTTP client/server framework for asyncio and Python. In affected versions an attacker can send a specially crafted POST (multipart/form-data) request. When the aiohttp server processes it, the server will enter an infinite loop and be unable to process any further requests. An attacker can stop the application from serving requests after sending a single request. This issue has been addressed in version 3.9.4. Users are advised to upgrade. Users unable to upgrade may manually apply a patch to their systems. Please see the linked GHSA for instructions.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-96",
      "code": "def proxy(\n    request,\n    url=None,\n    response_callback=None,\n    sec_chk_hosts=True,\n    sec_chk_rules=True,\n    timeout=None,\n    allowed_hosts=[],\n    headers=None,\n    access_token=None,\n    **kwargs,\n):\n    from geonode.geoserver.helpers import ogc_server_settings\n    if not timeout:\n        timeout = getattr(ogc_server_settings, \"TIMEOUT\", TIMEOUT)\n    PROXY_ALLOWED_HOSTS = getattr(settings, \"PROXY_ALLOWED_HOSTS\", ())\n    if \"url\" not in request.GET and not url:\n        return HttpResponse(\n            \"The proxy service requires a URL-encoded URL as a parameter.\", status=400, content_type=\"text/plain\"\n        )\n    raw_url = url or request.GET[\"url\"]\n    raw_url = urljoin(settings.SITEURL, raw_url) if raw_url.startswith(\"/\") else raw_url\n    url = urlsplit(raw_url)\n    scheme = str(url.scheme)\n    locator = str(url.path)\n    if url.query != \"\":\n        locator += f\"?{url.query}\"\n    if url.fragment != \"\":\n        locator += f\"\n    site_url = urlsplit(settings.SITEURL)\n    if sec_chk_hosts and not settings.DEBUG:\n        if site_url.hostname not in PROXY_ALLOWED_HOSTS:\n            PROXY_ALLOWED_HOSTS += (site_url.hostname,)\n        hostname = (ogc_server_settings.hostname,) if ogc_server_settings else ()\n        if hostname not in PROXY_ALLOWED_HOSTS:\n            PROXY_ALLOWED_HOSTS += hostname\n        if url.query and ows_regexp.match(url.query):\n            ows_tokens = ows_regexp.match(url.query).groups()\n            if (\n                len(ows_tokens) == 4\n                and \"version\" == ows_tokens[0]\n                and StrictVersion(ows_tokens[1]) >= StrictVersion(\"1.0.0\")\n                and StrictVersion(ows_tokens[1]) <= StrictVersion(\"3.0.0\")\n                and ows_tokens[2].lower() in (\"getcapabilities\")\n                and ows_tokens[3].upper() in (\"OWS\", \"WCS\", \"WFS\", \"WMS\", \"WPS\", \"CSW\")\n            ):\n                if url.hostname not in PROXY_ALLOWED_HOSTS:\n                    PROXY_ALLOWED_HOSTS += (url.hostname,)\n        from geonode.services.models import Service\n        for _s in Service.objects.all():\n            _remote_host = urlsplit(_s.base_url).hostname\n            PROXY_ALLOWED_HOSTS += (_remote_host,)\n        if not validate_host(url.hostname, PROXY_ALLOWED_HOSTS):\n            return HttpResponse(\n                \"DEBUG is set to False but the host of the path provided to the proxy service\"\n                \" is not in the PROXY_ALLOWED_HOSTS setting.\",\n                status=403,\n                content_type=\"text/plain\",\n            )\n    if sec_chk_rules:\n        pass\n    if not headers:\n        headers, access_token = get_headers(request, url, raw_url, allowed_hosts=allowed_hosts)\n    if not access_token:\n        auth_header = None\n        if \"Authorization\" in headers:\n            auth_header = headers[\"Authorization\"]\n        elif \"HTTP_AUTHORIZATION\" in request.META:\n            auth_header = request.META.get(\"HTTP_AUTHORIZATION\", request.META.get(\"HTTP_AUTHORIZATION2\"))\n        if auth_header:\n            access_token = get_token_from_auth_header(auth_header, create_if_not_exists=True)\n    user = get_auth_user(access_token)\n    parsed = urlparse(raw_url)\n    parsed._replace(path=locator.encode(\"utf8\"))\n    if parsed.netloc == site_url.netloc and scheme != site_url.scheme:\n        parsed = parsed._replace(scheme=site_url.scheme)\n    _url = parsed.geturl()\n    _url = URL.from_text(_url).normalize().to_text()\n    if request.method == \"GET\" and access_token and \"access_token\" not in _url:\n        query_separator = \"&\" if \"?\" in _url else \"?\"\n        _url = f\"{_url}{query_separator}access_token={access_token}\"\n    _data = request.body.decode(\"utf-8\")\n    if check_ogc_backend(geoserver.BACKEND_PACKAGE):\n        from geonode.geoserver.helpers import ogc_server_settings\n        _url = _url.replace(f\"{settings.SITEURL}geoserver\", ogc_server_settings.LOCATION.rstrip(\"/\"))\n        _data = _data.replace(f\"{settings.SITEURL}geoserver\", ogc_server_settings.LOCATION.rstrip(\"/\"))\n    response, content = http_client.request(\n        _url, method=request.method, data=_data.encode(\"utf-8\"), headers=headers, timeout=timeout, user=user\n    )\n    if response is None:\n        return HttpResponse(content=content, reason=content, status=500)\n    content = response.content or response.reason\n    status = response.status_code\n    response_headers = response.headers\n    content_type = response.headers.get(\"Content-Type\")\n    if status >= 400:\n        _response = HttpResponse(content=content, reason=content, status=status, content_type=content_type)\n        return fetch_response_headers(_response, response_headers)\n    if content and content_type and content_type == \"gzip\":\n        buf = io.BytesIO(content)\n        with gzip.GzipFile(fileobj=buf) as f:\n            content = f.read()\n        buf.close()\n    PLAIN_CONTENT_TYPES = [\"text\", \"plain\", \"html\", \"json\", \"xml\", \"gml\"]\n    for _ct in PLAIN_CONTENT_TYPES:\n        if content_type and _ct in content_type and not isinstance(content, str):\n            try:\n                content = content.decode()\n                break\n            except Exception:\n                pass\n    if response and response_callback:\n        kwargs = {} if not kwargs else kwargs\n        kwargs.update(\n            {\n                \"response\": response,\n                \"content\": content,\n                \"status\": status,\n                \"response_headers\": response_headers,\n                \"content_type\": content_type,\n            }\n        )\n        return response_callback(**kwargs)\n    else:\n        if status and status in (301, 302, 303, 307):\n            _response = HttpResponse(\n                (\n                    f\"This proxy does not support redirects. The server in '{url}' \"\n                    f\"asked for a redirect to '{response.getheader('Location')}'\"\n                ),\n                status=status,\n                content_type=content_type,\n            )\n            _response[\"Location\"] = response.getheader(\"Location\")\n            return fetch_response_headers(_response, response_headers)\n        else:\n            def _get_message(text):\n                _s = text\n                if isinstance(text, bytes):\n                    _s = text.decode(\"utf-8\", \"replace\")\n                try:\n                    found = re.search(\"<b>Message</b>(.+?)</p>\", _s).group(1).strip()\n                except Exception:\n                    found = _s\n                return found\n            _response = HttpResponse(\n                content=content,\n                reason=_get_message(content) if status not in (200, 201) else None,\n                status=status,\n                content_type=content_type,\n            )\n            return fetch_response_headers(_response, response_headers)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-40017",
        "description": "[{'lang': 'en', 'value': 'GeoNode is an open source platform that facilitates the creation, sharing, and collaborative use of geospatial data. In versions 3.2.0 through 4.1.2, the endpoint `/proxy/?url=` does not properly protect against server-side request forgery. This allows an attacker to port scan internal hosts and request information from internal hosts. A patch is available at commit a9eebae80cb362009660a1fd49e105e7cdb499b9.'}]",
        "cwe_number": 918
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-97",
      "code": "def full_domain_validator(hostname):\n    \"\"\"\n    Fully validates a domain name as compilant with the standard rules:\n        - Composed of series of labels concatenated with dots, as are all domain names.\n        - Each label must be between 1 and 63 characters long.\n        - The entire hostname (including the delimiting dots) has a maximum of 255 characters.\n        - Only characters 'a' through 'z' (in a case-insensitive manner), the digits '0' through '9'.\n        - Labels can't start or end with a hyphen.\n    \"\"\"\n    HOSTNAME_LABEL_PATTERN = re.compile(r\"(?!-)[A-Z\\d-]+(?<!-)$\", re.IGNORECASE)\n    if not hostname:\n        return\n    if len(hostname) > 255:\n        raise Exception(\n            \"The domain name cannot be composed of more than 255 characters.\"\n        )\n    if hostname[-1:] == \".\":\n        hostname = hostname[:-1]\n    for label in hostname.split(\".\"):\n        if len(label) > 63:\n            raise Exception(\n                \"The label '%(label)s' is too long (maximum is 63 characters).\"\n                % {\"label\": label}\n            )\n        if not HOSTNAME_LABEL_PATTERN.match(label):\n            raise Exception(f\"Unallowed characters in label '{label}'.\")\n    return hostname\ndef check_soa_record(target: str) -> bool:\n    \"\"\"Checks the presence of a SOA record for the Email Systems Testing.\"\"\"\n    result = False\n    try:\n        answers = dns.resolver.query(target, \"SOA\")\n        result = 0 != len(answers)\n    except Exception:\n        result = False\n    return result",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-48310",
        "description": "[{'lang': 'en', 'value': 'TestingPlatform is a testing platform for Internet Security Standards. Prior to version 2.1.1, user input is not filtered correctly. Nmap options are accepted. In this particular case, the option to create log files is accepted in addition to a host name (and even without). A log file is created at the location specified. These files are created as root. If the file exists, the existing file is being rendered useless. This can result in denial of service. Additionally, input for scanning can be any CIDR blocks passed to nmap. An attacker can scan 0.0.0.0/0 or even local networks. Version 2.1.1 contains a patch for this issue.'}]",
        "cwe_number": 20
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-98",
      "code": "    def get(self, request, format=None, **kwargs):\n        hosts = (\n            Host.objects.prefetch_related(\"addresses\")\n            .filter(\n                structured_attributes__structured_attribute_value__attribute__name=\"nac-profile\",\n                structured_attributes__structured_attribute_value__value__startswith=CONFIG_DEFAULTS[\n                    \"NAC_PROFILE_IS_SERVER_PREFIX\"\n                ],\n            )\n            .annotate(\n                nac_profile=F(\n                    \"structured_attributes__structured_attribute_value__value\"\n                ),\n            )\n        )\n        user_perms_prefetch = UserObjectPermission.objects.select_related(\n            \"permission\", \"user\"\n        ).filter(\n            content_type=ContentType.objects.get_for_model(Host),\n            object_pk__in=[str(host.mac) for host in hosts],\n            permission__codename=\"is_owner_host\",\n        )\n        group_perms_prefetch = GroupObjectPermission.objects.select_related(\n            \"permission\", \"group\"\n        ).filter(\n            content_type=ContentType.objects.get_for_model(Host),\n            object_pk__in=[str(host.mac) for host in hosts],\n            permission__codename=\"is_owner_host\",\n        )\n        data = []\n        for host in hosts:\n            owners = host.get_owners(\n                name_only=True,\n                user_perms_prefetch=user_perms_prefetch,\n                group_perms_prefetch=group_perms_prefetch,\n            )\n            data.append(\n                {\n                    \"hostname\": host.hostname,\n                    \"mac\": str(host.mac),\n                    \"description\": host.description,\n                    \"master_ip_address\": host.ip_addresses[0]\n                    if host.ip_addresses\n                    else None,\n                    \"user_owners\": \", \".join(owners[0]),\n                    \"group_owners\": \", \".join(owners[1]),\n                    \"nac_profile\": host.nac_profile,\n                }\n            )\n        if request.accepted_renderer.format == \"json\":\n            return Response({\"data\": data}, status=status.HTTP_200_OK)\n        else:\n            return Response(data, status=status.HTTP_200_OK)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-4595",
        "description": "[{'lang': 'en', 'value': 'A vulnerability classified as problematic has been found in django-openipam. This affects an unknown part of the file openipam/report/templates/report/exposed_hosts.html. The manipulation of the argument description leads to cross site scripting. It is possible to initiate the attack remotely. The name of the patch is a6223a1150d60cd036106ba6a8e676c1bfc3cc85. It is recommended to apply a patch to fix this issue. The identifier VDB-216189 was assigned to this vulnerability.'}]",
        "cwe_number": 707
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-99",
      "code": "def get_parser():\n    parser = configargparse.ArgumentParser(\n        prog='rdiffweb',\n        description='Web interface to browse and restore rdiff-backup repositories.',\n        default_config_files=['/etc/rdiffweb/rdw.conf', '/etc/rdiffweb/rdw.conf.d/*.conf'],\n        add_env_var_help=True,\n        auto_env_var_prefix='RDIFFWEB_',\n        config_file_parser_class=ConfigFileParser,\n        conflict_handler='resolve',\n    )\n    parser.add_argument(\n        '-f', '--config', is_config_file=True, metavar='FILE', help='location of Rdiffweb configuration file'\n    )\n    parser.add(\n        '--database-uri',\n        '--sqlitedb-file',\n        '--sqlitedbfile',\n        metavar='URI',\n        help=\"\"\"Location of the database used for persistence. SQLite and PostgreSQL\n            database are supported officially. To use a SQLite database you may\n            define the location using a file path or a URI.\n            e.g.: /srv/rdiffweb/file.db or sqlite:///srv/rdiffweb/file.db`.\n            To use PostgreSQL server you must provide\n            a URI similar to postgresql://user:pass@10.255.1.34/dbname and you\n            must install required dependencies.\n            By default, Rdiffweb uses a SQLite embedded database located at\n            /etc/rdiffweb/rdw.db.\"\"\",\n        default='/etc/rdiffweb/rdw.db',\n    )\n    parser.add_argument(\n        '-d',\n        '--debug',\n        action='store_true',\n        help='enable rdiffweb debug mode - change the log level to DEBUG, print exception stack trace to the web interface and show SQL query in logs',\n    )\n    parser.add_argument(\n        '--admin-user',\n        '--adminuser',\n        metavar='USERNAME',\n        help='administrator username. The administrator user get created on startup if the database is empty.',\n        default='admin',\n    )\n    parser.add_argument(\n        '--admin-password',\n        metavar='USERNAME',\n        help=\"\"\"administrator encrypted password as SSHA. Read online\n            documentation to know more about how to encrypt your password\n            into SSHA or use http://projects.marsching.org/weave4j/util/genpassword.php\n            When defined, administrator password cannot be updated using the web interface.\n            When undefined, default administrator password is `admin123` and\n            it can be updated using the web interface.\"\"\",\n    )\n    parser.add_argument(\n        '--default-theme',\n        '--defaulttheme',\n        help='define the default theme. Either: default, blue or orange. Define the CSS file to be loaded in the web interface. You may manually edit a CSS file to customize it. The location is similar to `/usr/local/lib/python3.9/dist-packages/rdiffweb/static/`',\n        choices=['default', 'blue', 'orange'],\n        default='default',\n    )\n    parser.add_argument(\n        '--environment',\n        choices=['development', 'production'],\n        help='define the type of environment: development, production. This is used to limit the information shown to the user when an error occur.',\n        default='production',\n    )\n    parser.add_argument(\n        '--email-encryption',\n        '--emailencryption',\n        choices=['none', 'ssl', 'starttls'],\n        help='type of encryption to be used when establishing communication with SMTP server. Default: none',\n        default='none',\n    )\n    parser.add_argument(\n        '--email-host',\n        '--emailhost',\n        metavar='HOST',\n        help='SMTP server used to send email in the form <host>:<port>. If the port is not provided, default to standard port 25 or 465 is used. e.g.: smtp.gmail.com:587',\n    )\n    parser.add_argument(\n        '--email-sender',\n        '--emailsender',\n        metavar='EMAIL',\n        help='email addres used for the `from:` field when sending email.',\n    )\n    parser.add_argument(\n        '--email-notification-time',\n        '--emailnotificationtime',\n        metavar='TIME',\n        help='time when the email notifcation should be sent for inactive backups. e.g.: 22:00 Default value: 23:00',\n        default='23:00',\n    )\n    parser.add_argument(\n        '--email-username',\n        '--emailusername',\n        metavar='USERNAME',\n        help='username used for authentication with the SMTP server.',\n    )\n    parser.add_argument(\n        '--email-password',\n        '--emailpassword',\n        metavar='PASSWORD',\n        help='password used for authentication with the SMTP server.',\n    )\n    parser.add_argument(\n        '--email-send-changed-notification',\n        '--emailsendchangednotification',\n        help='True to send notification when sensitive information get change in user profile.',\n        action='store_true',\n        default=False,\n    )\n    parser.add_argument(\n        '--favicon',\n        help='location of an icon to be used as a favicon displayed in web browser.',\n        default=pkg_resources.resource_filename('rdiffweb', 'static/favicon.ico'),\n    )\n    parser.add_argument(\n        '--footer-name', '--footername', help=argparse.SUPPRESS, default='rdiffweb'\n    )\n    parser.add_argument(\n        '--footer-url', '--footerurl', help=argparse.SUPPRESS, default='https://rdiffweb.org/'\n    )\n    parser.add_argument(\n        '--header-logo',\n        '--headerlogo',\n        help='location of an image (preferably a .png) to be used as a replacement for the rdiffweb logo.',\n    )\n    parser.add_argument(\n        '--header-name',\n        '--headername',\n        help='application name displayed in the title bar and header menu.',\n        default='Rdiffweb',\n    )\n    parser.add_argument(\n        '--ldap-add-missing-user',\n        '--addmissinguser',\n        action='store_true',\n        help='enable creation of users from LDAP when the credential are valid.',\n        default=False,\n    )\n    parser.add_argument(\n        '--ldap-add-user-default-role',\n        help='default role used when creating users from LDAP. This parameter is only useful when `--ldap-add-missing-user` is enabled.',\n        default='user',\n        choices=['admin', 'maintainer', 'user'],\n    )\n    parser.add_argument(\n        '--ldap-add-user-default-userroot',\n        help='default user root directory used when creating users from LDAP. LDAP attributes may be used to define the default location. e.g.: `/backups/{uid[0]}/`. This parameter is only useful when `--ldap-add-missing-user` is enabled.',\n        default='',\n    )\n    parser.add_argument(\n        '--ldap-uri',\n        '--ldapuri',\n        help='URL to the LDAP server used to validate user credentials. e.g.: ldap://localhost:389',\n    )\n    parser.add_argument(\n        '--ldap-base-dn',\n        '--ldapbasedn',\n        metavar='DN',\n        help='DN of the branch of the directory where all searches should start from. e.g.: dc=my,dc=domain',\n        default=\"\",\n    )\n    parser.add_argument(\n        '--ldap-scope',\n        '--ldapscope',\n        help='scope of the search. Can be either base, onelevel or subtree',\n        choices=['base', 'onelevel', 'subtree'],\n        default=\"subtree\",\n    )\n    parser.add_argument('--ldap-tls', '--ldaptls', action='store_true', help='enable TLS')\n    parser.add_argument(\n        '--ldap-username-attribute',\n        '--ldapattribute',\n        metavar='ATTRIBUTE',\n        help=\"The attribute to search username. If no attributes are provided, the default is to use `uid`. It's a good idea to choose an attribute that will be unique across all entries in the subtree you will be using.\",\n        default='uid',\n    )\n    parser.add_argument(\n        '--ldap-filter',\n        '--ldapfilter',\n        help=\"search filter to limit LDAP lookup. If not provided, defaults to (objectClass=*), which searches for all objects in the tree.\",\n        default='(objectClass=*)',\n    )\n    parser.add_argument(\n        '--ldap-required-group',\n        '--ldaprequiredgroup',\n        metavar='GROUPNAME',\n        help=\"name of the group of which the user must be a member to access rdiffweb. Should be used with ldap-group-attribute and ldap-group-attribute-is-dn.\",\n    )\n    parser.add_argument(\n        '--ldap-group-attribute',\n        '--ldapgroupattribute',\n        metavar='ATTRIBUTE',\n        help=\"name of the attribute defining the groups of which the user is a member. Should be used with ldap-required-group and ldap-group-attribute-is-dn.\",\n        default='member',\n    )\n    parser.add_argument(\n        '--ldap-group-attribute-is-dn',\n        '--ldapgroupattributeisdn',\n        help=\"True if the content of the attribute `ldap-group-attribute` is a DN.\",\n        action='store_true',\n    )\n    parser.add_argument(\n        '--ldap-bind-dn',\n        '--ldapbinddn',\n        metavar='DN',\n        help=\"optional DN used to bind to the server when searching for entries. If not provided, will use an anonymous bind.\",\n        default=\"\",\n    )\n    parser.add_argument(\n        '--ldap-bind-password',\n        '--ldapbindpassword',\n        metavar='PASSWORD',\n        help=\"password to use in conjunction with LdapBindDn. Note that the bind password is probably sensitive data, and should be properly protected. You should only use the LdapBindDn and LdapBindPassword if you absolutely need them to search the directory.\",\n        default=\"\",\n    )\n    parser.add_argument(\n        '--ldap-version',\n        '--ldapversion',\n        '--ldapprotocolversion',\n        help=\"version of LDAP in use either 2 or 3. Default to 3.\",\n        default=3,\n        type=int,\n        choices=[2, 3],\n    )\n    parser.add_argument(\n        '--ldap-network-timeout',\n        '--ldapnetworktimeout',\n        metavar='SECONDS',\n        help=\"timeout in seconds value used for LDAP connection\",\n        default=100,\n        type=int,\n    )\n    parser.add_argument(\n        '--ldap-timeout',\n        '--ldaptimeout',\n        metavar='SECONDS',\n        help=\"timeout in seconds value used for LDAP request\",\n        default=300,\n        type=int,\n    )\n    parser.add_argument(\n        '--ldap-encoding',\n        '--ldapencoding',\n        metavar='ENCODING',\n        help=\"encoding used by your LDAP server.\",\n        default=\"utf-8\",\n    )\n    parser.add_argument(\n        '--log-access-file', '--logaccessfile', metavar='FILE', help='location of Rdiffweb log access file.'\n    )\n    parser.add_argument(\n        '--log-file',\n        '--logfile',\n        metavar='FILE',\n        help='location of Rdiffweb log file. Print log to the console if not define in config file.',\n    )\n    parser.add_argument(\n        '--log-level',\n        '--loglevel',\n        help='Define the log level.',\n        choices=['ERROR', 'WARN', 'INFO', 'DEBUG'],\n        default='INFO',\n    )\n    parser.add_argument(\n        '--max-depth',\n        '--maxdepth',\n        metavar='DEPTH',\n        help=\"define the maximum folder depthness to search into the user's root directory to find repositories. This is commonly used if you repositories are organised with multiple sub-folder.\",\n        type=int,\n        default=3,\n    )\n    parser.add('--quota-set-cmd', '--quotasetcmd', metavar='COMMAND', help=\"command line to set the user's quota.\")\n    parser.add('--quota-get-cmd', '--quotagetcmd', metavar='COMMAND', help=\"command line to get the user's quota.\")\n    parser.add(\n        '--quota-used-cmd', '--quotausedcmd', metavar='COMMAND', help=\"Command line to get user's quota disk usage.\"\n    )\n    parser.add(\n        '--remove-older-time',\n        '--removeoldertime',\n        metavar='TIME',\n        help=\"Time when to execute the remove older scheduled job. e.g.: 22:30\",\n        default='23:00',\n    )\n    parser.add('--server-host', '--serverhost', metavar='IP', default='127.0.0.1', help='IP address to listen to')\n    parser.add(\n        '--server-port',\n        '--serverport',\n        metavar='PORT',\n        help='port to listen to for HTTP request',\n        default='8080',\n        type=int,\n    )\n    parser.add(\n        '--rate-limit-dir',\n        '--session-dir',\n        '--sessiondir',\n        metavar='FOLDER',\n        help='location where to store rate-limit information. When undefined, the data is kept in memory. `--session-dir` are deprecated and kept for backward compatibility.',\n    )\n    parser.add(\n        '--session-timeout',\n        metavar='MINUTES',\n        help='Sessions will be revoke after this period of inactivity, unless the user selected \"remember me\". Default 15 minutes.',\n        default=15,\n    )\n    parser.add(\n        '--session-persistent-timeout',\n        metavar='MINUTES',\n        help='Persistent sessions (remember me) will be revoke after this period of inactivity. Default 30 days.',\n        default=43200,\n    )\n    parser.add(\n        '--rate-limit',\n        metavar='LIMIT',\n        type=int,\n        default=30,\n        help='maximum number of requests per minute that can be made by an IP address for an unauthenticated connection. When this limit is reached, an HTTP 429 message is returned to the user. This security measure is used to limit brute force attacks on the login page and the RESTful API.',\n    )\n    parser.add(\n        '--ssl-certificate',\n        '--sslcertificate',\n        metavar='CERT',\n        help='location of the SSL Certification to enable HTTPS (not recommended)',\n    )\n    parser.add(\n        '--ssl-private-key',\n        '--sslprivatekey',\n        metavar='KEY',\n        help='location of the SSL Private Key to enable HTTPS (not recommended)',\n    )\n    parser.add(\n        '--tempdir',\n        metavar='FOLDER',\n        help='alternate temporary folder to be used when restoring files. Might be useful if the default location has limited disk space. Default to TEMPDIR environment or `/tmp`.',\n    )\n    parser.add(\n        '--disable-ssh-keys',\n        action='store_true',\n        help='used to hide SSH Key management to avoid users to add or remove SSH Key using the web application',\n        default=False,\n    )\n    parser.add(\n        '--password-min-length',\n        type=int,\n        help=\"Minimum length of the user's password\",\n        default=8,\n    )\n    parser.add(\n        '--password-max-length',\n        type=int,\n        help=\"Maximum length of the user's password\",\n        default=128,\n    )\n    parser.add(\n        '--password-score',\n        type=lambda x: max(1, min(int(x), 4)),\n        help=\"Minimum zxcvbn's score for password. Value from 1 to 4. Default value 2. Read more about it here: https://github.com/dropbox/zxcvbn\",\n        default=2,\n    )\n    parser.add_argument('--version', action='version', version='%(prog)s ' + VERSION)\n    flags = ['--welcome-msg'] + ['--welcome-msg-' + i for i in ['ca', 'en', 'es', 'fr', 'ru']] + ['--welcomemsg']\n    parser.add_argument(\n        *flags,\n        metavar='HTML',\n        help='replace the welcome message displayed in the login page for default locale or for a specific locale',\n        action=LocaleAction\n    )\n    return parser\n    def __init__(self, cfg):\n        self.cfg = cfg\n        db_uri = self.cfg.database_uri if '://' in self.cfg.database_uri else \"sqlite:///\" + self.cfg.database_uri\n        cherrypy.config.update(\n            {\n                'environment': 'development' if cfg.debug else cfg.environment,\n                'tools.db.uri': db_uri,\n                'tools.db.debug': cfg.debug,\n                'ldap.uri': cfg.ldap_uri,\n                'ldap.base_dn': cfg.ldap_base_dn,\n                'ldap.bind_dn': cfg.ldap_bind_dn,\n                'ldap.bind_password': cfg.ldap_bind_password,\n                'ldap.scope': cfg.ldap_scope,\n                'ldap.tls': cfg.ldap_tls,\n                'ldap.username_attribute': cfg.ldap_username_attribute,\n                'ldap.required_group': cfg.ldap_required_group,\n                'ldap.group_attribute': cfg.ldap_group_attribute,\n                'ldap.group_attribute_is_dn': cfg.ldap_group_attribute_is_dn,\n                'ldap.version': cfg.ldap_version,\n                'ldap.network_timeout': cfg.ldap_network_timeout,\n                'ldap.timeout': cfg.ldap_timeout,\n                'ldap.encoding': cfg.ldap_encoding,\n                'login.add_missing_user': cfg.ldap_add_missing_user,\n                'login.add_user_default_role': cfg.ldap_add_user_default_role,\n                'login.add_user_default_userroot': cfg.ldap_add_user_default_userroot,\n                'smtp.server': cfg.email_host,\n                'smtp.username': cfg.email_username,\n                'smtp.password': cfg.email_password,\n                'smtp.email_from': cfg.email_sender\n                and '%s <%s>'\n                % (\n                    cfg.header_name,\n                    cfg.email_sender,\n                ),\n                'smtp.encryption': cfg.email_encryption,\n                'remove_older.execution_time': self.cfg.remove_older_time,\n                'notification.execution_time': self.cfg.email_notification_time,\n                'notification.send_changed': self.cfg.email_send_changed_notification,\n                'quota.set_quota_cmd': self.cfg.quota_set_cmd,\n                'quota.get_quota_cmd': self.cfg.quota_get_cmd,\n                'quota.get_usage_cmd': self.cfg.quota_used_cmd,\n            }\n        )\n        cherrypy.tools.db.create_all()\n        self.templates = rdw_templating.TemplateManager()\n        rate_limit_storage_class = rdiffweb.tools.ratelimit.RamRateLimit\n        if cfg.rate_limit_dir:\n            rate_limit_storage_class = rdiffweb.tools.ratelimit.FileRateLimit\n        config = {\n            '/': {\n                'request.uri_encoding': 'ISO-8859-1',\n                'tools.i18n.on': True,\n                'tools.i18n.default': 'en_US',\n                'tools.i18n.mo_dir': pkg_resources.resource_filename('rdiffweb', 'locales'),\n                'tools.i18n.domain': 'messages',\n                'tools.encode.on': True,\n                'tools.encode.encoding': 'utf-8',\n                'tools.gzip.on': True,\n                'error_page.default': self.error_page,\n                'tools.sessions.on': True,\n                'tools.sessions.debug': cfg.debug,\n                'tools.sessions.storage_class': DbSession,\n                'tools.sessions.httponly': True,\n                'tools.sessions.timeout': cfg.session_timeout,\n                'tools.sessions.persistent': False,\n                'tools.auth_form.timeout': cfg.session_persistent_timeout,\n                'tools.ratelimit.debug': cfg.debug,\n                'tools.ratelimit.delay': 60,\n                'tools.ratelimit.anonymous_limit': cfg.rate_limit,\n                'tools.ratelimit.storage_class': rate_limit_storage_class,\n                'tools.ratelimit.storage_path': cfg.rate_limit_dir,\n            },\n        }\n        Application.__init__(self, root=Root(), config=config)\n        self.root.favicon_ico = staticfile(self._favicon)\n        if self._header_logo:\n            self.root.header_logo = staticfile(self._header_logo)\n        if self._tempdir:\n            os.environ[\"TMPDIR\"] = self._tempdir\n        UserObject.create_admin_user(cfg.admin_user, cfg.admin_password)\n    def get_and_increment(self, token, delay):\n        lock = self._locks.setdefault(token, threading.RLock())\n        with lock:\n            tracker = self._load(token)\n            if tracker is None or tracker.timeout < time.time():\n                tracker = Tracker(token=token, hits=0, timeout=int(time.time() + delay))\n            tracker = tracker._replace(hits=tracker.hits + 1)\n            self._save(tracker)\n        return tracker.hits\n    def _load(self, token):\n        path = self._path(token)\n        try:\n            f = open(path, 'rb')\n            try:\n                return pickle.load(f)\n            finally:\n                f.close()\n        except (IOError, EOFError):\n            pass\n        return None\ndef check_ratelimit(delay=60, anonymous_limit=0, registered_limit=0, rate_exceed_status=429, debug=False, **conf):\n    \"\"\"\n    Verify the ratelimit. By default return a 429 HTTP error code (Too Many Request).\n    Usage:\n    @cherrypy.tools.ratelimit(on=True, anonymous_limit=5, registered_limit=50, storage_class=FileRateLimit, storage_path='/tmp')\n    def index(self):\n        pass\n    \"\"\"\n    datastore = getattr(cherrypy, '_ratelimit_datastore', None)\n    if datastore is None:\n        storage_class = conf.get('storage_class', RamRateLimit)\n        datastore = storage_class(**conf)\n        cherrypy._ratelimit_datastore = datastore\n    token = cherrypy.request.login or cherrypy.request.remote.ip\n    limit = registered_limit if cherrypy.request.login else anonymous_limit\n    if limit is None or limit <= 0:\n        return\n    hits = datastore.get_and_increment(token, delay)\n    if debug:\n        cherrypy.log(\n            'check and increase rate limit for token %s, limit %s, hits %s' % (token, limit, hits), 'TOOLS.RATELIMIT'\n        )\n    if limit <= hits:\n        raise cherrypy.HTTPError(rate_exceed_status)\ncherrypy.tools.ratelimit = cherrypy.Tool('before_handler', check_ratelimit, priority=60)\ndef _checkpassword(realm, username, password):\n    \"\"\"\n    Check basic authentication.\n    \"\"\"\n    userobj = UserObject.get_user(username)\n    if userobj is not None:\n        if userobj.validate_access_token(password):\n            return True\n        if userobj.mfa == UserObject.ENABLED_MFA:\n            return False\n    return any(cherrypy.engine.publish('login', username, password))\nclass ApiCurrentUser(Controller):\n    def default(self):\n        u = self.app.currentuser\n        u.refresh_repos()\n        return {\n            \"email\": u.email,\n            \"username\": u.username,\n            \"repos\": [\n                {\n                    \"name\": repo_obj.name,\n                    \"maxage\": repo_obj.maxage,\n                    \"keepdays\": repo_obj.keepdays,\n                    \"display_name\": repo_obj.display_name,\n                    \"last_backup_date\": repo_obj.last_backup_date,\n                    \"status\": repo_obj.status[0],\n                    \"encoding\": repo_obj.encoding,\n                }\n                for repo_obj in u.repo_objs\n            ],\n        }\n    def __init__(self, **kwargs):\n        if 'formdata' in kwargs:\n            formdata = kwargs.pop('formdata')\n        else:\n            formdata = _AUTO if CherryForm.is_submitted(self) else None\n        super().__init__(formdata=formdata, **kwargs)\n    def is_submitted(self):\n        \"\"\"\n        Consider the form submitted if there is an active request and\n        the method is ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.\n        \"\"\"\n        return cherrypy.request.method in SUBMIT_METHODS",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-3439",
        "description": "[{'lang': 'en', 'value': 'Allocation of Resources Without Limits or Throttling in GitHub repository ikus060/rdiffweb prior to 2.5.0.'}]",
        "cwe_number": 770
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-100",
      "code": "def login(request):\n    form = forms.PasswordLoginForm()\n    magic_form = forms.EmailLoginForm()\n    if request.method == \"POST\":\n        if request.POST.get(\"action\") == \"login\":\n            form = forms.PasswordLoginForm(request.POST)\n            if form.is_valid():\n                return _check_2fa(request, form.user)\n        else:\n            magic_form = forms.EmailLoginForm(request.POST)\n            if magic_form.is_valid():\n                redirect_url = request.GET.get(\"next\")\n                if not _allow_redirect(redirect_url):\n                    redirect_url = None\n                profile = Profile.objects.for_user(magic_form.user)\n                profile.send_instant_login_link(redirect_url=redirect_url)\n                response = redirect(\"hc-login-link-sent\")\n                response.set_cookie(\"auto-login\", \"1\", max_age=300, httponly=True)\n                return response\n    if request.user.is_authenticated:\n        return _redirect_after_login(request)\n    bad_link = request.session.pop(\"bad_link\", None)\n    ctx = {\n        \"page\": \"login\",\n        \"form\": form,\n        \"magic_form\": magic_form,\n        \"bad_link\": bad_link,\n        \"registration_open\": settings.REGISTRATION_OPEN,\n        \"support_email\": settings.SUPPORT_EMAIL,\n    }\n    return render(request, \"accounts/login.html\", ctx)\ndef signup(request):\n    if not settings.REGISTRATION_OPEN:\n        return HttpResponseForbidden()\n    ctx = {}\n    form = forms.SignupForm(request.POST)\n    if form.is_valid():\n        email = form.cleaned_data[\"identity\"]\n        tz = form.cleaned_data[\"tz\"]\n        user = _make_user(email, tz)\n        profile = Profile.objects.for_user(user)\n        profile.send_instant_login_link()\n        ctx[\"created\"] = True\n    else:\n        ctx = {\"form\": form}\n    response = render(request, \"accounts/signup_result.html\", ctx)\n    if ctx.get(\"created\"):\n        response.set_cookie(\"auto-login\", \"1\", max_age=300, httponly=True)\n    return response\nclass EmailLoginForm(forms.Form):\n    def clean_identity(self):\n        v = self.cleaned_data[\"identity\"]\n        if not TokenBucket.authorize_login_email(v):\n            raise forms.ValidationError(\"Too many attempts, please try later.\")\n        try:\n            self.user = User.objects.get(email=v)\n        except User.DoesNotExist:\n            raise forms.ValidationError(\"Unknown email address.\")\n        return v",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-0440",
        "description": "[{'lang': 'en', 'value': 'Observable Discrepancy in GitHub repository healthchecks/healthchecks prior to v2.6.'}]",
        "cwe_number": 203
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-101",
      "code": "    def __init__(self, cfg, **kwargs):\n        self.vpns = cfg.vpns\n        if kwargs.get('vpn_id'):\n            vpn = self.vpns[kwargs['vpn_id']]\n            self._socket_connect(vpn)\n            if vpn['socket_connected']:\n                release = self.send_command('version\\n')\n                version = semver(self.parse_version(release).split(' ')[1])\n                if version.major == 2 and \\\n                        version.minor >= 4 and \\\n                        kwargs.get('client_id'):\n                    command = 'client-kill {0!s}\\n'.format(kwargs['client_id'])\n                else:\n                    command = 'kill {0!s}:{1!s}\\n'.format(kwargs['ip'], kwargs['port'])\n                self.send_command(command)\n                self._socket_disconnect()\n        geoip_data = cfg.settings['geoip_data']\n        self.geoip_version = None\n        self.gi = None\n        try:\n            if geoip_data.endswith('.mmdb') and geoip2_available:\n                self.gi = database.Reader(geoip_data)\n                self.geoip_version = 2\n            elif geoip_data.endswith('.dat') and geoip1_available:\n                self.gi = geoip1.open(geoip_data, geoip1.GEOIP_STANDARD)\n                self.geoip_version = 1\n            else:\n                warning('No compatible geoip1 or geoip2 data/libraries found.')\n        except IOError:\n            warning('No compatible geoip1 or geoip2 data/libraries found.')\n        for _, vpn in list(self.vpns.items()):\n            self._socket_connect(vpn)\n            if vpn['socket_connected']:\n                self.collect_data(vpn)\n                self._socket_disconnect()\n    def collect_data(self, vpn):\n        ver = self.send_command('version\\n')\n        vpn['release'] = self.parse_version(ver)\n        vpn['version'] = semver(vpn['release'].split(' ')[1])\n        state = self.send_command('state\\n')\n        vpn['state'] = self.parse_state(state)\n        stats = self.send_command('load-stats\\n')\n        vpn['stats'] = self.parse_stats(stats)\n        status = self.send_command('status 3\\n')\n        vpn['sessions'] = self.parse_status(status, vpn['version'])",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-31606",
        "description": "[{'lang': 'en', 'value': 'furlongm openvpn-monitor through 1.1.3 allows Authorization Bypass to disconnect arbitrary clients.'}]",
        "cwe_number": 287
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-102",
      "code": "    def __init__(\n        self,\n        route_handlers: List[\"ControllerRouterHandler\"],\n        *,\n        after_exception: Optional[\"SingleOrList[AfterExceptionHookHandler]\"] = None,\n        after_request: Optional[\"AfterRequestHookHandler\"] = None,\n        after_response: Optional[\"AfterResponseHookHandler\"] = None,\n        after_shutdown: Optional[\"SingleOrList[LifeSpanHookHandler]\"] = None,\n        after_startup: Optional[\"SingleOrList[LifeSpanHookHandler]\"] = None,\n        allowed_hosts: Optional[Union[List[str], \"AllowedHostsConfig\"]] = None,\n        before_request: Optional[\"BeforeRequestHookHandler\"] = None,\n        before_send: Optional[\"SingleOrList[BeforeMessageSendHookHandler]\"] = None,\n        before_shutdown: Optional[\"SingleOrList[LifeSpanHookHandler]\"] = None,\n        before_startup: Optional[\"SingleOrList[LifeSpanHookHandler]\"] = None,\n        cache_config: CacheConfig = DEFAULT_CACHE_CONFIG,\n        cache_control: Optional[\"CacheControlHeader\"] = None,\n        compression_config: Optional[\"CompressionConfig\"] = None,\n        cors_config: Optional[\"CORSConfig\"] = None,\n        csrf_config: Optional[\"CSRFConfig\"] = None,\n        debug: bool = False,\n        dependencies: Optional[Dict[str, \"Provide\"]] = None,\n        etag: Optional[\"ETag\"] = None,\n        exception_handlers: Optional[\"ExceptionHandlersMap\"] = None,\n        guards: Optional[List[\"Guard\"]] = None,\n        initial_state: Optional[\"InitialStateType\"] = None,\n        logging_config: Union[\"BaseLoggingConfig\", \"EmptyType\", None] = Empty,\n        middleware: Optional[List[\"Middleware\"]] = None,\n        on_app_init: Optional[List[\"OnAppInitHandler\"]] = None,\n        on_shutdown: Optional[List[\"LifeSpanHandler\"]] = None,\n        on_startup: Optional[List[\"LifeSpanHandler\"]] = None,\n        openapi_config: Optional[OpenAPIConfig] = DEFAULT_OPENAPI_CONFIG,\n        opt: Optional[Dict[str, Any]] = None,\n        parameters: Optional[\"ParametersMap\"] = None,\n        plugins: Optional[List[\"PluginProtocol\"]] = None,\n        request_class: Optional[Type[\"Request\"]] = None,\n        response_class: Optional[\"ResponseType\"] = None,\n        response_cookies: Optional[\"ResponseCookies\"] = None,\n        response_headers: Optional[\"ResponseHeadersMap\"] = None,\n        security: Optional[List[\"SecurityRequirement\"]] = None,\n        static_files_config: Optional[Union[\"StaticFilesConfig\", List[\"StaticFilesConfig\"]]] = None,\n        tags: Optional[List[str]] = None,\n        template_config: Optional[\"TemplateConfig\"] = None,\n        type_encoders: Optional[\"TypeEncodersMap\"] = None,\n        websocket_class: Optional[Type[\"WebSocket\"]] = None,\n    ) -> None:\n        \"\"\"Initialize a ``Starlite`` application.\n        Args:\n            after_exception: An application level :class:`exception hook handler <starlite.types.AfterExceptionHookHandler>`\n                or list thereof.This hook is called after an exception occurs. In difference to exception handlers,\n                it is not meant to return a response - only to process the exception (e.g. log it, send it to Sentry etc.).\n            after_request: A sync or async function executed after the route handler function returned and the response\n                object has been resolved. Receives the response object.\n            after_response: A sync or async function called after the response has been awaited. It receives the\n                :class:`Request <starlite.connection.Request>` object and should not return any values.\n            after_shutdown: An application level :class:`life-span hook handler <starlite.types.LifeSpanHookHandler>` or\n                list thereof. This hook is called during the ASGI shutdown, after all callables in the 'on_shutdown'\n                list have been called.\n            after_startup: An application level :class:`life-span hook handler <starlite.types.LifeSpanHookHandler>` or\n                list thereof. This hook is called during the ASGI startup, after all callables in the 'on_startup'\n                list have been called.\n            allowed_hosts: A list of allowed hosts - enables the builtin allowed hosts middleware.\n            before_request: A sync or async function called immediately before calling the route handler.\n                Receives the :class:`Request <starlite.connection.Request>` instance and any non-``None`` return value is\n                used for the response, bypassing the route handler.\n            before_send: An application level :class:`before send hook handler <starlite.types.BeforeMessageSendHookHandler>` or\n                list thereof. This hook is called when the ASGI send function is called.\n            before_shutdown: An application level :class:`life-span hook handler <starlite.types.LifeSpanHookHandler>` or\n                list thereof. This hook is called during the ASGI shutdown, before any callables in the 'on_shutdown'\n                list have been called.\n            before_startup: An application level :class:`life-span hook handler <starlite.types.LifeSpanHookHandler>` or\n                list thereof. This hook is called during the ASGI startup, before any callables in the 'on_startup'\n                list have been called.\n            cache_config: Configures caching behavior of the application.\n            cache_control: A ``cache-control`` header of type\n                :class:`CacheControlHeader <starlite.datastructures.CacheControlHeader>` to add to route handlers of this app.\n                Can be overridden by route handlers.\n            compression_config: Configures compression behaviour of the application, this enabled a builtin or user\n                defined Compression middleware.\n            cors_config: If set this enables the builtin CORS middleware.\n            csrf_config: If set this enables the builtin CSRF middleware.\n            debug: If ``True``, app errors rendered as HTML with a stack trace.\n            dependencies: A string keyed dictionary of dependency :class:`Provider <starlite.datastructures.Provide>` instances.\n            etag: An ``etag`` header of type :class:`ETag <datastructures.ETag>` to add to route handlers of this app.\n                Can be overridden by route handlers.\n            exception_handlers: A dictionary that maps handler functions to status codes and/or exception types.\n            guards: A list of :class:`Guard <starlite.types.Guard>` callables.\n            initial_state: An object from which to initialize the app state.\n            logging_config: A subclass of :class:`BaseLoggingConfig <starlite.config.logging.BaseLoggingConfig>`.\n            middleware: A list of :class:`Middleware <starlite.types.Middleware>`.\n            on_app_init: A sequence of :class:`OnAppInitHandler <starlite.types.OnAppInitHandler>` instances. Handlers receive\n                an instance of :class:`AppConfig <starlite.config.app.AppConfig>` that will have been initially populated with\n                the parameters passed to :class:`Starlite <starlite.app.Starlite>`, and must return an instance of same. If more\n                than one handler is registered they are called in the order they are provided.\n            on_shutdown: A list of :class:`LifeSpanHandler <starlite.types.LifeSpanHandler>` called during\n                application shutdown.\n            on_startup: A list of :class:`LifeSpanHandler <starlite.types.LifeSpanHandler>` called during\n                application startup.\n            openapi_config: Defaults to :attr:`DEFAULT_OPENAPI_CONFIG`\n            opt: A string keyed dictionary of arbitrary values that can be accessed in :class:`Guards <starlite.types.Guard>` or wherever you\n                have access to :class:`Request <starlite.connection.request.Request>` or :class:`ASGI Scope <starlite.types.Scope>`.\n            parameters: A mapping of :class:`Parameter <starlite.params.Parameter>` definitions available to all\n                application paths.\n            plugins: List of plugins.\n            request_class: An optional subclass of :class:`Request <starlite.connection.request.Request>` to use for\n                http connections.\n            response_class: A custom subclass of [starlite.response.Response] to be used as the app's default response.\n            response_cookies: A list of [Cookie](starlite.datastructures.Cookie] instances.\n            response_headers: A string keyed dictionary mapping :class:`ResponseHeader <starlite.datastructures.ResponseHeader>`\n                instances.\n            route_handlers: A required list of route handlers, which can include instances of\n                :class:`Router <starlite.router.Router>`, subclasses of :class:`Controller <starlite.controller.Controller>` or\n                any function decorated by the route handler decorators.\n            security: A list of dictionaries that will be added to the schema of all route handlers in the application.\n                See :class:`SecurityRequirement <pydantic_openapi_schema.v3_1_0.security_requirement.SecurityRequirement>` for details.\n            static_files_config: An instance or list of :class:`StaticFilesConfig <starlite.config.StaticFilesConfig>`\n            tags: A list of string tags that will be appended to the schema of all route handlers under the application.\n            template_config: An instance of :class:`TemplateConfig <starlite.config.TemplateConfig>`\n            type_encoders: A mapping of types to callables that transform them into types supported for serialization.\n            websocket_class: An optional subclass of :class:`WebSocket <starlite.connection.websocket.WebSocket>` to use for\n                websocket connections.\n        \"\"\"\n        self.openapi_schema: Optional[\"OpenAPI\"] = None\n        self.get_logger: \"GetLogger\" = get_logger_placeholder\n        self.logger: Optional[\"Logger\"] = None\n        self.routes: List[Union[\"HTTPRoute\", \"ASGIRoute\", \"WebSocketRoute\"]] = []\n        self.asgi_router = ASGIRouter(app=self)\n        config = AppConfig(\n            after_exception=after_exception or [],\n            after_request=after_request,\n            after_response=after_response,\n            after_shutdown=after_shutdown or [],\n            after_startup=after_startup or [],\n            allowed_hosts=allowed_hosts or [],\n            before_request=before_request,\n            before_send=before_send or [],\n            before_shutdown=before_shutdown or [],\n            before_startup=before_startup or [],\n            cache_config=cache_config,\n            cache_control=cache_control,\n            compression_config=compression_config,\n            cors_config=cors_config,\n            csrf_config=csrf_config,\n            debug=debug,\n            dependencies=dependencies or {},\n            etag=etag,\n            exception_handlers=exception_handlers or {},\n            guards=guards or [],\n            initial_state=initial_state or {},\n            logging_config=logging_config if logging_config is not Empty else LoggingConfig() if debug else None,\n            middleware=middleware or [],\n            on_shutdown=on_shutdown or [],\n            on_startup=on_startup or [],\n            openapi_config=openapi_config,\n            opt=opt or {},\n            parameters=parameters or {},\n            plugins=plugins or [],\n            request_class=request_class,\n            response_class=response_class,\n            response_cookies=response_cookies or [],\n            response_headers=response_headers or {},\n            route_handlers=route_handlers,\n            security=security or [],\n            static_files_config=static_files_config or [],\n            tags=tags or [],\n            template_config=template_config,\n            type_encoders=type_encoders,\n            websocket_class=websocket_class,\n        )\n        for handler in on_app_init or []:\n            config = handler(config)\n        self.allowed_hosts = cast(\"Optional[AllowedHostsConfig]\", config.allowed_hosts)\n        self.after_exception = as_async_callable_list(config.after_exception)\n        self.after_shutdown = as_async_callable_list(config.after_shutdown)\n        self.after_startup = as_async_callable_list(config.after_startup)\n        self.before_send = as_async_callable_list(config.before_send)\n        self.before_shutdown = as_async_callable_list(config.before_shutdown)\n        self.before_startup = as_async_callable_list(config.before_startup)\n        self.cache = config.cache_config.to_cache()\n        self.compression_config = config.compression_config\n        self.cors_config = config.cors_config\n        self.csrf_config = config.csrf_config\n        self.debug = config.debug\n        self.logging_config = config.logging_config\n        self.on_shutdown = config.on_shutdown\n        self.on_startup = config.on_startup\n        self.openapi_config = config.openapi_config\n        self.plugins = config.plugins\n        self.request_class = config.request_class or Request\n        self.state = State(config.initial_state, deep_copy=True)\n        self.static_files_config = config.static_files_config\n        self.template_engine = config.template_config.engine_instance if config.template_config else None\n        self.websocket_class = config.websocket_class or WebSocket\n        super().__init__(\n            after_request=config.after_request,\n            after_response=config.after_response,\n            before_request=config.before_request,\n            cache_control=config.cache_control,\n            dependencies=config.dependencies,\n            etag=config.etag,\n            exception_handlers=config.exception_handlers,\n            guards=config.guards,\n            middleware=config.middleware,\n            opt=config.opt,\n            parameters=config.parameters,\n            path=\"\",\n            response_class=config.response_class,\n            response_cookies=config.response_cookies,\n            response_headers=config.response_headers,\n            route_handlers=[],\n            security=config.security,\n            tags=config.tags,\n            type_encoders=config.type_encoders,\n        )\n        for plugin in self.plugins:\n            plugin.on_app_init(app=self)\n        for route_handler in config.route_handlers:\n            self.register(route_handler)\n        if self.debug and isinstance(self.logging_config, LoggingConfig):\n            self.logging_config.loggers[\"starlite\"][\"level\"] = \"DEBUG\"\n        if self.logging_config:\n            self.get_logger = self.logging_config.configure()\n            self.logger = self.get_logger(\"starlite\")\n        if self.openapi_config:\n            self.openapi_schema = self.openapi_config.to_openapi_schema()\n            self.update_openapi_schema()\n            self.register(self.openapi_config.openapi_controller)\n        for static_config in (\n            self.static_files_config if isinstance(self.static_files_config, list) else [self.static_files_config]\n        ):\n            self.register(static_config.to_static_files_app())\n        self.asgi_handler = self._create_asgi_handler()\n    async def __call__(\n        self,\n        scope: Union[\"Scope\", \"LifeSpanScope\"],\n        receive: Union[\"Receive\", \"LifeSpanReceive\"],\n        send: Union[\"Send\", \"LifeSpanSend\"],\n    ) -> None:\n        \"\"\"Application entry point.\n        Lifespan events (startup / shutdown) are sent to the lifespan handler, otherwise the ASGI handler is used\n        Args:\n            scope: The ASGI connection scope.\n            receive: The ASGI receive function.\n            send: The ASGI send function.\n        Returns:\n            None\n        \"\"\"\n        scope[\"app\"] = self\n        if scope[\"type\"] == \"lifespan\":\n            await self.asgi_router.lifespan(receive=receive, send=send)\n            return\n        scope[\"state\"] = {}\n        await self.asgi_handler(scope, receive, self._wrap_send(send=send, scope=scope))\ndef Body(\n    *,\n    media_type: Union[str, \"RequestEncodingType\"] = RequestEncodingType.JSON,\n    examples: Optional[List[\"Example\"]] = None,\n    external_docs: Optional[\"ExternalDocumentation\"] = None,\n    content_encoding: Optional[str] = None,\n    default: Any = Empty,\n    title: Optional[str] = None,\n    description: Optional[str] = None,\n    const: Optional[bool] = None,\n    gt: Optional[float] = None,\n    ge: Optional[float] = None,\n    lt: Optional[float] = None,\n    le: Optional[float] = None,\n    multiple_of: Optional[float] = None,\n    min_items: Optional[int] = None,\n    max_items: Optional[int] = None,\n    min_length: Optional[int] = None,\n    max_length: Optional[int] = None,\n    regex: Optional[str] = None\n) -> Any:\n    \"\"\"Create an extended request body kwarg definition.\n    Args:\n        media_type: Defaults to RequestEncodingType.JSON.\n        examples: A list of Example models.\n        external_docs: A url pointing at external documentation for the given\n            parameter.\n        content_encoding: The content encoding of the value. Applicable on to string values. See\n            OpenAPI 3.1 for details.\n        default: A default value. If const is true, this value is required.\n        title: String value used in the title section of the OpenAPI schema for the given\n            parameter.\n        description: String value used in the description section of the OpenAPI schema for the\n            given parameter.\n        const: A boolean flag dictating whether this parameter is a constant. If True, the value passed\n            to the parameter must equal its default value. This also causes the OpenAPI const field to be populated with\n            the default value.\n        gt: Constrict value to be greater than a given float or int. Equivalent to\n            exclusiveMinimum in the OpenAPI specification.\n        ge: Constrict value to be greater or equal to a given float or int. Equivalent to\n            minimum in the OpenAPI specification.\n        lt: Constrict value to be less than a given float or int. Equivalent to\n            exclusiveMaximum in the OpenAPI specification.\n        le: Constrict value to be less or equal to a given float or int. Equivalent to maximum\n            in the OpenAPI specification.\n        multiple_of: Constrict value to a multiple of a given float or int. Equivalent to\n            multipleOf in the OpenAPI specification.\n        min_items: Constrict a set or a list to have a minimum number of items. Equivalent to\n            minItems in the OpenAPI specification.\n        max_items: Constrict a set or a list to have a maximum number of items. Equivalent to\n            maxItems in the OpenAPI specification.\n        min_length: Constrict a string or bytes value to have a minimum length. Equivalent to\n            minLength in the OpenAPI specification.\n        max_length: Constrict a string or bytes value to have a maximum length. Equivalent to\n            maxLength in the OpenAPI specification.\n        regex: A string representing a regex against which the given string will be matched.\n            Equivalent to pattern in the OpenAPI specification.\n    \"\"\"\n    return BodyKwarg(\n        media_type=media_type,\n        examples=examples,\n        external_docs=external_docs,\n        content_encoding=content_encoding,\n        default=default,\n        title=title,\n        description=description,\n        const=const,\n        gt=gt,\n        ge=ge,\n        lt=lt,\n        le=le,\n        multiple_of=multiple_of,\n        min_items=min_items,\n        max_items=max_items,\n        min_length=min_length,\n        max_length=max_length,\n        regex=regex,\n    )\nclass DependencyKwarg:\ndef parse_multipart_form(body: bytes, boundary: bytes) -> Dict[str, Any]:\n    \"\"\"Parse multipart form data.\n    Args:\n        body: Body of the request.\n        boundary: Boundary of the multipart message.\n    Returns:\n        A dictionary of parsed results.\n    \"\"\"\n    fields: DefaultDict[str, List[Any]] = defaultdict(list)\n    if body and boundary:\n        form_parts = body.split(boundary)\n        for form_part in form_parts[1:-1]:\n            file_name = None\n            content_type = \"text/plain\"\n            content_charset = \"utf-8\"\n            field_name = None\n            line_index = 2\n            line_end_index = 0\n            headers: List[Tuple[str, str]] = []\n            while line_end_index != -1:\n                line_end_index = form_part.find(b\"\\r\\n\", line_index)\n                form_line = form_part[line_index:line_end_index].decode(\"utf-8\")\n                if not form_line:\n                    break\n                line_index = line_end_index + 2\n                colon_index = form_line.index(\":\")\n                current_idx = colon_index + 2\n                form_header_field = form_line[0:colon_index].lower()\n                form_header_value, form_parameters = parse_content_header(form_line[current_idx:])\n                if form_header_field == \"content-disposition\":\n                    field_name = form_parameters.get(\"name\")\n                    file_name = form_parameters.get(\"filename\")\n                    if file_name is None and (filename_with_asterisk := form_parameters.get(\"filename*\")):\n                        encoding, _, value = decode_rfc2231(filename_with_asterisk)\n                        file_name = unquote(value, encoding=encoding or content_charset)\n                elif form_header_field == \"content-type\":\n                    content_type = form_header_value\n                    content_charset = form_parameters.get(\"charset\", \"utf-8\")\n                headers.append((form_header_field, form_header_value))\n            if field_name:\n                post_data = form_part[line_index:-4].lstrip(b\"\\r\\n\")\n                if file_name:\n                    form_file = UploadFile(\n                        content_type=content_type, filename=file_name, file_data=post_data, headers=dict(headers)\n                    )\n                    fields[field_name].append(form_file)\n                else:\n                    try:\n                        fields[field_name].append(decode_json(post_data))\n                    except SerializationException:\n                        fields[field_name].append(post_data.decode(content_charset))\n    return {k: v if len(v) > 1 else v[0] for k, v in fields.items()}\ndef create_multipart_extractor(\n    signature_field: \"SignatureField\", is_data_optional: bool\n) -> Callable[[\"ASGIConnection[Any, Any, Any]\"], Coroutine[Any, Any, Any]]:\n    \"\"\"Create a multipart form-data extractor.\n    Args:\n        signature_field: A SignatureField instance.\n        is_data_optional: Boolean dictating whether the field is optional.\n    Returns:\n        An extractor function.\n    \"\"\"\n    async def extract_multipart(\n        connection: \"Request[Any, Any]\",\n    ) -> Any:\n        connection.scope[\"_form\"] = form_values = (\n            connection.scope[\"_form\"]\n            if \"_form\" in connection.scope\n            else parse_multipart_form(\n                body=await connection.body(), boundary=connection.content_type[-1].get(\"boundary\", \"\").encode()\n            )\n        )\n        if signature_field.is_non_string_sequence:\n            return list(form_values.values())\n        if signature_field.is_simple_type and signature_field.field_type is UploadFile and form_values:\n            return [v for v in form_values.values() if isinstance(v, UploadFile)][0]\n        return form_values if form_values or not is_data_optional else None\n    return cast(\"Callable[[ASGIConnection[Any, Any, Any]], Coroutine[Any, Any, Any]]\", extract_multipart)\ndef create_url_encoded_data_extractor(\n    is_data_optional: bool,\n) -> Callable[[\"ASGIConnection[Any, Any, Any]\"], Coroutine[Any, Any, Any]]:\n    \"\"\"Create extractor for url encoded form-data.\n    Args:\n        is_data_optional: Boolean dictating whether the field is optional.\n    Returns:\n        An extractor function.\n    \"\"\"\n    async def extract_url_encoded_extractor(\n        connection: \"Request[Any, Any]\",\n    ) -> Any:\n        connection.scope[\"_form\"] = form_values = (\n            connection.scope[\"_form\"]\n            if \"_form\" in connection.scope\n            else parse_url_encoded_form_data(await connection.body())\n        )\n        return form_values if form_values or not is_data_optional else None\n    return cast(\"Callable[[ASGIConnection[Any, Any, Any]], Coroutine[Any, Any, Any]]\", extract_url_encoded_extractor)\n    async def form(self) -> FormMultiDict:\n        \"\"\"Retrieve form data from the request. If the request is either a 'multipart/form-data' or an\n        'application/x-www-form- urlencoded', return a FormMultiDict instance populated with the values sent in the\n        request, otherwise, an empty instance.\n        Returns:\n            A FormMultiDict instance\n        \"\"\"\n        if self._form is Empty:\n            content_type, options = self.content_type\n            if content_type == RequestEncodingType.MULTI_PART:\n                self._form = self.scope[\"_form\"] = form_values = parse_multipart_form(\n                    body=await self.body(), boundary=options.get(\"boundary\", \"\").encode()\n                )\n                return FormMultiDict(form_values)\n            if content_type == RequestEncodingType.URL_ENCODED:\n                self._form = self.scope[\"_form\"] = form_values = parse_url_encoded_form_data(\n                    await self.body(),\n                )\n                return FormMultiDict(form_values)\n            return FormMultiDict()\n        return FormMultiDict(self._form)\n    async def send_push_promise(self, path: str) -> None:\n        \"\"\"Send a push promise.\n        This method requires the `http.response.push` extension to be sent from the ASGI server.\n        Args:\n            path: Path to send the promise to.\n        Returns:\n            None\n        \"\"\"\n        extensions: Dict[str, Dict[Any, Any]] = self.scope.get(\"extensions\") or {}\n        if \"http.response.push\" in extensions:\n            raw_headers = []\n            for name in SERVER_PUSH_HEADERS:\n                for value in self.headers.getall(name, []):\n                    raw_headers.append((name.encode(\"latin-1\"), value.encode(\"latin-1\")))\n            await self.send({\"type\": \"http.response.push\", \"path\": path, \"headers\": raw_headers})",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-25578",
        "description": "[{'lang': 'en', 'value': 'Starlite is an Asynchronous Server Gateway Interface (ASGI) framework. Prior to version 1.5.2, the request body parsing in `starlite` allows a potentially unauthenticated attacker to consume a large amount of CPU time and RAM. The multipart body parser processes an unlimited number of file parts and an unlimited number of field parts. This is a remote, potentially unauthenticated Denial of Service vulnerability. This vulnerability affects applications with a request handler that accepts a `Body(media_type=RequestEncodingType.MULTI_PART)`. The large amount of CPU time required for processing requests can block all available worker processes and significantly delay or slow down the processing of legitimate user requests. The large amount of RAM accumulated while processing requests can lead to Out-Of-Memory kills. Complete DoS is achievable by sending many concurrent multipart requests in a loop. Version 1.51.2 contains a patch for this issue.\\n'}]",
        "cwe_number": 770
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-103",
      "code": "    def to_dot_graph(\n        self,\n        priorities=True,\n        addnodes=[],\n        savefn=None,\n        savelayout=\"plain\",\n        saveformat=None,\n        color_edges=True,\n    ):\n        \"\"\"\n        Converts this transform graph to the graphviz_ DOT format.\n        Optionally saves it (requires `graphviz`_ be installed and on your path).\n        .. _graphviz: http://www.graphviz.org/\n        Parameters\n        ----------\n        priorities : bool\n            If `True`, show the priority values for each transform.  Otherwise,\n            the will not be included in the graph.\n        addnodes : sequence of str\n            Additional coordinate systems to add (this can include systems\n            already in the transform graph, but they will only appear once).\n        savefn : None or str\n            The file name to save this graph to or `None` to not save\n            to a file.\n        savelayout : str\n            The graphviz program to use to layout the graph (see\n            graphviz_ for details) or 'plain' to just save the DOT graph\n            content. Ignored if ``savefn`` is `None`.\n        saveformat : str\n            The graphviz output format. (e.g. the ``-Txxx`` option for\n            the command line program - see graphviz docs for details).\n            Ignored if ``savefn`` is `None`.\n        color_edges : bool\n            Color the edges between two nodes (frames) based on the type of\n            transform. ``FunctionTransform``: red, ``StaticMatrixTransform``:\n            blue, ``DynamicMatrixTransform``: green.\n        Returns\n        -------\n        dotgraph : str\n            A string with the DOT format graph.\n        \"\"\"\n        nodes = []\n        for a in self._graph:\n            if a not in nodes:\n                nodes.append(a)\n            for b in self._graph[a]:\n                if b not in nodes:\n                    nodes.append(b)\n        for node in addnodes:\n            if node not in nodes:\n                nodes.append(node)\n        nodenames = []\n        invclsaliases = {\n            f: [k for k, v in self._cached_names.items() if v == f]\n            for f in self.frame_set\n        }\n        for n in nodes:\n            if n in invclsaliases:\n                aliases = \"`\\\\n`\".join(invclsaliases[n])\n                nodenames.append(\n                    '{0} [shape=oval label=\"{0}\\\\n`{1}`\"]'.format(n.__name__, aliases)\n                )\n            else:\n                nodenames.append(n.__name__ + \"[ shape=oval ]\")\n        edgenames = []\n        for a in self._graph:\n            agraph = self._graph[a]\n            for b in agraph:\n                transform = agraph[b]\n                pri = transform.priority if hasattr(transform, \"priority\") else 1\n                color = trans_to_color[transform.__class__] if color_edges else \"black\"\n                edgenames.append((a.__name__, b.__name__, pri, color))\n        lines = [\"digraph AstropyCoordinateTransformGraph {\"]\n        lines.append(\"graph [rankdir=LR]\")\n        lines.append(\"; \".join(nodenames) + \";\")\n        for enm1, enm2, weights, color in edgenames:\n            labelstr_fmt = \"[ {0} {1} ]\"\n            if priorities:\n                priority_part = f'label = \"{weights}\"'\n            else:\n                priority_part = \"\"\n            color_part = f'color = \"{color}\"'\n            labelstr = labelstr_fmt.format(priority_part, color_part)\n            lines.append(f\"{enm1} -> {enm2}{labelstr};\")\n        lines.append(\"\")\n        lines.append(\"overlap=false\")\n        lines.append(\"}\")\n        dotgraph = \"\\n\".join(lines)\n        if savefn is not None:\n            if savelayout == \"plain\":\n                with open(savefn, \"w\") as f:\n                    f.write(dotgraph)\n            else:\n                args = [savelayout]\n                if saveformat is not None:\n                    args.append(\"-T\" + saveformat)\n                proc = subprocess.Popen(\n                    args,\n                    stdin=subprocess.PIPE,\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.PIPE,\n                )\n                stdout, stderr = proc.communicate(dotgraph)\n                if proc.returncode != 0:\n                    raise OSError(\"problem running graphviz: \\n\" + stderr)\n                with open(savefn, \"w\") as f:\n                    f.write(stdout)\n        return dotgraph",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-41334",
        "description": "[{'lang': 'en', 'value': 'Astropy is a project for astronomy in Python that fosters interoperability between Python astronomy packages. Version 5.3.2 of the Astropy core package is vulnerable to remote code execution due to improper input validation in the `TranformGraph().to_dot_graph` function. A malicious user can provide a command or a script file as a value to the `savelayout` argument, which will be placed as the first value in a list of arguments passed to `subprocess.Popen`.  Although an error will be raised, the command or script will be executed successfully. Version 5.3.3 fixes this issue.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-104",
      "code": "def escape(s):\n    s = s.replace(\"&\", \"&amp;\")\n    s = s.replace(\"<\", \"&lt;\")\n    s = s.replace(\">\", \"&gt;\")\n    return s\ndef handleInputParameterError(f):\n    def new_f(*args, **kwargs):\n        try:\n            return f(*args, **kwargs)\n        except InputParameterError as e:\n            msgStr = str(e)\n            log.warning('%s', msgStr)\n            return HttpResponseBadRequest(escape(msgStr))\n    return new_f",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-4728",
        "description": "[{'lang': 'en', 'value': 'A vulnerability has been found in Graphite Web and classified as problematic. This vulnerability affects unknown code of the component Cookie Handler. The manipulation leads to cross site scripting. The attack can be initiated remotely. The exploit has been disclosed to the public and may be used. The name of the patch is 2f178f490e10efc03cd1d27c72f64ecab224eb23. It is recommended to apply a patch to fix this issue. VDB-216742 is the identifier assigned to this vulnerability.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-105",
      "code": "    def _create_database(self, last_upgrade_to_run):\n        \"\"\"\n        Make sure that the database is created and sets the file permissions.\n        This should be done before storing any sensitive data in it.\n        \"\"\"\n        conn = self._connect()\n        try:\n            with conn:\n                self._create_tables(conn, last_upgrade_to_run)\n        finally:\n            conn.close()\n        os.chmod(self.filename, stat.S_IRUSR | stat.S_IWUSR)\n    def _create_tables(self, conn, last_upgrade_to_run):\n        conn.execute(\n            \"\"\"\n            CREATE TABLE IF NOT EXISTS\n            update_done (\n                update_number INT NOT NULL\n            );\n        \"\"\"\n        )\n        conn.execute(\n            \"\"\"\n           CREATE TABLE IF NOT EXISTS\n           account (\n               account_id TEXT NOT NULL,\n               application_key TEXT NOT NULL,\n               account_auth_token TEXT NOT NULL,\n               api_url TEXT NOT NULL,\n               download_url TEXT NOT NULL,\n               minimum_part_size INT NOT NULL,\n               realm TEXT NOT NULL\n           );\n        \"\"\"\n        )\n        conn.execute(\n            \"\"\"\n           CREATE TABLE IF NOT EXISTS\n           bucket (\n               bucket_name TEXT NOT NULL,\n               bucket_id TEXT NOT NULL\n           );\n        \"\"\"\n        )\n        conn.execute(\n            \"\"\"\n           CREATE TABLE IF NOT EXISTS\n           bucket_upload_url (\n               bucket_id TEXT NOT NULL,\n               upload_url TEXT NOT NULL,\n               upload_auth_token TEXT NOT NULL\n           );\n        \"\"\"\n        )\n        last_upgrade_to_run = 4 if last_upgrade_to_run is None else last_upgrade_to_run\n        if 1 <= last_upgrade_to_run:\n            self._ensure_update(1, ['ALTER TABLE account ADD COLUMN allowed TEXT;'])\n        if 2 <= last_upgrade_to_run:\n            self._ensure_update(\n                2, ['ALTER TABLE account ADD COLUMN account_id_or_app_key_id TEXT;']\n            )\n        if 3 <= last_upgrade_to_run:\n            self._ensure_update(3, ['ALTER TABLE account ADD COLUMN s3_api_url TEXT;'])\n        if 4 <= last_upgrade_to_run:\n            self._ensure_update(\n                4, [\n                    \"\"\"\n                    CREATE TABLE\n                    tmp_account (\n                        account_id TEXT NOT NULL,\n                        application_key TEXT NOT NULL,\n                        account_auth_token TEXT NOT NULL,\n                        api_url TEXT NOT NULL,\n                        download_url TEXT NOT NULL,\n                        absolute_minimum_part_size INT NOT NULL DEFAULT {},\n                        recommended_part_size INT NOT NULL,\n                        realm TEXT NOT NULL,\n                        allowed TEXT,\n                        account_id_or_app_key_id TEXT,\n                        s3_api_url TEXT\n                    );\n                    \"\"\".format(DEFAULT_ABSOLUTE_MINIMUM_PART_SIZE),\n                    \"\"\"INSERT INTO tmp_account(\n                        account_id,\n                        application_key,\n                        account_auth_token,\n                        api_url,\n                        download_url,\n                        recommended_part_size,\n                        realm,\n                        allowed,\n                        account_id_or_app_key_id,\n                        s3_api_url\n                    )\n                    SELECT\n                        account_id,\n                        application_key,\n                        account_auth_token,\n                        api_url,\n                        download_url,\n                        minimum_part_size,\n                        realm,\n                        allowed,\n                        account_id_or_app_key_id,\n                        s3_api_url\n                    FROM account;\n                    \"\"\",\n                    'DROP TABLE account;',\n                    \"\"\"\n                    CREATE TABLE account (\n                        account_id TEXT NOT NULL,\n                        application_key TEXT NOT NULL,\n                        account_auth_token TEXT NOT NULL,\n                        api_url TEXT NOT NULL,\n                        download_url TEXT NOT NULL,\n                        absolute_minimum_part_size INT NOT NULL,\n                        recommended_part_size INT NOT NULL,\n                        realm TEXT NOT NULL,\n                        allowed TEXT,\n                        account_id_or_app_key_id TEXT,\n                        s3_api_url TEXT\n                    );\n                    \"\"\",\n                    \"\"\"INSERT INTO account(\n                                    account_id,\n                                    application_key,\n                                    account_auth_token,\n                                    api_url,\n                                    download_url,\n                                    absolute_minimum_part_size,\n                                    recommended_part_size,\n                                    realm,\n                                    allowed,\n                                    account_id_or_app_key_id,\n                                    s3_api_url\n                                )\n                                SELECT\n                                    account_id,\n                                    application_key,\n                                    account_auth_token,\n                                    api_url,\n                                    download_url,\n                                    absolute_minimum_part_size,\n                                    recommended_part_size,\n                                    realm,\n                                    allowed,\n                                    account_id_or_app_key_id,\n                                    s3_api_url\n                                FROM tmp_account;\n                                \"\"\",\n                    'DROP TABLE tmp_account;',\n                ]\n            )",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-23651",
        "description": "[{'lang': 'en', 'value': 'b2-sdk-python is a python library to access cloud storage provided by backblaze. Linux and Mac releases of the SDK version 1.14.0 and below contain a key disclosure vulnerability that, in certain conditions, can be exploited by local attackers through a time-of-check-time-of-use (TOCTOU) race condition. SDK users of the SqliteAccountInfo format are vulnerable while users of the InMemoryAccountInfo format are safe. The SqliteAccountInfo saves API keys (and bucket name-to-id mapping) in a local database file ($XDG_CONFIG_HOME/b2/account_info, ~/.b2_account_info or a user-defined path). When first created, the file is world readable and is (typically a few milliseconds) later altered to be private to the user. If the directory containing the file is readable by a local attacker then during the brief period between file creation and permission modification, a local attacker can race to open the file and maintain a handle to it. This allows the local attacker to read the contents after the file after the sensitive information has been saved to it. Consumers of this SDK who rely on it to save data using SqliteAccountInfo class should upgrade to the latest version of the SDK. Those who believe a local user might have opened a handle using this race condition, should remove the affected database files and regenerate all application keys. Users should upgrade to b2-sdk-python 1.14.1 or later.'}]",
        "cwe_number": 367
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-106",
      "code": "def loginForm(request, next=''):\n    if request.method == 'POST':\n        form = Login_Form(request.POST)\n        if form.is_valid():\n            username = request.POST.get('username')\n            password = request.POST.get('password')\n            user = user = authenticate(username=username, password=password)\n            if user and user.is_active:\n                login(request, user)\n                next_url = request.POST.get('next', '')\n                if next_url and next_url.startswith('/'):\n                    return HttpResponseRedirect(next_url)\n                else:\n                    return HttpResponseRedirect(reverse('CalendarinhoApp:Dashboard'))\n            else:\n                messages.error(request, \"Invalid login details given\")\n                form = Login_Form()\n                return render(request, 'CalendarinhoApp/login.html', {'form': form})\n    else:\n        form = Login_Form()\n        return render(request, 'CalendarinhoApp/login.html', {'form': form})",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-49281",
        "description": "[{'lang': 'en', 'value': 'Calendarinho is an open source calendaring application to manage large teams of consultants. An Open Redirect issue occurs when a web application redirects users to external URLs without proper validation. This can lead to phishing attacks, where users are tricked into visiting malicious sites, potentially leading to information theft and reputational damage to the website used for redirection. The problem is has been patched in commit `15b2393`. Users are advised to update to a commit after `15b2393`. There are no known workarounds for this vulnerability. '}]",
        "cwe_number": 601
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-107",
      "code": "    def cookies(self):\n        cookies = flask.Request.cookies.__get__(self)\n        result = {}\n        desuffixed = {}\n        for key, value in cookies.items():\n            if key.endswith(self.cookie_suffix):\n                desuffixed[key[: -len(self.cookie_suffix)]] = value\n            else:\n                result[key] = value\n        result.update(desuffixed)\n        return result\n    def set_cookie(self, key, *args, **kwargs):\n        kwargs[\"path\"] = flask.request.script_root + kwargs.get(\"path\", \"/\")\n        samesite = settings().get([\"server\", \"cookies\", \"samesite\"])\n        if samesite is not None:\n            samesite = samesite.lower()\n        if samesite == \"none\":\n            samesite = \"None\"\n        if samesite not in (\"None\", \"strict\", \"lax\"):\n            samesite = None\n        kwargs[\"samesite\"] = samesite\n        kwargs[\"secure\"] = settings().getBoolean([\"server\", \"cookies\", \"secure\"])\n        flask.Response.set_cookie(\n            self, key + flask.request.cookie_suffix, *args, **kwargs\n        )\ndef load_user(id):\n    if id is None:\n        return None\n    if id == \"_api\":\n        return userManager.api_user_factory()\n    if session and \"usersession.id\" in session:\n        sessionid = session[\"usersession.id\"]\n    else:\n        sessionid = None\n    if sessionid:\n        user = userManager.find_user(userid=id, session=sessionid)\n    else:\n        user = userManager.find_user(userid=id)\n    if user and user.is_active:\n        return user\n    return None\n    def _cleanup_sessions(self):\n        for session, user in list(self._session_users_by_session.items()):\n            if not isinstance(user, SessionUser):\n                continue\n            if user.created + (24 * 60 * 60) < time.monotonic():\n                self._logger.info(\n                    \"Cleaning up user session {} for user {}\".format(\n                        session, user.get_id()\n                    )\n                )\n                self.logout_user(user, stale=True)\n    def find_user(self, userid=None, session=None):\n        if session is not None and session in self._session_users_by_session:\n            user = self._session_users_by_session[session]\n            if userid is None or userid == user.get_id():\n                return user\n        return None\n    def find_sessions_for(self, matcher):\n        result = []\n        for user in self.get_all_users():\n            if matcher(user):\n                try:\n                    session_ids = self._sessionids_by_userid[user.get_id()]\n                    for session_id in session_ids:\n                        try:\n                            result.append(self._session_users_by_session[session_id])\n                        except KeyError:\n                            continue\n                except KeyError:\n                    pass\n        return result\n    def find_user(self, userid=None, apikey=None, session=None):\n        user = UserManager.find_user(self, userid=userid, session=session)\n        if user is not None:\n            return user\n        if userid is not None:\n            if userid not in self._users:\n                return None\n            return self._users[userid]\n        elif apikey is not None:\n            for user in self._users.values():\n                if apikey == user._apikey:\n                    return user\n            return None\n        else:\n            return None\n    def __init__(self, user):\n        wrapt.ObjectProxy.__init__(self, user)\n        self._self_session = \"\".join(\"%02X\" % z for z in bytes(uuid.uuid4().bytes))\n        self._self_created = time.monotonic()\n        self._self_touched = time.monotonic()",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-2888",
        "description": "[{'lang': 'en', 'value': \"If an attacker comes into the possession of a victim's OctoPrint session cookie through whatever means, the attacker can use this cookie to authenticate as long as the victim's account exists.\"}]",
        "cwe_number": 613
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-108",
      "code": "    def runTest(self):\n        for mode in (self.module.MODE_ECB, self.module.MODE_CBC, self.module.MODE_CFB, self.module.MODE_OFB, self.module.MODE_OPENPGP):\n            encryption_cipher = self.module.new(a2b_hex(self.key), mode, self.iv)\n            ciphertext = encryption_cipher.encrypt(self.plaintext)\n            if mode != self.module.MODE_OPENPGP:\n                decryption_cipher = self.module.new(a2b_hex(self.key), mode, self.iv)\n            else:\n                eiv = ciphertext[:self.module.block_size+2]\n                ciphertext = ciphertext[self.module.block_size+2:]\n                decryption_cipher = self.module.new(a2b_hex(self.key), mode, eiv)\n            decrypted_plaintext = decryption_cipher.decrypt(ciphertext)\n            self.assertEqual(self.plaintext, decrypted_plaintext)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2013-7459",
        "description": "[{'lang': 'en', 'value': 'Heap-based buffer overflow in the ALGnew function in block_templace.c in Python Cryptography Toolkit (aka pycrypto) allows remote attackers to execute arbitrary code as demonstrated by a crafted iv parameter to cryptmsg.py.'}]",
        "cwe_number": 119
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-109",
      "code": "    def get_csv_incident_list(self) -> str:\n        csvOutput = io.StringIO()\n        writer = csv.writer(csvOutput)\n        if len(self.triggered_details.hits) > 0:\n            hit_class_dict = dict(self.triggered_details.hits[0])\n            headers = [\n                (i)\n                for i in hit_class_dict.keys()\n                if i not in [\"token_type\", \"time_of_hit\"]\n            ]\n            writer.writerow([\"Timestamp\"] + headers)\n            for hit in self.triggered_details.hits:\n                timestamp = hit.time_of_hit\n                hit_id = datetime.fromtimestamp(timestamp).strftime(\n                    \"%Y-%m-%d %H:%M:%S.%f\"\n                )\n                hit_dict = dict(hit)\n                data = [hit_id]\n                for key in headers:\n                    data.append(hit_dict.get(key, \"N/A\"))\n                writer.writerow(data)\n        else:\n            writer.writerow(\"the token has not been triggered\")\n        return csvOutput.getvalue()\n    def _do_ns_response(self, name=None):\n        \"\"\"\n        Calculate the response to a query.\n        \"\"\"\n        answer = dns.RRHeader(\n            name=name,\n            payload=dns.Record_NS(\n                ttl=300,\n                name=\".\".join([\"ns1\", name.decode()]),\n            ),\n            type=dns.NS,\n            auth=True,\n            ttl=300\n        )\n        additional = dns.RRHeader(\n            name=\".\".join([\"ns1\", name.decode()]),\n            payload=dns.Record_A(ttl=10, address=self.frontend_settings.PUBLIC_IP),\n            type=dns.A,\n            auth=True,\n            ttl=300,\n        )\n        answers = [answer]\n        authority: list[str] = []\n        additional = [additional]\n        return answers, authority, additional",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-28111",
        "description": "[{'lang': 'en', 'value': \"Canarytokens helps track activity and actions on a network. Canarytokens.org supports exporting the history of a Canarytoken's incidents in CSV format. The generation of these CSV files is vulnerable to a CSV Injection vulnerability. This flaw can be used by an attacker who discovers an HTTP-based Canarytoken to target the Canarytoken's owner, if the owner exports the incident history to CSV and opens in a reader application such as Microsoft Excel. The impact is that this issue could lead to code execution on the machine on which the CSV file is opened. Version sha-c595a1f8 contains a fix for this issue.\"}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-110",
      "code": "  def testQIntArgAndRet(self):\n    @function.Defun(dtypes.qint8)\n    def func(x):\n      return x\n    with self.cached_session(force_gpu=test.is_gpu_available()) as sess:\n      qint = constant_op.constant(np.array([42]), dtypes.qint8)\n      result = func(qint)\n      self.evaluate(result)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2020-15190",
        "description": "[{'lang': 'en', 'value': 'In Tensorflow before versions 1.15.4, 2.0.3, 2.1.2, 2.2.1 and 2.3.1, the `tf.raw_ops.Switch` operation takes as input a tensor and a boolean and outputs two tensors. Depending on the boolean value, one of the tensors is exactly the input tensor whereas the other one should be an empty tensor. However, the eager runtime traverses all tensors in the output. Since only one of the tensors is defined, the other one is `nullptr`, hence we are binding a reference to `nullptr`. This is undefined behavior and reported as an error if compiling with `-fsanitize=null`. In this case, this results in a segmentation fault The issue is patched in commit da8558533d925694483d2c136a9220d6d49d843c, and is released in TensorFlow versions 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1.'}]",
        "cwe_number": 20
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-111",
      "code": "    def redirect_request(self, req, fp, code, msg, headers, newurl):\n        if code not in (301, 302, 303, 307, 308):\n            raise urllib.error.HTTPError(req.full_url, code, msg, headers, fp)\n        new_method = req.get_method()\n        new_data = req.data\n        remove_headers = []\n        if code == 303 and req.get_method() != 'HEAD':\n            new_method = 'GET'\n        elif code in (301, 302) and req.get_method() == 'POST':\n            new_method = 'GET'\n        if new_method != req.get_method():\n            new_data = None\n            remove_headers.extend(['Content-Length', 'Content-Type'])\n        new_headers = {k: v for k, v in req.headers.items() if k.lower() not in remove_headers}\n        return urllib.request.Request(\n            newurl, headers=new_headers, origin_req_host=req.origin_req_host,\n            unverifiable=True, method=new_method, data=new_data)\ndef extract_timezone(date_str):\n    m = re.search(\n        r'''(?x)\n            ^.{8,}?\n            (?P<tz>Z|\n                (?:(?<=.\\b\\d{4}|\\b\\d{2}:\\d\\d)|\n                   (?<!.\\b[a-zA-Z]{3}|[a-zA-Z]{4}|..\\b\\d\\d))\n                   [ ]?\n                (?P<sign>\\+|-)\n                (?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})\n            $)\n        ''', date_str)\n    if not m:\n        m = re.search(r'\\d{1,2}:\\d{1,2}(?:\\.\\d+)?(?P<tz>\\s*[A-Z]+)$', date_str)\n        timezone = TIMEZONE_NAMES.get(m and m.group('tz').strip())\n        if timezone is not None:\n            date_str = date_str[:-len(m.group('tz'))]\n        timezone = datetime.timedelta(hours=timezone or 0)\n    else:\n        date_str = date_str[:-len(m.group('tz'))]\n        if not m.group('sign'):\n            timezone = datetime.timedelta()\n        else:\n            sign = 1 if m.group('sign') == '+' else -1\n            timezone = datetime.timedelta(\n                hours=sign * int(m.group('hours')),\n                minutes=sign * int(m.group('minutes')))\n    return timezone, date_str",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-35934",
        "description": "[{'lang': 'en', 'value': \"yt-dlp is a command-line program to download videos from video sites. During file downloads, yt-dlp or the external downloaders that yt-dlp employs may leak cookies on HTTP redirects to a different host, or leak them when the host for download fragments differs from their parent manifest's host. This vulnerable behavior is present in yt-dlp prior to 2023.07.06 and nightly 2023.07.06.185519. All native and external downloaders are affected, except for `curl` and `httpie` (version 3.1.0 or later).\\n\\nAt the file download stage, all cookies are passed by yt-dlp to the file downloader as a `Cookie` header, thereby losing their scope. This also occurs in yt-dlp's info JSON output, which may be used by external tools. As a result, the downloader or external tool may indiscriminately send cookies with requests to domains or paths for which the cookies are not scoped.\\n\\nyt-dlp version 2023.07.06 and nightly 2023.07.06.185519 fix this issue by removing the `Cookie` header upon HTTP redirects; having native downloaders calculate the `Cookie` header from the cookiejar, utilizing external downloaders' built-in support for cookies instead of passing them as header arguments, disabling HTTP redirectiong if the external downloader does not have proper cookie support, processing cookies passed as HTTP headers to limit their scope, and having a separate field for cookies in the info dict storing more information about scoping\\n\\nSome workarounds are available for those who are unable to upgrade. Avoid using cookies and user authentication methods. While extractors may set custom cookies, these usually do not contain sensitive information. Alternatively, avoid using `--load-info-json`. Or, if authentication is a must: verify the integrity of download links from unknown sources in browser (including redirects) before passing them to yt-dlp; use `curl` as external downloader, since it is not impacted; and/or avoid fragmented formats such as HLS/m3u8, DASH/mpd and ISM.\"}]",
        "cwe_number": 200
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-112",
      "code": "    def set_user_password(self, context, user_id, user):\n        return self.update_user(context, user_id, user)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2012-3426",
        "description": "[{'lang': 'en', 'value': 'OpenStack Keystone before 2012.1.1, as used in OpenStack Folsom before Folsom-1 and OpenStack Essex, does not properly implement token expiration, which allows remote authenticated users to bypass intended authorization restrictions by (1) creating new tokens through token chaining, (2) leveraging possession of a token for a disabled user account, or (3) leveraging possession of a token for an account with a changed password.'}]",
        "cwe_number": 264
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-113",
      "code": "def parse_soap_enveloped_saml(text, body_class, header_class=None):\n    \"\"\"Parses a SOAP enveloped SAML thing and returns header parts and body\n    :param text: The SOAP object as XML\n    :return: header parts and body as saml.samlbase instances\n    \"\"\"\n    envelope = ElementTree.fromstring(text)\n    assert envelope.tag == '{%s}Envelope' % NAMESPACE\n    body = None\n    header = {}\n    for part in envelope:\n        if part.tag == '{%s}Body' % NAMESPACE:\n            for sub in part:\n                try:\n                    body = saml2.create_class_from_element_tree(body_class, sub)\n                except Exception:\n                    raise Exception(\n                        \"Wrong body type (%s) in SOAP envelope\" % sub.tag)\n        elif part.tag == '{%s}Header' % NAMESPACE:\n            if not header_class:\n                raise Exception(\"Header where I didn't expect one\")\n            for sub in part:\n                for klass in header_class:\n                    if sub.tag == \"{%s}%s\" % (klass.c_namespace, klass.c_tag):\n                        header[sub.tag] = \\\n                            saml2.create_class_from_element_tree(klass, sub)\n                        break\n    return body, header\ndef parse_soap_enveloped_saml_thingy(text, expected_tags):\n    \"\"\"Parses a SOAP enveloped SAML thing and returns the thing as\n    a string.\n    :param text: The SOAP object as XML string\n    :param expected_tags: What the tag of the SAML thingy is expected to be.\n    :return: SAML thingy as a string\n    \"\"\"\n    envelope = ElementTree.fromstring(text)\n    assert envelope.tag == '{%s}Envelope' % soapenv.NAMESPACE\n    assert len(envelope) >= 1\n    body = None\n    for part in envelope:\n        if part.tag == '{%s}Body' % soapenv.NAMESPACE:\n            assert len(part) == 1\n            body = part\n            break\n    if body is None:\n        return \"\"\n    saml_part = body[0]\n    if saml_part.tag in expected_tags:\n        return ElementTree.tostring(saml_part, encoding=\"UTF-8\")\n    else:\n        raise WrongMessageType(\"Was '%s' expected one of %s\" % (saml_part.tag,\n                                                                expected_tags))\ndef class_instances_from_soap_enveloped_saml_thingies(text, modules):\n    \"\"\"Parses a SOAP enveloped header and body SAML thing and returns the\n    thing as a dictionary class instance.\n    :param text: The SOAP object as XML\n    :param modules: modules representing xsd schemas\n    :return: The body and headers as class instances\n    \"\"\"\n    try:\n        envelope = ElementTree.fromstring(text)\n    except Exception as exc:\n        raise XmlParseError(\"%s\" % exc)\n    assert envelope.tag == '{%s}Envelope' % soapenv.NAMESPACE\n    assert len(envelope) >= 1\n    env = {\"header\": [], \"body\": None}\n    for part in envelope:\n        if part.tag == '{%s}Body' % soapenv.NAMESPACE:\n            assert len(part) == 1\n            env[\"body\"] = instanciate_class(part[0], modules)\n        elif part.tag == \"{%s}Header\" % soapenv.NAMESPACE:\n            for item in part:\n                env[\"header\"].append(instanciate_class(item, modules))\n    return env\ndef open_soap_envelope(text):\n    \"\"\"\n    :param text: SOAP message\n    :return: dictionary with two keys \"body\"/\"header\"\n    \"\"\"\n    try:\n        envelope = ElementTree.fromstring(text)\n    except Exception as exc:\n        raise XmlParseError(\"%s\" % exc)\n    assert envelope.tag == '{%s}Envelope' % soapenv.NAMESPACE\n    assert len(envelope) >= 1\n    content = {\"header\": [], \"body\": None}\n    for part in envelope:\n        if part.tag == '{%s}Body' % soapenv.NAMESPACE:\n            assert len(part) == 1\n            content[\"body\"] = ElementTree.tostring(part[0], encoding=\"UTF-8\")\n        elif part.tag == \"{%s}Header\" % soapenv.NAMESPACE:\n            for item in part:\n                _str = ElementTree.tostring(item, encoding=\"UTF-8\")\n                content[\"header\"].append(_str)\n    return content\ndef create_class_from_xml_string(target_class, xml_string):\n    \"\"\"Creates an instance of the target class from a string.\n    :param target_class: The class which will be instantiated and populated\n        with the contents of the XML. This class must have a c_tag and a\n        c_namespace class variable.\n    :param xml_string: A string which contains valid XML. The root element\n        of the XML string should match the tag and namespace of the desired\n        class.\n    :return: An instance of the target class with members assigned according to\n        the contents of the XML - or None if the root XML tag and namespace did\n        not match those of the target class.\n    \"\"\"\n    if not isinstance(xml_string, six.binary_type):\n        xml_string = xml_string.encode('utf-8')\n    tree = ElementTree.fromstring(xml_string)\n    return create_class_from_element_tree(target_class, tree)\ndef extension_element_from_string(xml_string):\n    element_tree = ElementTree.fromstring(xml_string)\n    return _extension_element_from_element_tree(element_tree)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2016-10149",
        "description": "[{'lang': 'en', 'value': 'XML External Entity (XXE) vulnerability in PySAML2 4.4.0 and earlier allows remote attackers to read arbitrary files via a crafted SAML XML request or response.'}]",
        "cwe_number": 611
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-114",
      "code": "    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.password.validators += [\n            validators.length(\n                min=self.app.cfg.password_min_length,\n                max=self.app.cfg.password_max_length,\n                message=_('Password must have between %(min)d and %(max)d characters.'),\n            )\n        ]\n    def users(self, username=None, criteria=u\"\", search=u\"\", action=u\"\", **kwargs):\n        if action == \"add\":\n            form = UserForm()\n            if form.validate_on_submit():\n                try:\n                    user = self.app.store.add_user(username)\n                    form.populate_obj(user)\n                    flash(_(\"User added successfully.\"))\n                except Exception as e:\n                    flash(str(e), level='error')\n            else:\n                flash(form.error_message, level='error')\n        elif action == \"edit\":\n            user = self.app.store.get_user(username)\n            if user:\n                form = EditUserForm(obj=user)\n                if form.validate_on_submit():\n                    try:\n                        form.populate_obj(user)\n                        flash(_(\"User information modified successfully.\"))\n                    except Exception as e:\n                        flash(str(e), level='error')\n                else:\n                    flash(form.error_message, level='error')\n            else:\n                flash(_(\"Cannot edit user `%s`: user doesn't exists\") % username, level='error')\n        elif action == 'delete':\n            self._delete_user(action, DeleteUserForm())\n        params = {\n            \"add_form\": UserForm(formdata=None),\n            \"edit_form\": EditUserForm(formdata=None),\n            \"criteria\": criteria,\n            \"search\": search,\n            \"users\": list(self.app.store.users(search=search, criteria=criteria)),\n        }\n        return self._compile_template(\"admin_users.html\", **params)\nclass DeletePage(Controller):\n    def default(self, path=b\"\", **kwargs):\n        repo, path = self.app.store.get_repo_path(path)\n        path_obj = repo.fstat(path)\n        is_maintainer()\n        form = DeleteRepoForm()\n        if not form.validate():\n            raise cherrypy.HTTPError(400, form.error_message)\n        if form.confirm.data != path_obj.display_name:\n            _logger.info(\"do not delete repo, bad confirmation %r != %r\", form.confirm.data, path_obj.display_name)\n            raise cherrypy.HTTPError(400, 'bad confirmation')\n        scheduled = cherrypy.engine.publish('schedule_task', repo.delete, path)\n        assert scheduled\n        raise cherrypy.HTTPRedirect(form.redirect.data)\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.new.validators += [\n            Length(\n                min=self.app.cfg.password_min_length,\n                max=self.app.cfg.password_max_length,\n                message=_('Password must have between %(min)d and %(max)d characters.'),\n            )\n        ]",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-3232",
        "description": "[{'lang': 'en', 'value': 'Cross-Site Request Forgery (CSRF) in GitHub repository ikus060/rdiffweb prior to 2.4.5.'}]",
        "cwe_number": 352
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-115",
      "code": "    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-34359",
        "description": "[{'lang': 'en', 'value': \"llama-cpp-python is the Python bindings for llama.cpp. `llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to remote code execution by a carefully constructed payload.\"}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-116",
      "code": "    def validate_code(cls, code: str, code_validations: PALValidation) -> None:\n        try:\n            code_tree = ast.parse(code)\n        except (SyntaxError, UnicodeDecodeError):\n            raise ValueError(f\"Generated code is not valid python code: {code}\")\n        except TypeError:\n            raise ValueError(\n                f\"Generated code is expected to be a string, \"\n                f\"instead found {type(code)}\"\n            )\n        except OverflowError:\n            raise ValueError(\n                f\"Generated code too long / complex to be parsed by ast: {code}\"\n            )\n        found_solution_expr = False\n        if code_validations.solution_expression_name is None:\n            found_solution_expr = True\n        has_imports = False\n        top_level_nodes = list(ast.iter_child_nodes(code_tree))\n        for node in top_level_nodes:\n            if (\n                code_validations.solution_expression_name is not None\n                and code_validations.solution_expression_type is not None\n            ):\n                if (\n                    isinstance(node, code_validations.solution_expression_type)\n                    and hasattr(node, \"name\")\n                    and node.name == code_validations.solution_expression_name\n                ):\n                    found_solution_expr = True\n                if isinstance(node, ast.Assign):\n                    for target_node in node.targets:\n                        if (\n                            isinstance(\n                                target_node, code_validations.solution_expression_type\n                            )\n                            and hasattr(target_node, \"id\")\n                            and target_node.id\n                            == code_validations.solution_expression_name\n                        ):\n                            found_solution_expr = True\n            if isinstance(node, ast.Import) or isinstance(node, ast.ImportFrom):\n                has_imports = True\n        if not found_solution_expr:\n            raise ValueError(\n                f\"Generated code is missing the solution expression: \"\n                f\"{code_validations.solution_expression_name} of type: \"\n                f\"{code_validations.solution_expression_type}\"\n            )\n        if not code_validations.allow_imports and has_imports:\n            raise ValueError(f\"Generated code has disallowed imports: {code}\")\n        if (\n            not code_validations.allow_command_exec\n            or not code_validations.allow_imports\n        ):\n            for node in ast.walk(code_tree):\n                if (not code_validations.allow_command_exec) and isinstance(\n                    node, ast.Call\n                ):\n                    if (\n                        hasattr(node.func, \"id\")\n                        and node.func.id in COMMAND_EXECUTION_FUNCTIONS\n                    ):\n                        raise ValueError(\n                            f\"Found illegal command execution function \"\n                            f\"{node.func.id} in code {code}\"\n                        )\n                    if (\n                        isinstance(node.func, ast.Attribute)\n                        and node.func.attr in COMMAND_EXECUTION_FUNCTIONS\n                    ):\n                        raise ValueError(\n                            f\"Found illegal command execution function \"\n                            f\"{node.func.attr} in code {code}\"\n                        )\n                if (not code_validations.allow_imports) and (\n                    isinstance(node, ast.Import) or isinstance(node, ast.ImportFrom)\n                ):\n                    raise ValueError(f\"Generated code has disallowed imports: {code}\")\n    def from_math_prompt(cls, llm: BaseLanguageModel, **kwargs: Any) -> PALChain:\n        \"\"\"Load PAL from math prompt.\n        Args:\n            llm (BaseLanguageModel): The language model to use for generating code.\n        Returns:\n            PALChain: An instance of PALChain.\n        \"\"\"\n        llm_chain = LLMChain(llm=llm, prompt=MATH_PROMPT)\n        code_validations = PALValidation(\n            solution_expression_name=\"solution\",\n            solution_expression_type=PALValidation.SOLUTION_EXPRESSION_TYPE_FUNCTION,\n        )\n        return cls(\n            llm_chain=llm_chain,\n            stop=\"\\n\\n\",\n            get_answer_expr=\"print(solution())\",\n            code_validations=code_validations,\n            **kwargs,\n        )",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-27444",
        "description": "[{'lang': 'en', 'value': 'langchain_experimental (aka LangChain Experimental) in LangChain before 0.1.8 allows an attacker to bypass the CVE-2023-44467 fix and execute arbitrary code via the __import__, __subclasses__, __builtins__, __globals__, __getattribute__, __bases__, __mro__, or __base__ attribute in Python code. These are not prohibited by pal_chain/base.py.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-117",
      "code": "    def oauth_callback(self):\n        LOG.debug(\"Handling Oauth callback...\")\n        if request.args.get(\"error\"):\n            return f\"<h1>Error: {request.args.get('error')}</h1>\"\n        code = request.args.get(\"code\")\n        try:\n            access_token = self._fetch_access_token(code)\n            username, email, fullname = self._get_user_profile(access_token)\n            with DBSession() as session:\n                flask_login.login_user(\n                    AuthUser(\n                        self.login_user(username, email, fullname, session=session)\n                    )\n                )\n        except AuthenticationError as e:\n            LOG.error(\"Failed authenticate oauth user\", e)\n            abort_unauthorized()\n        next_url = QuerybookSettings.PUBLIC_URL\n        if \"next\" in flask_session:\n            next_url = flask_session[\"next\"]\n            del flask_session[\"next\"]\n        return redirect(next_url)\n    def oauth_callback(self):\n        LOG.debug(\"Handling Oauth callback...\")\n        if request.args.get(\"error\"):\n            return f\"<h1>Error: {request.args.get('error')}</h1>\"\n        code = request.args.get(\"code\")\n        try:\n            access_token = self._fetch_access_token(code)\n            username, email = self._get_user_profile(access_token)\n            with DBSession() as session:\n                flask_login.login_user(\n                    AuthUser(self.login_user(username, email, session=session))\n                )\n        except AuthenticationError as e:\n            LOG.error(\"Failed authenticate oauth user\", e)\n            abort_unauthorized()\n        next_url = QuerybookSettings.PUBLIC_URL\n        if \"next\" in flask_session:\n            next_url = flask_session[\"next\"]\n            del flask_session[\"next\"]\n        return redirect(next_url)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-46151",
        "description": "[{'lang': 'en', 'value': 'Querybook is an open source data querying UI. In affected versions user provided data is not escaped in the error field of the auth callback url in `querybook/server/app/auth/oauth_auth.py` and `querybook/server/app/auth/okta_auth.py`. This may allow attackers to perform reflected cross site scripting (XSS) if Content Security Policy (CSP) is not enabled or `unsafe-inline` is allowed. Users are advised to upgrade to the latest, patched version of querybook (version 3.14.2 or greater). Users unable to upgrade may enable CSP and not allow unsafe-inline or manually escape query parameters in a reverse proxy.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-118",
      "code": "    def format_details(self, replace=False):\n        env = self.environment\n        text = _(\"Additional Information:\\n\")\n        text += format_2_column_name_value(_(\"Source Context\"),        self.scontext.format())\n        text += format_2_column_name_value(_(\"Target Context\"),        self.tcontext.format())\n        text += format_2_column_name_value(_(\"Target Objects\"),        self.format_target_object())\n        text += format_2_column_name_value(_(\"Source\"),                default_text(self.source))\n        text += format_2_column_name_value(_(\"Source Path\"),           default_text(self.spath))\n        text += format_2_column_name_value(_(\"Port\"),                  default_text(self.port))\n        if (replace):\n            text += format_2_column_name_value(_(\"Host\"),              \"(removed)\")\n        else:\n            text += format_2_column_name_value(_(\"Host\"),                  default_text(self.sig.host))\n        text += format_2_column_name_value(_(\"Source RPM Packages\"),   default_text(self.format_rpm_list(self.src_rpm_list)))\n        text += format_2_column_name_value(_(\"Target RPM Packages\"),   default_text(self.format_rpm_list(self.tgt_rpm_list)))\n        text += format_2_column_name_value(_(\"Policy RPM\"),            default_text(env.policy_rpm))\n        text += format_2_column_name_value(_(\"Selinux Enabled\"),       default_text(env.selinux_enabled))\n        text += format_2_column_name_value(_(\"Policy Type\"),           default_text(env.policy_type))\n        text += format_2_column_name_value(_(\"Enforcing Mode\"),        default_text(env.enforce))\n        if replace:\n            text += format_2_column_name_value(_(\"Host Name\"),\"(removed)\")\n        else:\n            text += format_2_column_name_value(_(\"Host Name\"),         default_text(env.hostname))\n        if replace:\n            uname = env.uname.split()\n            uname[1] = \"(removed)\"\n            text += format_2_column_name_value(_(\"Platform\"),          default_text(\" \".join(uname)))\n        else:\n            text += format_2_column_name_value(_(\"Platform\"),              default_text(env.uname))\n        text += format_2_column_name_value(_(\"Alert Count\"),           default_text(self.report_count))\n        date_format = \"%Y-%m-%d %H:%M:%S %Z\"\n        text += format_2_column_name_value(_(\"First Seen\"),            self.first_seen_date.format(date_format))\n        text += format_2_column_name_value(_(\"Last Seen\"),             self.last_seen_date.format(date_format))\n        text += format_2_column_name_value(_(\"Local ID\"),              default_text(self.local_id))\n        text += '\\n' + _(\"Raw Audit Messages\")\n        avcbuf = \"\"\n        for audit_record in self.audit_event.records:\n            if audit_record.record_type == 'AVC':\n                avcbuf += \"\\n\" + audit_record.to_text() + \"\\n\"\n            else:\n                avcbuf += \"\\ntype=%s msg=%s: \" % (audit_record.record_type, audit_record.event_id)\n                avcbuf += ' '.join([\"%s=%s\" % (k, audit_record.fields[k]) for k in audit_record.fields_ord]) +\"\\n\"\n        avcbuf += \"\\nHash: \" + self.get_hash_str()\n        try:\n            audit2allow = \"/usr/bin/audit2allow\"\n            if os.path.exist(audit2allow):\n                newbuf = \"\\n\\naudit2allow\"\n                p =  Popen([audit2allow], shell=True,stdin=PIPE, stdout=PIPE)\n                newbuf += p.communicate(avcbuf)[0]\n                if os.path.exists(\"/var/lib/sepolgen/interface_info\"):\n                    newbuf += \"\\naudit2allow -R\"\n                    p =  Popen([\"%s -R\" % audit2allow ], shell=True,stdin=PIPE, stdout=PIPE)\n                    newbuf += p.communicate(avcbuf)[0]\n                avcbuf += newbuf\n        except:\n            pass\n        text += avcbuf + '\\n'\n        return text",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2016-4445",
        "description": "[{'lang': 'en', 'value': 'The fix_lookup_id function in sealert in setroubleshoot before 3.2.23 allows local users to execute arbitrary commands as root by triggering an SELinux denial with a crafted file name, related to executing external commands with the commands.getstatusoutput function.'}]",
        "cwe_number": 77
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-119",
      "code": "def get_markdown(text):\n    if not text:\n        return \"\"\n    pattern = fr'([\\[\\s\\S\\]]*?)\\(([\\s\\S]*?):([\\[\\s\\S\\]]*?)\\)'\n    if re.match(pattern, text):\n        scheme = re.search(pattern, text, re.IGNORECASE).group(2)\n        if scheme in helpdesk_settings.ALLOWED_URL_SCHEMES:\n            replacement = '\\\\1(\\\\2:\\\\3)'\n        else:\n            replacement = '\\\\1(\\\\3)'\n        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n    return mark_safe(\n        markdown(\n            text,\n            extensions=[\n                EscapeHtml(), 'markdown.extensions.nl2br',\n                'markdown.extensions.fenced_code'\n            ]\n        )\n    )",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-3994",
        "description": "[{'lang': 'en', 'value': \"django-helpdesk is vulnerable to Improper Neutralization of Input During Web Page Generation ('Cross-site Scripting')\"}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-120",
      "code": "    def handle_get(self) -> bool:\n        if self.do_log:\n            logmsg = \"%-4s %s @%s\" % (self.mode, self.req, self.uname)\n            if \"range\" in self.headers:\n                try:\n                    rval = self.headers[\"range\"].split(\"=\", 1)[1]\n                except:\n                    rval = self.headers[\"range\"]\n                logmsg += \" [\\033[36m\" + rval + \"\\033[0m]\"\n            self.log(logmsg)\n        if self.vpath.startswith(\".cpr\"):\n            if self.vpath.startswith(\".cpr/ico/\"):\n                return self.tx_ico(self.vpath.split(\"/\")[-1], exact=True)\n            if self.vpath.startswith(\".cpr/ssdp\"):\n                return self.conn.hsrv.ssdp.reply(self)\n            if self.vpath.startswith(\".cpr/dd/\") and self.args.mpmc:\n                if self.args.mpmc == \".\":\n                    raise Pebkac(404)\n                loc = self.args.mpmc.rstrip(\"/\") + self.vpath[self.vpath.rfind(\"/\") :]\n                h = {\"Location\": loc, \"Cache-Control\": \"max-age=39\"}\n                self.reply(b\"\", 301, headers=h)\n                return True\n            path_base = os.path.join(self.E.mod, \"web\")\n            static_path = absreal(os.path.join(path_base, self.vpath[5:]))\n            if not static_path.startswith(path_base):\n                t = \"attempted path traversal [{}] => [{}]\"\n                self.log(t.format(self.vpath, static_path), 1)\n                self.tx_404()\n                return False\n            return self.tx_file(static_path)\n        if \"cf_challenge\" in self.uparam:\n            self.reply(self.j2s(\"cf\").encode(\"utf-8\", \"replace\"))\n            return True\n        if not self.can_read and not self.can_write and not self.can_get:\n            t = \"@{} has no access to [{}]\"\n            self.log(t.format(self.uname, self.vpath))\n            if \"on403\" in self.vn.flags:\n                ret = self.on40x(self.vn.flags[\"on403\"], self.vn, self.rem)\n                if ret == \"true\":\n                    return True\n                elif ret == \"false\":\n                    return False\n                elif ret == \"allow\":\n                    self.log(\"plugin override; access permitted\")\n                    self.can_read = self.can_write = self.can_move = True\n                    self.can_delete = self.can_get = self.can_upget = True\n                    self.can_admin = True\n                else:\n                    return self.tx_404(True)\n            else:\n                if self.vpath:\n                    return self.tx_404(True)\n                self.uparam[\"h\"] = \"\"\n        if \"tree\" in self.uparam:\n            return self.tx_tree()\n        if \"scan\" in self.uparam:\n            return self.scanvol()\n        if self.args.getmod:\n            if \"delete\" in self.uparam:\n                return self.handle_rm([])\n            if \"move\" in self.uparam:\n                return self.handle_mv()\n        if not self.vpath:\n            if \"reload\" in self.uparam:\n                return self.handle_reload()\n            if \"stack\" in self.uparam:\n                return self.tx_stack()\n            if \"ups\" in self.uparam:\n                return self.tx_ups()\n            if \"k304\" in self.uparam:\n                return self.set_k304()\n            if \"setck\" in self.uparam:\n                return self.setck()\n            if \"reset\" in self.uparam:\n                return self.set_cfg_reset()\n            if \"hc\" in self.uparam:\n                return self.tx_svcs()\n        if \"h\" in self.uparam:\n            return self.tx_mounts()\n        if self.vpath == \"\" and not self.ouparam:\n            nread = len(self.rvol)\n            nwrite = len(self.wvol)\n            if nread + nwrite == 1 or (self.rvol == self.wvol and nread == 1):\n                if nread == 1:\n                    vpath = self.rvol[0]\n                else:\n                    vpath = self.wvol[0]\n                if self.vpath != vpath:\n                    self.redirect(vpath, flavor=\"redirecting to\", use302=True)\n                    return True\n        return self.tx_browser()\n    def set_k304(self) -> bool:\n        ck = gencookie(\"k304\", self.uparam[\"k304\"], self.args.R, False, 86400 * 299)\n        self.out_headerlist.append((\"Set-Cookie\", ck))\n        self.redirect(\"\", \"?h\n        return True",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-38501",
        "description": "[{'lang': 'en', 'value': \"copyparty is file server software. Prior to version 1.8.7, the application contains a reflected cross-site scripting via URL-parameter `?k304=...` and `?setck=...`. The worst-case outcome of this is being able to move or delete existing files on the server, or upload new files, using the account of the person who clicks the malicious link. It is recommended to change the passwords of one's copyparty accounts, unless one have inspected one's logs and found no trace of attacks. Version 1.8.7 contains a patch for the issue.\"}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-121",
      "code": "if __name__ == \"__main__\":\n  googletest.main()",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-35986",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. If `RaggedBincount` is given an empty input tensor `splits`, it results in a segfault that can be used to trigger a denial of service attack. We have patched the issue in GitHub commit 7a4591fd4f065f4fa903593bc39b2f79530a74b8. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-122",
      "code": "def create_app(name=\"\"):\n    if \"mscolab.server\" in name:\n        from mslib.mscolab.app import APP\n    else:\n        from mslib.mswms.app import APP\n    @APP.route('/xstatic/<name>/', defaults=dict(filename=''))\n    @APP.route('/xstatic/<name>/<path:filename>')\n    def files(name, filename):\n        base_path = _xstatic(name)\n        if base_path is None:\n            abort(404)\n        if not filename:\n            abort(404)\n        return send_from_directory(base_path, filename)\n    @APP.route('/mss_theme/<name>/', defaults=dict(filename=''))\n    @APP.route('/mss_theme/<name>/<path:filename>')\n    def mss_theme(name, filename):\n        if name != 'img':\n            abort(404)\n        base_path = os.path.join(DOCS_SERVER_PATH, 'static', 'img')\n        return send_from_directory(base_path, filename)\n    def get_topmenu():\n        if \"mscolab\" in \" \".join(sys.argv):\n            menu = [\n                (url_for('index'), 'Mission Support System',\n                 ((url_for('about'), 'About'),\n                  (url_for('install'), 'Install'),\n                  (url_for('help'), 'Help'),\n                  )),\n            ]\n        else:\n            menu = [\n                (url_for('index'), 'Mission Support System',\n                 ((url_for('about'), 'About'),\n                  (url_for('install'), 'Install'),\n                  (url_for(\"plots\"), 'Gallery'),\n                  (url_for('help'), 'Help'),\n                  )),\n            ]\n        return menu\n    APP.jinja_env.globals.update(get_topmenu=get_topmenu)\n    def get_content(filename, overrides=None):\n        markdown = Markdown(extensions=[\"fenced_code\"])\n        content = \"\"\n        if os.path.isfile(filename):\n            with codecs.open(filename, 'r', 'utf-8') as f:\n                md_data = f.read()\n            md_data = md_data.replace(':ref:', '')\n            if overrides is not None:\n                v1, v2 = overrides\n                md_data = md_data.replace(v1, v2)\n            content = markdown.convert(md_data)\n        return content\n    @APP.route(\"/index\")\n    def index():\n        return render_template(\"/index.html\")\n    @APP.route(\"/mss/about\")\n    @APP.route(\"/mss\")\n    def about():\n        _file = os.path.join(DOCS_SERVER_PATH, 'static', 'docs', 'about.md')\n        img_url = url_for('overview')\n        overrides = ['![image](/mss/overview.png)', f'![image]({img_url})']\n        content = get_content(_file,\n                              overrides=overrides)\n        return render_template(\"/content.html\", act=\"about\", content=content)\n    @APP.route(\"/mss/install\")\n    def install():\n        _file = os.path.join(DOCS_SERVER_PATH, 'static', 'docs', 'installation.md')\n        content = get_content(_file)\n        return render_template(\"/content.html\", act=\"install\", content=content)\n    @APP.route(\"/mss/plots\")\n    def plots():\n        if STATIC_LOCATION != \"\" and os.path.exists(os.path.join(STATIC_LOCATION, 'plots.html')):\n            _file = os.path.join(STATIC_LOCATION, 'plots.html')\n            content = get_content(_file)\n        else:\n            content = \"Gallery was not generated for this server.<br>\" \\\n                      \"For further info on how to generate it, run the \" \\\n                      \"<b>gallery --help</b> command line parameter of mswms.<br>\" \\\n                      \"An example of the gallery can be seen \" \\\n                      \"<a href=\\\"https://mss.readthedocs.io/en/stable/gallery/index.html\\\">here</a>\"\n        return render_template(\"/content.html\", act=\"plots\", content=content)\n    @APP.route(\"/mss/code/<path:filename>\")\n    def code(filename):\n        download = request.args.get(\"download\", False)\n        _file = os.path.join(STATIC_LOCATION, 'code', filename)\n        content = get_content(_file)\n        if not download:\n            return render_template(\"/content.html\", act=\"code\", content=content)\n        else:\n            with open(_file) as f:\n                text = f.read()\n            return Response(\"\".join([s.replace(\"\\t\", \"\", 1) for s in text.split(\"```python\")[-1]\n                                    .splitlines(keepends=True)][1:-2]),\n                            mimetype=\"text/plain\",\n                            headers={\"Content-disposition\": f\"attachment; filename={filename.split('-')[0]}.py\"})\n    @APP.route(\"/mss/help\")\n    def help():\n        _file = os.path.join(DOCS_SERVER_PATH, 'static', 'docs', 'help.md')\n        content = get_content(_file)\n        return render_template(\"/content.html\", act=\"help\", content=content)\n    @APP.route(\"/mss/imprint\")\n    def imprint():\n        _file = os.path.join(DOCS_SERVER_PATH, 'static', 'docs', 'imprint.md')\n        content = get_content(_file)\n        return render_template(\"/content.html\", act=\"imprint\", content=content)\n    @APP.route('/mss/favicon.ico')\n    def favicons():\n        base_path = icons(\"16x16\", \"favicon.ico\")\n        return send_file(base_path)\n    @APP.route('/mss/logo.png')\n    def logo():\n        base_path = icons(\"64x64\", \"mss-logo.png\")\n        return send_file(base_path)\n    @APP.route('/mss/overview.png')\n    def overview():\n        base_path = os.path.join(DOCS_SERVER_PATH, 'static', 'img', 'wise12_overview.png')\n        return send_file(base_path)\n    return APP\ndef uploads(name=None, filename=None):\n    base_path = mscolab_settings.UPLOAD_FOLDER\n    if name is None:\n        abort(404)\n    if filename is None:\n        abort(404)\n    return send_from_directory(fs.path.join(base_path, name), filename)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-25123",
        "description": "[{'lang': 'en', 'value': 'MSS (Mission Support System) is an open source package designed for planning atmospheric research flights. In file: `index.py`, there is a method that is vulnerable to path manipulation attack. By modifying file paths, an attacker can acquire sensitive information from different resources. The `filename` variable is joined with other variables to form a file path in `_file`. However, `filename` is a route parameter that can capture path type values i.e. values including slashes (\\\\). So it is possible for an attacker to manipulate the file being read by assigning a value containing ../ to `filename` and so the attacker may be able to gain access to other files on the host filesystem. This issue has been addressed in MSS version 8.3.3. Users are advised to upgrade. There are no known workarounds for this vulnerability.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-123",
      "code": "def file(path):\n    path = secure_filename(path)\n    if app.interface.encrypt and isinstance(app.interface.examples, str) and path.startswith(app.interface.examples):\n        with open(os.path.join(app.cwd, path), \"rb\") as encrypted_file:\n            encrypted_data = encrypted_file.read()\n        file_data = encryptor.decrypt(\n            app.interface.encryption_key, encrypted_data)\n        return send_file(io.BytesIO(file_data), attachment_filename=os.path.basename(path))\n    else:\n        return send_file(os.path.join(app.cwd, path))",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-43831",
        "description": "[{'lang': 'en', 'value': 'Gradio is an open source framework for building interactive machine learning models and demos. In versions prior to 2.5.0 there is a vulnerability that affects anyone who creates and publicly shares Gradio interfaces. File paths are not restricted and users who receive a Gradio link can access any files on the host computer if they know the file names or file paths. This is limited only by the host operating system. Paths are opened in read only mode. The problem has been patched in gradio 2.5.0.'}]",
        "cwe_number": 22
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-124",
      "code": "  def testScatterFailsWhenIndexLargerThanNumElements(self):\n    c0 = constant_op.constant([1.0, 2.0])\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        \"TensorListScatter: Trying to scatter at index 3 in list with size 3\"):\n      l = gen_list_ops.tensor_list_scatter_v2(\n          c0, [1, 3], list_ops._build_element_shape([]), num_elements=3)\n      self.evaluate(l)\n  def testScatterFailsWithInvalidNumElements(self):\n    c0 = constant_op.constant([1.0, 2.0])\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        \"TensorListScatter expects num_elements >= -1, found: -2\"):\n      l = gen_list_ops.tensor_list_scatter_v2(\n          c0, [1, 3], list_ops._build_element_shape([]), num_elements=-2)\n      self.evaluate(l)\n  def testScatterWithInvalidRowsInInputTensorFails(self):\n    c0 = constant_op.constant([1.0, 2.0])\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        \"Invalid number of rows in input tensor. Expected: 3 Actual: 2\"):\n      l = list_ops.tensor_list_scatter(c0, [1, 0, 2], [])\n      self.evaluate(l)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-35991",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. When `TensorListScatter` and `TensorListScatterV2` receive an `element_shape` of a rank greater than one, they give a `CHECK` fail that can trigger a denial of service attack. We have patched the issue in GitHub commit bb03fdf4aae944ab2e4b35c7daa051068a8b7f61. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue.'}]",
        "cwe_number": 617
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-125",
      "code": "    def write_error(self, status_code: int, **kwargs: Any) -> None:\n        \"\"\"APIHandler errors are JSON, not human pages\"\"\"\n        self.set_header(\"Content-Type\", \"application/json\")\n        message = responses.get(status_code, \"Unknown HTTP Error\")\n        reply: dict[str, Any] = {\n            \"message\": message,\n        }\n        exc_info = kwargs.get(\"exc_info\")\n        if exc_info:\n            e = exc_info[1]\n            if isinstance(e, HTTPError):\n                reply[\"message\"] = e.log_message or message\n                reply[\"reason\"] = e.reason\n            else:\n                reply[\"message\"] = \"Unhandled error\"\n                reply[\"reason\"] = None\n                reply[\"traceback\"] = \"\".join(traceback.format_exception(*exc_info))\n        self.log.warning(\"wrote error: %r\", reply[\"message\"], exc_info=True)\n        self.finish(json.dumps(reply))\n    async def post(self, kernel_id, action):\n        \"\"\"Interrupt or restart a kernel.\"\"\"\n        km = self.kernel_manager\n        if action == \"interrupt\":\n            await ensure_async(km.interrupt_kernel(kernel_id))\n            self.set_status(204)\n        if action == \"restart\":\n            try:\n                await km.restart_kernel(kernel_id)\n            except Exception as e:\n                message = \"Exception restarting kernel\"\n                self.log.error(message, exc_info=True)\n                traceback = format_tb(e.__traceback__)\n                self.write(json.dumps({\"message\": message, \"traceback\": traceback}))\n                self.set_status(500)\n            else:\n                model = await ensure_async(km.kernel_model(kernel_id))\n                self.write(json.dumps(model, default=json_default))\n        self.finish()",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-49080",
        "description": "[{'lang': 'en', 'value': 'The Jupyter Server provides the backend (i.e. the core services, APIs, and REST endpoints) for Jupyter web applications like Jupyter notebook, JupyterLab, and Voila. Unhandled errors in API requests coming from an authenticated user include traceback information, which can include path information. There is no known mechanism by which to trigger these errors without authentication, so the paths revealed are not considered particularly sensitive, given that the requesting user has arbitrary execution permissions already in the same environment. A fix has been introduced in commit `0056c3aa52` which no longer includes traceback information in JSON error responses. For compatibility, the traceback field is present, but always empty. This commit has been included in version 2.11.2. Users are advised to upgrade. There are no known workarounds for this vulnerability.'}]",
        "cwe_number": 209
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-126",
      "code": "def get_install_requires():\n    requires = ['psutil>=5.3.0', 'future']\n    if sys.platform.startswith('win'):\n        requires.append('bottle')\n        requires.append('requests')\n    return requires\ndef get_install_extras_require():\n    extras_require = {\n        'action': ['chevron'],\n        'browser': ['zeroconf==0.19.1' if PY2 else 'zeroconf>=0.19.1'],\n        'cloud': ['requests'],\n        'docker': ['docker>=2.0.0'],\n        'export': ['bernhard', 'cassandra-driver', 'couchdb', 'elasticsearch',\n                   'graphitesender', 'influxdb>=1.0.0', 'kafka-python', 'pika',\n                   'paho-mqtt', 'potsdb', 'prometheus_client', 'pyzmq',\n                   'statsd'],\n        'folders': ['scandir'],\n        'gpu': ['py3nvml'],\n        'graph': ['pygal'],\n        'ip': ['netifaces'],\n        'raid': ['pymdstat'],\n        'smart': ['pySMART.smartx'],\n        'snmp': ['pysnmp'],\n        'sparklines': ['sparklines'],\n        'web': ['bottle', 'requests'],\n        'wifi': ['wifi']\n    }\n    extras_require.update({'all': [i[0] for i in extras_require.values()]})\n    return extras_require\nclass tests(Command):",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-23418",
        "description": "[{'lang': 'en', 'value': 'The package glances before 3.2.1 are vulnerable to XML External Entity (XXE) Injection via the use of Fault to parse untrusted XML data, which is known to be vulnerable to XML attacks.'}]",
        "cwe_number": 611
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-127",
      "code": "def publish(request, user_id=None):\n    initial = None\n    if user_id:\n        user_to = get_object_or_404(User, pk=user_id)\n        initial = {'users': [user_to.st.nickname]}\n    user = request.user\n    tform = TopicForPrivateForm(\n        user=user, data=post_data(request))\n    cform = CommentForm(\n        user=user, data=post_data(request))\n    tpform = TopicPrivateManyForm(\n        user=user, data=post_data(request), initial=initial)\n    if (is_post(request) and\n            all([tform.is_valid(), cform.is_valid(), tpform.is_valid()]) and\n            not request.is_limited()):\n        if not user.st.update_post_hash(tform.get_topic_hash()):\n            return redirect(\n                request.POST.get('next', None) or\n                tform.category.get_absolute_url())\n        topic = tform.save()\n        cform.topic = topic\n        comment = cform.save()\n        comment_posted(comment=comment, mentions=None)\n        tpform.topic = topic\n        tpform.save_m2m()\n        TopicNotification.bulk_create(\n            users=tpform.get_users(), comment=comment)\n        return redirect(topic.get_absolute_url())\n    return render(\n        request=request,\n        template_name='spirit/topic/private/publish.html',\n        context={\n            'tform': tform,\n            'cform': cform,\n            'tpform': tpform})\ndef create_access(request, topic_id):\n    topic_private = TopicPrivate.objects.for_create_or_404(topic_id, request.user)\n    form = TopicPrivateInviteForm(\n        topic=topic_private.topic,\n        data=post_data(request))\n    if form.is_valid():\n        form.save()\n        notify_access(user=form.get_user(), topic_private=topic_private)\n    else:\n        messages.error(request, utils.render_form_errors(form))\n    return redirect(request.POST.get('next', topic_private.get_absolute_url()))\ndef delete_access(request, pk):\n    topic_private = TopicPrivate.objects.for_delete_or_404(pk, request.user)\n    if request.method == 'POST':\n        topic_private.delete()\n        if request.user.pk == topic_private.user_id:\n            return redirect(reverse(\"spirit:topic:private:index\"))\n        return redirect(request.POST.get('next', topic_private.get_absolute_url()))\n    return render(\n        request=request,\n        template_name='spirit/topic/private/delete.html',\n        context={'topic_private': topic_private})\ndef join_in(request, topic_id):\n    topic = get_object_or_404(\n        Topic,\n        pk=topic_id,\n        user=request.user,\n        category_id=settings.ST_TOPIC_PRIVATE_CATEGORY_PK)\n    form = TopicPrivateJoinForm(\n        topic=topic,\n        user=request.user,\n        data=post_data(request))\n    if is_post(request) and form.is_valid():\n        topic_private = form.save()\n        notify_access(user=form.get_user(), topic_private=topic_private)\n        return redirect(request.POST.get('next', topic.get_absolute_url()))\n    return render(\n        request=request,\n        template_name='spirit/topic/private/join.html',\n        context={\n            'topic': topic,\n            'form': form})\ndef publish(request, topic_id, pk=None):\n    initial = None\n    if pk:\n        comment = get_object_or_404(\n            Comment.objects.for_access(user=request.user), pk=pk)\n        quote = markdown.quotify(comment.comment, comment.user.st.nickname)\n        initial = {'comment': quote}\n    user = request.user\n    topic = get_object_or_404(\n        Topic.objects.opened().for_access(user),\n        pk=topic_id)\n    form = CommentForm(\n        user=user,\n        topic=topic,\n        data=post_data(request),\n        initial=initial)\n    if is_post(request) and not request.is_limited() and form.is_valid():\n        if not user.st.update_post_hash(form.get_comment_hash()):\n            return redirect(\n                request.POST.get('next', None) or\n                Comment\n                .get_last_for_topic(topic_id)\n                .get_absolute_url())\n        comment = form.save()\n        comment_posted(comment=comment, mentions=form.mentions)\n        return redirect(request.POST.get('next', comment.get_absolute_url()))\n    return render(\n        request=request,\n        template_name='spirit/comment/publish.html',\n        context={\n            'form': form,\n            'topic': topic})\ndef update(request, pk):\n    comment = Comment.objects.for_update_or_404(pk, request.user)\n    form = CommentForm(data=post_data(request), instance=comment)\n    if is_post(request) and form.is_valid():\n        pre_comment_update(comment=Comment.objects.get(pk=comment.pk))\n        comment = form.save()\n        post_comment_update(comment=comment)\n        return redirect(request.POST.get('next', comment.get_absolute_url()))\n    return render(\n        request=request,\n        template_name='spirit/comment/update.html',\n        context={'form': form})\ndef delete(request, pk, remove=True):\n    comment = get_object_or_404(Comment, pk=pk)\n    if is_post(request):\n        (Comment.objects\n         .filter(pk=pk)\n         .update(is_removed=remove))\n        return redirect(request.GET.get('next', comment.get_absolute_url()))\n    return render(\n        request=request,\n        template_name='spirit/comment/moderate.html',\n        context={'comment': comment})\ndef move(request, topic_id):\n    topic = get_object_or_404(Topic, pk=topic_id)\n    form = CommentMoveForm(topic=topic, data=request.POST)\n    if form.is_valid():\n        comments = form.save()\n        for comment in comments:\n            comment_posted(comment=comment, mentions=None)\n            topic.decrease_comment_count()\n            post_comment_move(comment=comment, topic=topic)\n    else:\n        messages.error(request, render_form_errors(form))\n    return redirect(request.POST.get('next', topic.get_absolute_url()))\ndef publish(request, category_id=None):\n    if category_id:\n        get_object_or_404(\n            Category.objects.visible(),\n            pk=category_id)\n    user = request.user\n    form = TopicForm(\n        user=user,\n        data=post_data(request),\n        initial={'category': category_id})\n    cform = CommentForm(\n        user=user,\n        data=post_data(request))\n    if (is_post(request) and\n            all([form.is_valid(), cform.is_valid()]) and\n            not request.is_limited()):\n        if not user.st.update_post_hash(form.get_topic_hash()):\n            return redirect(\n                request.POST.get('next', None) or\n                form.get_category().get_absolute_url())\n        topic = form.save()\n        cform.topic = topic\n        comment = cform.save()\n        comment_posted(comment=comment, mentions=cform.mentions)\n        return redirect(topic.get_absolute_url())\n    return render(\n        request=request,\n        template_name='spirit/topic/publish.html',\n        context={'form': form, 'cform': cform})\ndef update(request, pk):\n    topic = Topic.objects.for_update_or_404(pk, request.user)\n    category_id = topic.category_id\n    form = TopicForm(\n        user=request.user,\n        data=post_data(request),\n        instance=topic)\n    if is_post(request) and form.is_valid():\n        topic = form.save()\n        if topic.category_id != category_id:\n            Comment.create_moderation_action(\n                user=request.user, topic=topic, action=Comment.MOVED)\n        return redirect(request.POST.get('next', topic.get_absolute_url()))\n    return render(\n        request=request,\n        template_name='spirit/topic/update.html',\n        context={'form': form})\ndef _moderate(request, pk, field_name, to_value, action=None, message=None):\n    topic = get_object_or_404(Topic, pk=pk)\n    if is_post(request):\n        count = (\n            Topic.objects\n            .filter(pk=pk)\n            .exclude(**{field_name: to_value})\n            .update(**{\n                field_name: to_value,\n                'reindex_at': timezone.now()}))\n        if count and action is not None:\n            Comment.create_moderation_action(\n                user=request.user,\n                topic=topic,\n                action=action)\n        if message is not None:\n            messages.info(request, message)\n        return redirect(request.POST.get(\n            'next', topic.get_absolute_url()))\n    return render(\n        request=request,\n        template_name='spirit/topic/moderate.html',\n        context={'topic': topic})\ndef custom_login(request, **kwargs):\n    if request.user.is_authenticated:\n        return redirect(request.GET.get('next', request.user.st.get_absolute_url()))\n    if request.method == \"POST\" and request.is_limited():\n        return redirect(request.get_full_path())\n    return _login_view(request, authentication_form=LoginForm, **kwargs)\ndef custom_logout(request, **kwargs):\n    if not request.user.is_authenticated:\n        return redirect(request.GET.get('next', reverse(settings.LOGIN_URL)))\n    if request.method == 'POST':\n        return _logout_view(request, **kwargs)\n    return render(request, 'spirit/user/auth/logout.html')\ndef register(request, registration_form=RegistrationForm):\n    if request.user.is_authenticated:\n        return redirect(request.GET.get('next', reverse('spirit:user:update')))\n    form = registration_form(data=post_data(request))\n    if (is_post(request) and\n            not request.is_limited() and\n            form.is_valid()):\n        user = form.save()\n        send_activation_email(request, user)\n        messages.info(\n            request, _(\n                \"We have sent you an email to %(email)s \"\n                \"so you can activate your account!\") % {'email': form.get_email()})\n        return redirect(reverse(settings.LOGIN_URL))\n    return render(\n        request=request,\n        template_name='spirit/user/auth/register.html',\n        context={'form': form})\ndef resend_activation_email(request):\n    if request.user.is_authenticated:\n        return redirect(request.GET.get('next', reverse('spirit:user:update')))\n    form = ResendActivationForm(data=post_data(request))\n    if is_post(request):\n        if not request.is_limited() and form.is_valid():\n            user = form.get_user()\n            send_activation_email(request, user)\n        messages.info(\n            request, _(\n                \"If you don't receive an email, please make sure you've entered \"\n                \"the address you registered with, and check your spam folder.\"))\n        return redirect(reverse(settings.LOGIN_URL))\n    return render(\n        request=request,\n        template_name='spirit/user/auth/activation_resend.html',\n        context={'form': form})\ndef create(request, comment_id):\n    comment = get_object_or_404(\n        Comment.objects.exclude(user=request.user),\n        pk=comment_id)\n    form = LikeForm(\n        user=request.user,\n        comment=comment,\n        data=post_data(request))\n    if is_post(request) and form.is_valid():\n        like = form.save()\n        like.comment.increase_likes_count()\n        if is_ajax(request):\n            return json_response({'url_delete': like.get_delete_url()})\n        return redirect(request.POST.get('next', comment.get_absolute_url()))\n    return render(\n        request=request,\n        template_name='spirit/comment/like/create.html',\n        context={\n            'form': form,\n            'comment': comment})\ndef delete(request, pk):\n    like = get_object_or_404(CommentLike, pk=pk, user=request.user)\n    if is_post(request):\n        like.delete()\n        like.comment.decrease_likes_count()\n        if is_ajax(request):\n            url = reverse(\n                'spirit:comment:like:create',\n                kwargs={'comment_id': like.comment.pk})\n            return json_response({'url_create': url, })\n        return redirect(request.POST.get('next', like.comment.get_absolute_url()))\n    return render(\n        request=request,\n        template_name='spirit/comment/like/delete.html',\n        context={'like': like})\ndef close_or_open(request, pk, close=True):\n    poll = get_object_or_404(\n        CommentPoll,\n        pk=pk,\n        comment__user=request.user\n    )\n    if close:\n        close_at = timezone.now()\n    else:\n        close_at = None\n    (CommentPoll.objects\n     .filter(pk=poll.pk)\n     .update(close_at=close_at))\n    return redirect(request.GET.get('next', poll.get_absolute_url()))\ndef vote(request, pk):\n    poll = get_object_or_404(\n        CommentPoll.objects.unremoved(),\n        pk=pk\n    )\n    if not request.user.is_authenticated:\n        return redirect_to_login(next=poll.get_absolute_url())\n    form = PollVoteManyForm(user=request.user, poll=poll, data=request.POST)\n    if form.is_valid():\n        CommentPollChoice.decrease_vote_count(poll=poll, voter=request.user)\n        form.save_m2m()\n        CommentPollChoice.increase_vote_count(poll=poll, voter=request.user)\n        return redirect(request.POST.get('next', poll.get_absolute_url()))\n    messages.error(request, utils.render_form_errors(form))\n    return redirect(request.POST.get('next', poll.get_absolute_url()))\ndef create(request, comment_id):\n    comment = get_object_or_404(Comment, pk=comment_id)\n    form = FlagForm(\n        user=request.user,\n        comment=comment,\n        data=post_data(request))\n    if is_post(request) and form.is_valid():\n        form.save()\n        return redirect(request.POST.get('next', comment.get_absolute_url()))\n    return render(\n        request=request,\n        template_name='spirit/comment/flag/create.html',\n        context={\n            'form': form,\n            'comment': comment})\ndef create(request, topic_id):\n    topic = get_object_or_404(\n        Topic.objects.for_access(request.user),\n        pk=topic_id)\n    form = NotificationCreationForm(\n        user=request.user,\n        topic=topic,\n        data=request.POST)\n    if form.is_valid():\n        form.save()\n    else:\n        messages.error(request, utils.render_form_errors(form))\n    return redirect(request.POST.get('next', topic.get_absolute_url()))\ndef update(request, pk):\n    notification = get_object_or_404(TopicNotification, pk=pk, user=request.user)\n    form = NotificationForm(data=request.POST, instance=notification)\n    if form.is_valid():\n        form.save()\n    else:\n        messages.error(request, utils.render_form_errors(form))\n    return redirect(request.POST.get(\n        'next', notification.topic.get_absolute_url()))\ndef mark_all_as_read(request):\n    (TopicNotification.objects\n        .for_access(request.user)\n        .filter(is_read=False)\n        .update(is_read=True))\n    return redirect(request.POST.get(\n        'next', reverse('spirit:topic:notification:index')))\ndef edit(request, user_id):\n    user = get_object_or_404(User, pk=user_id)\n    uform = UserForm(data=post_data(request), instance=user)\n    form = UserProfileForm(data=post_data(request), instance=user.st)\n    if is_post(request) and all([uform.is_valid(), form.is_valid()]):\n        uform.save()\n        form.save()\n        messages.info(request, _(\"This profile has been updated!\"))\n        return redirect(request.GET.get(\"next\", request.get_full_path()))\n    return render(\n        request=request,\n        template_name='spirit/user/admin/edit.html',\n        context={'form': form, 'uform': uform})\ndef config_basic(request):\n    form = BasicConfigForm(data=post_data(request))\n    if is_post(request) and form.is_valid():\n        form.save()\n        messages.info(request, _(\"Settings updated!\"))\n        return redirect(request.GET.get(\"next\", request.get_full_path()))\n    return render(\n        request=request,\n        template_name='spirit/admin/config_basic.html',\n        context={'form': form})\ndef guest_only(view_func):\n    @wraps(view_func)\n    def wrapper(request, *args, **kwargs):\n        if request.user.is_authenticated:\n            return redirect(request.GET.get('next', request.user.st.get_absolute_url()))\n        return view_func(request, *args, **kwargs)\n    return wrapper\ndef create(request, topic_id):\n    topic = get_object_or_404(Topic, pk=topic_id)\n    form = FavoriteForm(user=request.user, topic=topic, data=request.POST)\n    if form.is_valid():\n        form.save()\n    else:\n        messages.error(request, utils.render_form_errors(form))\n    return redirect(request.POST.get('next', topic.get_absolute_url()))\ndef delete(request, pk):\n    favorite = get_object_or_404(TopicFavorite, pk=pk, user=request.user)\n    favorite.delete()\n    return redirect(request.POST.get('next', favorite.topic.get_absolute_url()))",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-0869",
        "description": "[{'lang': 'en', 'value': 'Multiple Open Redirect in GitHub repository nitely/spirit prior to 0.12.3.'}]",
        "cwe_number": 601
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-128",
      "code": "def is_allowed_hostname(hostname):\n    allowed_lists = [\"atl-paas.net\", \"atlassian.com\", \"atlassian.net\", \"jira.com\"]\n    parsed_uri = urlparse(f\"https://{hostname}\")\n    domain = parsed_uri.netloc.split(\":\")[0]\n    base_domain = \".\".join(domain.split(\".\")[-2:])\n    return base_domain in allowed_lists\ndef jira_project_issue_summary(email, api_token, project_key, hostname):\n    try:\n        if not is_allowed_hostname(hostname):\n            print(\"Errored Hostname\")\n            return {\"error\": \"Invalid or unauthorized hostname\"}\n        auth = HTTPBasicAuth(email, api_token)\n        headers = {\"Accept\": \"application/json\"}\n        issue_url = f\"https://{hostname}/rest/api/3/search?jql=project={project_key} AND issuetype!=Epic\"\n        issue_response = requests.request(\n            \"GET\", issue_url, headers=headers, auth=auth\n        ).json()[\"total\"]\n        module_url = f\"https://{hostname}/rest/api/3/search?jql=project={project_key} AND issuetype=Epic\"\n        module_response = requests.request(\n            \"GET\", module_url, headers=headers, auth=auth\n        ).json()[\"total\"]\n        status_url = f\"https://{hostname}/rest/api/3/project/${project_key}/statuses\"\n        status_response = requests.request(\n            \"GET\", status_url, headers=headers, auth=auth\n        ).json()\n        labels_url = f\"https://{hostname}/rest/api/3/label/?jql=project={project_key}\"\n        labels_response = requests.request(\n            \"GET\", labels_url, headers=headers, auth=auth\n        ).json()[\"total\"]\n        users_url = (\n            f\"https://{hostname}/rest/api/3/users/search?jql=project={project_key}\"\n        )\n        users_response = requests.request(\n            \"GET\", users_url, headers=headers, auth=auth\n        ).json()\n        return {\n            \"issues\": issue_response,\n            \"modules\": module_response,\n            \"labels\": labels_response,\n            \"states\": len(status_response),\n            \"users\": (\n                [\n                    user\n                    for user in users_response\n                    if user.get(\"accountType\") == \"atlassian\"\n                ]\n            ),\n        }\n    except Exception as e:\n        capture_exception(e)\n        return {\"error\": \"Something went wrong could not fetch information from jira\"}",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-31461",
        "description": "[{'lang': 'en', 'value': 'Plane, an open-source project management tool, has a Server-Side Request Forgery (SSRF) vulnerability in versions prior to 0.17-dev. This issue may allow an attacker to send arbitrary requests from the server hosting the application, potentially leading to unauthorized access to internal systems. The impact of this vulnerability includes, but is not limited to, unauthorized access to internal services accessible from the server, potential leakage of sensitive information from internal services, manipulation of internal systems by interacting with internal APIs. Version 0.17-dev contains a patch for this issue. Those who are unable to update immediately may mitigate the issue by restricting outgoing network connections from servers hosting the application to essential services only and/or implementing strict input validation on URLs or parameters that are used to generate server-side requests.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-129",
      "code": "    def _setup_master(self):\n        Router.max_message_size = self.config['max_message_size']\n        if self.config['profiling']:\n            enable_profiling()\n        self.broker = Broker(activate_compat=False)\n        self.router = Router(self.broker)\n        self.router.debug = self.config.get('debug', False)\n        self.router.undirectional = self.config['unidirectional']\n        self.router.add_handler(\n            fn=self._on_shutdown_msg,\n            handle=SHUTDOWN,\n            policy=has_parent_authority,\n        )\n        self.master = Context(self.router, 0, 'master')\n        parent_id = self.config['parent_ids'][0]\n        if parent_id == 0:\n            self.parent = self.master\n        else:\n            self.parent = Context(self.router, parent_id, 'parent')\n        in_fd = self.config.get('in_fd', 100)\n        in_fp = os.fdopen(os.dup(in_fd), 'rb', 0)\n        os.close(in_fd)\n        out_fp = os.fdopen(os.dup(self.config.get('out_fd', 1)), 'wb', 0)\n        self.stream = MitogenProtocol.build_stream(self.router, parent_id)\n        self.stream.accept(in_fp, out_fp)\n        self.stream.name = 'parent'\n        self.stream.receive_side.keep_alive = False\n        listen(self.stream, 'disconnect', self._on_parent_disconnect)\n        listen(self.broker, 'exit', self._on_broker_exit)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2019-15149",
        "description": "[{'lang': 'en', 'value': 'core.py in Mitogen before 0.2.8 has a typo that drops the unidirectional-routing protection mechanism in the case of a child that is initiated by another child. The Ansible extension is unaffected. NOTE: the vendor disputes this issue because it is exploitable only in conjunction with hypothetical other factors, i.e., an affected use case within a library caller, and a bug in the message receiver policy code that led to reliance on this extra protection mechanism'}]",
        "cwe_number": 254
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-130",
      "code": "def get_nodes_like(word,requested_limit):\n    debug = False\n    t0 = timeit.default_timer()\n    requested_limit = int(requested_limit)\n    values = []\n    n_values = 0\n    if len(word) < 2:\n        return values\n    floor = word[:-1]\n    ceiling = floor + 'zz'\n    if debug:\n        print(f\"INFO: Query 1\")\n    cursor.execute(f\"SELECT term FROM terms WHERE term > \\\"{floor}\\\" AND term < \\\"{ceiling}\\\" AND term LIKE \\\"{word}%%\\\" ORDER BY length(term),term LIMIT {requested_limit}\")\n    rows = cursor.fetchall()\n    values_dict = {}\n    for row in rows:\n        term = row[0]\n        if term.upper() not in values_dict:\n            if debug:\n                print(f\"    - {term}\")\n            properties = { \"curie\": '??', \"name\": term, \"type\": '??' }\n            values.append(properties)\n            values_dict[term.upper()] = 1\n            n_values += 1\n            if n_values >= requested_limit:\n                break\n    t1 = timeit.default_timer()\n    if debug:\n        print(f\"INFO: Query 1 in {t1-t0} sec\")\n    if n_values < requested_limit:\n        if debug:\n            print(f\"INFO: Query 2\")\n        word_part = word\n        found_fragment = None\n        while len(word_part) > 2:\n            cursor.execute(f\"SELECT rowid, fragment FROM cached_fragments WHERE fragment == \\\"{word_part}\\\"\")\n            rows = cursor.fetchall()\n            if len(rows) > 0:\n                fragment_id = rows[0][0]\n                found_fragment = rows[0][1]\n                break\n            word_part = word_part[:-1]\n        if found_fragment:\n            if debug:\n                print(f\"Found matching fragment {found_fragment} as fragment_id {fragment_id}\")\n            cursor.execute(f\"SELECT term FROM cached_fragment_terms WHERE fragment_id = {fragment_id} AND term LIKE \\\"%%{word}%%\\\"\")\n            rows = cursor.fetchall()\n            for row in rows:\n                term = row[0]\n                if term.upper() not in values_dict:\n                    if n_values < requested_limit:\n                        if debug:\n                            print(f\"    - {term}\")\n                        properties = { \"curie\": '??', \"name\": term, \"type\": '??' }\n                        values.append(properties)\n                        n_values += 1\n        if found_fragment is None:\n            cursor.execute(\"INSERT INTO cached_fragments(fragment) VALUES(?)\", (word,))\n            fragment_id = cursor.lastrowid\n            if debug:\n                print(f\"fragment_id = {fragment_id}\")\n            cursor.execute(\"SELECT term FROM terms WHERE term LIKE \\\"%%%s%%\\\" ORDER BY length(term),term LIMIT %s\" % (word,10000))\n            rows = cursor.fetchall()\n            for row in rows:\n                term = row[0]\n                if term.upper() not in values_dict:\n                    if n_values < requested_limit:\n                        if debug:\n                            print(f\"    - {term}\")\n                        properties = { \"curie\": '??', \"name\": term, \"type\": '??' }\n                        values.append(properties)\n                        n_values += 1\n                    values_dict[term.upper()] = 1\n                    cursor.execute(\"INSERT INTO cached_fragment_terms(fragment_id, term) VALUES(?,?)\", (fragment_id, term,))\n            conn.commit()\n        t2 = timeit.default_timer()\n        if debug:\n            print(f\"INFO: Query 2 in {t2-t1} sec\")\n    return(values)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-1531",
        "description": "[{'lang': 'en', 'value': 'SQL injection vulnerability in ARAX-UI Synonym Lookup functionality in GitHub repository rtxteam/rtx prior to checkpoint_2022-04-20 . This vulnerability is critical as it can lead to remote code execution and thus complete server takeover.'}]",
        "cwe_number": 89
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-131",
      "code": "    def __call__(self, iterable):\n        if self._jobs:\n            raise ValueError('This Parallel instance is already running')\n        self._aborting = False\n        if not self._managed_backend:\n            n_jobs = self._initialize_backend()\n        else:\n            n_jobs = self._effective_n_jobs()\n        if isinstance(self._backend, LokyBackend):\n            def _batched_calls_reducer_callback():\n                self._backend._workers._temp_folder_manager.set_current_context(\n                    self._id\n                )\n            self._reducer_callback = _batched_calls_reducer_callback\n        self._cached_effective_n_jobs = n_jobs\n        backend_name = self._backend.__class__.__name__\n        if n_jobs == 0:\n            raise RuntimeError(\"%s has no active worker.\" % backend_name)\n        self._print(\"Using backend %s with %d concurrent workers.\",\n                    (backend_name, n_jobs))\n        if hasattr(self._backend, 'start_call'):\n            self._backend.start_call()\n        iterator = iter(iterable)\n        pre_dispatch = self.pre_dispatch\n        if pre_dispatch == 'all' or n_jobs == 1:\n            self._original_iterator = None\n            self._pre_dispatch_amount = 0\n        else:\n            self._original_iterator = iterator\n            if hasattr(pre_dispatch, 'endswith'):\n                pre_dispatch = eval(pre_dispatch)\n            self._pre_dispatch_amount = pre_dispatch = int(pre_dispatch)\n            iterator = itertools.islice(iterator, self._pre_dispatch_amount)\n        self._start_time = time.time()\n        self.n_dispatched_batches = 0\n        self.n_dispatched_tasks = 0\n        self.n_completed_tasks = 0\n        self._pickle_cache = dict()\n        try:\n            self._iterating = False\n            if self.dispatch_one_batch(iterator):\n                self._iterating = self._original_iterator is not None\n            while self.dispatch_one_batch(iterator):\n                pass\n            if pre_dispatch == \"all\" or n_jobs == 1:\n                self._iterating = False\n            with self._backend.retrieval_context():\n                self.retrieve()\n            elapsed_time = time.time() - self._start_time\n            self._print('Done %3i out of %3i | elapsed: %s finished',\n                        (len(self._output), len(self._output),\n                         short_format_time(elapsed_time)))\n        finally:\n            if hasattr(self._backend, 'stop_call'):\n                self._backend.stop_call()\n            if not self._managed_backend:\n                self._terminate_backend()\n            self._jobs = list()\n            self._pickle_cache = None\n        output = self._output\n        self._output = None\n        return output",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-21797",
        "description": "[{'lang': 'en', 'value': 'The package joblib from 0 and before 1.2.0 are vulnerable to Arbitrary Code Execution via the pre_dispatch flag in Parallel() class due to the eval() statement.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-132",
      "code": "    def write(self):\n        self._create_dirs()\n        for component in self.components:\n            text = ical.serialize(\n                self.tag, self.headers, [component] + self.timezones)\n            name = (\n                component.name if sys.version_info[0] >= 3 else\n                component.name.encode(filesystem.FILESYSTEM_ENCODING))\n            filesystem_path = os.path.join(self._filesystem_path, name)\n            with filesystem.open(filesystem_path, \"w\") as fd:\n                fd.write(text)\n    def delete(self):\n        shutil.rmtree(self._filesystem_path)\n        os.remove(self._props_path)\n    def text(self):\n        components = (\n            ical.Timezone, ical.Event, ical.Todo, ical.Journal, ical.Card)\n        items = set()\n        try:\n            filenames = os.listdir(self._filesystem_path)\n        except (OSError, IOError) as e:\n            log.LOGGER.info('Error while reading collection %r: %r'\n                            % (self._filesystem_path, e))\n            return \"\"\n        for filename in filenames:\n            path = os.path.join(self._filesystem_path, filename)\n            try:\n                with filesystem.open(path) as fd:\n                    items.update(self._parse(fd.read(), components))\n            except (OSError, IOError) as e:\n                log.LOGGER.warning('Error while reading item %r: %r'\n                                   % (path, e))\n        return ical.serialize(\n            self.tag, self.headers, sorted(items, key=lambda x: x.name))",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2015-8747",
        "description": "[{'lang': 'en', 'value': 'The multifilesystem storage backend in Radicale before 1.1 allows remote attackers to read or write to arbitrary files via a crafted component name.'}]",
        "cwe_number": 20
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-133",
      "code": "    def load(klass):\n        \"\"\"\n        Insantiates the configuration by attempting to load the\n        configuration from YAML files specified by the CONF_PATH module\n        variable. This should be the main entry point for configuration.\n        \"\"\"\n        config = klass()\n        for path in klass.CONF_PATHS:\n            if os.path.exists(path):\n                with open(path, 'r') as conf:\n                    config.configure(yaml.load(conf))\n        return config",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2017-16763",
        "description": "[{'lang': 'en', 'value': 'An exploitable vulnerability exists in the YAML parsing functionality in config.py in Confire 0.2.0. Due to the user-specific configuration being loaded from \"~/.confire.yaml\" using the yaml.load function, a YAML parser can execute arbitrary Python commands resulting in command execution. An attacker can insert Python into loaded YAML to trigger this vulnerability.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-134",
      "code": "    username = serializers.CharField(max_length=50)\n    password = serializers.CharField(max_length=50)\n    otp = serializers.CharField(max_length=6)\n    def login_user(self, username, password, otp, context, **kwargs):\n        user = authenticate(request=context.get('request'),\n                            username=username, password=password)\n        if not user:\n            login_reject.send(sender=self.__class__, username=username, reason='creds')\n            raise FormattedException(m='incorrect_username_or_password', d={'reason': 'incorrect_username_or_password'},\n                                     status_code=HTTP_401_UNAUTHORIZED)\n        if not user.email_verified and not user.is_superuser:\n            login_reject.send(sender=self.__class__, username=username, reason='email')\n            raise FormattedException(m='email_verification_required', d={'reason': 'email_verification_required'},\n                                     status_code=HTTP_401_UNAUTHORIZED)\n        if not user.can_login():\n            login_reject.send(sender=self.__class__, username=username, reason='closed')\n            raise FormattedException(m='login_not_open', d={'reason': 'login_not_open'},\n                                     status_code=HTTP_401_UNAUTHORIZED)\n        if user.totp_status == TOTPStatus.ENABLED:\n            if not otp or otp == '':\n                login_reject.send(sender=self.__class__, username=username, reason='no_2fa')\n                raise FormattedException(m='2fa_required', d={'reason': '2fa_required'},\n                                         status_code=HTTP_401_UNAUTHORIZED)\n            totp = pyotp.TOTP(user.totp_secret)\n            if not totp.verify(otp, valid_window=1):\n                login_reject.send(sender=self.__class__, username=username, reason='incorrect_2fa')\n                raise FormattedException(m='incorrect_2fa', d={'reason': 'incorrect_2fa'},\n                                         status_code=HTTP_401_UNAUTHORIZED)\n        login.send(sender=self.__class__, user=user)\n        return user\n    def dispatch(self, *args, **kwargs):\n        return super(LoginView, self).dispatch(*args, **kwargs)\n    def post(self, request, *args, **kwargs):\n        serializer = self.serializer_class(data=request.data, context={'request': request})\n        serializer.is_valid(raise_exception=True)\n        user = serializer.validated_data['user']\n        if user is None:\n            return FormattedResponse(status=HTTP_401_UNAUTHORIZED, d={'reason': 'login_failed'}, m='login_failed')\n        token = providers.get_provider('token').issue_token(user)\n        return FormattedResponse({'token': token})\n    def post(self, request):\n        totp_secret = pyotp.random_base32()\n        request.user.totp_secret = totp_secret\n        request.user.totp_status = TOTPStatus.VERIFYING\n        request.user.save()\n        add_2fa.send(sender=self.__class__, user=request.user)\n        return FormattedResponse({\"totp_secret\": totp_secret})\n    def post(self, request):\n        email = request.data[\"email\"]\n        email_validator = EmailValidator()\n        email_validator(email)\n        try:\n            user = get_user_model().objects.get(email=email)\n            token = PasswordResetToken(user=user, token=secrets.token_hex(64))\n            token.save()\n            uid = user.id\n            token = token.token\n            password_reset_start.send(sender=self.__class__, user=user)\n        except get_user_model().DoesNotExist:\n            password_reset_start_reject.send(sender=self.__class__, email=email)\n            uid = -1\n            token = \"\"\n            email = \"noreply@ractf.co.uk\"\n        send_email(\n            email,\n            \"RACTF - Reset Your Password\",\n            \"password_reset\",\n            url=\"password_reset?id={}&secret={}\".format(uid, token),\n        )\n        return FormattedResponse()\n    def validate(self, data):\n        user = providers.get_provider('login').login_user(**data, context=self.context)\n        if user is not None:\n            data['user'] = user\n        return data\n    def issue_token(self):\n        token, created = Token.objects.get_or_create(user=self)\n        return token.key\n    def is_2fa_enabled(self):\n        return self.totp_status == TOTPStatus.ENABLED\n    def should_deny_admin(self):\n        return self.totp_status != TOTPStatus.ENABLED and config.get(\n            \"enable_force_admin_2fa\"\n        )",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-21329",
        "description": "[{'lang': 'en', 'value': 'RATCF is an open-source framework for hosting Cyber-Security Capture the Flag events. In affected versions of RATCF users with multi factor authentication enabled are able to log in without a valid token. This is fixed in commit cebb67b.'}]",
        "cwe_number": 287
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-135",
      "code": "def extract_ip_or_domain(url):\n    ip_regex = re.compile(\"^(?:http://|https://)(\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3})\")\n    domain_regex = re.compile(\"^(?:http://|https://)([a-zA-Z0-9.-]+)\")\n    match = ip_regex.findall(url)\n    if len(match):\n        ip_address = match[0]\n        try:\n            ipaddress.ip_address(ip_address)\n            return ip_address\n        except ValueError:\n            pass\n    match = domain_regex.findall(url)\n    if len(match):\n        return match[0]\n    return None",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-42439",
        "description": "[{'lang': 'en', 'value': 'GeoNode is an open source platform that facilitates the creation, sharing, and collaborative use of geospatial data. A SSRF vulnerability exists starting in version 3.2.0, bypassing existing controls on the software. This can allow a user to request internal services for a full read SSRF, returning any data from the internal network. The application is using a whitelist, but the whitelist can be bypassed. The bypass will trick the application that the first host is a whitelisted address, but the browser will use `@` or `%40` as a credential to the host geoserver on port 8080, this will return the data to that host on the response. Version 4.1.3.post1 is the first available version that contains a patch.'}]",
        "cwe_number": 918
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-136",
      "code": "async def largest_content_len(urls: list[str]) -> tuple[str, int]:\n    largest_url = \"\"\n    largest_len = 0\n    async def do(client: AsyncClient, url: str) -> Response:\n        return await client.head(url, headers={\"User-Agent\": _FIREFOX_UA})\n    async with AsyncClient() as client:\n        tasks = [do(client, url) for url in urls]\n        responses: list[Response] = await gather_with_concurrency(10, *tasks, ignore_exceptions=True)\n        for response in responses:\n            len_int = int(response.headers.get(\"Content-Length\", 0))\n            if len_int > largest_len:\n                largest_url = str(response.url)\n                largest_len = len_int\n    return largest_url, largest_len\nclass NotAnImageError(Exception):\n    def _validate_image_url(url: str) -> bool:\n        \"\"\"\n        Validates that the URL is of an allowed source and restricts certain sources to prevent\n        malicious images from being downloaded.\n        \"\"\"\n        invalid_domains = {\"127.0.0.1\", \"localhost\"}\n        for domain in invalid_domains:\n            if domain in url:\n                return False\n        return True\n    async def scrape_image(self, image_url) -> None:\n        self.logger.info(f\"Image URL: {image_url}\")\n        if not self._validate_image_url(image_url):\n            self.logger.error(f\"Invalid image URL: {image_url}\")\n            raise InvalidDomainError(f\"Invalid domain: {image_url}\")\n        if isinstance(image_url, str):\n            pass\n        elif isinstance(image_url, list):\n            image_url, _ = await largest_content_len(image_url)\n        elif isinstance(image_url, dict):\n            for key in image_url:\n                if key == \"url\":\n                    image_url = image_url.get(\"url\")\n        ext = image_url.split(\".\")[-1]\n        if ext not in img.IMAGE_EXTENSIONS:\n            ext = \"jpg\"\n        file_name = f\"{str(self.recipe_id)}.{ext}\"\n        file_path = Recipe.directory_from_id(self.recipe_id).joinpath(\"images\", file_name)\n        async with AsyncClient() as client:\n            try:\n                r = await client.get(image_url, headers={\"User-Agent\": _FIREFOX_UA})\n            except Exception:\n                self.logger.exception(\"Fatal Image Request Exception\")\n                return None\n            if r.status_code != 200:\n                return None\n            content_type = r.headers.get(\"content-type\", \"\")\n            if \"image\" not in content_type:\n                self.logger.error(f\"Content-Type: {content_type} is not an image\")\n                raise NotAnImageError(f\"Content-Type {content_type} is not an image\")\n            self.logger.debug(f\"File Name Suffix {file_path.suffix}\")\n            self.write_image(r.read(), file_path.suffix)\n            file_path.unlink(missing_ok=True)\nasync def safe_scrape_html(url: str) -> str:\n    \"\"\"\n    Scrapes the html from a url but will cancel the request\n    if the request takes longer than 15 seconds. This is used to mitigate\n    DDOS attacks from users providing a url with arbitrary large content.\n    \"\"\"\n    async with AsyncClient() as client:\n        html_bytes = b\"\"\n        async with client.stream(\"GET\", url, timeout=SCRAPER_TIMEOUT, headers={\"User-Agent\": _FIREFOX_UA}) as resp:\n            start_time = time.time()\n            async for chunk in resp.aiter_bytes(chunk_size=1024):\n                html_bytes += chunk\n                if time.time() - start_time > SCRAPER_TIMEOUT:\n                    raise ForceTimeoutException()\n        content = None\n        encoding = resp.encoding\n        if not html_bytes:\n            return \"\"\n        if encoding is None:\n            encoding = resp.apparent_encoding\n        try:\n            content = str(html_bytes, encoding, errors=\"replace\")\n        except (LookupError, TypeError):\n            content = str(html_bytes, errors=\"replace\")\n        return content\nasync def create_from_url(url: str, translator: Translator) -> tuple[Recipe, ScrapedExtras | None]:\n    \"\"\"Main entry point for generating a recipe from a URL. Pass in a URL and\n    a Recipe object will be returned if successful.\n    Args:\n        url (str): a valid string representing a URL\n    Returns:\n        Recipe: Recipe Object\n    \"\"\"\n    scraper = RecipeScraper(translator)\n    new_recipe, extras = await scraper.scrape(url)\n    if not new_recipe:\n        raise HTTPException(status.HTTP_400_BAD_REQUEST, {\"details\": ParserErrors.BAD_RECIPE_DATA.value})\n    new_recipe.id = uuid4()\n    logger = get_logger()\n    logger.debug(f\"Image {new_recipe.image}\")\n    recipe_data_service = RecipeDataService(new_recipe.id)\n    try:\n        await recipe_data_service.scrape_image(new_recipe.image)\n        if new_recipe.name is None:\n            new_recipe.name = \"Untitled\"\n        new_recipe.slug = slugify(new_recipe.name)\n        new_recipe.image = cache.new_key(4)\n    except Exception as e:\n        recipe_data_service.logger.exception(f\"Error Scraping Image: {e}\")\n        new_recipe.image = \"no image\"\n    if new_recipe.name is None or new_recipe.name == \"\":\n        new_recipe.name = f\"No Recipe Name Found - {str(uuid4())}\"\n        new_recipe.slug = slugify(new_recipe.name)\n    return new_recipe, extras",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-31994",
        "description": "[{'lang': 'en', 'value': 'Mealie is a self hosted recipe manager and meal planner. Prior to 1.4.0, an attacker can point the image request to an arbitrarily large file. Mealie will attempt to retrieve this file in whole. If it can be retrieved, it may be stored on the file system in whole (leading to possible disk consumption), however the more likely scenario given resource limitations is that the container will OOM during file retrieval if the target file size is greater than the allocated memory of the container. At best this can be used to force the container to infinitely restart due to OOM (if so configured in `docker-compose.yml), or at worst this can be used to force the Mealie container to crash and remain offline. In the event that the file can be retrieved, the lack of rate limiting on this endpoint also permits an attacker to generate ongoing requests to any target of their choice, potentially contributing to an external-facing DoS attack. This vulnerability is fixed in 1.4.0.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-137",
      "code": "    def value(self):\n        \"\"\"Returns a formatted version of the data for final output.\n        This takes into consideration the\n        :attr:`~horizon.tables.Column.link`` and\n        :attr:`~horizon.tables.Column.empty_value`\n        attributes.\n        \"\"\"\n        try:\n            data = self.column.get_data(self.datum)\n            if data is None:\n                if callable(self.column.empty_value):\n                    data = self.column.empty_value(self.datum)\n                else:\n                    data = self.column.empty_value\n        except Exception:\n            data = None\n            exc_info = sys.exc_info()\n            raise template.TemplateSyntaxError, exc_info[1], exc_info[2]\n        if self.url:\n            link_classes = ' '.join(self.column.link_classes)\n            data = mark_safe('<a href=\"%s\" class=\"%s\">%s</a>' %\n                             (self.url, link_classes, escape(unicode(data))))\n        return data",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2014-125078",
        "description": "[{'lang': 'en', 'value': 'A vulnerability was found in yanheven console and classified as problematic. Affected by this issue is some unknown functionality of the file horizon/static/horizon/js/horizon.instances.js. The manipulation leads to cross site scripting. The attack may be launched remotely. The patch is identified as 32a7b713468161282f2ea01d5e2faff980d924cd. It is recommended to apply a patch to fix this issue. VDB-218354 is the identifier assigned to this vulnerability.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-138",
      "code": "    def _create_token(cls, user_id: Optional[str], issued_on: float) -> str:\n        \"\"\"Creates a new CSRF token.\n        Args:\n            user_id: str|None. The user_id for which the token is generated.\n            issued_on: float. The timestamp at which the token was issued.\n        Returns:\n            str. The generated CSRF token.\n        \"\"\"\n        cls.init_csrf_secret()\n        if user_id is None:\n            user_id = cls._USER_ID_DEFAULT\n        issued_on_str = str(int(issued_on))\n        digester = hmac.new(\n            key=CSRF_SECRET.value.encode('utf-8'),\n            digestmod='md5'\n        )\n        digester.update(user_id.encode('utf-8'))\n        digester.update(b':')\n        digester.update(issued_on_str.encode('utf-8'))\n        digest = digester.digest()\n        token = '%s/%s' % (\n            issued_on_str, base64.urlsafe_b64encode(digest).decode('utf-8'))\n        return token\n    def _get_current_time(cls) -> float:\n        \"\"\"Returns the current server time.\n        Returns:\n            float. The time in seconds as floating point number.\n        \"\"\"\n        return time.time()\n    def create_csrf_token(cls, user_id: Optional[str]) -> str:\n        \"\"\"Creates a CSRF token for the given user_id.\n        Args:\n            user_id: str|None. The user_id for whom the token is generated.\n        Returns:\n            str. The generated CSRF token.\n        \"\"\"\n        return cls._create_token(user_id, cls._get_current_time())\n    def is_csrf_token_valid(cls, user_id: Optional[str], token: str) -> bool:\n        \"\"\"Validates a given CSRF token.\n        Args:\n            user_id: str|None. The user_id to validate the CSRF token against.\n            token: str. The CSRF token to validate.\n        Returns:\n            bool. Whether the given CSRF token is valid.\n        \"\"\"\n        try:\n            parts = token.split('/')\n            if len(parts) != 2:\n                return False\n            issued_on = int(parts[0])\n            age = cls._get_current_time() - issued_on\n            if age > cls._CSRF_TOKEN_AGE_SECS:\n                return False\n            authentic_token = cls._create_token(user_id, issued_on)\n            if authentic_token == token:\n                return True\n            return False\n        except Exception:\n            return False\nclass CsrfTokenHandler(BaseHandler[Dict[str, str], Dict[str, str]]):\n    def get(self) -> None:\n        csrf_token = CsrfTokenManager.create_csrf_token(\n            self.user_id)\n        self.render_json({\n            'token': csrf_token,\n        })\nclass OppiaMLVMHandler(\n    BaseHandler[_NormalizedPayloadDictType, _NormalizedRequestDictType]\n):\n    def test_create_and_validate_token(self) -> None:\n        uid = 'user_id'\n        token = base.CsrfTokenManager.create_csrf_token(uid)\n        self.assertTrue(base.CsrfTokenManager.is_csrf_token_valid(\n            uid, token))\n        self.assertFalse(\n            base.CsrfTokenManager.is_csrf_token_valid('bad_user', token))\n        self.assertFalse(\n            base.CsrfTokenManager.is_csrf_token_valid(uid, 'new_token'))\n        self.assertFalse(\n            base.CsrfTokenManager.is_csrf_token_valid(uid, 'new/token'))",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-40021",
        "description": "[{'lang': 'en', 'value': \"Oppia is an online learning platform. When comparing a received CSRF token against the expected token, Oppia uses the string equality operator (`==`), which is not safe against timing attacks. By repeatedly submitting invalid tokens, an attacker can brute-force the expected CSRF token character by character. Once they have recovered the token, they can then submit a forged request on behalf of a logged-in user and execute privileged actions on that user's behalf. In particular the function to validate received CSRF tokens is at `oppia.core.controllers.base.CsrfTokenManager.is_csrf_token_valid`. An attacker who can lure a logged-in Oppia user to a malicious website can perform any change on Oppia that the user is authorized to do, including changing profile information; creating, deleting, and changing explorations; etc. Note that the attacker cannot change a user's login credentials. An attack would need to complete within 1 second because every second, the time used in computing the token changes. This issue has been addressed in commit `b89bf80837` which has been included in release `3.3.2-hotfix-2`. Users are advised to upgrade. There are no known workarounds for this vulnerability.\"}]",
        "cwe_number": 203
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-139",
      "code": "    def run(self):\n        self.sys = sys\n        _active_threads.append(self)\n        tid = hex(long(id(self)) & xffffffff)\n        if self.server_mode:\n            self._log(DEBUG, 'starting thread (server mode): %s' % tid)\n        else:\n            self._log(DEBUG, 'starting thread (client mode): %s' % tid)\n        try:\n            try:\n                self.packetizer.write_all(b(self.local_version + '\\r\\n'))\n                self._log(DEBUG, 'Local version/idstring: %s' % self.local_version)\n                self._check_banner()\n                self.packetizer.start_handshake(self.handshake_timeout)\n                self._send_kex_init()\n                self._expect_packet(MSG_KEXINIT)\n                while self.active:\n                    if self.packetizer.need_rekey() and not self.in_kex:\n                        self._send_kex_init()\n                    try:\n                        ptype, m = self.packetizer.read_message()\n                    except NeedRekeyException:\n                        continue\n                    if ptype == MSG_IGNORE:\n                        continue\n                    elif ptype == MSG_DISCONNECT:\n                        self._parse_disconnect(m)\n                        self.active = False\n                        self.packetizer.close()\n                        break\n                    elif ptype == MSG_DEBUG:\n                        self._parse_debug(m)\n                        continue\n                    if len(self._expected_packet) > 0:\n                        if ptype not in self._expected_packet:\n                            raise SSHException('Expecting packet from %r, got %d' % (self._expected_packet, ptype))\n                        self._expected_packet = tuple()\n                        if (ptype >= 30) and (ptype <= 41):\n                            self.kex_engine.parse_next(ptype, m)\n                            continue\n                    if ptype in self._handler_table:\n                        self._handler_table[ptype](self, m)\n                    elif ptype in self._channel_handler_table:\n                        chanid = m.get_int()\n                        chan = self._channels.get(chanid)\n                        if chan is not None:\n                            self._channel_handler_table[ptype](chan, m)\n                        elif chanid in self.channels_seen:\n                            self._log(DEBUG, 'Ignoring message for dead channel %d' % chanid)\n                        else:\n                            self._log(ERROR, 'Channel request for unknown channel %d' % chanid)\n                            self.active = False\n                            self.packetizer.close()\n                    elif (\n                        self.auth_handler is not None and\n                        ptype in self.auth_handler._handler_table\n                    ):\n                        handler = self.auth_handler._handler_table[ptype]\n                        handler(self.auth_handler, m)\n                        if len(self._expected_packet) > 0:\n                            continue\n                    else:\n                        self._log(WARNING, 'Oops, unhandled type %d' % ptype)\n                        msg = Message()\n                        msg.add_byte(cMSG_UNIMPLEMENTED)\n                        msg.add_int(m.seqno)\n                        self._send_message(msg)\n                    self.packetizer.complete_handshake()\n            except SSHException as e:\n                self._log(ERROR, 'Exception: ' + str(e))\n                self._log(ERROR, util.tb_strings())\n                self.saved_exception = e\n            except EOFError as e:\n                self._log(DEBUG, 'EOF in transport thread')\n                self.saved_exception = e\n            except socket.error as e:\n                if type(e.args) is tuple:\n                    if e.args:\n                        emsg = '%s (%d)' % (e.args[1], e.args[0])\n                    else:\n                        emsg = str(e) or repr(e)\n                else:\n                    emsg = e.args\n                self._log(ERROR, 'Socket exception: ' + emsg)\n                self.saved_exception = e\n            except Exception as e:\n                self._log(ERROR, 'Unknown exception: ' + str(e))\n                self._log(ERROR, util.tb_strings())\n                self.saved_exception = e\n            _active_threads.remove(self)\n            for chan in list(self._channels.values()):\n                chan._unlink()\n            if self.active:\n                self.active = False\n                self.packetizer.close()\n                if self.completion_event is not None:\n                    self.completion_event.set()\n                if self.auth_handler is not None:\n                    self.auth_handler.abort()\n                for event in self.channel_events.values():\n                    event.set()\n                try:\n                    self.lock.acquire()\n                    self.server_accept_cv.notify()\n                finally:\n                    self.lock.release()\n            self.sock.close()\n        except:\n            if self.sys.modules is not None:\n                raise",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2018-7750",
        "description": "[{'lang': 'en', 'value': 'transport.py in the SSH server implementation of Paramiko before 1.17.6, 1.18.x before 1.18.5, 2.0.x before 2.0.8, 2.1.x before 2.1.5, 2.2.x before 2.2.3, 2.3.x before 2.3.2, and 2.4.x before 2.4.1 does not properly check whether authentication is completed before processing other requests, as demonstrated by channel-open. A customized SSH client can simply skip the authentication step.'}]",
        "cwe_number": 287
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-140",
      "code": "def get_markdown(text):\n    if not text:\n        return \"\"\n    pattern = fr'([\\[\\s\\S\\]]*?)\\(([\\s\\S]*?):([\\[\\s\\S\\]]*?)\\)'\n    if re.match(pattern, text):\n        scheme = re.search(pattern, text, re.IGNORECASE).group(2)\n        if scheme in helpdesk_settings.ALLOWED_URL_SCHEMES:\n            replacement = '\\\\1(\\\\2:\\\\3)'\n        else:\n            replacement = '\\\\1(\\\\3)'\n        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n    return mark_safe(\n        markdown(\n            text,\n            extensions=[\n                EscapeHtml(), 'markdown.extensions.nl2br',\n                'markdown.extensions.fenced_code'\n            ]\n        )\n    )",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-3994",
        "description": "[{'lang': 'en', 'value': \"django-helpdesk is vulnerable to Improper Neutralization of Input During Web Page Generation ('Cross-site Scripting')\"}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-141",
      "code": "    def parse_header(self, header_plus):\n        \"\"\"\n        Parses the header_plus block of text (the headers plus the\n        first line of the request).\n        \"\"\"\n        index = header_plus.find(b\"\\r\\n\")\n        if index >= 0:\n            first_line = header_plus[:index].rstrip()\n            header = header_plus[index + 2 :]\n        else:\n            raise ParsingError(\"HTTP message header invalid\")\n        if b\"\\r\" in first_line or b\"\\n\" in first_line:\n            raise ParsingError(\"Bare CR or LF found in HTTP message\")\n        self.first_line = first_line\n        lines = get_header_lines(header)\n        headers = self.headers\n        for line in lines:\n            index = line.find(b\":\")\n            if index > 0:\n                key = line[:index]\n                if key != key.strip():\n                    raise ParsingError(\"Invalid whitespace after field-name\")\n                if b\"_\" in key:\n                    continue\n                value = line[index + 1 :].strip()\n                key1 = tostr(key.upper().replace(b\"-\", b\"_\"))\n                try:\n                    headers[key1] += tostr(b\", \" + value)\n                except KeyError:\n                    headers[key1] = tostr(value)\n        command, uri, version = crack_first_line(first_line)\n        version = tostr(version)\n        command = tostr(command)\n        self.command = command\n        self.version = version\n        (\n            self.proxy_scheme,\n            self.proxy_netloc,\n            self.path,\n            self.query,\n            self.fragment,\n        ) = split_uri(uri)\n        self.url_scheme = self.adj.url_scheme\n        connection = headers.get(\"CONNECTION\", \"\")\n        if version == \"1.0\":\n            if connection.lower() != \"keep-alive\":\n                self.connection_close = True\n        if version == \"1.1\":\n            te = headers.pop(\"TRANSFER_ENCODING\", \"\")\n            if te.lower() == \"chunked\":\n                self.chunked = True\n                buf = OverflowableBuffer(self.adj.inbuf_overflow)\n                self.body_rcv = ChunkedReceiver(buf)\n            expect = headers.get(\"EXPECT\", \"\").lower()\n            self.expect_continue = expect == \"100-continue\"\n            if connection.lower() == \"close\":\n                self.connection_close = True\n        if not self.chunked:\n            try:\n                cl = int(headers.get(\"CONTENT_LENGTH\", 0))\n            except ValueError:\n                cl = 0\n            self.content_length = cl\n            if cl > 0:\n                buf = OverflowableBuffer(self.adj.inbuf_overflow)\n                self.body_rcv = FixedStreamReceiver(cl, buf)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2019-16792",
        "description": "[{'lang': 'en', 'value': 'Waitress through version 1.3.1 allows request smuggling by sending the Content-Length header twice. Waitress would header fold a double Content-Length header and due to being unable to cast the now comma separated value to an integer would set the Content-Length to 0 internally. If two Content-Length headers are sent in a single request, Waitress would treat the request as having no body, thereby treating the body of the request as a new request in HTTP pipelining. This issue is fixed in Waitress 1.4.0.'}]",
        "cwe_number": 444
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-142",
      "code": "    def create_app(\n        blocks: gradio.Blocks, app_kwargs: Dict[str, Any] | None = None\n    ) -> App:\n        app_kwargs = app_kwargs or {}\n        app_kwargs.setdefault(\"default_response_class\", ORJSONResponse)\n        app = App(**app_kwargs)\n        app.configure_app(blocks)\n        if not wasm_utils.IS_WASM:\n            app.add_middleware(\n                CORSMiddleware,\n                allow_origins=[\"*\"],\n                allow_methods=[\"*\"],\n                allow_headers=[\"*\"],\n            )\n        @app.get(\"/user\")\n        @app.get(\"/user/\")\n        def get_current_user(request: fastapi.Request) -> Optional[str]:\n            token = request.cookies.get(\n                f\"access-token-{app.cookie_id}\"\n            ) or request.cookies.get(f\"access-token-unsecure-{app.cookie_id}\")\n            return app.tokens.get(token)\n        @app.get(\"/login_check\")\n        @app.get(\"/login_check/\")\n        def login_check(user: str = Depends(get_current_user)):\n            if app.auth is None or user is not None:\n                return\n            raise HTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Not authenticated\"\n            )\n        @app.get(\"/token\")\n        @app.get(\"/token/\")\n        def get_token(request: fastapi.Request) -> dict:\n            token = request.cookies.get(f\"access-token-{app.cookie_id}\")\n            return {\"token\": token, \"user\": app.tokens.get(token)}\n        @app.get(\"/app_id\")\n        @app.get(\"/app_id/\")\n        def app_id(request: fastapi.Request) -> dict:\n            return {\"app_id\": app.get_blocks().app_id}\n        @app.get(\"/dev/reload\", dependencies=[Depends(login_check)])\n        async def notify_changes(\n            request: fastapi.Request,\n        ):\n            async def reload_checker(request: fastapi.Request):\n                heartbeat_rate = 15\n                check_rate = 0.05\n                last_heartbeat = time.perf_counter()\n                while True:\n                    if await request.is_disconnected():\n                        return\n                    if app.change_event and app.change_event.is_set():\n                        app.change_event.clear()\n                        yield \"\"\"data: CHANGE\\n\\n\"\"\"\n                    await asyncio.sleep(check_rate)\n                    if time.perf_counter() - last_heartbeat > heartbeat_rate:\n                        yield \"\"\"data: HEARTBEAT\\n\\n\"\"\"\n                        last_heartbeat = time.time()\n            return StreamingResponse(\n                reload_checker(request),\n                media_type=\"text/event-stream\",\n            )\n        @app.post(\"/login\")\n        @app.post(\"/login/\")\n        def login(form_data: OAuth2PasswordRequestForm = Depends()):\n            username, password = form_data.username.strip(), form_data.password\n            if app.auth is None:\n                return RedirectResponse(url=\"/\", status_code=status.HTTP_302_FOUND)\n            if (\n                not callable(app.auth)\n                and username in app.auth\n                and app.auth[username] == password\n            ) or (callable(app.auth) and app.auth.__call__(username, password)):\n                token = secrets.token_urlsafe(16)\n                app.tokens[token] = username\n                response = JSONResponse(content={\"success\": True})\n                response.set_cookie(\n                    key=f\"access-token-{app.cookie_id}\",\n                    value=token,\n                    httponly=True,\n                    samesite=\"none\",\n                    secure=True,\n                )\n                response.set_cookie(\n                    key=f\"access-token-unsecure-{app.cookie_id}\",\n                    value=token,\n                    httponly=True,\n                )\n                return response\n            else:\n                raise HTTPException(status_code=400, detail=\"Incorrect credentials.\")\n        if app.blocks is not None and app.blocks.expects_oauth:\n            attach_oauth(app)\n        @app.head(\"/\", response_class=HTMLResponse)\n        @app.get(\"/\", response_class=HTMLResponse)\n        def main(request: fastapi.Request, user: str = Depends(get_current_user)):\n            mimetypes.add_type(\"application/javascript\", \".js\")\n            blocks = app.get_blocks()\n            root_path = (\n                request.scope.get(\"root_path\")\n                or request.headers.get(\"X-Direct-Url\")\n                or \"\"\n            )\n            if app.auth is None or user is not None:\n                config = app.get_blocks().config\n                config[\"root\"] = route_utils.strip_url(root_path)\n            else:\n                config = {\n                    \"auth_required\": True,\n                    \"auth_message\": blocks.auth_message,\n                    \"space_id\": app.get_blocks().space_id,\n                    \"root\": route_utils.strip_url(root_path),\n                }\n            try:\n                template = (\n                    \"frontend/share.html\" if blocks.share else \"frontend/index.html\"\n                )\n                return templates.TemplateResponse(\n                    template,\n                    {\"request\": request, \"config\": config},\n                )\n            except TemplateNotFound as err:\n                if blocks.share:\n                    raise ValueError(\n                        \"Did you install Gradio from source files? Share mode only \"\n                        \"works when Gradio is installed through the pip package.\"\n                    ) from err\n                else:\n                    raise ValueError(\n                        \"Did you install Gradio from source files? You need to build \"\n                        \"the frontend by running /scripts/build_frontend.sh\"\n                    ) from err\n        @app.get(\"/info/\", dependencies=[Depends(login_check)])\n        @app.get(\"/info\", dependencies=[Depends(login_check)])\n        def api_info(serialize: bool = True):\n            return app.get_blocks().get_api_info()\n        @app.get(\"/config/\", dependencies=[Depends(login_check)])\n        @app.get(\"/config\", dependencies=[Depends(login_check)])\n        def get_config(request: fastapi.Request):\n            root_path = (\n                request.scope.get(\"root_path\")\n                or request.headers.get(\"X-Direct-Url\")\n                or \"\"\n            )\n            config = app.get_blocks().config\n            config[\"root\"] = route_utils.strip_url(root_path)\n            return config\n        @app.get(\"/static/{path:path}\")\n        def static_resource(path: str):\n            static_file = safe_join(STATIC_PATH_LIB, path)\n            return FileResponse(static_file)\n        @app.get(\"/custom_component/{id}/{type}/{file_name}\")\n        def custom_component_path(id: str, type: str, file_name: str):\n            config = app.get_blocks().config\n            components = config[\"components\"]\n            location = next(\n                (item for item in components if item[\"component_class_id\"] == id), None\n            )\n            if location is None:\n                raise HTTPException(status_code=404, detail=\"Component not found.\")\n            component_instance = app.get_blocks().get_component(location[\"id\"])\n            module_name = component_instance.__class__.__module__\n            module_path = sys.modules[module_name].__file__\n            if module_path is None or component_instance is None:\n                raise HTTPException(status_code=404, detail=\"Component not found.\")\n            return FileResponse(\n                safe_join(\n                    str(Path(module_path).parent),\n                    f\"{component_instance.__class__.TEMPLATE_DIR}/{type}/{file_name}\",\n                )\n            )\n        @app.get(\"/assets/{path:path}\")\n        def build_resource(path: str):\n            build_file = safe_join(BUILD_PATH_LIB, path)\n            return FileResponse(build_file)\n        @app.get(\"/favicon.ico\")\n        async def favicon():\n            blocks = app.get_blocks()\n            if blocks.favicon_path is None:\n                return static_resource(\"img/logo.svg\")\n            else:\n                return FileResponse(blocks.favicon_path)\n        @app.head(\"/proxy={url_path:path}\", dependencies=[Depends(login_check)])\n        @app.get(\"/proxy={url_path:path}\", dependencies=[Depends(login_check)])\n        async def reverse_proxy(url_path: str):\n            try:\n                rp_req = app.build_proxy_request(url_path)\n            except PermissionError as err:\n                raise HTTPException(status_code=400, detail=str(err)) from err\n            rp_resp = await client.send(rp_req, stream=True)\n            return StreamingResponse(\n                rp_resp.aiter_raw(),\n                status_code=rp_resp.status_code,\n                headers=rp_resp.headers,\n                background=BackgroundTask(rp_resp.aclose),\n            )\n        @app.head(\"/file={path_or_url:path}\", dependencies=[Depends(login_check)])\n        @app.get(\"/file={path_or_url:path}\", dependencies=[Depends(login_check)])\n        async def file(path_or_url: str, request: fastapi.Request):\n            blocks = app.get_blocks()\n            if client_utils.is_http_url_like(path_or_url):\n                return RedirectResponse(\n                    url=path_or_url, status_code=status.HTTP_302_FOUND\n                )\n            abs_path = utils.abspath(path_or_url)\n            in_blocklist = any(\n                utils.is_in_or_equal(abs_path, blocked_path)\n                for blocked_path in blocks.blocked_paths\n            )\n            is_dir = abs_path.is_dir()\n            if in_blocklist or is_dir:\n                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")\n            created_by_app = str(abs_path) in set().union(*blocks.temp_file_sets)\n            in_allowlist = any(\n                utils.is_in_or_equal(abs_path, allowed_path)\n                for allowed_path in blocks.allowed_paths\n            )\n            was_uploaded = utils.is_in_or_equal(abs_path, app.uploaded_file_dir)\n            is_cached_example = utils.is_in_or_equal(\n                abs_path, utils.abspath(utils.get_cache_folder())\n            )\n            if not (\n                created_by_app or in_allowlist or was_uploaded or is_cached_example\n            ):\n                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")\n            if not abs_path.exists():\n                raise HTTPException(404, f\"File not found: {path_or_url}.\")\n            range_val = request.headers.get(\"Range\", \"\").strip()\n            if range_val.startswith(\"bytes=\") and \"-\" in range_val:\n                range_val = range_val[6:]\n                start, end = range_val.split(\"-\")\n                if start.isnumeric() and end.isnumeric():\n                    start = int(start)\n                    end = int(end)\n                    response = ranged_response.RangedFileResponse(\n                        abs_path,\n                        ranged_response.OpenRange(start, end),\n                        dict(request.headers),\n                        stat_result=os.stat(abs_path),\n                    )\n                    return response\n            return FileResponse(abs_path, headers={\"Accept-Ranges\": \"bytes\"})\n        @app.get(\n            \"/stream/{session_hash}/{run}/{component_id}\",\n            dependencies=[Depends(login_check)],\n        )\n        async def stream(\n            session_hash: str, run: int, component_id: int, request: fastapi.Request\n        ):\n            stream: list = (\n                app.get_blocks()\n                .pending_streams[session_hash]\n                .get(run, {})\n                .get(component_id, None)\n            )\n            if stream is None:\n                raise HTTPException(404, \"Stream not found.\")\n            def stream_wrapper():\n                check_stream_rate = 0.01\n                max_wait_time = 120\n                wait_time = 0\n                while True:\n                    if len(stream) == 0:\n                        if wait_time > max_wait_time:\n                            return\n                        wait_time += check_stream_rate\n                        time.sleep(check_stream_rate)\n                        continue\n                    wait_time = 0\n                    next_stream = stream.pop(0)\n                    if next_stream is None:\n                        return\n                    yield next_stream\n            return StreamingResponse(stream_wrapper())\n        @app.get(\"/file/{path:path}\", dependencies=[Depends(login_check)])\n        async def file_deprecated(path: str, request: fastapi.Request):\n            return await file(path, request)\n        @app.post(\"/reset/\")\n        @app.post(\"/reset\")\n        async def reset_iterator(body: ResetBody):\n            if body.event_id not in app.iterators:\n                return {\"success\": False}\n            async with app.lock:\n                del app.iterators[body.event_id]\n                app.iterators_to_reset.add(body.event_id)\n                await app.get_blocks()._queue.clean_events(event_id=body.event_id)\n            return {\"success\": True}\n        @app.post(\"/run/{api_name}\", dependencies=[Depends(login_check)])\n        @app.post(\"/run/{api_name}/\", dependencies=[Depends(login_check)])\n        @app.post(\"/api/{api_name}\", dependencies=[Depends(login_check)])\n        @app.post(\"/api/{api_name}/\", dependencies=[Depends(login_check)])\n        async def predict(\n            api_name: str,\n            body: PredictBody,\n            request: fastapi.Request,\n            username: str = Depends(get_current_user),\n        ):\n            fn_index_inferred = route_utils.infer_fn_index(\n                app=app, api_name=api_name, body=body\n            )\n            if not app.get_blocks().api_open and app.get_blocks().queue_enabled_for_fn(\n                fn_index_inferred\n            ):\n                raise HTTPException(\n                    detail=\"This API endpoint does not accept direct HTTP POST requests. Please join the queue to use this API.\",\n                    status_code=status.HTTP_404_NOT_FOUND,\n                )\n            gr_request = route_utils.compile_gr_request(\n                app,\n                body,\n                fn_index_inferred=fn_index_inferred,\n                username=username,\n                request=request,\n            )\n            try:\n                output = await route_utils.call_process_api(\n                    app=app,\n                    body=body,\n                    gr_request=gr_request,\n                    fn_index_inferred=fn_index_inferred,\n                )\n            except BaseException as error:\n                show_error = app.get_blocks().show_error or isinstance(error, Error)\n                traceback.print_exc()\n                return JSONResponse(\n                    content={\"error\": str(error) if show_error else None},\n                    status_code=500,\n                )\n            return output\n        @app.get(\"/queue/data\", dependencies=[Depends(login_check)])\n        async def queue_data(\n            request: fastapi.Request,\n            session_hash: str,\n        ):\n            blocks = app.get_blocks()\n            async def sse_stream(request: fastapi.Request):\n                try:\n                    last_heartbeat = time.perf_counter()\n                    while True:\n                        if await request.is_disconnected():\n                            await blocks._queue.clean_events(session_hash=session_hash)\n                            return\n                        if (\n                            session_hash\n                            not in blocks._queue.pending_messages_per_session\n                        ):\n                            raise HTTPException(\n                                status_code=status.HTTP_404_NOT_FOUND,\n                                detail=\"Session not found.\",\n                            )\n                        heartbeat_rate = 15\n                        check_rate = 0.05\n                        message = None\n                        try:\n                            messages = blocks._queue.pending_messages_per_session[\n                                session_hash\n                            ]\n                            message = messages.get_nowait()\n                        except EmptyQueue:\n                            await asyncio.sleep(check_rate)\n                            if time.perf_counter() - last_heartbeat > heartbeat_rate:\n                                message = {\n                                    \"msg\": ServerMessage.heartbeat,\n                                }\n                                last_heartbeat = time.perf_counter()\n                        if blocks._queue.stopped:\n                            message = {\n                                \"msg\": \"unexpected_error\",\n                                \"message\": \"Server stopped unexpectedly.\",\n                                \"success\": False,\n                            }\n                        if message:\n                            yield f\"data: {json.dumps(message)}\\n\\n\"\n                            if message[\"msg\"] == ServerMessage.process_completed:\n                                blocks._queue.pending_event_ids_session[\n                                    session_hash\n                                ].remove(message[\"event_id\"])\n                                if message[\"msg\"] == ServerMessage.server_stopped or (\n                                    message[\"msg\"] == ServerMessage.process_completed\n                                    and (\n                                        len(\n                                            blocks._queue.pending_event_ids_session[\n                                                session_hash\n                                            ]\n                                        )\n                                        == 0\n                                    )\n                                ):\n                                    return\n                except BaseException as e:\n                    message = {\n                        \"msg\": \"unexpected_error\",\n                        \"success\": False,\n                        \"message\": str(e),\n                    }\n                    yield f\"data: {json.dumps(message)}\\n\\n\"\n                    if isinstance(e, asyncio.CancelledError):\n                        del blocks._queue.pending_messages_per_session[session_hash]\n                        await blocks._queue.clean_events(session_hash=session_hash)\n                    raise e\n            return StreamingResponse(\n                sse_stream(request),\n                media_type=\"text/event-stream\",\n            )\n        @app.post(\"/queue/join\", dependencies=[Depends(login_check)])\n        async def queue_join(\n            body: PredictBody,\n            request: fastapi.Request,\n            username: str = Depends(get_current_user),\n        ):\n            if blocks._queue.server_app is None:\n                blocks._queue.set_server_app(app)\n            if blocks._queue.stopped:\n                raise HTTPException(\n                    status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n                    detail=\"Queue is stopped.\",\n                )\n            success, event_id = await blocks._queue.push(body, request, username)\n            if not success:\n                status_code = (\n                    status.HTTP_503_SERVICE_UNAVAILABLE\n                    if \"Queue is full.\" in event_id\n                    else status.HTTP_400_BAD_REQUEST\n                )\n                raise HTTPException(status_code=status_code, detail=event_id)\n            return {\"event_id\": event_id}\n        @app.post(\"/component_server\", dependencies=[Depends(login_check)])\n        @app.post(\"/component_server/\", dependencies=[Depends(login_check)])\n        def component_server(body: ComponentServerBody):\n            state = app.state_holder[body.session_hash]\n            component_id = body.component_id\n            block: Block\n            if component_id in state:\n                block = state[component_id]\n            else:\n                block = app.get_blocks().blocks[component_id]\n            fn = getattr(block, body.fn_name)\n            return fn(body.data)\n        @app.get(\n            \"/queue/status\",\n            dependencies=[Depends(login_check)],\n            response_model=Estimation,\n        )\n        async def get_queue_status():\n            return app.get_blocks()._queue.get_status()\n        @app.get(\"/upload_progress\")\n        def get_upload_progress(upload_id: str, request: fastapi.Request):\n            async def sse_stream(request: fastapi.Request):\n                last_heartbeat = time.perf_counter()\n                is_done = False\n                while True:\n                    if await request.is_disconnected():\n                        file_upload_statuses.stop_tracking(upload_id)\n                        return\n                    if is_done:\n                        file_upload_statuses.stop_tracking(upload_id)\n                        return\n                    heartbeat_rate = 15\n                    check_rate = 0.05\n                    message = None\n                    try:\n                        if update := file_upload_statuses.status(upload_id).popleft():\n                            if update.is_done:\n                                message = {\"msg\": \"done\"}\n                                is_done = True\n                            else:\n                                message = {\n                                    \"msg\": \"update\",\n                                    \"orig_name\": update.filename,\n                                    \"chunk_size\": update.chunk_size,\n                                }\n                        else:\n                            await asyncio.sleep(check_rate)\n                            if time.perf_counter() - last_heartbeat > heartbeat_rate:\n                                message = {\"msg\": \"heartbeat\"}\n                                last_heartbeat = time.perf_counter()\n                        if message:\n                            yield f\"data: {json.dumps(message)}\\n\\n\"\n                    except IndexError:\n                        if not file_upload_statuses.is_tracked(upload_id):\n                            return\n                        continue\n            return StreamingResponse(\n                sse_stream(request),\n                media_type=\"text/event-stream\",\n            )\n        @app.post(\"/upload\", dependencies=[Depends(login_check)])\n        async def upload_file(\n            request: fastapi.Request,\n            bg_tasks: BackgroundTasks,\n            upload_id: Optional[str] = None,\n        ):\n            content_type_header = request.headers.get(\"Content-Type\")\n            content_type: bytes\n            content_type, _ = parse_options_header(content_type_header)\n            if content_type != b\"multipart/form-data\":\n                raise HTTPException(status_code=400, detail=\"Invalid content type.\")\n            try:\n                if upload_id:\n                    file_upload_statuses.track(upload_id)\n                multipart_parser = GradioMultiPartParser(\n                    request.headers,\n                    request.stream(),\n                    max_files=1000,\n                    max_fields=1000,\n                    upload_id=upload_id if upload_id else None,\n                    upload_progress=file_upload_statuses if upload_id else None,\n                )\n                form = await multipart_parser.parse()\n            except MultiPartException as exc:\n                raise HTTPException(status_code=400, detail=exc.message) from exc\n            output_files = []\n            files_to_copy = []\n            locations: list[str] = []\n            for temp_file in form.getlist(\"files\"):\n                assert isinstance(temp_file, GradioUploadFile)\n                if temp_file.filename:\n                    file_name = Path(temp_file.filename).name\n                    name = client_utils.strip_invalid_filename_characters(file_name)\n                else:\n                    name = f\"tmp{secrets.token_hex(5)}\"\n                directory = Path(app.uploaded_file_dir) / temp_file.sha.hexdigest()\n                directory.mkdir(exist_ok=True, parents=True)\n                dest = (directory / name).resolve()\n                temp_file.file.close()\n                try:\n                    os.rename(temp_file.file.name, dest)\n                except OSError:\n                    files_to_copy.append(temp_file.file.name)\n                    locations.append(str(dest))\n                output_files.append(dest)\n            if files_to_copy:\n                bg_tasks.add_task(\n                    move_uploaded_files_to_cache, files_to_copy, locations\n                )\n            return output_files\n        @app.on_event(\"startup\")\n        @app.get(\"/startup-events\")\n        async def startup_events():\n            if not app.startup_events_triggered:\n                app.get_blocks().startup_events()\n                app.startup_events_triggered = True\n                return True\n            return False\n        @app.get(\"/theme.css\", response_class=PlainTextResponse)\n        def theme_css():\n            return PlainTextResponse(app.get_blocks().theme_css, media_type=\"text/css\")\n        @app.get(\"/robots.txt\", response_class=PlainTextResponse)\n        def robots_txt():\n            if app.get_blocks().share:\n                return \"User-agent: *\\nDisallow: /\"\n            else:\n                return \"User-agent: *\\nDisallow: \"\n        return app\n    def ls(self, value=None) -> list[dict[str, str]] | None:\n        \"\"\"\n        Parameters:\n            value: file path as a list of strings for each directory level relative to the root.\n        Returns:\n            tuple of list of files in directory, then list of folders in directory\n        \"\"\"\n        def expand_braces(text, seen=None):\n            if seen is None:\n                seen = set()\n            spans = [m.span() for m in re.finditer(\"{[^{}]*}\", text)][::-1]\n            alts = [text[start + 1 : stop - 1].split(\",\") for start, stop in spans]\n            if len(spans) == 0:\n                if text not in seen:\n                    yield text\n                seen.add(text)\n            else:\n                for combo in itertools.product(*alts):\n                    replaced = list(text)\n                    for (start, stop), replacement in zip(spans, combo):\n                        replaced[start:stop] = replacement\n                    yield from expand_braces(\"\".join(replaced), seen)\n        def make_tree(files):\n            tree = []\n            for file in files:\n                parts = file.split(os.path.sep)\n                make_node(parts, tree)\n            return tree\n        def make_node(parts, tree):\n            _tree = tree\n            for i in range(len(parts)):\n                if _tree is None:\n                    continue\n                if i == len(parts) - 1:\n                    type = \"file\"\n                    _tree.append({\"path\": parts[i], \"type\": type, \"children\": None})\n                    continue\n                type = \"folder\"\n                j = next(\n                    (index for (index, v) in enumerate(_tree) if v[\"path\"] == parts[i]),\n                    None,\n                )\n                if j is not None:\n                    _tree = _tree[j][\"children\"]\n                else:\n                    _tree.append({\"path\": parts[i], \"type\": type, \"children\": []})\n                    _tree = _tree[-1][\"children\"]\n        files: list[Path] = []\n        for result in expand_braces(self.glob):\n            files += list(Path(self.root).resolve().glob(result))\n        files = [f for f in files if f != Path(self.root).resolve()]\n        ignore_files = []\n        if self.ignore_glob:\n            for result in expand_braces(self.ignore_glob):\n                ignore_files += list(Path(self.root).resolve().glob(result))\n            files = list(set(files) - set(ignore_files))\n        files_with_sep = []\n        for f in files:\n            file = str(f.relative_to(self.root))\n            if f.is_dir():\n                file += os.path.sep\n            files_with_sep.append(file)\n        tree = make_tree(files_with_sep)\n        return tree",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-1561",
        "description": "[{'lang': 'en', 'value': 'An issue was discovered in gradio-app/gradio, where the `/component_server` endpoint improperly allows the invocation of any method on a `Component` class with attacker-controlled arguments. Specifically, by exploiting the `move_resource_to_block_cache()` method of the `Block` class, an attacker can copy any file on the filesystem to a temporary directory and subsequently retrieve it. This vulnerability enables unauthorized local file read access, posing a significant risk especially when the application is exposed to the internet via `launch(share=True)`, thereby allowing remote attackers to read files on the host machine. Furthermore, gradio apps hosted on `huggingface.co` are also affected, potentially leading to the exposure of sensitive information such as API keys and credentials stored in environment variables.'}]",
        "cwe_number": 29
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-143",
      "code": "def generate_auth_token(user_id):\n    host_list = request.host.rsplit(':')\n    if len(host_list) == 1:\n        host = ':'.join(host_list)\n    else:\n        host = ':'.join(host_list[0:-1])\n    if host.startswith('127.') or host.lower() == 'localhost' or host.startswith('[::ffff:7f'):\n        warning = _('PLease access calibre-web from non localhost to get valid api_endpoint for kobo device')\n        return render_title_template(\n            \"generate_kobo_auth_url.html\",\n            title=_(u\"Kobo Setup\"),\n            warning = warning\n        )\n    else:\n        auth_token = ub.session.query(ub.RemoteAuthToken).filter(\n            ub.RemoteAuthToken.user_id == user_id\n        ).filter(ub.RemoteAuthToken.token_type==1).first()\n        if not auth_token:\n            auth_token = ub.RemoteAuthToken()\n            auth_token.user_id = user_id\n            auth_token.expiration = datetime.max\n            auth_token.auth_token = (hexlify(urandom(16))).decode(\"utf-8\")\n            auth_token.token_type = 1\n            ub.session.add(auth_token)\n            ub.session_commit()\n        books = calibre_db.session.query(db.Books).join(db.Data).all()\n        for book in books:\n            formats = [data.format for data in book.data]\n            if not 'KEPUB' in formats and config.config_kepubifypath and 'EPUB' in formats:\n                helper.convert_book_format(book.id, config.config_calibre_dir, 'EPUB', 'KEPUB', current_user.name)\n        return render_title_template(\n            \"generate_kobo_auth_url.html\",\n            title=_(u\"Kobo Setup\"),\n            kobo_auth_url=url_for(\n                \"kobo.TopLevelEndpoint\", auth_token=auth_token.auth_token, _external=True\n            ),\n            warning = False\n        )\ndef delete_auth_token(user_id):\n    ub.session.query(ub.RemoteAuthToken).filter(ub.RemoteAuthToken.user_id == user_id)\\\n        .filter(ub.RemoteAuthToken.token_type==1).delete()\n    return ub.session_commit()\ndef _delete_user(content):\n    if ub.session.query(ub.User).filter(ub.User.role.op('&')(constants.ROLE_ADMIN) == constants.ROLE_ADMIN,\n                                        ub.User.id != content.id).count():\n        if content.name != \"Guest\":\n            ub.session.query(ub.ReadBook).filter(content.id == ub.ReadBook.user_id).delete()\n            ub.session.query(ub.Downloads).filter(content.id == ub.Downloads.user_id).delete()\n            for us in ub.session.query(ub.Shelf).filter(content.id == ub.Shelf.user_id):\n                ub.session.query(ub.BookShelf).filter(us.id == ub.BookShelf.shelf).delete()\n            ub.session.query(ub.Shelf).filter(content.id == ub.Shelf.user_id).delete()\n            ub.session.query(ub.Bookmark).filter(content.id == ub.Bookmark.user_id).delete()\n            ub.session.query(ub.User).filter(ub.User.id == content.id).delete()\n            ub.session.query(ub.ArchivedBook).filter(ub.ArchivedBook.user_id == content.id).delete()\n            ub.session.query(ub.RemoteAuthToken).filter(ub.RemoteAuthToken.user_id == content.id).delete()\n            ub.session.query(ub.User_Sessions).filter(ub.User_Sessions.user_id == content.id).delete()\n            ub.session.query(ub.KoboSyncedBooks).filter(ub.KoboSyncedBooks.user_id == content.id).delete()\n            kobo_entries = ub.session.query(ub.KoboReadingState).filter(ub.KoboReadingState.user_id == content.id).all()\n            for kobo_entry in kobo_entries:\n                ub.session.delete(kobo_entry)\n            ub.session_commit()\n            log.info(u\"User {} deleted\".format(content.name))\n            return(_(u\"User '%(nick)s' deleted\", nick=content.name))\n        else:\n            log.warning(_(u\"Can't delete Guest User\"))\n            raise Exception(_(u\"Can't delete Guest User\"))\n    else:\n        log.warning(u\"No admin user remaining, can't delete user\")\n        raise Exception(_(u\"No admin user remaining, can't delete user\"))",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-0405",
        "description": "[{'lang': 'en', 'value': 'Improper Access Control in GitHub repository janeczku/calibre-web prior to 0.6.16.'}]",
        "cwe_number": 284
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-144",
      "code": "    def __init__(\n        self,\n        agent: IAgent,\n        ip_whitelist: Optional[IPSet] = None,\n        ip_blacklist: Optional[IPSet] = None,\n    ):\n        \"\"\"\n        Args:\n            agent: The Agent to wrap.\n            ip_whitelist: IP addresses to allow.\n            ip_blacklist: IP addresses to disallow.\n        \"\"\"\n        self._agent = agent\n        self._ip_whitelist = ip_whitelist\n        self._ip_blacklist = ip_blacklist\n    def request(\n        self,\n        method: bytes,\n        uri: bytes,\n        headers: Optional[Headers] = None,\n        bodyProducer: Optional[IBodyProducer] = None,\n    ) -> defer.Deferred:\n        h = urllib.parse.urlparse(uri.decode(\"ascii\"))\n        try:\n            ip_address = IPAddress(h.hostname)\n            if check_against_blacklist(\n                ip_address, self._ip_whitelist, self._ip_blacklist\n            ):\n                logger.info(\"Blocking access to %s due to blacklist\" % (ip_address,))\n                e = SynapseError(403, \"IP address blocked by IP blacklist entry\")\n                return defer.fail(Failure(e))\n        except Exception:\n            pass\n        return self._agent.request(\n            method, uri, headers=headers, bodyProducer=bodyProducer\n        )\n    def __init__(\n        self,\n        hs: \"HomeServer\",\n        treq_args: Dict[str, Any] = {},\n        ip_whitelist: Optional[IPSet] = None,\n        ip_blacklist: Optional[IPSet] = None,\n        http_proxy: Optional[bytes] = None,\n        https_proxy: Optional[bytes] = None,\n    ):\n        \"\"\"\n        Args:\n            hs\n            treq_args: Extra keyword arguments to be given to treq.request.\n            ip_blacklist: The IP addresses that are blacklisted that\n                we may not request.\n            ip_whitelist: The whitelisted IP addresses, that we can\n               request if it were otherwise caught in a blacklist.\n            http_proxy: proxy server to use for http connections. host[:port]\n            https_proxy: proxy server to use for https connections. host[:port]\n        \"\"\"\n        self.hs = hs\n        self._ip_whitelist = ip_whitelist\n        self._ip_blacklist = ip_blacklist\n        self._extra_treq_args = treq_args\n        self.user_agent = hs.version_string\n        self.clock = hs.get_clock()\n        if hs.config.user_agent_suffix:\n            self.user_agent = \"%s %s\" % (self.user_agent, hs.config.user_agent_suffix)\n        self._cooperator = Cooperator(scheduler=_make_scheduler(hs.get_reactor()))\n        self.user_agent = self.user_agent.encode(\"ascii\")\n        if self._ip_blacklist:\n            real_reactor = hs.get_reactor()\n            nameResolver = IPBlacklistingResolver(\n                real_reactor, self._ip_whitelist, self._ip_blacklist\n            )\n            @implementer(IReactorPluggableNameResolver)\n            class Reactor:\n                def __getattr__(_self, attr):\n                    if attr == \"nameResolver\":\n                        return nameResolver\n                    else:\n                        return getattr(real_reactor, attr)\n            self.reactor = Reactor()\n        else:\n            self.reactor = hs.get_reactor()\n        pool = HTTPConnectionPool(self.reactor)\n        pool.maxPersistentPerHost = max((100 * hs.config.caches.global_factor, 5))\n        pool.cachedConnectionTimeout = 2 * 60\n        self.agent = ProxyAgent(\n            self.reactor,\n            connectTimeout=15,\n            contextFactory=self.hs.get_http_client_context_factory(),\n            pool=pool,\n            http_proxy=http_proxy,\n            https_proxy=https_proxy,\n        )\n        if self._ip_blacklist:\n            self.agent = BlacklistingAgentWrapper(\n                self.agent,\n                ip_whitelist=self._ip_whitelist,\n                ip_blacklist=self._ip_blacklist,\n            )\n    async def request(\n        self,\n        method: str,\n        uri: str,\n        data: Optional[bytes] = None,\n        headers: Optional[Headers] = None,\n    ) -> IResponse:\n        \"\"\"\n        Args:\n            method: HTTP method to use.\n            uri: URI to query.\n            data: Data to send in the request body, if applicable.\n            headers: Request headers.\n        Returns:\n            Response object, once the headers have been read.\n        Raises:\n            RequestTimedOutError if the request times out before the headers are read\n        \"\"\"\n        outgoing_requests_counter.labels(method).inc()\n        logger.debug(\"Sending request %s %s\", method, redact_uri(uri))\n        with start_active_span(\n            \"outgoing-client-request\",\n            tags={\n                tags.SPAN_KIND: tags.SPAN_KIND_RPC_CLIENT,\n                tags.HTTP_METHOD: method,\n                tags.HTTP_URL: uri,\n            },\n            finish_on_close=True,\n        ):\n            try:\n                body_producer = None\n                if data is not None:\n                    body_producer = QuieterFileBodyProducer(\n                        BytesIO(data), cooperator=self._cooperator,\n                    )\n                request_deferred = treq.request(\n                    method,\n                    uri,\n                    agent=self.agent,\n                    data=body_producer,\n                    headers=headers,\n                    **self._extra_treq_args,\n                )\n                request_deferred = timeout_deferred(\n                    request_deferred, 60, self.hs.get_reactor(),\n                )\n                request_deferred.addErrback(_timeout_to_request_timed_out_error)\n                response = await make_deferred_yieldable(request_deferred)\n                incoming_responses_counter.labels(method, response.code).inc()\n                logger.info(\n                    \"Received response to %s %s: %s\",\n                    method,\n                    redact_uri(uri),\n                    response.code,\n                )\n                return response\n            except Exception as e:\n                incoming_responses_counter.labels(method, \"ERR\").inc()\n                logger.info(\n                    \"Error sending request to  %s %s: %s %s\",\n                    method,\n                    redact_uri(uri),\n                    type(e).__name__,\n                    e.args[0],\n                )\n                set_tag(tags.ERROR, True)\n                set_tag(\"error_reason\", e.args[0])\n                raise\n    def __init__(\n        self,\n        reactor: IReactorCore,\n        tls_client_options_factory: Optional[FederationPolicyForHTTPS],\n        user_agent: bytes,\n        _srv_resolver: Optional[SrvResolver] = None,\n        _well_known_resolver: Optional[WellKnownResolver] = None,\n    ):\n        self._reactor = reactor\n        self._clock = Clock(reactor)\n        self._pool = HTTPConnectionPool(reactor)\n        self._pool.retryAutomatically = False\n        self._pool.maxPersistentPerHost = 5\n        self._pool.cachedConnectionTimeout = 2 * 60\n        self._agent = Agent.usingEndpointFactory(\n            self._reactor,\n            MatrixHostnameEndpointFactory(\n                reactor, tls_client_options_factory, _srv_resolver\n            ),\n            pool=self._pool,\n        )\n        self.user_agent = user_agent\n        if _well_known_resolver is None:\n            _well_known_resolver = WellKnownResolver(\n                self._reactor,\n                agent=Agent(\n                    self._reactor,\n                    pool=self._pool,\n                    contextFactory=tls_client_options_factory,\n                ),\n                user_agent=self.user_agent,\n            )\n        self._well_known_resolver = _well_known_resolver\n    def request(\n        self,\n        method: bytes,\n        uri: bytes,\n        headers: Optional[Headers] = None,\n        bodyProducer: Optional[IBodyProducer] = None,\n    ) -> defer.Deferred:\n        \"\"\"\n        Args:\n            method: HTTP method: GET/POST/etc\n            uri: Absolute URI to be retrieved\n            headers:\n                HTTP headers to send with the request, or None to send no extra headers.\n            bodyProducer:\n                An object which can generate bytes to make up the\n                body of this request (for example, the properly encoded contents of\n                a file for a file upload).  Or None if the request is to have\n                no body.\n        Returns:\n            Deferred[twisted.web.iweb.IResponse]:\n                fires when the header of the response has been received (regardless of the\n                response status code). Fails if there is any problem which prevents that\n                response from being received (including problems that prevent the request\n                from being sent).\n        \"\"\"\n        parsed_uri = urllib.parse.urlparse(uri)\n        assert parsed_uri.hostname\n        delegated_server = None\n        if (\n            parsed_uri.scheme == b\"matrix\"\n            and not _is_ip_literal(parsed_uri.hostname)\n            and not parsed_uri.port\n        ):\n            well_known_result = yield defer.ensureDeferred(\n                self._well_known_resolver.get_well_known(parsed_uri.hostname)\n            )\n            delegated_server = well_known_result.delegated_server\n        if delegated_server:\n            uri = urllib.parse.urlunparse(\n                (\n                    parsed_uri.scheme,\n                    delegated_server,\n                    parsed_uri.path,\n                    parsed_uri.params,\n                    parsed_uri.query,\n                    parsed_uri.fragment,\n                )\n            )\n            parsed_uri = urllib.parse.urlparse(uri)\n        if headers is None:\n            headers = Headers()\n        else:\n            headers = headers.copy()\n        if not headers.hasHeader(b\"host\"):\n            headers.addRawHeader(b\"host\", parsed_uri.netloc)\n        if not headers.hasHeader(b\"user-agent\"):\n            headers.addRawHeader(b\"user-agent\", self.user_agent)\n        res = yield make_deferred_yieldable(\n            self._agent.request(method, uri, headers, bodyProducer)\n        )\n        return res\n    def __init__(self, hs: \"HomeServer\"):\n        super().__init__(hs)\n        self.hs = hs\n        self.store = hs.get_datastore()\n        self.storage = hs.get_storage()\n        self.state_store = self.storage.state\n        self.federation_client = hs.get_federation_client()\n        self.state_handler = hs.get_state_handler()\n        self._state_resolution_handler = hs.get_state_resolution_handler()\n        self.server_name = hs.hostname\n        self.keyring = hs.get_keyring()\n        self.action_generator = hs.get_action_generator()\n        self.is_mine_id = hs.is_mine_id\n        self.spam_checker = hs.get_spam_checker()\n        self.event_creation_handler = hs.get_event_creation_handler()\n        self._message_handler = hs.get_message_handler()\n        self._server_notices_mxid = hs.config.server_notices_mxid\n        self.config = hs.config\n        self.http_client = hs.get_simple_http_client()\n        self._instance_name = hs.get_instance_name()\n        self._replication = hs.get_replication_data_handler()\n        self._send_events = ReplicationFederationSendEventsRestServlet.make_client(hs)\n        self._clean_room_for_join_client = ReplicationCleanRoomRestServlet.make_client(\n            hs\n        )\n        if hs.config.worker_app:\n            self._user_device_resync = ReplicationUserDevicesResyncRestServlet.make_client(\n                hs\n            )\n            self._maybe_store_room_on_outlier_membership = ReplicationStoreRoomOnOutlierMembershipRestServlet.make_client(\n                hs\n            )\n        else:\n            self._device_list_updater = hs.get_device_handler().device_list_updater\n            self._maybe_store_room_on_outlier_membership = (\n                self.store.maybe_store_room_on_outlier_membership\n            )\n        self.room_queues = {}\n        self._room_pdu_linearizer = Linearizer(\"fed_room_pdu\")\n        self.third_party_event_rules = hs.get_third_party_event_rules()\n        self._ephemeral_messages_enabled = hs.config.enable_ephemeral_messages\n    def read_config(self, config, **kwargs):\n        self.federation_domain_whitelist = None\n        federation_domain_whitelist = config.get(\"federation_domain_whitelist\", None)\n        if federation_domain_whitelist is not None:\n            self.federation_domain_whitelist = {}\n            for domain in federation_domain_whitelist:\n                self.federation_domain_whitelist[domain] = True\n        self.federation_ip_range_blacklist = config.get(\n            \"federation_ip_range_blacklist\", []\n        )\n        try:\n            self.federation_ip_range_blacklist = IPSet(\n                self.federation_ip_range_blacklist\n            )\n            self.federation_ip_range_blacklist.update([\"0.0.0.0\", \"::\"])\n        except Exception as e:\n            raise ConfigError(\n                \"Invalid range(s) provided in federation_ip_range_blacklist: %s\" % e\n            )\n        federation_metrics_domains = config.get(\"federation_metrics_domains\") or []\n        validate_config(\n            _METRICS_FOR_DOMAINS_SCHEMA,\n            federation_metrics_domains,\n            (\"federation_metrics_domains\",),\n        )\n        self.federation_metrics_domains = set(federation_metrics_domains)\n    def generate_config_section(self, config_dir_path, server_name, **kwargs):\n        return \"\"\"\\\n        federation_ip_range_blacklist:\n          - '127.0.0.0/8'\n          - '10.0.0.0/8'\n          - '172.16.0.0/12'\n          - '192.168.0.0/16'\n          - '100.64.0.0/10'\n          - '169.254.0.0/16'\n          - '::1/128'\n          - 'fe80::/64'\n          - 'fc00::/7'\n        \"\"\"\n_METRICS_FOR_DOMAINS_SCHEMA = {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n    def __init__(self, hs, tls_client_options_factory):\n        self.hs = hs\n        self.signing_key = hs.signing_key\n        self.server_name = hs.hostname\n        real_reactor = hs.get_reactor()\n        nameResolver = IPBlacklistingResolver(\n            real_reactor, None, hs.config.federation_ip_range_blacklist\n        )\n        @implementer(IReactorPluggableNameResolver)\n        class Reactor:\n            def __getattr__(_self, attr):\n                if attr == \"nameResolver\":\n                    return nameResolver\n                else:\n                    return getattr(real_reactor, attr)\n        self.reactor = Reactor()\n        user_agent = hs.version_string\n        if hs.config.user_agent_suffix:\n            user_agent = \"%s %s\" % (user_agent, hs.config.user_agent_suffix)\n        user_agent = user_agent.encode(\"ascii\")\n        self.agent = MatrixFederationAgent(\n            self.reactor, tls_client_options_factory, user_agent\n        )\n        self.agent = BlacklistingAgentWrapper(\n            self.agent, ip_blacklist=hs.config.federation_ip_range_blacklist,\n        )\n        self.clock = hs.get_clock()\n        self._store = hs.get_datastore()\n        self.version_string_bytes = hs.version_string.encode(\"ascii\")\n        self.default_timeout = 60\n        def schedule(x):\n            self.reactor.callLater(_EPSILON, x)\n        self._cooperator = Cooperator(scheduler=schedule)\n    def __init__(self, hs, pusherdict):\n        self.hs = hs\n        self.store = self.hs.get_datastore()\n        self.storage = self.hs.get_storage()\n        self.clock = self.hs.get_clock()\n        self.state_handler = self.hs.get_state_handler()\n        self.user_id = pusherdict[\"user_name\"]\n        self.app_id = pusherdict[\"app_id\"]\n        self.app_display_name = pusherdict[\"app_display_name\"]\n        self.device_display_name = pusherdict[\"device_display_name\"]\n        self.pushkey = pusherdict[\"pushkey\"]\n        self.pushkey_ts = pusherdict[\"ts\"]\n        self.data = pusherdict[\"data\"]\n        self.last_stream_ordering = pusherdict[\"last_stream_ordering\"]\n        self.backoff_delay = HttpPusher.INITIAL_BACKOFF_SEC\n        self.failing_since = pusherdict[\"failing_since\"]\n        self.timed_call = None\n        self._is_processing = False\n        self._group_unread_count_by_room = hs.config.push_group_unread_count_by_room\n        self.max_stream_ordering = None\n        if \"data\" not in pusherdict:\n            raise PusherConfigException(\"No 'data' key for HTTP pusher\")\n        self.data = pusherdict[\"data\"]\n        self.name = \"%s/%s/%s\" % (\n            pusherdict[\"user_name\"],\n            pusherdict[\"app_id\"],\n            pusherdict[\"pushkey\"],\n        )\n        if self.data is None:\n            raise PusherConfigException(\"data can not be null for HTTP pusher\")\n        if \"url\" not in self.data:\n            raise PusherConfigException(\"'url' required in data for HTTP pusher\")\n        self.url = self.data[\"url\"]\n        self.http_client = hs.get_proxied_http_client()\n        self.data_minus_url = {}\n        self.data_minus_url.update(self.data)\n        del self.data_minus_url[\"url\"]\n    def __init__(self, hs):\n        self.hs = hs\n        self.auth = hs.get_auth()\n        self.client = hs.get_http_client()\n        self.clock = hs.get_clock()\n        self.server_name = hs.hostname\n        self.store = hs.get_datastore()\n        self.max_upload_size = hs.config.max_upload_size\n        self.max_image_pixels = hs.config.max_image_pixels\n        self.primary_base_path = hs.config.media_store_path\n        self.filepaths = MediaFilePaths(self.primary_base_path)\n        self.dynamic_thumbnails = hs.config.dynamic_thumbnails\n        self.thumbnail_requirements = hs.config.thumbnail_requirements\n        self.remote_media_linearizer = Linearizer(name=\"media_remote\")\n        self.recently_accessed_remotes = set()\n        self.recently_accessed_locals = set()\n        self.federation_domain_whitelist = hs.config.federation_domain_whitelist\n        storage_providers = []\n        for clz, provider_config, wrapper_config in hs.config.media_storage_providers:\n            backend = clz(hs, provider_config)\n            provider = StorageProviderWrapper(\n                backend,\n                store_local=wrapper_config.store_local,\n                store_remote=wrapper_config.store_remote,\n                store_synchronous=wrapper_config.store_synchronous,\n            )\n            storage_providers.append(provider)\n        self.media_storage = MediaStorage(\n            self.hs, self.primary_base_path, self.filepaths, storage_providers\n        )\n        self.clock.looping_call(\n            self._start_update_recently_accessed, UPDATE_RECENTLY_ACCESSED_TS\n        )\n    def __init__(self, hs):\n        super().__init__(hs)\n        self.http_client = SimpleHttpClient(hs)\n        self.blacklisting_http_client = SimpleHttpClient(\n            hs, ip_blacklist=hs.config.federation_ip_range_blacklist\n        )\n        self.federation_http_client = hs.get_http_client()\n        self.hs = hs\n    def get_simple_http_client(self) -> SimpleHttpClient:\n        return SimpleHttpClient(self)\n    def get_proxied_http_client(self) -> SimpleHttpClient:\n        return SimpleHttpClient(\n            self,\n            http_proxy=os.getenvb(b\"http_proxy\"),\n            https_proxy=os.getenvb(b\"HTTPS_PROXY\"),\n        )\n    def get_room_creation_handler(self) -> RoomCreationHandler:\n        return RoomCreationHandler(self)\n    def get_sendmail(self) -> sendmail:\n        return sendmail\n    def get_state_handler(self) -> StateHandler:\n        return StateHandler(self)\n    def get_state_resolution_handler(self) -> StateResolutionHandler:\n        return StateResolutionHandler(self)\n    def get_presence_handler(self) -> PresenceHandler:\n        return PresenceHandler(self)\n    def get_typing_handler(self):\n        if self.config.worker.writers.typing == self.get_instance_name():\n            return TypingWriterHandler(self)\n        else:\n            return FollowerTypingHandler(self)\n    def __init__(self, hs):\n        super().__init__(hs)\n        self.clock = hs.get_clock()\n        self.client = hs.get_http_client()\n        self.key_servers = self.config.key_servers\n    def __init__(self, hs):\n        self.server_name = hs.hostname\n        self.client = hs.get_http_client()",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-21273",
        "description": "[{'lang': 'en', 'value': 'Synapse is a Matrix reference homeserver written in python (pypi package matrix-synapse). Matrix is an ecosystem for open federated Instant Messaging and VoIP. In Synapse before version 1.25.0, requests to user provided domains were not restricted to external IP addresses when calculating the key validity for third-party invite events and sending push notifications. This could cause Synapse to make requests to internal infrastructure. The type of request was not controlled by the user, although limited modification of request bodies was possible. For the most thorough protection server administrators should remove the deprecated `federation_ip_range_blacklist` from their settings after upgrading to Synapse v1.25.0 which will result in Synapse using the improved default IP address restrictions. See the new `ip_range_blacklist` and `ip_range_whitelist` settings if more specific control is necessary.'}]",
        "cwe_number": 601
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-145",
      "code": "def special_args(\n    fn: Callable,\n    inputs: list[Any] | None = None,\n    request: routes.Request | None = None,\n    event_data: EventData | None = None,\n) -> tuple[list, int | None, int | None]:\n    \"\"\"\n    Checks if function has special arguments Request or EventData (via annotation) or Progress (via default value).\n    If inputs is provided, these values will be loaded into the inputs array.\n    Parameters:\n        fn: function to check.\n        inputs: array to load special arguments into.\n        request: request to load into inputs.\n        event_data: event-related data to load into inputs.\n    Returns:\n        updated inputs, progress index, event data index.\n    \"\"\"\n    try:\n        signature = inspect.signature(fn)\n    except ValueError:\n        return inputs or [], None, None\n    type_hints = utils.get_type_hints(fn)\n    positional_args = []\n    for param in signature.parameters.values():\n        if param.kind not in (param.POSITIONAL_ONLY, param.POSITIONAL_OR_KEYWORD):\n            break\n        positional_args.append(param)\n    progress_index = None\n    event_data_index = None\n    for i, param in enumerate(positional_args):\n        type_hint = type_hints.get(param.name)\n        if isinstance(param.default, Progress):\n            progress_index = i\n            if inputs is not None:\n                inputs.insert(i, param.default)\n        elif type_hint == routes.Request:\n            if inputs is not None:\n                inputs.insert(i, request)\n        elif type_hint in (\n            Optional[oauth.OAuthProfile],\n            Optional[oauth.OAuthToken],\n            oauth.OAuthProfile,\n            oauth.OAuthToken,\n        ):\n            if inputs is not None:\n                session = (\n                    getattr(request, \"session\", {})\n                    or\n                    getattr(getattr(request, \"request\", None), \"session\", {})\n                )\n                if type_hint in (Optional[oauth.OAuthProfile], oauth.OAuthProfile):\n                    oauth_profile = (\n                        session[\"oauth_info\"][\"userinfo\"]\n                        if \"oauth_info\" in session\n                        else None\n                    )\n                    if oauth_profile is not None:\n                        oauth_profile = oauth.OAuthProfile(oauth_profile)\n                    elif type_hint == oauth.OAuthProfile:\n                        raise Error(\n                            \"This action requires a logged in user. Please sign in and retry.\"\n                        )\n                    inputs.insert(i, oauth_profile)\n                elif type_hint in (Optional[oauth.OAuthToken], oauth.OAuthToken):\n                    oauth_info = session.get(\"oauth_info\", None)\n                    oauth_token = (\n                        oauth.OAuthToken(\n                            token=oauth_info[\"access_token\"],\n                            scope=oauth_info[\"scope\"],\n                            expires_at=oauth_info[\"expires_at\"],\n                        )\n                        if oauth_info is not None\n                        else None\n                    )\n                    if oauth_token is None and type_hint == oauth.OAuthToken:\n                        raise Error(\n                            \"This action requires a logged in user. Please sign in and retry.\"\n                        )\n                    inputs.insert(i, oauth_token)\n        elif (\n            type_hint\n            and inspect.isclass(type_hint)\n            and issubclass(type_hint, EventData)\n        ):\n            event_data_index = i\n            if inputs is not None and event_data is not None:\n                inputs.insert(i, type_hint(event_data.target, event_data._data))\n        elif (\n            param.default is not param.empty and inputs is not None and len(inputs) <= i\n        ):\n            inputs.insert(i, param.default)\n    if inputs is not None:\n        while len(inputs) < len(positional_args):\n            i = len(inputs)\n            param = positional_args[i]\n            if param.default == param.empty:\n                warnings.warn(\"Unexpected argument. Filling with None.\")\n                inputs.append(None)\n            else:\n                inputs.append(param.default)\n    return inputs or [], progress_index, event_data_index\ndef move_files_to_cache(\n    data: Any,\n    block: Block,\n    postprocess: bool = False,\n    check_in_upload_folder=False,\n    keep_in_cache=False,\n):\n    \"\"\"Move any files in `data` to cache and (optionally), adds URL prefixes (/file=...) needed to access the cached file.\n    Also handles the case where the file is on an external Gradio app (/proxy=...).\n    Runs after .postprocess() and before .preprocess().\n    Args:\n        data: The input or output data for a component. Can be a dictionary or a dataclass\n        block: The component whose data is being processed\n        postprocess: Whether its running from postprocessing\n        check_in_upload_folder: If True, instead of moving the file to cache, checks if the file is in already in cache (exception if not).\n        keep_in_cache: If True, the file will not be deleted from cache when the server is shut down.\n    \"\"\"\n    def _move_to_cache(d: dict):\n        payload = FileData(**d)\n        if payload.url and postprocess and client_utils.is_http_url_like(payload.url):\n            payload.path = payload.url\n        elif utils.is_static_file(payload):\n            pass\n        elif not block.proxy_url:\n            if check_in_upload_folder and not client_utils.is_http_url_like(\n                payload.path\n            ):\n                path = os.path.abspath(payload.path)\n                if not is_in_or_equal(path, get_upload_folder()):\n                    raise ValueError(\n                        f\"File {path} is not in the upload folder and cannot be accessed.\"\n                    )\n            if not payload.is_stream:\n                temp_file_path = block.move_resource_to_block_cache(payload.path)\n                if temp_file_path is None:\n                    raise ValueError(\"Did not determine a file path for the resource.\")\n                payload.path = temp_file_path\n                if keep_in_cache:\n                    block.keep_in_cache.add(payload.path)\n        url_prefix = \"/stream/\" if payload.is_stream else \"/file=\"\n        if block.proxy_url:\n            proxy_url = block.proxy_url.rstrip(\"/\")\n            url = f\"/proxy={proxy_url}{url_prefix}{payload.path}\"\n        elif client_utils.is_http_url_like(payload.path) or payload.path.startswith(\n            f\"{url_prefix}\"\n        ):\n            url = payload.path\n        else:\n            url = f\"{url_prefix}{payload.path}\"\n        payload.url = url\n        return payload.model_dump()\n    if isinstance(data, (GradioRootModel, GradioModel)):\n        data = data.model_dump()\n    return client_utils.traverse(data, _move_to_cache, client_utils.is_file_obj)\n    def postprocess(self, value: dict | list | str | None) -> dict | list | None:\n        \"\"\"\n        Parameters:\n            value: Expects a valid JSON `str` -- or a `list` or `dict` that can be serialized to a JSON string. The `list` or `dict` value can contain numpy arrays.\n        Returns:\n            Returns the JSON as a `list` or `dict`.\n        \"\"\"\n        if value is None:\n            return None\n        if isinstance(value, str):\n            return orjson.loads(value)\n        else:\n            return orjson.loads(\n                orjson.dumps(\n                    value,\n                    option=orjson.OPT_SERIALIZE_NUMPY | orjson.OPT_PASSTHROUGH_DATETIME,\n                    default=str,\n                )\n            )\n    def example_payload(self) -> Any:\n        return {\"foo\": \"bar\"}\n    def flag(self, payload: Any, flag_dir: str | Path = \"\") -> str:\n        \"\"\"\n        Write the component's value to a format that can be stored in a csv or jsonl format for flagging.\n        \"\"\"\n        if self.data_model:\n            payload = self.data_model.from_json(payload)\n            Path(flag_dir).mkdir(exist_ok=True)\n            payload = payload.copy_to_dir(flag_dir).model_dump()\n        if not isinstance(payload, str):\n            payload = json.dumps(payload)\n        return payload\n    def read_from_flag(self, payload: Any):\n        \"\"\"\n        Convert the data from the csv or jsonl file into the component state.\n        \"\"\"\n        if self.data_model:\n            return self.data_model.from_json(json.loads(payload))\n        return payload",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-4941",
        "description": "[{'lang': 'en', 'value': 'A local file inclusion vulnerability exists in the JSON component of gradio-app/gradio version 4.25. The vulnerability arises from improper input validation in the `postprocess()` function within `gradio/components/json_component.py`, where a user-controlled string is parsed as JSON. If the parsed JSON object contains a `path` key, the specified file is moved to a temporary directory, making it possible to retrieve it later via the `/file=..` endpoint. This issue is due to the `processing_utils.move_files_to_cache()` function traversing any object passed to it, looking for a dictionary with a `path` key, and then copying the specified file to a temporary directory. The vulnerability can be exploited by an attacker to read files on the remote system, posing a significant security risk.'}]",
        "cwe_number": 20
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-146",
      "code": "def get_parser():\n    parser = configargparse.ArgumentParser(\n        prog='rdiffweb',\n        description='Web interface to browse and restore rdiff-backup repositories.',\n        default_config_files=['/etc/rdiffweb/rdw.conf', '/etc/rdiffweb/rdw.conf.d/*.conf'],\n        add_env_var_help=True,\n        auto_env_var_prefix='RDIFFWEB_',\n        config_file_parser_class=ConfigFileParser,\n        conflict_handler='resolve',\n    )\n    parser.add_argument(\n        '-f', '--config', is_config_file=True, metavar='FILE', help='location of Rdiffweb configuration file'\n    )\n    parser.add(\n        '--database-uri',\n        '--sqlitedb-file',\n        '--sqlitedbfile',\n        metavar='URI',\n        help=\"\"\"Location of the database used for persistence. SQLite and PostgreSQL\n            database are supported officially. To use a SQLite database you may\n            define the location using a file path or a URI.\n            e.g.: /srv/rdiffweb/file.db or sqlite:///srv/rdiffweb/file.db`.\n            To use PostgreSQL server you must provide\n            a URI similar to postgresql://user:pass@10.255.1.34/dbname and you\n            must install required dependencies.\n            By default, Rdiffweb uses a SQLite embedded database located at\n            /etc/rdiffweb/rdw.db.\"\"\",\n        default='/etc/rdiffweb/rdw.db',\n    )\n    parser.add_argument(\n        '-d',\n        '--debug',\n        action='store_true',\n        help='enable rdiffweb debug mode - change the log level to DEBUG, print exception stack trace to the web interface and show SQL query in logs',\n    )\n    parser.add_argument(\n        '--admin-user',\n        '--adminuser',\n        metavar='USERNAME',\n        help='administrator username. The administrator user get created on startup if the database is empty.',\n        default='admin',\n    )\n    parser.add_argument(\n        '--admin-password',\n        metavar='USERNAME',\n        help=\"\"\"administrator encrypted password as SSHA. Read online\n            documentation to know more about how to encrypt your password\n            into SSHA or use http://projects.marsching.org/weave4j/util/genpassword.php\n            When defined, administrator password cannot be updated using the web interface.\n            When undefined, default administrator password is `admin123` and\n            it can be updated using the web interface.\"\"\",\n    )\n    parser.add_argument(\n        '--default-theme',\n        '--defaulttheme',\n        help='define the default theme. Either: default, blue or orange. Define the CSS file to be loaded in the web interface. You may manually edit a CSS file to customize it. The location is similar to `/usr/local/lib/python3.9/dist-packages/rdiffweb/static/`',\n        choices=['default', 'blue', 'orange'],\n        default='default',\n    )\n    parser.add_argument(\n        '--environment',\n        choices=['development', 'production'],\n        help='define the type of environment: development, production. This is used to limit the information shown to the user when an error occur.',\n        default='production',\n    )\n    parser.add_argument(\n        '--email-encryption',\n        '--emailencryption',\n        choices=['none', 'ssl', 'starttls'],\n        help='type of encryption to be used when establishing communication with SMTP server. Default: none',\n        default='none',\n    )\n    parser.add_argument(\n        '--email-host',\n        '--emailhost',\n        metavar='HOST',\n        help='SMTP server used to send email in the form <host>:<port>. If the port is not provided, default to standard port 25 or 465 is used. e.g.: smtp.gmail.com:587',\n    )\n    parser.add_argument(\n        '--email-sender',\n        '--emailsender',\n        metavar='EMAIL',\n        help='email addres used for the `from:` field when sending email.',\n    )\n    parser.add_argument(\n        '--email-notification-time',\n        '--emailnotificationtime',\n        metavar='TIME',\n        help='time when the email notifcation should be sent for inactive backups. e.g.: 22:00 Default value: 23:00',\n        default='23:00',\n    )\n    parser.add_argument(\n        '--email-username',\n        '--emailusername',\n        metavar='USERNAME',\n        help='username used for authentication with the SMTP server.',\n    )\n    parser.add_argument(\n        '--email-password',\n        '--emailpassword',\n        metavar='PASSWORD',\n        help='password used for authentication with the SMTP server.',\n    )\n    parser.add_argument(\n        '--email-send-changed-notification',\n        '--emailsendchangednotification',\n        help='True to send notification when sensitive information get change in user profile.',\n        action='store_true',\n        default=False,\n    )\n    parser.add_argument(\n        '--favicon',\n        help='location of an icon to be used as a favicon displayed in web browser.',\n        default=pkg_resources.resource_filename('rdiffweb', 'static/favicon.ico'),\n    )\n    parser.add_argument(\n        '--footer-name', '--footername', help=argparse.SUPPRESS, default='rdiffweb'\n    )\n    parser.add_argument(\n        '--footer-url', '--footerurl', help=argparse.SUPPRESS, default='https://rdiffweb.org/'\n    )\n    parser.add_argument(\n        '--header-logo',\n        '--headerlogo',\n        help='location of an image (preferably a .png) to be used as a replacement for the rdiffweb logo.',\n    )\n    parser.add_argument(\n        '--header-name',\n        '--headername',\n        help='application name displayed in the title bar and header menu.',\n        default='Rdiffweb',\n    )\n    parser.add_argument(\n        '--ldap-add-missing-user',\n        '--addmissinguser',\n        action='store_true',\n        help='enable creation of users from LDAP when the credential are valid.',\n        default=False,\n    )\n    parser.add_argument(\n        '--ldap-add-user-default-role',\n        help='default role used when creating users from LDAP. This parameter is only useful when `--ldap-add-missing-user` is enabled.',\n        default='user',\n        choices=['admin', 'maintainer', 'user'],\n    )\n    parser.add_argument(\n        '--ldap-add-user-default-userroot',\n        help='default user root directory used when creating users from LDAP. LDAP attributes may be used to define the default location. e.g.: `/backups/{uid[0]}/`. This parameter is only useful when `--ldap-add-missing-user` is enabled.',\n        default='',\n    )\n    parser.add_argument(\n        '--ldap-uri',\n        '--ldapuri',\n        help='URL to the LDAP server used to validate user credentials. e.g.: ldap://localhost:389',\n    )\n    parser.add_argument(\n        '--ldap-base-dn',\n        '--ldapbasedn',\n        metavar='DN',\n        help='DN of the branch of the directory where all searches should start from. e.g.: dc=my,dc=domain',\n        default=\"\",\n    )\n    parser.add_argument(\n        '--ldap-scope',\n        '--ldapscope',\n        help='scope of the search. Can be either base, onelevel or subtree',\n        choices=['base', 'onelevel', 'subtree'],\n        default=\"subtree\",\n    )\n    parser.add_argument('--ldap-tls', '--ldaptls', action='store_true', help='enable TLS')\n    parser.add_argument(\n        '--ldap-username-attribute',\n        '--ldapattribute',\n        metavar='ATTRIBUTE',\n        help=\"The attribute to search username. If no attributes are provided, the default is to use `uid`. It's a good idea to choose an attribute that will be unique across all entries in the subtree you will be using.\",\n        default='uid',\n    )\n    parser.add_argument(\n        '--ldap-filter',\n        '--ldapfilter',\n        help=\"search filter to limit LDAP lookup. If not provided, defaults to (objectClass=*), which searches for all objects in the tree.\",\n        default='(objectClass=*)',\n    )\n    parser.add_argument(\n        '--ldap-required-group',\n        '--ldaprequiredgroup',\n        metavar='GROUPNAME',\n        help=\"name of the group of which the user must be a member to access rdiffweb. Should be used with ldap-group-attribute and ldap-group-attribute-is-dn.\",\n    )\n    parser.add_argument(\n        '--ldap-group-attribute',\n        '--ldapgroupattribute',\n        metavar='ATTRIBUTE',\n        help=\"name of the attribute defining the groups of which the user is a member. Should be used with ldap-required-group and ldap-group-attribute-is-dn.\",\n        default='member',\n    )\n    parser.add_argument(\n        '--ldap-group-attribute-is-dn',\n        '--ldapgroupattributeisdn',\n        help=\"True if the content of the attribute `ldap-group-attribute` is a DN.\",\n        action='store_true',\n    )\n    parser.add_argument(\n        '--ldap-bind-dn',\n        '--ldapbinddn',\n        metavar='DN',\n        help=\"optional DN used to bind to the server when searching for entries. If not provided, will use an anonymous bind.\",\n        default=\"\",\n    )\n    parser.add_argument(\n        '--ldap-bind-password',\n        '--ldapbindpassword',\n        metavar='PASSWORD',\n        help=\"password to use in conjunction with LdapBindDn. Note that the bind password is probably sensitive data, and should be properly protected. You should only use the LdapBindDn and LdapBindPassword if you absolutely need them to search the directory.\",\n        default=\"\",\n    )\n    parser.add_argument(\n        '--ldap-version',\n        '--ldapversion',\n        '--ldapprotocolversion',\n        help=\"version of LDAP in use either 2 or 3. Default to 3.\",\n        default=3,\n        type=int,\n        choices=[2, 3],\n    )\n    parser.add_argument(\n        '--ldap-network-timeout',\n        '--ldapnetworktimeout',\n        metavar='SECONDS',\n        help=\"timeout in seconds value used for LDAP connection\",\n        default=100,\n        type=int,\n    )\n    parser.add_argument(\n        '--ldap-timeout',\n        '--ldaptimeout',\n        metavar='SECONDS',\n        help=\"timeout in seconds value used for LDAP request\",\n        default=300,\n        type=int,\n    )\n    parser.add_argument(\n        '--ldap-encoding',\n        '--ldapencoding',\n        metavar='ENCODING',\n        help=\"encoding used by your LDAP server.\",\n        default=\"utf-8\",\n    )\n    parser.add_argument(\n        '--log-access-file', '--logaccessfile', metavar='FILE', help='location of Rdiffweb log access file.'\n    )\n    parser.add_argument(\n        '--log-file',\n        '--logfile',\n        metavar='FILE',\n        help='location of Rdiffweb log file. Print log to the console if not define in config file.',\n    )\n    parser.add_argument(\n        '--log-level',\n        '--loglevel',\n        help='Define the log level.',\n        choices=['ERROR', 'WARN', 'INFO', 'DEBUG'],\n        default='INFO',\n    )\n    parser.add_argument(\n        '--max-depth',\n        '--maxdepth',\n        metavar='DEPTH',\n        help=\"define the maximum folder depthness to search into the user's root directory to find repositories. This is commonly used if you repositories are organised with multiple sub-folder.\",\n        type=int,\n        default=3,\n    )\n    parser.add('--quota-set-cmd', '--quotasetcmd', metavar='COMMAND', help=\"command line to set the user's quota.\")\n    parser.add('--quota-get-cmd', '--quotagetcmd', metavar='COMMAND', help=\"command line to get the user's quota.\")\n    parser.add(\n        '--quota-used-cmd', '--quotausedcmd', metavar='COMMAND', help=\"Command line to get user's quota disk usage.\"\n    )\n    parser.add(\n        '--remove-older-time',\n        '--removeoldertime',\n        metavar='TIME',\n        help=\"Time when to execute the remove older scheduled job. e.g.: 22:30\",\n        default='23:00',\n    )\n    parser.add('--server-host', '--serverhost', metavar='IP', default='127.0.0.1', help='IP address to listen to')\n    parser.add(\n        '--server-port',\n        '--serverport',\n        metavar='PORT',\n        help='port to listen to for HTTP request',\n        default='8080',\n        type=int,\n    )\n    parser.add(\n        '--rate-limit-dir',\n        '--session-dir',\n        '--sessiondir',\n        metavar='FOLDER',\n        help='location where to store rate-limit information. When undefined, the data is kept in memory. `--session-dir` are deprecated and kept for backward compatibility.',\n    )\n    parser.add(\n        '--rate-limit',\n        metavar='LIMIT',\n        type=int,\n        default=20,\n        help='maximum number of requests per hour that can be made on sensitive endpoints. When this limit is reached, an HTTP 429 message is returned to the user or the user is logged out. This security measure is used to limit brute force attacks on the login page and the RESTful API.',\n    )\n    parser.add(\n        '--session-idle-timeout',\n        metavar='MINUTES',\n        help='This timeout defines the amount of time a session will remain active in case there is no activity in the session. User Session will be revoke after this period of inactivity, unless the user selected \"remember me\". Default 5 minutes.',\n        default=5,\n    )\n    parser.add(\n        '--session-absolute-timeout',\n        metavar='MINUTES',\n        help='This timeout defines the maximum amount of time a session can be active. After this period, user is forced to (re)authenticate, unless the user selected \"remember me\". Default 20 minutes.',\n        default=20,\n    )\n    parser.add(\n        '--session-persistent-timeout',\n        metavar='MINUTES',\n        help='This timeout defines the maximum amount of time to remember and trust a user device. This timeout is used when user select \"remember me\". Default 30 days.',\n        default=43200,\n    )\n    parser.add(\n        '--ssl-certificate',\n        '--sslcertificate',\n        metavar='CERT',\n        help='location of the SSL Certification to enable HTTPS (not recommended)',\n    )\n    parser.add(\n        '--ssl-private-key',\n        '--sslprivatekey',\n        metavar='KEY',\n        help='location of the SSL Private Key to enable HTTPS (not recommended)',\n    )\n    parser.add(\n        '--tempdir',\n        metavar='FOLDER',\n        help='alternate temporary folder to be used when restoring files. Might be useful if the default location has limited disk space. Default to TEMPDIR environment or `/tmp`.',\n    )\n    parser.add(\n        '--disable-ssh-keys',\n        action='store_true',\n        help='used to hide SSH Key management to avoid users to add or remove SSH Key using the web application',\n        default=False,\n    )\n    parser.add(\n        '--password-min-length',\n        type=int,\n        help=\"Minimum length of the user's password\",\n        default=8,\n    )\n    parser.add(\n        '--password-max-length',\n        type=int,\n        help=\"Maximum length of the user's password\",\n        default=128,\n    )\n    parser.add(\n        '--password-score',\n        type=lambda x: max(1, min(int(x), 4)),\n        help=\"Minimum zxcvbn's score for password. Value from 1 to 4. Default value 2. Read more about it here: https://github.com/dropbox/zxcvbn\",\n        default=2,\n    )\n    parser.add_argument('--version', action='version', version='%(prog)s ' + VERSION)\n    flags = ['--welcome-msg'] + ['--welcome-msg-' + i for i in ['ca', 'en', 'es', 'fr', 'ru']] + ['--welcomemsg']\n    parser.add_argument(\n        *flags,\n        metavar='HTML',\n        help='replace the welcome message displayed in the login page for default locale or for a specific locale',\n        action=LocaleAction\n    )\n    return parser\n    def add_user(cls, username, password=None, **attrs):\n        \"\"\"\n        Used to add a new user with an optional password.\n        \"\"\"\n        assert password is None or isinstance(password, str)\n        if UserObject.get_user(username):\n            raise ValueError(_(\"User %s already exists.\" % (username,)))\n        logger.info(\"adding new user [%s]\", username)\n        userobj = UserObject(\n            username=username,\n            hash_password=hash_password(password) if password else '',\n            **attrs,\n        ).add()\n        cherrypy.engine.publish('user_added', userobj)\n        return userobj\n    def username(self):\n        return self._username\n    def username(self, value):\n        oldvalue = self._username\n        self._username = value\n        if oldvalue != value:\n            cherrypy.engine.publish('user_attr_changed', self, {'username': (oldvalue, value)})\n    def role(self):\n        if self._role is None:\n            return self.USER_ROLE\n        return self._role\n    def role(self, value):\n        oldvalue = self._role\n        self._role = value\n        if oldvalue != value:\n            cherrypy.engine.publish('user_attr_changed', self, {'role': (oldvalue, value)})\n    def email(self):\n        return self._email\n    def email(self, value):\n        oldvalue = self._email\n        self._email = value\n        if oldvalue != value:\n            cherrypy.engine.publish('user_attr_changed', self, {'email': (oldvalue, value)})\n    def user_root(self):\n        return self._user_root\n    def user_root(self, value):\n        oldvalue = self._user_root\n        self._user_root = value\n        if oldvalue != value:\n            cherrypy.engine.publish('user_attr_changed', self, {'user_root': (oldvalue, value)})\n    def user_attr_changed(self, userobj, attrs={}):\n        if not self.send_changed:\n            return\n        if 'email' not in attrs:\n            return\n        old_email = attrs['email'][0]\n        if not old_email:\n            logger.info(\"can't sent mail to user [%s] without an email\", userobj.username)\n            return\n        body = self.app.templates.compile_template(\n            \"email_changed.html\", **{\"header_name\": self.app.cfg.header_name, 'user': userobj}\n        )\n        self.bus.publish('queue_mail', to=old_email, subject=_(\"Email address changed\"), message=body)\n    def send_code(self):\n        userobj = cherrypy.serving.request.currentuser\n        if not userobj.email:\n            flash(\n                _(\n                    \"Multi-factor authentication is enabled for your account, but your account does not have a valid email address to send the verification code to. Check your account settings with your administrator.\"\n                )\n            )\n        else:\n            code = cherrypy.tools.auth_mfa.generate_code()\n            body = self.app.templates.compile_template(\n                \"email_mfa.html\", **{\"header_name\": self.app.cfg.header_name, 'user': userobj, 'code': code}\n            )\n            cherrypy.engine.publish('queue_mail', to=userobj.email, subject=_(\"Your verification code\"), message=body)\n            flash(_(\"A new verification code has been sent to your email.\"))\n    def send_code(self):\n        userobj = self.app.currentuser\n        if not userobj.email:\n            flash(_(\"To continue, you must set up an email address for your account.\"), level='warning')\n            return\n        code = cherrypy.tools.auth_mfa.generate_code()\n        body = self.app.templates.compile_template(\n            \"email_mfa.html\", **{\"header_name\": self.app.cfg.header_name, 'user': userobj, 'code': code}\n        )\n        cherrypy.engine.publish('queue_mail', to=userobj.email, subject=_(\"Your verification code\"), message=body)\n        flash(_(\"A new verification code has been sent to your email.\"))",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-3363",
        "description": "[{'lang': 'en', 'value': 'Business Logic Errors in GitHub repository ikus060/rdiffweb prior to 2.5.0a7.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-147",
      "code": "    def _sqrt(x):\n        if isinstance(x, complex) or x < 0:\n            return cmath.sqrt(x)\n        else:\n            return math.sqrt(x)\n    def _cbrt(x):\n        return math.pow(x, 1.0/3)\n    def _factorial(x):\n        if x<=10000:\n            return float(math.factorial(x))\n        else:\n            raise Exception('factorial argument too large')\n    def calc(self, irc, msg, args, text):\n        \"\"\"<math expression>\n        Returns the value of the evaluated <math expression>.  The syntax is\n        Python syntax; the type of arithmetic is floating point.  Floating\n        point arithmetic is used in order to prevent a user from being able to\n        crash to the bot with something like '10**10**10**10'.  One consequence\n        is that large values such as '10**24' might not be exact.\n        \"\"\"\n        try:\n            text = str(text)\n        except UnicodeEncodeError:\n            irc.error(_(\"There's no reason you should have fancy non-ASCII \"\n                            \"characters in your mathematical expression. \"\n                            \"Please remove them.\"))\n            return\n        if self._calc_match_forbidden_chars.match(text):\n            irc.error(_('There\\'s really no reason why you should have '\n                           'underscores or brackets in your mathematical '\n                           'expression.  Please remove them.'))\n            return\n        text = self._calc_remover(text)\n        if 'lambda' in text:\n            irc.error(_('You can\\'t use lambda in this command.'))\n            return\n        text = text.lower()\n        def handleMatch(m):\n            s = m.group(1)\n            if s.startswith('0x'):\n                i = int(s, 16)\n            elif s.startswith('0') and '.' not in s:\n                try:\n                    i = int(s, 8)\n                except ValueError:\n                    i = int(s)\n            else:\n                i = float(s)\n            x = complex(i)\n            if x.imag == 0:\n                x = x.real\n                return '%.16f' % x\n            return str(x)\n        text = self._mathRe.sub(handleMatch, text)\n        try:\n            self.log.info('evaluating %q from %s', text, msg.prefix)\n            x = complex(eval(text, self._mathSafeEnv, self._mathSafeEnv))\n            irc.reply(self._complexToString(x))\n        except OverflowError:\n            maxFloat = math.ldexp(0.9999999999999999, 1024)\n            irc.error(_('The answer exceeded %s or so.') % maxFloat)\n        except TypeError:\n            irc.error(_('Something in there wasn\\'t a valid number.'))\n        except NameError as e:\n            irc.error(_('%s is not a defined function.') % str(e).split()[1])\n        except Exception as e:\n            irc.error(str(e))\n    def icalc(self, irc, msg, args, text):\n        \"\"\"<math expression>\n        This is the same as the calc command except that it allows integer\n        math, and can thus cause the bot to suck up CPU.  Hence it requires\n        the 'trusted' capability to use.\n        \"\"\"\n        if self._calc_match_forbidden_chars.match(text):\n            irc.error(_('There\\'s really no reason why you should have '\n                           'underscores or brackets in your mathematical '\n                           'expression.  Please remove them.'))\n            return\n        text = self._calc_remover(text)\n        if 'lambda' in text:\n            irc.error(_('You can\\'t use lambda in this command.'))\n            return\n        text = text.replace('lambda', '')\n        try:\n            self.log.info('evaluating %q from %s', text, msg.prefix)\n            irc.reply(str(eval(text, self._mathEnv, self._mathEnv)))\n        except OverflowError:\n            maxFloat = math.ldexp(0.9999999999999999, 1024)\n            irc.error(_('The answer exceeded %s or so.') % maxFloat)\n        except TypeError:\n            irc.error(_('Something in there wasn\\'t a valid number.'))\n        except NameError as e:\n            irc.error(_('%s is not a defined function.') % str(e).split()[1])\n        except Exception as e:\n            irc.error(utils.exnToString(e))\n    def rpn(self, irc, msg, args):\n        \"\"\"<rpn math expression>\n        Returns the value of an RPN expression.\n        \"\"\"\n        stack = []\n        for arg in args:\n            try:\n                x = complex(arg)\n                if x == abs(x):\n                    x = abs(x)\n                stack.append(x)\n            except ValueError:\n                if arg in self._mathSafeEnv:\n                    f = self._mathSafeEnv[arg]\n                    if callable(f):\n                        called = False\n                        arguments = []\n                        while not called and stack:\n                            arguments.append(stack.pop())\n                            try:\n                                stack.append(f(*arguments))\n                                called = True\n                            except TypeError:\n                                pass\n                        if not called:\n                            irc.error(_('Not enough arguments for %s') % arg)\n                            return\n                    else:\n                        stack.append(f)\n                elif arg in self._rpnEnv:\n                    self._rpnEnv[arg](stack)\n                else:\n                    arg2 = stack.pop()\n                    arg1 = stack.pop()\n                    s = '%s%s%s' % (arg1, arg, arg2)\n                    try:\n                        stack.append(eval(s, self._mathSafeEnv, self._mathSafeEnv))\n                    except SyntaxError:\n                        irc.error(format(_('%q is not a defined function.'),\n                                         arg))\n                        return\n        if len(stack) == 1:\n            irc.reply(str(self._complexToString(complex(stack[0]))))\n        else:\n            s = ', '.join(map(self._complexToString, list(map(complex, stack))))\n            irc.reply(_('Stack: [%s]') % s)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2019-19010",
        "description": "[{'lang': 'en', 'value': 'Eval injection in the Math plugin of Limnoria (before 2019.11.09) and Supybot (through 2018-05-09) allows remote unprivileged attackers to disclose information or possibly have unspecified other impact via the calc and icalc IRC commands.'}]",
        "cwe_number": 94
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-148",
      "code": "    def run(self):\n        '''\n        Launch the Ansible task configured in self.config (A RunnerConfig object), returns once the\n        invocation is complete\n        '''\n        password_patterns = []\n        password_values = []\n        self.status_callback('starting')\n        stdout_filename = os.path.join(self.config.artifact_dir, 'stdout')\n        command_filename = os.path.join(self.config.artifact_dir, 'command')\n        stderr_filename = os.path.join(self.config.artifact_dir, 'stderr')\n        try:\n            os.makedirs(self.config.artifact_dir, mode=0o700)\n        except OSError as exc:\n            if exc.errno == errno.EEXIST and os.path.isdir(self.config.artifact_dir):\n                pass\n            else:\n                raise\n        os.close(os.open(stdout_filename, os.O_CREAT, stat.S_IRUSR | stat.S_IWUSR))\n        job_events_path = os.path.join(self.config.artifact_dir, 'job_events')\n        if not os.path.exists(job_events_path):\n            os.mkdir(job_events_path, 0o700)\n        command = self.config.command\n        with codecs.open(command_filename, 'w', encoding='utf-8') as f:\n            os.chmod(command_filename, stat.S_IRUSR | stat.S_IWUSR)\n            json.dump(\n                {'command': command,\n                 'cwd': self.config.cwd,\n                 'env': self.config.env}, f, ensure_ascii=False\n            )\n        if self.config.ident is not None:\n            cleanup_artifact_dir(os.path.join(self.config.artifact_dir, \"..\"), self.config.rotate_artifacts)\n        if hasattr(self.config, 'suppress_ansible_output'):\n            suppress_ansible_output = self.config.suppress_ansible_output\n        else:\n            suppress_ansible_output = False\n        stdout_handle = codecs.open(stdout_filename, 'w', encoding='utf-8')\n        stdout_handle = OutputEventFilter(stdout_handle, self.event_callback, suppress_ansible_output, output_json=self.config.json_mode)\n        stderr_handle = codecs.open(stderr_filename, 'w', encoding='utf-8')\n        stderr_handle = OutputEventFilter(stderr_handle, self.event_callback, suppress_ansible_output, output_json=self.config.json_mode)\n        if self.runner_mode == 'pexpect' and not isinstance(self.config.expect_passwords, collections.OrderedDict):\n            expect_passwords = collections.OrderedDict(self.config.expect_passwords)\n            password_patterns = list(expect_passwords.keys())\n            password_values = list(expect_passwords.values())\n        if self.config.containerized:\n            cwd = os.getcwd()\n            pexpect_env = os.environ.copy()\n            pexpect_env.update(self.config.env)\n            env_file_host = os.path.join(self.config.artifact_dir, 'env.list')\n            with open(env_file_host, 'w') as f:\n                f.write(\n                    '\\n'.join(\n                        [\"{}={}\".format(key, value) for key, value in self.config.env.items()]\n                    )\n                )\n        else:\n            cwd = self.config.cwd\n            pexpect_env = self.config.env\n        env = {\n            ensure_str(k): ensure_str(v) if k != 'PATH' and isinstance(v, six.text_type) else v\n            for k, v in pexpect_env.items()\n        }\n        if self.resource_profiling:\n            cgroup_path = '{0}/{1}'.format(self.config.resource_profiling_base_cgroup, self.config.ident)\n            import getpass\n            import grp\n            user = getpass.getuser()\n            group = grp.getgrgid(os.getgid()).gr_name\n            cmd = 'cgcreate -a {user}:{group} -t {user}:{group} -g cpuacct,memory,pids:{}'.format(cgroup_path, user=user, group=group)\n            proc = Popen(cmd, stdout=PIPE, stderr=PIPE, shell=True)\n            _, stderr = proc.communicate()\n            if proc.returncode:\n                logger.error('Unable to create cgroup: {}'.format(stderr))\n                raise RuntimeError('Unable to create cgroup: {}'.format(stderr))\n            else:\n                logger.info(\"Created cgroup '{}'\".format(cgroup_path))\n        self.status_callback('running')\n        self.last_stdout_update = time.time()\n        if self.runner_mode == 'subprocess':\n            if hasattr(self.config, 'input_fd') and self.config.input_fd:\n                input_fd = self.config.input_fd\n            else:\n                input_fd = None\n            if hasattr(self.config, 'output_fd') and self.config.output_fd:\n                output_fd = self.config.output_fd\n            else:\n                output_fd = PIPE\n            if hasattr(self.config, 'error_fd') and self.config.error_fd:\n                error_fd = self.config.error_fd\n            else:\n                error_fd = PIPE\n            subprocess_timeout = self.config.subprocess_timeout if hasattr(self.config, 'subprocess_timeout') else None\n            try:\n                stdout_response = ''\n                stderr_response = ''\n                kwargs = {\n                    'cwd': cwd,\n                    'env': env,\n                    'stdin': input_fd,\n                    'stdout': output_fd,\n                    'stderr': error_fd,\n                    'check': True,\n                    'universal_newlines': True,\n                    'shell': True\n                }\n                if subprocess_timeout is not None:\n                    kwargs.update({'timeout': subprocess_timeout})\n                proc_out = run_subprocess(\" \".join(command), **kwargs)\n                stdout_response = proc_out.stdout\n                stderr_response = proc_out.stderr\n                self.rc = proc_out.returncode\n            except CalledProcessError as exc:\n                logger.debug(\"{cmd} execution failed, returncode: {rc}, output: {output}, stdout: {stdout}, stderr: {stderr}\".format(\n                    cmd=exc.cmd, rc=exc.returncode, output=exc.output, stdout=exc.stdout, stderr=exc.stderr))\n                self.rc = exc.returncode\n                self.errored = True\n                stdout_response = exc.stdout\n                stderr_response = exc.stderr\n            except TimeoutExpired as exc:\n                logger.debug(\"{cmd} execution timedout, timeout: {timeout}, output: {output}, stdout: {stdout}, stderr: {stderr}\".format(\n                    cmd=exc.cmd, timeout=exc.timeout, output=exc.output, stdout=exc.stdout, stderr=exc.stderr))\n                self.rc = 254\n                stdout_response = exc.stdout\n                stderr_response = exc.stderr\n                self.timed_out = True\n            except Exception as exc:\n                import traceback\n                stderr_response = traceback.format_exc()\n                self.rc = 254\n                self.errored = True\n                logger.debug(\"received execption: {exc}\".format(exc=str(exc)))\n            if self.timed_out or self.errored:\n                self.kill_container()\n            if stdout_response is not None:\n                if isinstance(stdout_response, bytes):\n                    stdout_response = stdout_response.decode()\n                stdout_handle.write(stdout_response)\n            if stderr_response is not None:\n                if isinstance(stderr_response, bytes):\n                    stderr_response = stderr_response.decode()\n                stderr_handle.write(stderr_response)\n        else:\n            try:\n                child = pexpect.spawn(\n                    command[0],\n                    command[1:],\n                    cwd=cwd,\n                    env=env,\n                    ignore_sighup=True,\n                    encoding='utf-8',\n                    codec_errors='replace',\n                    echo=False,\n                    use_poll=self.config.pexpect_use_poll,\n                )\n                child.logfile_read = stdout_handle\n            except pexpect.exceptions.ExceptionPexpect as e:\n                child = collections.namedtuple(\n                    'MissingProcess', 'exitstatus isalive close'\n                )(\n                    exitstatus=127,\n                    isalive=lambda: False,\n                    close=lambda: None,\n                )\n                def _decode(x):\n                    return x.decode('utf-8') if six.PY2 else x\n                events_directory = os.path.join(self.config.artifact_dir, 'job_events')\n                if not os.path.exists(events_directory):\n                    os.mkdir(events_directory, 0o700)\n                stdout_handle.write(_decode(str(e)))\n                stdout_handle.write(_decode('\\n'))\n            job_start = time.time()\n            while child.isalive():\n                result_id = child.expect(password_patterns, timeout=self.config.pexpect_timeout, searchwindowsize=100)\n                password = password_values[result_id]\n                if password is not None:\n                    child.sendline(password)\n                    self.last_stdout_update = time.time()\n                if self.cancel_callback:\n                    try:\n                        self.canceled = self.cancel_callback()\n                    except Exception as e:\n                        raise CallbackError(\"Exception in Cancel Callback: {}\".format(e))\n                if self.config.job_timeout and not self.canceled and (time.time() - job_start) > self.config.job_timeout:\n                    self.timed_out = True\n                if self.canceled or self.timed_out or self.errored:\n                    self.kill_container()\n                    Runner.handle_termination(child.pid, is_cancel=self.canceled)\n                if self.config.idle_timeout and (time.time() - self.last_stdout_update) > self.config.idle_timeout:\n                    self.kill_container()\n                    Runner.handle_termination(child.pid, is_cancel=False)\n                    self.timed_out = True\n            stdout_handle.flush()\n            stdout_handle.close()\n            child.close()\n            self.rc = child.exitstatus if not (self.timed_out or self.canceled) else 254\n        if self.canceled:\n            self.status_callback('canceled')\n        elif self.rc == 0 and not self.timed_out:\n            self.status_callback('successful')\n        elif self.timed_out:\n            self.status_callback('timeout')\n        else:\n            self.status_callback('failed')\n        for filename, data in [\n            ('status', self.status),\n            ('rc', self.rc),\n        ]:\n            artifact_path = os.path.join(self.config.artifact_dir, filename)\n            if not os.path.exists(artifact_path):\n                os.close(os.open(artifact_path, os.O_CREAT, stat.S_IRUSR | stat.S_IWUSR))\n            with open(artifact_path, 'w') as f:\n                f.write(str(data))\n        if self.directory_isolation_path and self.directory_isolation_cleanup:\n            shutil.rmtree(self.directory_isolation_path)\n        if self.process_isolation and self.process_isolation_path_actual:\n            def _delete(retries=15):\n                try:\n                    shutil.rmtree(self.process_isolation_path_actual)\n                except OSError as e:\n                    res = False\n                    if e.errno == 16 and retries > 0:\n                        time.sleep(1)\n                        res = _delete(retries=retries - 1)\n                    if not res:\n                        raise\n                return True\n            _delete()\n        if self.resource_profiling:\n            cmd = 'cgdelete -g cpuacct,memory,pids:{}'.format(cgroup_path)\n            proc = Popen(cmd, stdout=PIPE, stderr=PIPE, shell=True)\n            _, stderr = proc.communicate()\n            if proc.returncode:\n                logger.error('Failed to delete cgroup: {}'.format(stderr))\n                raise RuntimeError('Failed to delete cgroup: {}'.format(stderr))\n        if self.artifacts_handler is not None:\n            try:\n                self.artifacts_handler(self.config.artifact_dir)\n            except Exception as e:\n                raise CallbackError(\"Exception in Artifact Callback: {}\".format(e))\n        if self.finished_callback is not None:\n            try:\n                self.finished_callback(self)\n            except Exception as e:\n                raise CallbackError(\"Exception in Finished Callback: {}\".format(e))\n        return self.status, self.rc\n    def kill_container(self):\n        '''\n        Internal method to terminate a container being used for job isolation\n        '''\n        container_name = self.config.container_name\n        if container_name:\n            container_cli = self.config.process_isolation_executable\n            cmd = '{} kill {}'.format(container_cli, container_name)\n            proc = Popen(cmd, stdout=PIPE, stderr=PIPE, shell=True)\n            _, stderr = proc.communicate()\n            if proc.returncode:\n                logger.info('Error from {} kill {} command:\\n{}'.format(container_cli, container_name, stderr))\n            else:\n                logger.info(\"Killed container {}\".format(container_name))\n    def prepare_plugin_docs_command(self, plugin_names, plugin_type=None, response_format=None,\n                                    snippet=False, playbook_dir=None, module_path=None):\n        if response_format and response_format not in DocConfig._supported_response_formats:\n            raise ConfigurationError(\"Invalid response_format {0}, valid value is one of either {1}\".format(response_format,\n                                                                                                            \", \".join(DocConfig._supported_response_formats)))\n        if not isinstance(plugin_names, list):\n            raise ConfigurationError(\"plugin_names should be of type list, instead received {0} of type {1}\".format(plugin_names, type(plugin_names)))\n        self._prepare_env(runner_mode=self.runner_mode)\n        self.cmdline_args = []\n        if response_format == 'json':\n            self.cmdline_args.append('-j')\n        if snippet:\n            self.cmdline_args.append('-s')\n        if plugin_type:\n            self.cmdline_args.extend(['-t', plugin_type])\n        if playbook_dir:\n            self.cmdline_args.extend(['--playbook-dir', playbook_dir])\n        if module_path:\n            self.cmdline_args.extend(['-M', module_path])\n        self.cmdline_args.append(\" \".join(plugin_names))\n        self.command = [self._ansible_doc_exec_path] + self.cmdline_args\n        self._handle_command_wrap(self.execution_mode, self.cmdline_args)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-4041",
        "description": "[{'lang': 'en', 'value': \"A flaw was found in ansible-runner. An improper escaping of the shell command, while calling the ansible_runner.interface.run_command, can lead to parameters getting executed as host's shell command. A developer could unintentionally write code that gets executed in the host rather than the virtual environment.\"}]",
        "cwe_number": 116
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-149",
      "code": "    def __init__(\n        self,\n        method: str,\n        url: URL,\n        *,\n        params: Optional[Mapping[str, str]] = None,\n        headers: Optional[LooseHeaders] = None,\n        skip_auto_headers: Iterable[str] = frozenset(),\n        data: Any = None,\n        cookies: Optional[LooseCookies] = None,\n        auth: Optional[BasicAuth] = None,\n        version: http.HttpVersion = http.HttpVersion11,\n        compress: Optional[str] = None,\n        chunked: Optional[bool] = None,\n        expect100: bool = False,\n        loop: Optional[asyncio.AbstractEventLoop] = None,\n        response_class: Optional[Type[\"ClientResponse\"]] = None,\n        proxy: Optional[URL] = None,\n        proxy_auth: Optional[BasicAuth] = None,\n        timer: Optional[BaseTimerContext] = None,\n        session: Optional[\"ClientSession\"] = None,\n        ssl: Union[SSLContext, Literal[False], Fingerprint, None] = None,\n        proxy_headers: Optional[LooseHeaders] = None,\n        traces: Optional[List[\"Trace\"]] = None,\n        trust_env: bool = False,\n        server_hostname: Optional[str] = None,\n    ):\n        if loop is None:\n            loop = asyncio.get_event_loop()\n        assert isinstance(url, URL), url\n        assert isinstance(proxy, (URL, type(None))), proxy\n        self._session = cast(\"ClientSession\", session)\n        if params:\n            q = MultiDict(url.query)\n            url2 = url.with_query(params)\n            q.extend(url2.query)\n            url = url.with_query(q)\n        self.original_url = url\n        self.url = url.with_fragment(None)\n        self.method = method.upper()\n        self.chunked = chunked\n        self.compress = compress\n        self.loop = loop\n        self.length = None\n        if response_class is None:\n            real_response_class = ClientResponse\n        else:\n            real_response_class = response_class\n        self.response_class: Type[ClientResponse] = real_response_class\n        self._timer = timer if timer is not None else TimerNoop()\n        self._ssl = ssl\n        self.server_hostname = server_hostname\n        if loop.get_debug():\n            self._source_traceback = traceback.extract_stack(sys._getframe(1))\n        self.update_version(version)\n        self.update_host(url)\n        self.update_headers(headers)\n        self.update_auto_headers(skip_auto_headers)\n        self.update_cookies(cookies)\n        self.update_content_encoding(data)\n        self.update_auth(auth, trust_env)\n        self.update_proxy(proxy, proxy_auth, proxy_headers)\n        self.update_body_from_data(data)\n        if data is not None or self.method not in self.GET_METHODS:\n            self.update_transfer_encoding()\n        self.update_expect_continue(expect100)\n        if traces is None:\n            traces = []\n        self._traces = traces\n    def is_ssl(self) -> bool:\n        return self.url.scheme in (\"https\", \"wss\")\n    def ssl(self) -> Union[\"SSLContext\", None, Literal[False], Fingerprint]:\n        return self._ssl",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-49082",
        "description": "[{'lang': 'en', 'value': 'aiohttp is an asynchronous HTTP client/server framework for asyncio and Python. Improper validation makes it possible for an attacker to modify the HTTP request (e.g. insert a new header) or even create a new HTTP request if the attacker controls the HTTP method. The vulnerability occurs only if the attacker can control the HTTP method (GET, POST etc.) of the request. If the attacker can control the HTTP version of the request it will be able to modify the request (request smuggling). This issue has been patched in version 3.9.0.'}]",
        "cwe_number": 20
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-150",
      "code": "def sign_in():\n  if session.get('signed_in') != None: return redirect('/')\n  authorize_url = 'https://stage-id.valtech.com/oauth2/authorize?response_type=%s&client_id=%s&scope=%s' % ('code', CLIENT_ID, 'email openid')\n  return redirect(authorize_url)\ndef sign_in_callback():\n  code = request.args.get('code')\n  tokens = exchange_code_for_tokens(code)\n  user_info = jwt.decode(tokens[\"id_token\"], verify=False)\n  session['signed_in'] = True\n  session['email'] = user_info['email']\n  return redirect('/')\ndef sign_out():\n  session.clear()\n  return redirect('https://stage-id.valtech.com/oidc/end-session?client_id=%s' % CLIENT_ID)\ndef exchange_code_for_tokens(code):\n  data = {\n    'grant_type': 'authorization_code',\n    'code': code,\n    'client_id': CLIENT_ID,\n    'client_secret': CLIENT_SECRET\n  }\n  res = requests.post('https://stage-id.valtech.com/oauth2/token', data=data)\n  return res.json()\ndef fetch_user_info(access_token):\n  res = requests.get('https://stage-id.valtech.com/api/users/me', headers={ 'Authorization': 'Bearer %s' % access_token })\n  return res.json()",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2014-125028",
        "description": "[{'lang': 'en', 'value': 'A vulnerability was found in valtech IDP Test Client and classified as problematic. Affected by this issue is some unknown functionality of the file python-flask/main.py. The manipulation leads to cross-site request forgery. The attack may be launched remotely. The name of the patch is f1e7b3d431c8681ec46445557125890c14fa295f. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-217148.'}]",
        "cwe_number": 352
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-151",
      "code": "    def put(self, name: str):\n        \"\"\"add new file\n        params in FormData:\n            - file\n            - original_file_name [optional]\n        \"\"\"\n        data = {}\n        mindsdb_file_name = name\n        existing_file_names = ca.file_controller.get_files_names()\n        def on_field(field):\n            name = field.field_name.decode()\n            value = field.value.decode()\n            data[name] = value\n        file_object = None\n        def on_file(file):\n            nonlocal file_object\n            data[\"file\"] = file.file_name.decode()\n            file_object = file.file_object\n        temp_dir_path = tempfile.mkdtemp(prefix=\"mindsdb_file_\")\n        if request.headers[\"Content-Type\"].startswith(\"multipart/form-data\"):\n            parser = multipart.create_form_parser(\n                headers=request.headers,\n                on_field=on_field,\n                on_file=on_file,\n                config={\n                    \"UPLOAD_DIR\": temp_dir_path.encode(),\n                    \"UPLOAD_KEEP_FILENAME\": True,\n                    \"UPLOAD_KEEP_EXTENSIONS\": True,\n                    \"MAX_MEMORY_FILE_SIZE\": 0,\n                },\n            )\n            while True:\n                chunk = request.stream.read(8192)\n                if not chunk:\n                    break\n                parser.write(chunk)\n            parser.finalize()\n            parser.close()\n            if file_object is not None and not file_object.closed:\n                file_object.close()\n        else:\n            data = request.json\n        if mindsdb_file_name in existing_file_names:\n            return http_error(\n                400,\n                \"File already exists\",\n                f\"File with name '{data['file']}' already exists\",\n            )\n        if data.get(\"source_type\") == \"url\":\n            url = data[\"source\"]\n            data[\"file\"] = data[\"name\"]\n            config = Config()\n            is_cloud = config.get(\"cloud\", False)\n            if is_cloud is True and ctx.user_class != 1:\n                info = requests.head(url)\n                file_size = info.headers.get(\"Content-Length\")\n                try:\n                    file_size = int(file_size)\n                except Exception:\n                    pass\n                if file_size is None:\n                    return http_error(\n                        400,\n                        \"Error getting file info\",\n                        \"\u0421an't determine remote file size\",\n                    )\n                if file_size > 1024 * 1024 * 100:\n                    return http_error(\n                        400, \"File is too big\", \"Upload limit for file is 100Mb\"\n                    )\n            with requests.get(url, stream=True) as r:\n                if r.status_code != 200:\n                    return http_error(\n                        400, \"Error getting file\", f\"Got status code: {r.status_code}\"\n                    )\n                file_path = os.path.join(temp_dir_path, data[\"file\"])\n                with open(file_path, \"wb\") as f:\n                    for chunk in r.iter_content(chunk_size=8192):\n                        f.write(chunk)\n        original_file_name = data.get(\"original_file_name\")\n        file_path = os.path.join(temp_dir_path, data[\"file\"])\n        lp = file_path.lower()\n        if lp.endswith((\".zip\", \".tar.gz\")):\n            if lp.endswith(\".zip\"):\n                with zipfile.ZipFile(file_path) as f:\n                    f.extractall(temp_dir_path)\n            elif lp.endswith(\".tar.gz\"):\n                with tarfile.open(file_path) as f:\n                    safe_extract(f, temp_dir_path)\n            os.remove(file_path)\n            files = os.listdir(temp_dir_path)\n            if len(files) != 1:\n                os.rmdir(temp_dir_path)\n                return http_error(\n                    400, \"Wrong content.\", \"Archive must contain only one data file.\"\n                )\n            file_path = os.path.join(temp_dir_path, files[0])\n            mindsdb_file_name = files[0]\n            if not os.path.isfile(file_path):\n                os.rmdir(temp_dir_path)\n                return http_error(\n                    400, \"Wrong content.\", \"Archive must contain data file in root.\"\n                )\n        ca.file_controller.save_file(\n            mindsdb_file_name, file_path, file_name=original_file_name\n        )\n        os.rmdir(temp_dir_path)\n        return \"\", 200\n    def select(self, query: ast.Select) -> pd.DataFrame:\n        conditions = extract_comparison_conditions(query.where)\n        urls = []\n        for op, arg1, arg2 in conditions:\n            if op == 'or':\n                raise NotImplementedError(f'OR is not supported')\n            if arg1 == 'url':\n                url = arg2\n                if op == '=':\n                    urls = [str(url)]\n                elif op == 'in':\n                    if type(url) == str:\n                        urls = [str(url)]\n                    else:\n                        urls = url\n                else:\n                    raise NotImplementedError(\n                        f'url can be url = \"someurl\", you can also crawl multiple sites, as follows: url IN (\"url1\", \"url2\", ..)')\n            else:\n                pass\n        if len(urls) == 0:\n            raise NotImplementedError(\n                f'You must specify what url you want to crawl, for example: SELECT * FROM crawl WHERE url IN (\"someurl\", ..)')\n        if query.limit is None:\n            raise NotImplementedError(f'You must specify a LIMIT which defines the number of pages to crawl')\n        limit = query.limit.value\n        if limit < 0:\n            limit = 0\n        result = get_all_websites(urls, limit, html=False)\n        if len(result) > limit:\n            result = result[:limit]\n        result = project_dataframe(result, query.targets, self.get_columns())\n        return result\n    def create(self, target: str, df: Optional[pd.DataFrame] = None, args: Optional[Dict] = None) -> None:\n        if 'using' not in args:\n            raise Exception(\"LlamaIndex engine requires a USING clause! Refer to its documentation for more details.\")\n        if 'index_class' not in args['using']:\n            args['using']['index_class'] = self.default_index_class\n        elif args['using']['index_class'] not in self.supported_index_class:\n            raise Exception(f\"Invalid index class argument. Please use one of {self.supported_index_class}\")\n        if 'reader' not in args['using']:\n            args['using']['reader'] = self.default_reader\n        elif args['using']['reader'] not in self.supported_reader:\n            raise Exception(f\"Invalid operation mode. Please use one of {self.supported_reader}\")\n        if df is None or df.empty:\n            df = pd.DataFrame([{'text': ''}])\n        if args['using']['reader'] == 'DFReader':\n            dstrs = df.apply(lambda x: ', '.join([f'{col}: {str(entry)}' for col, entry in zip(df.columns, x)]), axis=1)\n            reader = list(map(lambda x: Document(x), dstrs.tolist()))\n        elif args['using']['reader'] == 'SimpleWebPageReader':\n            if 'source_url_link' not in args['using']:\n                raise Exception(\"SimpleWebPageReader requires a `source_url_link` parameter. Refer to LlamaIndex documentation for more details.\")\n            reader = SimpleWebPageReader(html_to_text=True).load_data([args['using']['source_url_link']])\n        else:\n            raise Exception(f\"Invalid operation mode. Please use one of {self.supported_reader}.\")\n        self.model_storage.json_set('args', args)\n        index = self._setup_index(reader)\n        path = self.model_storage.folder_get('context')\n        index.storage_context.persist(persist_dir=path)\n        self.model_storage.folder_sync('context')",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-49795",
        "description": "[{'lang': 'en', 'value': \"MindsDB connects artificial intelligence models to real time data. Versions prior to 23.11.4.1 contain a server-side request forgery vulnerability in `file.py`. This can lead to limited information disclosure. Users should use MindsDB's `staging` branch or v23.11.4.1, which contain a fix for the issue.\\n\"}]",
        "cwe_number": 918
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-152",
      "code": "  def testRandom(self):\n    np.random.seed(1618)\n    shapes = [(13,), (6, 8), (1, 7, 1)]\n    for shape in shapes:\n      for dtype in [np.int32, np.int64, np.float16, np.float32, np.float64]:\n        a_np = np.random.randn(*shape).astype(dtype)\n        b_np = np.random.randn(*shape).astype(dtype)\n        sp_a, unused_a_nnz = _sparsify(a_np, thresh=-.5)\n        sp_b, unused_b_nnz = _sparsify(b_np, thresh=-.5)\n        with self.cached_session(use_gpu=False):\n          maximum_tf = sparse_ops.sparse_maximum(sp_a, sp_b)\n          maximum_tf_densified = sparse_ops.sparse_tensor_to_dense(\n              maximum_tf).eval()\n          minimum_tf = sparse_ops.sparse_minimum(sp_a, sp_b)\n          minimum_tf_densified = sparse_ops.sparse_tensor_to_dense(\n              minimum_tf).eval()\n          a_densified = sparse_ops.sparse_tensor_to_dense(sp_a).eval()\n          b_densified = sparse_ops.sparse_tensor_to_dense(sp_b).eval()\n        self.assertAllEqual(\n            np.maximum(a_densified, b_densified), maximum_tf_densified)\n        self.assertAllEqual(\n            np.minimum(a_densified, b_densified), minimum_tf_densified)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-25665",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. Prior to versions 2.12.0 and 2.11.1, when `SparseSparseMaximum` is given invalid sparse tensors as inputs, it can give a null pointer error. A fix is included in TensorFlow version 2.12 and version 2.11.1.'}]",
        "cwe_number": 476
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-153",
      "code": "    def list_zones(self):\n        pipe = subprocess.Popen([self.zoneadm_cmd, 'list', '-ip'],\n                             cwd=self.runner.basedir,\n                             stdin=subprocess.PIPE,\n                             stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        zones = []\n        for l in pipe.stdout.readlines():\n          s = l.split(':')\n          if s[1] != 'global':\n            zones.append(s[1])\n        return zones\n    def _generate_cmd(self, executable, cmd):\n        if executable:\n            local_cmd = [self.zlogin_cmd, self.zone, executable, cmd]\n        else:\n            local_cmd = '%s \"%s\" %s' % (self.zlogin_cmd, self.zone, cmd)\n        return local_cmd\n    def exec_command(self, cmd, tmp_path, become_user=None, sudoable=False, executable=None, in_data=None):\n        ''' run a command on the zone '''\n        if sudoable and self.runner.become and self.runner.become_method not in self.become_methods_supported:\n            raise errors.AnsibleError(\"Internal Error: this module does not support running commands via %s\" % self.runner.become_method)\n        if in_data:\n            raise errors.AnsibleError(\"Internal Error: this module does not support optimized module pipelining\")\n        if executable == '/bin/sh':\n          executable = None\n        local_cmd = self._generate_cmd(executable, cmd)\n        vvv(\"EXEC %s\" % (local_cmd), host=self.zone)\n        p = subprocess.Popen(local_cmd, shell=isinstance(local_cmd, basestring),\n                             cwd=self.runner.basedir,\n                             stdin=subprocess.PIPE,\n                             stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        stdout, stderr = p.communicate()\n        return (p.returncode, '', stdout, stderr)\n    def _normalize_path(self, path, prefix):\n        if not path.startswith(os.path.sep):\n            path = os.path.join(os.path.sep, path)\n        normpath = os.path.normpath(path)\n        return os.path.join(prefix, normpath[1:])\n    def _copy_file(self, in_path, out_path):\n        if not os.path.exists(in_path):\n            raise errors.AnsibleFileNotFound(\"file or module does not exist: %s\" % in_path)\n        try:\n            shutil.copyfile(in_path, out_path)\n        except shutil.Error:\n            traceback.print_exc()\n            raise errors.AnsibleError(\"failed to copy: %s and %s are the same\" % (in_path, out_path))\n        except IOError:\n            traceback.print_exc()\n            raise errors.AnsibleError(\"failed to transfer file to %s\" % out_path)\n    def put_file(self, in_path, out_path):\n        ''' transfer a file from local to zone '''\n        out_path = self._normalize_path(out_path, self.get_zone_path())\n        vvv(\"PUT %s TO %s\" % (in_path, out_path), host=self.zone)\n        self._copy_file(in_path, out_path)\n    def fetch_file(self, in_path, out_path):\n        ''' fetch a file from zone to local '''\n        in_path = self._normalize_path(in_path, self.get_zone_path())\n        vvv(\"FETCH %s TO %s\" % (in_path, out_path), host=self.zone)\n        self._copy_file(in_path, out_path)\n    def close(self):\n        ''' terminate the connection; nothing to do here '''\n        pass\n    def connect(self, port=None):\n        ''' connect to the chroot; nothing to do here '''\n        vvv(\"THIS IS A LOCAL CHROOT DIR\", host=self.jail)\n        return self\n    def exec_command(self, cmd, tmp_path, become_user=None, sudoable=False, executable='/bin/sh', in_data=None):\n        ''' run a command on the chroot '''\n        if sudoable and self.runner.become and self.runner.become_method not in self.become_methods_supported:\n            raise errors.AnsibleError(\"Internal Error: this module does not support running commands via %s\" % self.runner.become_method)\n        if in_data:\n            raise errors.AnsibleError(\"Internal Error: this module does not support optimized module pipelining\")\n        local_cmd = self._generate_cmd(executable, cmd)\n        vvv(\"EXEC %s\" % (local_cmd), host=self.jail)\n        p = subprocess.Popen(local_cmd, shell=isinstance(local_cmd, basestring),\n                             cwd=self.runner.basedir,\n                             stdin=subprocess.PIPE,\n                             stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        stdout, stderr = p.communicate()\n        return (p.returncode, '', stdout, stderr)\n    def _normalize_path(self, path, prefix):\n        if not path.startswith(os.path.sep):\n            path = os.path.join(os.path.sep, path)\n        normpath = os.path.normpath(path)\n        return os.path.join(prefix, normpath[1:])\n    def _copy_file(self, in_path, out_path):\n        if not os.path.exists(in_path):\n            raise errors.AnsibleFileNotFound(\"file or module does not exist: %s\" % in_path)\n        try:\n            shutil.copyfile(in_path, out_path)\n        except shutil.Error:\n            traceback.print_exc()\n            raise errors.AnsibleError(\"failed to copy: %s and %s are the same\" % (in_path, out_path))\n        except IOError:\n            traceback.print_exc()\n            raise errors.AnsibleError(\"failed to transfer file to %s\" % out_path)\n    def put_file(self, in_path, out_path):\n        ''' transfer a file from local to chroot '''\n        out_path = self._normalize_path(out_path, self.get_jail_path())\n        vvv(\"PUT %s TO %s\" % (in_path, out_path), host=self.jail)\n        self._copy_file(in_path, out_path)\n    def fetch_file(self, in_path, out_path):\n        ''' fetch a file from chroot to local '''\n        in_path = self._normalize_path(in_path, self.get_jail_path())\n        vvv(\"FETCH %s TO %s\" % (in_path, out_path), host=self.jail)\n        self._copy_file(in_path, out_path)\n    def close(self):\n        ''' terminate the connection; nothing to do here '''\n        pass",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2015-6240",
        "description": "[{'lang': 'en', 'value': 'The chroot, jail, and zone connection plugins in ansible before 1.9.2 allow local users to escape a restricted environment via a symlink attack.'}]",
        "cwe_number": 59
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-154",
      "code": "    async def _has_watch_regex_match(self, text: str) -> Tuple[Union[bool, re.Match], Optional[str]]:\n        \"\"\"\n        Return True if `text` matches any regex from `word_watchlist` or `token_watchlist` configs.\n        `word_watchlist`'s patterns are placed between word boundaries while `token_watchlist` is\n        matched as-is. Spoilers are expanded, if any, and URLs are ignored.\n        Second return value is a reason written to database about blacklist entry (can be None).\n        \"\"\"\n        if SPOILER_RE.search(text):\n            text = self._expand_spoilers(text)\n        text = self.clean_input(text)\n        if URL_RE.search(text):\n            return False, None\n        watchlist_patterns = self._get_filterlist_items('filter_token', allowed=False)\n        for pattern in watchlist_patterns:\n            match = re.search(pattern, text, flags=re.IGNORECASE)\n            if match:\n                return match, self._get_filterlist_value('filter_token', pattern, allowed=False)['comment']\n        return False, None",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-41250",
        "description": "[{'lang': 'en', 'value': 'Python discord bot is the community bot for the Python Discord community. In affected versions when a non-blacklisted URL and an otherwise triggering filter token is included in the same message the token filter does not trigger. This means that by including any non-blacklisted URL moderation filters can be bypassed. This issue has been resolved in commit 67390298852513d13e0213870e50fb3cff1424e0'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-155",
      "code": "    def get(self, request):\n        model = self.queryset.model\n        content_type = ContentType.objects.get_for_model(model)\n        display_filter_params = []\n        dynamic_filter_form = None\n        filter_form = None\n        if self.filterset:\n            filter_params = self.get_filter_params(request)\n            filterset = self.filterset(filter_params, self.queryset)\n            self.queryset = filterset.qs\n            if not filterset.is_valid():\n                messages.error(\n                    request,\n                    mark_safe(f\"Invalid filters were specified: {filterset.errors}\"),\n                )\n                self.queryset = self.queryset.none()\n            display_filter_params = [\n                check_filter_for_display(filterset.filters, field_name, values)\n                for field_name, values in filter_params.items()\n            ]\n            if request.GET:\n                factory_formset_params = convert_querydict_to_factory_formset_acceptable_querydict(\n                    request.GET, filterset\n                )\n                dynamic_filter_form = DynamicFilterFormSet(filterset=filterset, data=factory_formset_params)\n            else:\n                dynamic_filter_form = DynamicFilterFormSet(filterset=filterset)\n            if self.filterset_form:\n                filter_form = self.filterset_form(filter_params, label_suffix=\"\")\n        if request.GET.get(\"export\"):\n            et = get_object_or_404(\n                ExportTemplate,\n                content_type=content_type,\n                name=request.GET.get(\"export\"),\n            )\n            try:\n                return et.render_to_response(self.queryset)\n            except Exception as e:\n                messages.error(\n                    request,\n                    f\"There was an error rendering the selected export template ({et.name}): {e}\",\n                )\n        elif \"export\" in request.GET and hasattr(model, \"to_yaml\"):\n            response = HttpResponse(self.queryset_to_yaml(), content_type=\"text/yaml\")\n            filename = f\"{settings.BRANDING_PREPENDED_FILENAME}{self.queryset.model._meta.verbose_name_plural}.yaml\"\n            response[\"Content-Disposition\"] = f'attachment; filename=\"{filename}\"'\n            return response\n        self.queryset = self.alter_queryset(request)\n        permissions = {}\n        for action in (\"add\", \"change\", \"delete\", \"view\"):\n            perm_name = get_permission_for_model(model, action)\n            permissions[action] = request.user.has_perm(perm_name)\n        table = None\n        table_config_form = None\n        if self.table:\n            order_by = self.request.GET.getlist(\"sort\")\n            table = self.table(self.queryset, user=request.user, order_by=order_by)\n            if \"pk\" in table.base_columns and (permissions[\"change\"] or permissions[\"delete\"]):\n                table.columns.show(\"pk\")\n            paginate = {\n                \"paginator_class\": EnhancedPaginator,\n                \"per_page\": get_paginate_count(request),\n            }\n            RequestConfig(request, paginate).configure(table)\n            table_config_form = TableConfigForm(table=table)\n            max_page_size = get_settings_or_config(\"MAX_PAGE_SIZE\")\n            if max_page_size and paginate[\"per_page\"] > max_page_size:\n                messages.warning(\n                    request,\n                    f'Requested \"per_page\" is too large. No more than {max_page_size} items may be displayed at a time.',\n                )\n        q_placeholder = \"Search \" + bettertitle(model._meta.verbose_name_plural)\n        search_form = SearchForm(data=request.GET, q_placeholder=q_placeholder)\n        valid_actions = self.validate_action_buttons(request)\n        context = {\n            \"content_type\": content_type,\n            \"table\": table,\n            \"permissions\": permissions,\n            \"action_buttons\": valid_actions,\n            \"table_config_form\": table_config_form,\n            \"filter_params\": display_filter_params,\n            \"filter_form\": filter_form,\n            \"dynamic_filter_form\": dynamic_filter_form,\n            \"search_form\": search_form,\n            \"list_url\": validated_viewname(model, \"list\"),\n            \"title\": bettertitle(model._meta.verbose_name_plural),\n        }\n        setattr(self, \"request\", request)\n        context.update(self.extra_context())\n        return render(request, self.template_name, context)\n    def post(self, request):\n        logger = logging.getLogger(__name__ + \".ObjectImportView\")\n        form = ImportForm(request.POST)\n        if form.is_valid():\n            logger.debug(\"Import form validation was successful\")\n            data = form.cleaned_data[\"data\"]\n            model_form = self.model_form(data)\n            restrict_form_fields(model_form, request.user)\n            for field_name, field in model_form.fields.items():\n                if field_name not in data and hasattr(field, \"initial\"):\n                    model_form.data[field_name] = field.initial\n            if model_form.is_valid():\n                try:\n                    with transaction.atomic():\n                        obj = model_form.save()\n                        self.queryset.get(pk=obj.pk)\n                        logger.debug(f\"Created {obj} (PK: {obj.pk})\")\n                        for (\n                            field_name,\n                            related_object_form,\n                        ) in self.related_object_forms.items():\n                            logger.debug(\"Processing form for related objects: {related_object_form}\")\n                            related_obj_pks = []\n                            for i, rel_obj_data in enumerate(data.get(field_name, [])):\n                                f = related_object_form(obj, rel_obj_data)\n                                for subfield_name, field in f.fields.items():\n                                    if subfield_name not in rel_obj_data and hasattr(field, \"initial\"):\n                                        f.data[subfield_name] = field.initial\n                                if f.is_valid():\n                                    related_obj = f.save()\n                                    related_obj_pks.append(related_obj.pk)\n                                else:\n                                    for subfield_name, errors in f.errors.items():\n                                        for err in errors:\n                                            err_msg = f\"{field_name}[{i}] {subfield_name}: {err}\"\n                                            model_form.add_error(None, err_msg)\n                                    raise AbortTransaction()\n                            model = related_object_form.Meta.model\n                            if model.objects.filter(pk__in=related_obj_pks).count() != len(related_obj_pks):\n                                raise ObjectDoesNotExist\n                except AbortTransaction:\n                    pass\n                except ObjectDoesNotExist:\n                    msg = \"Object creation failed due to object-level permissions violation\"\n                    logger.debug(msg)\n                    form.add_error(None, msg)\n            if not model_form.errors:\n                logger.info(f\"Import object {obj} (PK: {obj.pk})\")\n                messages.success(\n                    request,\n                    mark_safe(f'Imported object: <a href=\"{obj.get_absolute_url()}\">{obj}</a>'),\n                )\n                if \"_addanother\" in request.POST:\n                    return redirect(request.get_full_path())\n                return_url = form.cleaned_data.get(\"return_url\")\n                if return_url is not None and is_safe_url(url=return_url, allowed_hosts=request.get_host()):\n                    return redirect(return_url)\n                else:\n                    return redirect(self.get_return_url(request, obj))\n            else:\n                logger.debug(\"Model form validation failed\")\n                for field, errors in model_form.errors.items():\n                    for err in errors:\n                        if field == \"__all__\":\n                            form.add_error(None, err)\n                        else:\n                            form.add_error(None, f\"{field}: {err}\")\n        else:\n            logger.debug(\"Import form validation failed\")\n        return render(\n            request,\n            self.template_name,\n            {\n                \"form\": form,\n                \"obj_type\": self.queryset.model._meta.verbose_name,\n                \"return_url\": self.get_return_url(request),\n            },\n        )\n    def successful_post(self, request, obj, created, _logger):\n        \"\"\"Check for data that will be invalid in a future Nautobot release and warn the user if found.\"\"\"\n        edit_url = reverse(\"ipam:prefix_edit\", kwargs={\"pk\": obj.pk})\n        warning_msg = (\n            '<p>This <a href=\"'\n            + static(\"docs/models/ipam/prefix.html\")\n            + '\n        )\n        if obj.parent and obj.parent.type != constants.PREFIX_ALLOWED_PARENT_TYPES[obj.type]:\n            parent_edit_url = reverse(\"ipam:prefix_edit\", kwargs={\"pk\": obj.parent.pk})\n            messages.warning(\n                request,\n                mark_safe(\n                    f'{obj} is a {obj.type.title()} prefix but its parent <a href=\"{obj.parent.get_absolute_url()}\">'\n                    f\"{obj.parent}</a> is a {obj.parent.type.title()}. {warning_msg} \"\n                    f'Consider <a href=\"{edit_url}\">changing the type of {obj}</a> and/or '\n                    f'<a href=\"{parent_edit_url}\">{obj.parent}</a> to resolve this issue.'\n                ),\n            )\n        invalid_children = obj.children.filter(\n            ~Q(type__in=constants.PREFIX_ALLOWED_CHILD_TYPES[obj.type]),\n        )\n        if invalid_children.exists():\n            children_link = '<a href=\"' + reverse(\"ipam:prefix_list\") + f'?parent={obj.pk}\">its children</a>'\n            if obj.type == choices.PrefixTypeChoices.TYPE_CONTAINER:\n                messages.warning(\n                    request,\n                    mark_safe(\n                        f\"{obj} is a Container prefix and should not contain child prefixes of type Pool. \"\n                        f\"{warning_msg} Consider creating an intermediary Network prefix, or changing \"\n                        f\"the type of {children_link} to Network, to resolve this issue.\"\n                    ),\n                )\n            elif obj.type == choices.PrefixTypeChoices.TYPE_NETWORK:\n                messages.warning(\n                    request,\n                    mark_safe(\n                        f\"{obj} is a Network prefix and should not contain child prefixes of types Container or \"\n                        f'Network. {warning_msg} Consider <a href=\"{edit_url}\">changing the type of {obj}</a> '\n                        f\"to Container, or changing the type of {children_link} to Pool, to resolve this issue.\"\n                    ),\n                )\n            else:\n                messages.warning(\n                    request,\n                    mark_safe(\n                        f\"{obj} is a Pool prefix and should not contain other prefixes. {warning_msg} \"\n                        f'Consider either <a href=\"{edit_url}\">changing the type of {obj}</a> '\n                        f\"to Container or Network, or deleting {children_link}, to resolve this issue.\"\n                    ),\n                )\n        if obj.ip_addresses.exists() and obj.type == choices.PrefixTypeChoices.TYPE_CONTAINER:\n            ip_warning_msg = (\n                '<p>This <a href=\"'\n                + static(\"docs/models/ipam/ipaddress.html\")\n                + '\n                \"in a future release.</p>\"\n            )\n            shortest_child_mask_length = min([ip.mask_length for ip in obj.ip_addresses.all()])\n            if shortest_child_mask_length > obj.prefix_length:\n                ip_link = '<a href=\"' + reverse(\"ipam:ipaddress_list\") + f'?parent={obj.pk}\">these IP addresses</a>'\n                create_url = reverse(\"ipam:prefix_add\") + urlencode(\n                    {\n                        \"namespace\": obj.namespace.pk,\n                        \"type\": choices.PrefixTypeChoices.TYPE_NETWORK,\n                        \"prefix\": obj.prefix,\n                    }\n                )\n                messages.warning(\n                    request,\n                    mark_safe(\n                        f\"{obj} is a Container prefix and should not directly contain IP addresses. {ip_warning_msg} \"\n                        f'Consider either <a href=\"{edit_url}\">changing the type of {obj}</a> to Network, or '\n                        f'<a href=\"{create_url}\">creating one or more child prefix(es) of type Network</a> to contain '\n                        f\"{ip_link}, to resolve this issue.\"\n                    ),\n                )\n            else:\n                messages.warning(\n                    request,\n                    mark_safe(\n                        f\"{obj} is a Container prefix and should not directly contain IP addresses. {ip_warning_msg} \"\n                        f'Consider <a href=\"{edit_url}\">changing the type of {obj}</a> to Network '\n                        \"to resolve this issue.\"\n                    ),\n                )\n        super().successful_post(request, obj, created, _logger)\n    def post(self, request):\n        collapsed_ips = IPAddress.objects.filter(pk__in=request.POST.getlist(\"pk\"))\n        merged_attributes = request.POST\n        operation_invalid = len(collapsed_ips) < 2\n        if \"_skip\" not in request.POST and not operation_invalid:\n            with cache.lock(\"ipaddress_merge\", blocking_timeout=15, timeout=settings.REDIS_LOCK_TIMEOUT):\n                with transaction.atomic():\n                    namespace = Namespace.objects.get(pk=merged_attributes.get(\"namespace\"))\n                    status = Status.objects.get(pk=merged_attributes.get(\"status\"))\n                    if merged_attributes.get(\"tenant\"):\n                        tenant = Tenant.objects.get(pk=merged_attributes.get(\"tenant\"))\n                    else:\n                        tenant = None\n                    if merged_attributes.get(\"role\"):\n                        role = Role.objects.get(pk=merged_attributes.get(\"role\"))\n                    else:\n                        role = None\n                    if merged_attributes.get(\"tags\"):\n                        tag_pk_list = merged_attributes.get(\"tags\").split(\",\")\n                        tags = Tag.objects.filter(pk__in=tag_pk_list)\n                    else:\n                        tags = []\n                    if merged_attributes.get(\"nat_inside\"):\n                        nat_inside = IPAddress.objects.get(pk=merged_attributes.get(\"nat_inside\"))\n                    else:\n                        nat_inside = None\n                    ip_in_the_same_namespace = collapsed_ips.filter(parent__namespace=namespace).first()\n                    merged_ip = IPAddress(\n                        host=merged_attributes.get(\"host\"),\n                        ip_version=ip_in_the_same_namespace.ip_version,\n                        parent=ip_in_the_same_namespace.parent,\n                        type=merged_attributes.get(\"type\"),\n                        status=status,\n                        role=role,\n                        dns_name=merged_attributes.get(\"dns_name\", \"\"),\n                        description=merged_attributes.get(\"description\"),\n                        mask_length=merged_attributes.get(\"mask_length\"),\n                        tenant=tenant,\n                        nat_inside=nat_inside,\n                        _custom_field_data=ip_in_the_same_namespace._custom_field_data,\n                    )\n                    merged_ip.tags.set(tags)\n                    for key in merged_ip._custom_field_data.keys():\n                        ip_pk = merged_attributes.get(\"cf_\" + key)\n                        merged_ip._custom_field_data[key] = IPAddress.objects.get(pk=ip_pk)._custom_field_data[key]\n                    handle_relationship_changes_when_merging_ips(merged_ip, merged_attributes, collapsed_ips)\n                    device_ip4 = list(Device.objects.filter(primary_ip4__in=collapsed_ips).values_list(\"pk\", flat=True))\n                    device_ip6 = list(Device.objects.filter(primary_ip6__in=collapsed_ips).values_list(\"pk\", flat=True))\n                    vm_ip4 = list(\n                        VirtualMachine.objects.filter(primary_ip4__in=collapsed_ips).values_list(\"pk\", flat=True)\n                    )\n                    vm_ip6 = list(\n                        VirtualMachine.objects.filter(primary_ip6__in=collapsed_ips).values_list(\"pk\", flat=True)\n                    )\n                    ip_to_interface_assignments = []\n                    for assignment in IPAddressToInterface.objects.filter(ip_address__in=collapsed_ips):\n                        updated_attributes = model_to_dict(assignment)\n                        updated_attributes[\"ip_address\"] = merged_ip\n                        updated_attributes[\"interface\"] = Interface.objects.filter(\n                            pk=updated_attributes[\"interface\"]\n                        ).first()\n                        updated_attributes[\"vm_interface\"] = VMInterface.objects.filter(\n                            pk=updated_attributes[\"vm_interface\"]\n                        ).first()\n                        ip_to_interface_assignments.append(updated_attributes)\n                    services = list(Service.objects.filter(ip_addresses__in=collapsed_ips).values_list(\"pk\", flat=True))\n                    try:\n                        _, deleted_info = collapsed_ips.delete()\n                        deleted_count = deleted_info[IPAddress._meta.label]\n                    except ProtectedError as e:\n                        logger.info(\"Caught ProtectedError while attempting to delete objects\")\n                        handle_protectederror(collapsed_ips, request, e)\n                        return redirect(self.get_return_url(request))\n                    msg = (\n                        f\"Merged {deleted_count} {self.queryset.model._meta.verbose_name} \"\n                        f'into <a href=\"{merged_ip.get_absolute_url()}\">{escape(merged_ip)}</a>'\n                    )\n                    logger_msg = f\"Merged {deleted_count} {self.queryset.model._meta.verbose_name} into {merged_ip}\"\n                    merged_ip.validated_save()\n                    for assignment in ip_to_interface_assignments:\n                        IPAddressToInterface.objects.create(**assignment)\n                    Device.objects.filter(pk__in=device_ip4).update(primary_ip4=merged_ip)\n                    Device.objects.filter(pk__in=device_ip6).update(primary_ip6=merged_ip)\n                    VirtualMachine.objects.filter(pk__in=vm_ip4).update(primary_ip4=merged_ip)\n                    VirtualMachine.objects.filter(pk__in=vm_ip6).update(primary_ip6=merged_ip)\n                    for service in services:\n                        Service.objects.get(pk=service).ip_addresses.add(merged_ip)\n                    logger.info(logger_msg)\n                    messages.success(request, mark_safe(msg))\n        return self.find_duplicate_ips(request, merged_attributes)\n    def to_form_field(\n        self, set_initial=True, enforce_required=True, for_csv_import=False, simple_json_filter=False, label=None\n    ):\n        \"\"\"\n        Return a form field suitable for setting a CustomField's value for an object.\n        Args:\n            set_initial: Set initial date for the field. This should be False when generating a field for bulk editing.\n            enforce_required: Honor the value of CustomField.required. Set to False for filtering/bulk editing.\n            for_csv_import: Return a form field suitable for bulk import of objects. Despite the parameter name,\n                this is *not* used for CSV imports since 2.0, but it *is* used for JSON/YAML import of DeviceTypes.\n            simple_json_filter: Return a TextInput widget for JSON filtering instead of the default TextArea widget.\n            label: Set the input label manually (if required); otherwise, defaults to field's __str__() implementation.\n        \"\"\"\n        initial = self.default if set_initial else None\n        required = self.required if enforce_required else False\n        if self.type == CustomFieldTypeChoices.TYPE_INTEGER:\n            field = forms.IntegerField(\n                required=required,\n                initial=initial,\n                min_value=self.validation_minimum,\n                max_value=self.validation_maximum,\n            )\n        elif self.type == CustomFieldTypeChoices.TYPE_BOOLEAN:\n            choices = (\n                (None, \"---------\"),\n                (True, \"True\"),\n                (False, \"False\"),\n            )\n            field = forms.NullBooleanField(\n                required=required,\n                initial=initial,\n                widget=StaticSelect2(choices=choices),\n            )\n        elif self.type == CustomFieldTypeChoices.TYPE_DATE:\n            field = NullableDateField(\n                required=required,\n                initial=initial,\n                widget=DatePicker(),\n            )\n        elif self.type in (CustomFieldTypeChoices.TYPE_URL, CustomFieldTypeChoices.TYPE_TEXT):\n            if self.type == CustomFieldTypeChoices.TYPE_URL:\n                field = LaxURLField(required=required, initial=initial)\n            elif self.type == CustomFieldTypeChoices.TYPE_TEXT:\n                field = forms.CharField(max_length=255, required=required, initial=initial)\n            if self.validation_regex:\n                field.validators = [\n                    RegexValidator(\n                        regex=self.validation_regex,\n                        message=mark_safe(f\"Values must match this regex: <code>{self.validation_regex}</code>\"),\n                    )\n                ]\n        elif self.type == CustomFieldTypeChoices.TYPE_MARKDOWN:\n            field = CommentField(widget=SmallTextarea, label=None)\n        elif self.type == CustomFieldTypeChoices.TYPE_JSON:\n            if simple_json_filter:\n                field = JSONField(encoder=DjangoJSONEncoder, required=required, initial=None, widget=TextInput)\n            else:\n                field = JSONField(encoder=DjangoJSONEncoder, required=required, initial=initial)\n        else:\n            choices = [(cfc.value, cfc.value) for cfc in self.custom_field_choices.all()]\n            default_choice = self.custom_field_choices.filter(value=self.default).first()\n            if self.type == CustomFieldTypeChoices.TYPE_SELECT:\n                if not required or default_choice is None:\n                    choices = add_blank_choice(choices)\n                field_class = CSVChoiceField if for_csv_import else forms.ChoiceField\n                field = field_class(\n                    choices=choices,\n                    required=required,\n                    initial=initial,\n                    widget=StaticSelect2(),\n                )\n            else:\n                field_class = CSVMultipleChoiceField if for_csv_import else forms.MultipleChoiceField\n                field = field_class(choices=choices, required=required, initial=initial, widget=StaticSelect2Multiple())\n        field.model = self\n        if label is not None:\n            field.label = label\n        else:\n            field.label = str(self)\n        if self.description:\n            field.help_text = render_markdown(self.description)\n        return field\ndef job_buttons(context, obj):\n    \"\"\"\n    Render all applicable job buttons for the given object.\n    \"\"\"\n    content_type = ContentType.objects.get_for_model(obj)\n    buttons = JobButton.objects.filter(content_types=content_type)\n    if not buttons:\n        return \"\"\n    button_context = {\n        \"obj\": obj,\n        \"debug\": context.get(\"debug\", False),\n        \"request\": context[\"request\"],\n        \"user\": context[\"user\"],\n        \"perms\": context[\"perms\"],\n    }\n    buttons_html = forms_html = \"\"\n    group_names = OrderedDict()\n    hidden_inputs = HIDDEN_INPUTS.format(\n        csrf_token=context[\"csrf_token\"],\n        object_pk=obj.pk,\n        object_model_name=f\"{content_type.app_label}.{content_type.model}\",\n        redirect_path=context[\"request\"].path,\n    )\n    for jb in buttons:\n        template_args = {\n            \"button_id\": jb.pk,\n            \"button_text\": jb.text,\n            \"button_class\": jb.button_class,\n            \"button_url\": reverse(\"extras:jobbutton_run\", kwargs={\"pk\": jb.pk}),\n            \"object\": obj,\n            \"job\": jb.job,\n            \"hidden_inputs\": hidden_inputs,\n            \"disabled\": \"\" if context[\"user\"].has_perms((\"extras.run_jobbutton\", \"extras.run_job\")) else \"disabled\",\n        }\n        if jb.group_name:\n            group_names.setdefault(jb.group_name, [])\n            group_names[jb.group_name].append(jb)\n        else:\n            try:\n                text_rendered = render_jinja2(jb.text, button_context)\n                if text_rendered:\n                    template_args[\"button_text\"] = text_rendered\n                    if jb.confirmation:\n                        buttons_html += CONFIRM_BUTTON.format(**template_args)\n                        forms_html += CONFIRM_MODAL.format(**template_args)\n                    else:\n                        buttons_html += NO_CONFIRM_BUTTON.format(**template_args)\n                        forms_html += NO_CONFIRM_FORM.format(**template_args)\n            except Exception as e:\n                buttons_html += (\n                    f'<a class=\"btn btn-sm btn-default\" disabled=\"disabled\" title=\"{e}\">'\n                    f'<i class=\"mdi mdi-alert\"></i> {jb.name}</a>\\n'\n                )\n    for group_name, buttons in group_names.items():\n        group_button_class = buttons[0].button_class\n        buttons_rendered = \"\"\n        for jb in buttons:\n            template_args = {\n                \"button_id\": jb.pk,\n                \"button_text\": jb.text,\n                \"button_class\": \"link\",\n                \"button_url\": reverse(\"extras:jobbutton_run\", kwargs={\"pk\": jb.pk}),\n                \"object\": obj,\n                \"job\": jb.job,\n                \"hidden_inputs\": hidden_inputs,\n                \"disabled\": \"\" if context[\"user\"].has_perms((\"extras.run_jobbutton\", \"extras.run_job\")) else \"disabled\",\n            }\n            try:\n                text_rendered = render_jinja2(jb.text, button_context)\n                if text_rendered:\n                    template_args[\"button_text\"] = text_rendered\n                    if jb.confirmation:\n                        buttons_rendered += \"<li>\" + CONFIRM_BUTTON.format(**template_args) + \"</li>\"\n                        forms_html += CONFIRM_MODAL.format(**template_args)\n                    else:\n                        buttons_rendered += \"<li>\" + NO_CONFIRM_BUTTON.format(**template_args) + \"</li>\"\n                        forms_html += NO_CONFIRM_FORM.format(**template_args)\n            except Exception as e:\n                buttons_rendered += (\n                    f'<li><a disabled=\"disabled\" title=\"{e}\"><span class=\"text-muted\">'\n                    f'<i class=\"mdi mdi-alert\"></i> {jb.name}</span></a></li>'\n                )\n        if buttons_rendered:\n            buttons_html += GROUP_DROPDOWN.format(\n                group_button_class=group_button_class,\n                group_name=group_name,\n                grouped_buttons=buttons_rendered,\n            )\n    return mark_safe(buttons_html + forms_html)\n    def required_related_objects_errors(\n        cls, output_for=\"ui\", initial_data=None, relationships_key_specified=False, instance=None\n    ):\n        \"\"\"\n        Args:\n            output_for (str): either \"ui\" or \"api\" depending on usage\n            initial_data (dict): submitted form/serializer data to validate against\n            relationships_key_specified (bool): if the \"relationships\" key was provided or not\n            instance (Optional[BaseModel]): an optional model instance to validate against\n        Returns:\n            (list[dict]): List of field error dicts if any are found\n        \"\"\"\n        required_relationships = Relationship.objects.get_required_for_model(cls)\n        relationships_field_errors = {}\n        for relation in required_relationships:\n            opposite_side = RelationshipSideChoices.OPPOSITE[relation.required_on]\n            if relation.skip_required(cls, opposite_side):\n                continue\n            if relation.has_many(opposite_side):\n                num_required_verbose = \"at least one\"\n            else:\n                num_required_verbose = \"a\"\n            if output_for == \"api\":\n                if (\n                    getattr(instance, \"present_in_database\", False) is True\n                    and initial_data.get(relation, {}).get(opposite_side, {}) == {}\n                    and not relationships_key_specified\n                ):\n                    filter_kwargs = {\"relationship\": relation, f\"{relation.required_on}_id\": instance.pk}\n                    if RelationshipAssociation.objects.filter(**filter_kwargs).exists():\n                        continue\n            required_model_class = getattr(relation, f\"{opposite_side}_type\").model_class()\n            required_model_meta = required_model_class._meta\n            cr_field_name = f\"cr_{relation.key}__{opposite_side}\"\n            name_plural = cls._meta.verbose_name_plural\n            field_key = relation.key if output_for == \"api\" else cr_field_name\n            field_errors = {field_key: []}\n            if not required_model_class.objects.exists():\n                hint = (\n                    f\"You need to create {num_required_verbose} {required_model_meta.verbose_name} \"\n                    f\"before instantiating a {cls._meta.verbose_name}.\"\n                )\n                if output_for == \"ui\":\n                    try:\n                        add_url = reverse(get_route_for_model(required_model_class, \"add\"))\n                        hint = (\n                            f\"<a target='_blank' href='{add_url}'>Click here</a> to create \"\n                            f\"a {required_model_meta.verbose_name}.\"\n                        )\n                    except NoReverseMatch:\n                        pass\n                elif output_for == \"api\":\n                    try:\n                        api_post_url = reverse(get_route_for_model(required_model_class, \"list\", api=True))\n                        hint = f\"Create a {required_model_meta.verbose_name} by posting to {api_post_url}\"\n                    except NoReverseMatch:\n                        pass\n                error_message = mark_safe(\n                    f\"{name_plural[0].upper()}{name_plural[1:]} require \"\n                    f\"{num_required_verbose} {required_model_meta.verbose_name}, but no \"\n                    f\"{required_model_meta.verbose_name_plural} exist yet. {hint}\"\n                )\n                field_errors[field_key].append(error_message)\n            if initial_data is not None:\n                supplied_data = []\n                if output_for == \"ui\":\n                    supplied_data = initial_data.get(field_key, [])\n                elif output_for == \"api\":\n                    supplied_data = initial_data.get(relation, {}).get(opposite_side, {})\n                if not supplied_data:\n                    if output_for == \"ui\":\n                        field_errors[field_key].append(\n                            f\"You need to select {num_required_verbose} {required_model_meta.verbose_name}.\"\n                        )\n                    elif output_for == \"api\":\n                        field_errors[field_key].append(\n                            f'You need to specify [\"relationships\"][\"{relation.key}\"][\"{opposite_side}\"][\"objects\"].'\n                        )\n            if len(field_errors[field_key]) > 0:\n                relationships_field_errors[field_key] = field_errors[field_key]\n        return relationships_field_errors\n    def post(self, request, *args, **kwargs):\n        obj = self.alter_obj(self.get_object(kwargs), request, args, kwargs)\n        form = self.model_form(data=request.POST, files=request.FILES, instance=obj)\n        restrict_form_fields(form, request.user)\n        if form.is_valid():\n            logger.debug(\"Form validation was successful\")\n            try:\n                with transaction.atomic():\n                    object_created = not form.instance.present_in_database\n                    obj = form.save()\n                    self.queryset.get(pk=obj.pk)\n                    ctx = self.get_extra_context(request, obj)\n                    choices = ctx[\"choices\"]\n                    if choices.is_valid():\n                        choices.save()\n                    else:\n                        raise RuntimeError(choices.errors)\n                verb = \"Created\" if object_created else \"Modified\"\n                msg = f\"{verb} {self.queryset.model._meta.verbose_name}\"\n                logger.info(f\"{msg} {obj} (PK: {obj.pk})\")\n                if hasattr(obj, \"get_absolute_url\"):\n                    msg = f'{msg} <a href=\"{obj.get_absolute_url()}\">{escape(obj)}</a>'\n                else:\n                    msg = f\"{msg} {escape(obj)}\"\n                messages.success(request, mark_safe(msg))\n                if \"_addanother\" in request.POST:\n                    if hasattr(obj, \"clone_fields\"):\n                        url = f\"{request.path}?{prepare_cloned_fields(obj)}\"\n                        return redirect(url)\n                    return redirect(request.get_full_path())\n                return_url = form.cleaned_data.get(\"return_url\")\n                if return_url is not None and is_safe_url(url=return_url, allowed_hosts=request.get_host()):\n                    return redirect(return_url)\n                else:\n                    return redirect(self.get_return_url(request, obj))\n            except ObjectDoesNotExist:\n                msg = \"Object save failed due to object-level permissions violation\"\n                logger.debug(msg)\n                form.add_error(None, msg)\n            except RuntimeError:\n                msg = \"Errors encountered when saving custom field choices. See below.\"\n                logger.debug(msg)\n                form.add_error(None, msg)\n            except ProtectedError as err:\n                err_msg = err.args[0]\n                protected_obj = err.protected_objects[0]\n                msg = f\"{protected_obj.value}: {err_msg} Please cancel this edit and start again.\"\n                logger.debug(msg)\n                form.add_error(None, msg)\n        else:\n            logger.debug(\"Form validation failed\")\n        return render(\n            request,\n            self.template_name,\n            {\n                \"obj\": obj,\n                \"obj_type\": self.queryset.model._meta.verbose_name,\n                \"form\": form,\n                \"return_url\": self.get_return_url(request, obj),\n                \"editing\": obj.present_in_database,\n                **self.get_extra_context(request, obj),\n            },\n        )\ndef get_csv_form_fields_from_serializer_class(serializer_class):\n    \"\"\"From the given serializer class, build a list of field dicts suitable for rendering in the CSV import form.\"\"\"\n    serializer = serializer_class(context={\"request\": None, \"depth\": 0})\n    fields = []\n    for field_name, field in serializer.fields.items():\n        if field.read_only:\n            continue\n        if field_name == \"custom_fields\":\n            from nautobot.extras.choices import CustomFieldTypeChoices\n            from nautobot.extras.models import CustomField\n            cfs = CustomField.objects.get_for_model(serializer_class.Meta.model)\n            for cf in cfs:\n                cf_form_field = cf.to_form_field(set_initial=False)\n                field_info = {\n                    \"name\": cf.add_prefix_to_cf_key(),\n                    \"required\": cf_form_field.required,\n                    \"label\": cf_form_field.label,\n                    \"help_text\": cf_form_field.help_text,\n                }\n                if cf.type == CustomFieldTypeChoices.TYPE_BOOLEAN:\n                    field_info[\"format\"] = mark_safe(\"<code>true</code> or <code>false</code>\")\n                elif cf.type == CustomFieldTypeChoices.TYPE_DATE:\n                    field_info[\"format\"] = mark_safe(\"<code>YYYY-MM-DD</code>\")\n                elif cf.type == CustomFieldTypeChoices.TYPE_SELECT:\n                    field_info[\"choices\"] = {cfc.value: cfc.value for cfc in cf.custom_field_choices.all()}\n                elif cf.type == CustomFieldTypeChoices.TYPE_MULTISELECT:\n                    field_info[\"format\"] = mark_safe('<code>\"value,value\"</code>')\n                    field_info[\"choices\"] = {cfc.value: cfc.value for cfc in cf.custom_field_choices.all()}\n                fields.append(field_info)\n            continue\n        field_info = {\n            \"name\": field_name,\n            \"required\": field.required,\n            \"label\": field.label,\n            \"help_text\": field.help_text,\n        }\n        if isinstance(field, serializers.BooleanField):\n            field_info[\"format\"] = mark_safe(\"<code>true</code> or <code>false</code>\")\n        elif isinstance(field, serializers.DateField):\n            field_info[\"format\"] = mark_safe(\"<code>YYYY-MM-DD</code>\")\n        elif isinstance(field, TimeZoneSerializerField):\n            field_info[\"format\"] = mark_safe(\n                '<a href=\"https://en.wikipedia.org/wiki/List_of_tz_database_time_zones\">available options</a>'\n            )\n        elif isinstance(field, serializers.ManyRelatedField):\n            if field.field_name == \"tags\":\n                field_info[\"format\"] = mark_safe('<code>\"name,name\"</code> or <code>\"UUID,UUID\"</code>')\n            elif isinstance(field.child_relation, ContentTypeField):\n                field_info[\"format\"] = mark_safe('<code>\"app_label.model,app_label.model\"</code>')\n            else:\n                field_info[\"format\"] = mark_safe('<code>\"UUID,UUID\"</code>')\n        elif isinstance(field, serializers.RelatedField):\n            if isinstance(field, ContentTypeField):\n                field_info[\"format\"] = mark_safe(\"<code>app_label.model</code>\")\n            else:\n                field_info[\"format\"] = mark_safe(\"<code>UUID</code>\")\n        elif isinstance(field, (serializers.ListField, serializers.MultipleChoiceField)):\n            field_info[\"format\"] = mark_safe('<code>\"value,value\"</code>')\n        elif isinstance(field, (serializers.DictField, serializers.JSONField)):\n            pass\n        if isinstance(field, ChoiceField):\n            field_info[\"choices\"] = field.choices\n        fields.append(field_info)\n    fields = sorted(fields, key=lambda info: 1 if info[\"required\"] else 2)\n    return fields\ndef custom_links(context, obj):\n    \"\"\"\n    Render all applicable links for the given object.\n    \"\"\"\n    content_type = ContentType.objects.get_for_model(obj)\n    links = CustomLink.objects.filter(content_type=content_type)\n    if not links:\n        return \"\"\n    link_context = {\n        \"obj\": obj,\n        \"debug\": context.get(\"debug\", False),\n        \"request\": context[\"request\"],\n        \"user\": context[\"user\"],\n        \"perms\": context[\"perms\"],\n    }\n    template_code = \"\"\n    group_names = OrderedDict()\n    for cl in links:\n        if cl.group_name and cl.group_name in group_names:\n            group_names[cl.group_name].append(cl)\n        elif cl.group_name:\n            group_names[cl.group_name] = [cl]\n        else:\n            try:\n                text_rendered = render_jinja2(cl.text, link_context)\n                if text_rendered:\n                    link_rendered = render_jinja2(cl.target_url, link_context)\n                    link_target = ' target=\"_blank\"' if cl.new_window else \"\"\n                    template_code += LINK_BUTTON.format(link_rendered, link_target, cl.button_class, text_rendered)\n            except Exception as e:\n                template_code += (\n                    f'<a class=\"btn btn-sm btn-default\" disabled=\"disabled\" title=\"{e}\">'\n                    f'<i class=\"mdi mdi-alert\"></i> {cl.name}</a>\\n'\n                )\n    for group, links in group_names.items():\n        links_rendered = []\n        for cl in links:\n            try:\n                text_rendered = render_jinja2(cl.text, link_context)\n                if text_rendered:\n                    link_target = ' target=\"_blank\"' if cl.new_window else \"\"\n                    link_rendered = render_jinja2(cl.target_url, link_context)\n                    links_rendered.append(GROUP_LINK.format(link_rendered, link_target, text_rendered))\n            except Exception as e:\n                links_rendered.append(\n                    f'<li><a disabled=\"disabled\" title=\"{e}\"><span class=\"text-muted\">'\n                    f'<i class=\"mdi mdi-alert\"></i> {cl.name}</span></a></li>'\n                )\n        if links_rendered:\n            template_code += GROUP_BUTTON.format(links[0].button_class, group, \"\".join(links_rendered))\n    return mark_safe(template_code)\n    def clean(self):\n        super().clean()\n        if self.present_in_database and self.u_height > self._original_u_height:\n            for d in Device.objects.filter(device_type=self, position__isnull=False):\n                face_required = None if self.is_full_depth else d.face\n                u_available = d.rack.get_available_units(\n                    u_height=self.u_height, rack_face=face_required, exclude=[d.pk]\n                )\n                if d.position not in u_available:\n                    raise ValidationError(\n                        {\n                            \"u_height\": f\"Device {d} in rack {d.rack} does not have sufficient space to accommodate a height of {self.u_height}U\"\n                        }\n                    )\n        elif self.present_in_database and self._original_u_height > 0 and self.u_height == 0:\n            racked_instance_count = Device.objects.filter(device_type=self, position__isnull=False).count()\n            if racked_instance_count:\n                url = f\"{reverse('dcim:device_list')}?manufacturer={self.manufacturer_id}&device_type={self.pk}\"\n                raise ValidationError(\n                    {\n                        \"u_height\": mark_safe(\n                            f'Unable to set 0U height: Found <a href=\"{url}\">{racked_instance_count} instances</a> already '\n                            f\"mounted within racks.\"\n                        )\n                    }\n                )\n        if (self.subdevice_role != SubdeviceRoleChoices.ROLE_PARENT) and self.device_bay_templates.count():\n            raise ValidationError(\n                {\n                    \"subdevice_role\": \"Must delete all device bay templates associated with this device before \"\n                    \"declassifying it as a parent device.\"\n                }\n            )\n        if self.u_height and self.subdevice_role == SubdeviceRoleChoices.ROLE_CHILD:\n            raise ValidationError({\"u_height\": \"Child device types must be 0U.\"})\ndef _get_registered_content(obj, method, template_context, return_html=True):\n    \"\"\"\n    Given an object and a TemplateExtension method name and the template context, return all the\n    registered content for the object's model.\n    \"\"\"\n    context = {\n        \"object\": obj,\n        \"request\": template_context[\"request\"],\n        \"settings\": template_context[\"settings\"],\n        \"csrf_token\": template_context[\"csrf_token\"],\n        \"perms\": template_context[\"perms\"],\n    }\n    model_name = obj._meta.label_lower\n    template_extensions = registry[\"plugin_template_extensions\"].get(model_name, [])\n    objects = []\n    html = \"\"\n    for template_extension in template_extensions:\n        if getattr(template_extension, method) == getattr(TemplateExtension, method):\n            continue\n        plugin_name = template_extension.__module__.split(\".\")[0]\n        context[\"config\"] = settings.PLUGINS_CONFIG.get(plugin_name, {})\n        instance = template_extension(context)\n        content = getattr(instance, method)()\n        if not return_html:\n            for i, content in enumerate(content):\n                objects.append({f\"{plugin_name}:{i+1}\": content})\n        else:\n            html += content\n    if not return_html:\n        return objects\n    return mark_safe(html)\n    def _process_create_or_update_form(self, form):\n        \"\"\"\n        Helper method to create or update an object after the form is validated successfully.\n        \"\"\"\n        request = self.request\n        queryset = self.get_queryset()\n        with transaction.atomic():\n            object_created = not form.instance.present_in_database\n            obj = self.form_save(form)\n            queryset.get(pk=obj.pk)\n            if hasattr(form, \"save_note\") and callable(form.save_note):\n                form.save_note(instance=obj, user=request.user)\n            msg = f'{\"Created\" if object_created else \"Modified\"} {queryset.model._meta.verbose_name}'\n            self.logger.info(f\"{msg} {obj} (PK: {obj.pk})\")\n            if hasattr(obj, \"get_absolute_url\"):\n                msg = f'{msg} <a href=\"{obj.get_absolute_url()}\">{escape(obj)}</a>'\n            else:\n                msg = f\"{msg} { escape(obj)}\"\n            messages.success(request, mark_safe(msg))\n            if \"_addanother\" in request.POST:\n                if hasattr(obj, \"clone_fields\"):\n                    url = f\"{request.path}?{prepare_cloned_fields(obj)}\"\n                    self.success_url = url\n                self.success_url = request.get_full_path()\n            else:\n                return_url = form.cleaned_data.get(\"return_url\")\n                if return_url is not None and is_safe_url(url=return_url, allowed_hosts=request.get_host()):\n                    self.success_url = return_url\n                else:\n                    self.success_url = self.get_return_url(request, obj)\n    def post(self, request, pk):\n        virtual_chassis = get_object_or_404(self.queryset, pk=pk)\n        member_select_form = forms.VCMemberSelectForm(request.POST)\n        if member_select_form.is_valid():\n            device = member_select_form.cleaned_data[\"device\"]\n            device.virtual_chassis = virtual_chassis\n            data = {k: request.POST[k] for k in [\"vc_position\", \"vc_priority\"]}\n            membership_form = forms.DeviceVCMembershipForm(data=data, validate_vc_position=True, instance=device)\n            if membership_form.is_valid():\n                membership_form.save()\n                msg = f'Added member <a href=\"{device.get_absolute_url()}\">{escape(device)}</a>'\n                messages.success(request, mark_safe(msg))\n                if \"_addanother\" in request.POST:\n                    return redirect(request.get_full_path())\n                return redirect(self.get_return_url(request, device))\n        else:\n            membership_form = forms.DeviceVCMembershipForm(data=request.POST)\n        return render(\n            request,\n            \"dcim/virtualchassis_add_member.html\",\n            {\n                \"virtual_chassis\": virtual_chassis,\n                \"member_select_form\": member_select_form,\n                \"membership_form\": membership_form,\n                \"return_url\": self.get_return_url(request, virtual_chassis),\n            },\n        )\ndef render_boolean(value):\n    \"\"\"Render HTML from a computed boolean value.\n    Args:\n        value (any): Input value, can be any variable.\n            A truthy value (for example non-empty string / True / non-zero number) is considered True.\n            A falsey value other than None (for example \"\" or 0 or False) is considered False.\n            A value of None is considered neither True nor False.\n    Returns:\n        (str): HTML\n            '<span class=\"text-success\"><i class=\"mdi mdi-check-bold\" title=\"Yes\"></i></span>' if True value\n            - or -\n            '<span class=\"text-muted\">&mdash;</span>' if None value\n            - or -\n            '<span class=\"text-danger\"><i class=\"mdi mdi-close-thick\" title=\"No\"></i></span>' if False value\n    Examples:\n        >>> render_boolean(None)\n        '<span class=\"text-muted\">&mdash;</span>'\n        >>> render_boolean(True or \"arbitrary string\" or 1)\n        '<span class=\"text-success\"><i class=\"mdi mdi-check-bold\" title=\"Yes\"></i></span>'\n        >>> render_boolean(False or \"\" or 0)\n        '<span class=\"text-danger\"><i class=\"mdi mdi-close-thick\" title=\"No\"></i></span>'\n    \"\"\"\n    if value is None:\n        return mark_safe(HTML_NONE)\n    if bool(value):\n        return mark_safe(HTML_TRUE)\n    return mark_safe(HTML_FALSE)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-48705",
        "description": "[{'lang': 'en', 'value': \"Nautobot is a Network Source of Truth and Network Automation Platform built as a web application All users of Nautobot versions earlier than 1.6.6 or 2.0.5 are potentially affected by a cross-site scripting vulnerability. Due to incorrect usage of Django's `mark_safe()` API when rendering certain types of user-authored content; including custom links, job buttons, and computed fields; it is possible that users with permission to create or edit these types of content could craft a malicious payload (such as JavaScript code) that would be executed when rendering pages containing this content. The maintainers have fixed the incorrect uses of `mark_safe()` (generally by replacing them with appropriate use of `format_html()` instead) to prevent such malicious data from being executed. Users on Nautobot 1.6.x LTM should upgrade to v1.6.6 and users on Nautobot 2.0.x should upgrade to v2.0.5. Appropriate object permissions can and should be applied to restrict which users are permitted to create or edit the aforementioned types of user-authored content. Other than that, there is no direct workaround available.\"}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-156",
      "code": "    def _checkPolkitPrivilege(self, sender, conn, privilege):\n        \"\"\"\n        Verify that sender has a given PolicyKit privilege.\n        sender is the sender's (private) D-BUS name, such as \":1:42\"\n        (sender_keyword in @dbus.service.methods). conn is\n        the dbus.Connection object (connection_keyword in\n        @dbus.service.methods). privilege is the PolicyKit privilege string.\n        This method returns if the caller is privileged, and otherwise throws a\n        PermissionDeniedByPolicy exception.\n        \"\"\"\n        if sender is None and conn is None:\n            return\n        if not self.enforce_polkit:\n            return\n        info = SenderInfo(sender, conn)\n        pid = info.connectionPid()\n        self._initPolkit()\n        try:\n            (is_auth, _, details) = self.polkit.CheckAuthorization(\n                    ('unix-process', {'pid': dbus.UInt32(pid, variant_level=1),\n                    'start-time': dbus.UInt64(0, variant_level=1)}),\n                    privilege, {'': ''}, dbus.UInt32(1), '', timeout=3000)\n        except dbus.DBusException as e:\n            if e._dbus_error_name == 'org.freedesktop.DBus.Error.ServiceUnknown':\n                self.polkit = None\n                return self._checkPolkitPrivilege(sender, conn, privilege)\n            else:\n                raise\n        if not is_auth:\n            raise PermissionDeniedByPolicy(privilege)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2017-7572",
        "description": "[{'lang': 'en', 'value': 'The _checkPolkitPrivilege function in serviceHelper.py in Back In Time (aka backintime) 1.1.18 and earlier uses a deprecated polkit authorization method (unix-process) that is subject to a race condition (time of check, time of use). With this authorization method, the owner of a process requesting a polkit operation is checked by polkitd via /proc/<pid>/status, by which time the requesting process may have been replaced by a different process with the same PID that has different privileges then the original requester.'}]",
        "cwe_number": 362
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-157",
      "code": "  def testPoolingRatio(self):\n    with self.cached_session() as _:\n      with self.assertRaisesRegex(\n          errors.InvalidArgumentError,\n          r\"Pooling ratio is higher than input dimension size for dimension 1.*\"\n      ):\n        result = nn_ops.gen_nn_ops.fractional_max_pool(\n            value=constant_op.constant(\n                value=[[[[1, 4, 2, 3]]]], dtype=dtypes.int64),\n            pooling_ratio=[1.0, 1.44, 1.73, 1.0],\n            pseudo_random=False,\n            overlapping=False,\n            deterministic=False,\n            seed=0,\n            seed2=0,\n            name=None)\n        self.evaluate(result)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-41900",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. The security vulnerability results in FractionalMax(AVG)Pool with illegal pooling_ratio. Attackers using Tensorflow can exploit the vulnerability. They can access heap memory which is not in the control of user, leading to a crash or remote code execution. We have patched the issue in GitHub commit 216525144ee7c910296f5b05d214ca1327c9ce48. The fix will be included in TensorFlow 2.11.0. We will also cherry pick this commit on TensorFlow 2.10.1.'}]",
        "cwe_number": 125
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-158",
      "code": "def load_module(name):\n    if os.path.exists(name) and os.path.splitext(name)[1] == '.py':\n        sys.path.insert(0, os.path.dirname(os.path.abspath(name)))\n        try:\n            m = os.path.splitext(os.path.basename(name))[0]\n            module = importlib.import_module(m)\n        finally:\n            sys.path.pop(0)\n        return module\n    return importlib.import_module('dbusmock.templates.' + name)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2015-1326",
        "description": "[{'lang': 'en', 'value': 'python-dbusmock before version 0.15.1 AddTemplate() D-Bus method call or DBusTestCase.spawn_server_template() method could be tricked into executing malicious code if an attacker supplies a .pyc file.'}]",
        "cwe_number": 20
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-159",
      "code": "        def test_list_objects_depth_0(self):\n            \"\"\"\n            GET a list of objects using the \"?depth=0\" parameter.\n            \"\"\"\n            depth_fields = self.get_depth_fields()\n            self.add_permissions(f\"{self.model._meta.app_label}.view_{self.model._meta.model_name}\")\n            url = f\"{self._get_list_url()}?depth=0\"\n            response = self.client.get(url, **self.header)\n            self.assertHttpStatus(response, status.HTTP_200_OK)\n            self.assertIsInstance(response.data, dict)\n            self.assertIn(\"results\", response.data)\n            self.assertEqual(len(response.data[\"results\"]), self._get_queryset().count())\n            for response_data in response.data[\"results\"]:\n                for field in depth_fields:\n                    self.assertIn(field, response_data)\n                    if isinstance(response_data[field], list):\n                        for entry in response_data[field]:\n                            self.assertIsInstance(entry, dict)\n                            self.assertTrue(is_uuid(entry[\"id\"]))\n                    else:\n                        if response_data[field] is not None:\n                            self.assertIsInstance(response_data[field], dict)\n                            url = response_data[field][\"url\"]\n                            pk = response_data[field][\"id\"]\n                            object_type = response_data[field][\"object_type\"]\n                            self.assertTrue(is_uuid(url.split(\"/\")[-2]))\n                            self.assertTrue(is_uuid(pk))\n                            with self.subTest(f\"Assert object_type {object_type} is valid\"):\n                                app_label, model_name = object_type.split(\".\")\n                                ContentType.objects.get(app_label=app_label, model=model_name)\ndef nested_serializer_factory(relation_info, nested_depth):\n    \"\"\"\n    Return a NestedSerializer representation of a serializer field.\n    This method should only be called in build_nested_field()\n    in which relation_info and nested_depth are already given.\n    \"\"\"\n    nested_serializer_name = f\"Nested{nested_depth}{relation_info.related_model.__name__}\"\n    if nested_serializer_name in NESTED_SERIALIZER_CACHE:\n        field_class = NESTED_SERIALIZER_CACHE[nested_serializer_name]\n        field_kwargs = get_nested_relation_kwargs(relation_info)\n    else:\n        base_serializer_class = get_serializer_for_model(relation_info.related_model)\n        class NautobotNestedSerializer(base_serializer_class):\n            class Meta:\n                model = relation_info.related_model\n                is_nested = True\n                depth = nested_depth - 1\n                if hasattr(base_serializer_class.Meta, \"fields\"):\n                    fields = base_serializer_class.Meta.fields\n                if hasattr(base_serializer_class.Meta, \"exclude\"):\n                    exclude = base_serializer_class.Meta.exclude\n        NautobotNestedSerializer.__name__ = nested_serializer_name\n        NESTED_SERIALIZER_CACHE[nested_serializer_name] = NautobotNestedSerializer\n        field_class = NautobotNestedSerializer\n        field_kwargs = get_nested_relation_kwargs(relation_info)\n    return field_class, field_kwargs",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-46128",
        "description": "[{'lang': 'en', 'value': 'Nautobot is a Network Automation Platform built as a web application atop the Django Python framework with a PostgreSQL or MySQL database. In Nautobot 2.0.x, certain REST API endpoints, in combination with the `?depth=<N>` query parameter, can expose hashed user passwords as stored in the database to any authenticated user with access to these endpoints. The passwords are not exposed in plaintext. This vulnerability has been patched in version 2.0.3.\\n\\n'}]",
        "cwe_number": 312
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-160",
      "code": "    def parse_cmd(self, cmd, info):\n        tmpl, tmpl_dict = self._downloader.prepare_outtmpl(cmd, info)\n        if tmpl_dict:\n            return self._downloader.escape_outtmpl(tmpl) % tmpl_dict\n        filepath = info.get('filepath', info.get('_filename'))\n        if filepath:\n            if '{}' not in cmd:\n                cmd += ' {}'\n            cmd = cmd.replace('{}', compat_shlex_quote(filepath))\n        return cmd\n    def run(self, info):\n        for tmpl in self.exec_cmd:\n            cmd = self.parse_cmd(tmpl, info)\n            self.to_screen('Executing command: %s' % cmd)\n            retCode = subprocess.call(encodeArgument(cmd), shell=True)\n            if retCode != 0:\n                raise PostProcessingError('Command returned error code %d' % retCode)\n        return [], info\n    def __init__(self, *args, env=None, text=False, **kwargs):\n        if env is None:\n            env = os.environ.copy()\n        self._fix_pyinstaller_ld_path(env)\n        self.__text_mode = kwargs.get('encoding') or kwargs.get('errors') or text or kwargs.get('universal_newlines')\n        if text is True:\n            kwargs['universal_newlines'] = True\n            kwargs.setdefault('encoding', 'utf-8')\n            kwargs.setdefault('errors', 'replace')\n        super().__init__(*args, env=env, **kwargs, startupinfo=self._startupinfo)\n    def communicate_or_kill(self, *args, **kwargs):\n        try:\n            return self.communicate(*args, **kwargs)\n        except BaseException:\n            self.kill(timeout=None)\n            raise\n    def kill(self, *, timeout=0):\n        super().kill()\n        if timeout != 0:\n            self.wait(timeout=timeout)\ncompat_os_name = os._name if os.name == 'java' else os.name\nif compat_os_name == 'nt':\n    def compat_shlex_quote(s):\n        import re\n        return s if re.match(r'^[-_\\w./]+$', s) else '\"%s\"' % s.replace('\"', '\\\\\"')\nelse:\n    from shlex import quote as compat_shlex_quote",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-40581",
        "description": "[{'lang': 'en', 'value': 'yt-dlp is a youtube-dl fork with additional features and fixes. yt-dlp allows the user to provide shell command lines to be executed at various stages in its download steps through the `--exec` flag. This flag allows output template expansion in its argument, so that metadata values may be used in the shell commands. The metadata fields can be combined with the `%q` conversion, which is intended to quote/escape these values so they can be safely passed to the shell. However, the escaping used for `cmd` (the shell used by Python\\'s `subprocess` on Windows) does not properly escape special characters, which can allow for remote code execution if `--exec` is used directly with maliciously crafted remote data. This vulnerability only impacts `yt-dlp` on Windows, and the vulnerability is present regardless of whether `yt-dlp` is run from `cmd` or from `PowerShell`. Support for output template expansion in `--exec`, along with this vulnerable behavior, was added to `yt-dlp` in version 2021.04.11. yt-dlp version 2023.09.24 fixes this issue by properly escaping each special character. `\\\\n` will be replaced by `\\\\r` as no way of escaping it has been found. It is recommended to upgrade yt-dlp to version 2023.09.24 as soon as possible. Also, always be careful when using --exec, because while this specific vulnerability has been patched, using unvalidated input in shell commands is inherently dangerous. For Windows users who are not able to upgrade: 1. Avoid using any output template expansion in --exec other than {} (filepath). 2. If expansion in --exec is needed, verify the fields you are using do not contain \", | or &. 3. Instead of using --exec, write the info json and load the fields from it instead.\\n'}]",
        "cwe_number": 78
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-161",
      "code": "def SortEigenValues(e):\n  perm = np.argsort(e.real + e.imag, -1)\n  return np.take(e, perm, -1)\ndef SortEigenDecomposition(e, v):\n  if v.ndim < 2:\n    return e, v\n  perm = np.argsort(e.real + e.imag, -1)\n  return np.take(e, perm, -1), np.take(v, perm, -1)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-36000",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. When `mlir::tfg::ConvertGenericFunctionToFunctionDef` is given empty function attributes, it gives a null dereference. We have patched the issue in GitHub commit aed36912609fc07229b4d0a7b44f3f48efc00fd0. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue.'}]",
        "cwe_number": 476
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-162",
      "code": "def json_dumps_items(d, append=''):\n    \"\"\"Dumps a list of keys/values from a dictionary, without braces.\n    This works very much like ``json_dumps``, but doesn't output the\n    surrounding braces. This allows it to be used within a JavaScript\n    object definition alongside other custom keys.\n    If the dictionary is not empty, and ``append`` is passed, it will be\n    appended onto the results. This is most useful when you want to append\n    a comma after all the dictionary items, in order to provide further\n    keys in the template.\n    \"\"\"\n    if not d:\n        return ''\n    return mark_safe(json_dumps(d)[1:-1] + append)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2014-3994",
        "description": "[{'lang': 'en', 'value': 'Cross-site scripting (XSS) vulnerability in util/templatetags/djblets_js.py in Djblets before 0.7.30 and 0.8.x before 0.8.3 for Django, as used in Review Board, allows remote attackers to inject arbitrary web script or HTML via a JSON object, as demonstrated by the name field when changing a user name.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-163",
      "code": "    def access_token_added(self, userobj, name):\n        if not self.send_changed:\n            return\n        if not userobj.email:\n            logger.info(\"can't sent mail to user [%s] without an email\", userobj.username)\n            return\n        body = self.app.templates.compile_template(\n            \"access_token_added.html\", **{\"header_name\": self.app.cfg.header_name, 'user': userobj, 'name': name}\n        )\n        self.bus.publish('queue_mail', to=userobj.email, subject=_(\"A new access token has been created\"), message=body)\n    def user_password_changed(self, userobj):\n        if not self.send_changed:\n            return\n        if not userobj.email:\n            logger.info(\"can't sent mail to user [%s] without an email\", userobj.username)\n            return\n        body = self.app.templates.compile_template(\n            \"password_changed.html\", **{\"header_name\": self.app.cfg.header_name, 'user': userobj}\n        )\n        self.bus.publish('queue_mail', to=userobj.email, subject=_(\"Password changed\"), message=body)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-4721",
        "description": "[{'lang': 'en', 'value': 'Failure to Sanitize Special Elements into a Different Plane (Special Element Injection) in GitHub repository ikus060/rdiffweb prior to 2.5.5.'}]",
        "cwe_number": 75
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-164",
      "code": "    def emit(self, s, depth, reflow=True):\n        if reflow:\n            lines = reflow_lines(s, depth)\n        else:\n            lines = [s]\n        for line in lines:\n            line = (\" \" * TABSIZE * depth) + line + \"\\n\"\n            self.file.write(line)\n    def visitField(self, field, name, sum=None, prod=None, depth=0):\n        ctype = get_c_type(field.type)\n        if field.opt:\n            check = \"exists_not_none(obj, &PyId_%s)\" % (field.name,)\n        else:\n            check = \"_PyObject_HasAttrId(obj, &PyId_%s)\" % (field.name,)\n        self.emit(\"if (%s) {\" % (check,), depth, reflow=False)\n        self.emit(\"int res;\", depth+1)\n        if field.seq:\n            self.emit(\"Py_ssize_t len;\", depth+1)\n            self.emit(\"Py_ssize_t i;\", depth+1)\n        self.emit(\"tmp = _PyObject_GetAttrId(obj, &PyId_%s);\" % field.name, depth+1)\n        self.emit(\"if (tmp == NULL) goto failed;\", depth+1)\n        if field.seq:\n            self.emit(\"if (!PyList_Check(tmp)) {\", depth+1)\n            self.emit(\"PyErr_Format(PyExc_TypeError, \\\"%s field \\\\\\\"%s\\\\\\\" must \"\n                      \"be a list, not a %%.200s\\\", tmp->ob_type->tp_name);\" %\n                      (name, field.name),\n                      depth+2, reflow=False)\n            self.emit(\"goto failed;\", depth+2)\n            self.emit(\"}\", depth+1)\n            self.emit(\"len = PyList_GET_SIZE(tmp);\", depth+1)\n            if self.isSimpleType(field):\n                self.emit(\"%s = _Ta3_asdl_int_seq_new(len, arena);\" % field.name, depth+1)\n            else:\n                self.emit(\"%s = _Ta3_asdl_seq_new(len, arena);\" % field.name, depth+1)\n            self.emit(\"if (%s == NULL) goto failed;\" % field.name, depth+1)\n            self.emit(\"for (i = 0; i < len; i++) {\", depth+1)\n            self.emit(\"%s value;\" % ctype, depth+2)\n            self.emit(\"res = obj2ast_%s(PyList_GET_ITEM(tmp, i), &value, arena);\" %\n                      field.type, depth+2, reflow=False)\n            self.emit(\"if (res != 0) goto failed;\", depth+2)\n            self.emit(\"if (len != PyList_GET_SIZE(tmp)) {\", depth+2)\n            self.emit(\"PyErr_SetString(PyExc_RuntimeError, \\\"%s field \\\\\\\"%s\\\\\\\" \"\n                      \"changed size during iteration\\\");\" %\n                      (name, field.name),\n                      depth+3, reflow=False)\n            self.emit(\"goto failed;\", depth+3)\n            self.emit(\"}\", depth+2)\n            self.emit(\"asdl_seq_SET(%s, i, value);\" % field.name, depth+2)\n            self.emit(\"}\", depth+1)\n        else:\n            self.emit(\"res = obj2ast_%s(tmp, &%s, arena);\" %\n                      (field.type, field.name), depth+1)\n            self.emit(\"if (res != 0) goto failed;\", depth+1)\n        self.emit(\"Py_CLEAR(tmp);\", depth+1)\n        self.emit(\"} else {\", depth)\n        if not field.opt:\n            message = \"required field \\\\\\\"%s\\\\\\\" missing from %s\" % (field.name, name)\n            format = \"PyErr_SetString(PyExc_TypeError, \\\"%s\\\");\"\n            self.emit(format % message, depth+1, reflow=False)\n            self.emit(\"return 1;\", depth+1)\n        else:\n            if self.isNumeric(field):\n                self.emit(\"%s = 0;\" % field.name, depth+1)\n            elif not self.isSimpleType(field):\n                self.emit(\"%s = NULL;\" % field.name, depth+1)\n            else:\n                raise TypeError(\"could not determine the default value for %s\" % field.name)\n        self.emit(\"}\", depth)\n    def visitModule(self, mod):\n        self.emit(\"\"\"\ntypedef struct {\n    PyObject_HEAD\n    PyObject *dict;\n} AST_object;\nstatic void\nast_dealloc(AST_object *self)\n{\n    Py_CLEAR(self->dict);\n    Py_TYPE(self)->tp_free(self);\n}\nstatic int\nast_traverse(AST_object *self, visitproc visit, void *arg)\n{\n    Py_VISIT(self->dict);\n    return 0;\n}\nstatic void\nast_clear(AST_object *self)\n{\n    Py_CLEAR(self->dict);\n}\nstatic int\nast_type_init(PyObject *self, PyObject *args, PyObject *kw)\n{\n    _Py_IDENTIFIER(_fields);\n    Py_ssize_t i, numfields = 0;\n    int res = -1;\n    PyObject *key, *value, *fields;\n    fields = _PyObject_GetAttrId((PyObject*)Py_TYPE(self), &PyId__fields);\n    if (!fields)\n        PyErr_Clear();\n    if (fields) {\n        numfields = PySequence_Size(fields);\n        if (numfields == -1)\n            goto cleanup;\n    }\n    res = 0; /* if no error occurs, this stays 0 to the end */\n    if (PyTuple_GET_SIZE(args) > 0) {\n        if (numfields != PyTuple_GET_SIZE(args)) {\n            PyErr_Format(PyExc_TypeError, \"%.400s constructor takes %s\"\n                         \"%zd positional argument%s\",\n                         Py_TYPE(self)->tp_name,\n                         numfields == 0 ? \"\" : \"either 0 or \",\n                         numfields, numfields == 1 ? \"\" : \"s\");\n            res = -1;\n            goto cleanup;\n        }\n        for (i = 0; i < PyTuple_GET_SIZE(args); i++) {\n            /* cannot be reached when fields is NULL */\n            PyObject *name = PySequence_GetItem(fields, i);\n            if (!name) {\n                res = -1;\n                goto cleanup;\n            }\n            res = PyObject_SetAttr(self, name, PyTuple_GET_ITEM(args, i));\n            Py_DECREF(name);\n            if (res < 0)\n                goto cleanup;\n        }\n    }\n    if (kw) {\n        i = 0;  /* needed by PyDict_Next */\n        while (PyDict_Next(kw, &i, &key, &value)) {\n            res = PyObject_SetAttr(self, key, value);\n            if (res < 0)\n                goto cleanup;\n        }\n    }\n  cleanup:\n    Py_XDECREF(fields);\n    return res;\n}\n/* Pickling support */\nstatic PyObject *\nast_type_reduce(PyObject *self, PyObject *unused)\n{\n    PyObject *res;\n    _Py_IDENTIFIER(__dict__);\n    PyObject *dict = _PyObject_GetAttrId(self, &PyId___dict__);\n    if (dict == NULL) {\n        if (PyErr_ExceptionMatches(PyExc_AttributeError))\n            PyErr_Clear();\n        else\n            return NULL;\n    }\n    if (dict) {\n        res = Py_BuildValue(\"O()O\", Py_TYPE(self), dict);\n        Py_DECREF(dict);\n        return res;\n    }\n    return Py_BuildValue(\"O()\", Py_TYPE(self));\n}\nstatic PyMethodDef ast_type_methods[] = {\n    {\"__reduce__\", ast_type_reduce, METH_NOARGS, NULL},\n    {NULL}\n};\nstatic PyGetSetDef ast_type_getsets[] = {\n    {\"__dict__\", PyObject_GenericGetDict, PyObject_GenericSetDict},\n    {NULL}\n};\nstatic PyTypeObject AST_type = {\n    PyVarObject_HEAD_INIT(NULL, 0)\n    \"_ast3.AST\",\n    sizeof(AST_object),\n    0,\n    (destructor)ast_dealloc, /* tp_dealloc */\n    0,                       /* tp_print */\n    0,                       /* tp_getattr */\n    0,                       /* tp_setattr */\n    0,                       /* tp_reserved */\n    0,                       /* tp_repr */\n    0,                       /* tp_as_number */\n    0,                       /* tp_as_sequence */\n    0,                       /* tp_as_mapping */\n    0,                       /* tp_hash */\n    0,                       /* tp_call */\n    0,                       /* tp_str */\n    PyObject_GenericGetAttr, /* tp_getattro */\n    PyObject_GenericSetAttr, /* tp_setattro */\n    0,                       /* tp_as_buffer */\n    Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE | Py_TPFLAGS_HAVE_GC, /* tp_flags */\n    0,                       /* tp_doc */\n    (traverseproc)ast_traverse, /* tp_traverse */\n    (inquiry)ast_clear,      /* tp_clear */\n    0,                       /* tp_richcompare */\n    0,                       /* tp_weaklistoffset */\n    0,                       /* tp_iter */\n    0,                       /* tp_iternext */\n    ast_type_methods,        /* tp_methods */\n    0,                       /* tp_members */\n    ast_type_getsets,        /* tp_getset */\n    0,                       /* tp_base */\n    0,                       /* tp_dict */\n    0,                       /* tp_descr_get */\n    0,                       /* tp_descr_set */\n    offsetof(AST_object, dict),/* tp_dictoffset */\n    (initproc)ast_type_init, /* tp_init */\n    PyType_GenericAlloc,     /* tp_alloc */\n    PyType_GenericNew,       /* tp_new */\n    PyObject_GC_Del,         /* tp_free */\n};\nstatic PyTypeObject* make_type(char *type, PyTypeObject* base, char**fields, int num_fields)\n{\n    PyObject *fnames, *result;\n    int i;\n    fnames = PyTuple_New(num_fields);\n    if (!fnames) return NULL;\n    for (i = 0; i < num_fields; i++) {\n        PyObject *field = PyUnicode_FromString(fields[i]);\n        if (!field) {\n            Py_DECREF(fnames);\n            return NULL;\n        }\n        PyTuple_SET_ITEM(fnames, i, field);\n    }\n    result = PyObject_CallFunction((PyObject*)&PyType_Type, \"s(O){sOss}\",\n                    type, base, \"_fields\", fnames, \"__module__\", \"_ast3\");\n    Py_DECREF(fnames);\n    return (PyTypeObject*)result;\n}\nstatic int add_attributes(PyTypeObject* type, char**attrs, int num_fields)\n{\n    int i, result;\n    _Py_IDENTIFIER(_attributes);\n    PyObject *s, *l = PyTuple_New(num_fields);\n    if (!l)\n        return 0;\n    for (i = 0; i < num_fields; i++) {\n        s = PyUnicode_FromString(attrs[i]);\n        if (!s) {\n            Py_DECREF(l);\n            return 0;\n        }\n        PyTuple_SET_ITEM(l, i, s);\n    }\n    result = _PyObject_SetAttrId((PyObject*)type, &PyId__attributes, l) >= 0;\n    Py_DECREF(l);\n    return result;\n}\n/* Conversion AST -> Python */\nstatic PyObject* ast2obj_list(asdl_seq *seq, PyObject* (*func)(void*))\n{\n    Py_ssize_t i, n = asdl_seq_LEN(seq);\n    PyObject *result = PyList_New(n);\n    PyObject *value;\n    if (!result)\n        return NULL;\n    for (i = 0; i < n; i++) {\n        value = func(asdl_seq_GET(seq, i));\n        if (!value) {\n            Py_DECREF(result);\n            return NULL;\n        }\n        PyList_SET_ITEM(result, i, value);\n    }\n    return result;\n}\nstatic PyObject* ast2obj_object(void *o)\n{\n    if (!o)\n        o = Py_None;\n    Py_INCREF((PyObject*)o);\n    return (PyObject*)o;\n}\nstatic PyObject* ast2obj_int(long b)\n{\n    return PyLong_FromLong(b);\n}\n/* Conversion Python -> AST */\nstatic int obj2ast_singleton(PyObject *obj, PyObject** out, PyArena* arena)\n{\n    if (obj != Py_None && obj != Py_True && obj != Py_False) {\n        PyErr_SetString(PyExc_ValueError,\n                        \"AST singleton must be True, False, or None\");\n        return 1;\n    }\n    *out = obj;\n    return 0;\n}\nstatic int obj2ast_object(PyObject* obj, PyObject** out, PyArena* arena)\n{\n    if (obj == Py_None)\n        obj = NULL;\n    if (obj) {\n        if (PyArena_AddPyObject(arena, obj) < 0) {\n            *out = NULL;\n            return -1;\n        }\n        Py_INCREF(obj);\n    }\n    *out = obj;\n    return 0;\n}\nstatic int obj2ast_constant(PyObject* obj, PyObject** out, PyArena* arena)\n{\n    if (obj) {\n        if (PyArena_AddPyObject(arena, obj) < 0) {\n            *out = NULL;\n            return -1;\n        }\n        Py_INCREF(obj);\n    }\n    *out = obj;\n    return 0;\n}\nstatic int obj2ast_identifier(PyObject* obj, PyObject** out, PyArena* arena)\n{\n    if (!PyUnicode_CheckExact(obj) && obj != Py_None) {\n        PyErr_SetString(PyExc_TypeError, \"AST identifier must be of type str\");\n        return 1;\n    }\n    return obj2ast_object(obj, out, arena);\n}\nstatic int obj2ast_string(PyObject* obj, PyObject** out, PyArena* arena)\n{\n    if (!PyUnicode_CheckExact(obj) && !PyBytes_CheckExact(obj)) {\n        PyErr_SetString(PyExc_TypeError, \"AST string must be of type str\");\n        return 1;\n    }\n    return obj2ast_object(obj, out, arena);\n}\nstatic int obj2ast_bytes(PyObject* obj, PyObject** out, PyArena* arena)\n{\n    if (!PyBytes_CheckExact(obj)) {\n        PyErr_SetString(PyExc_TypeError, \"AST bytes must be of type bytes\");\n        return 1;\n    }\n    return obj2ast_object(obj, out, arena);\n}\nstatic int obj2ast_int(PyObject* obj, int* out, PyArena* arena)\n{\n    int i;\n    if (!PyLong_Check(obj)) {\n        PyErr_Format(PyExc_ValueError, \"invalid integer value: %R\", obj);\n        return 1;\n    }\n    i = _PyLong_AsInt(obj);\n    if (i == -1 && PyErr_Occurred())\n        return 1;\n    *out = i;\n    return 0;\n}\nstatic int add_ast_fields(void)\n{\n    PyObject *empty_tuple, *d;\n    if (PyType_Ready(&AST_type) < 0)\n        return -1;\n    d = AST_type.tp_dict;\n    empty_tuple = PyTuple_New(0);\n    if (!empty_tuple ||\n        PyDict_SetItemString(d, \"_fields\", empty_tuple) < 0 ||\n        PyDict_SetItemString(d, \"_attributes\", empty_tuple) < 0) {\n        Py_XDECREF(empty_tuple);\n        return -1;\n    }\n    Py_DECREF(empty_tuple);\n    return 0;\n}\nstatic int exists_not_none(PyObject *obj, _Py_Identifier *id)\n{\n    int isnone;\n    PyObject *attr = _PyObject_GetAttrId(obj, id);\n    if (!attr) {\n        PyErr_Clear();\n        return 0;\n    }\n    isnone = attr == Py_None;\n    Py_DECREF(attr);\n    return !isnone;\n}\n\"\"\", 0, reflow=False)\n        self.emit(\"static int init_types(void)\",0)\n        self.emit(\"{\", 0)\n        self.emit(\"static int initialized;\", 1)\n        self.emit(\"if (initialized) return 1;\", 1)\n        self.emit(\"if (add_ast_fields() < 0) return 0;\", 1)\n        for dfn in mod.dfns:\n            self.visit(dfn)\n        self.emit(\"initialized = 1;\", 1)\n        self.emit(\"return 1;\", 1);\n        self.emit(\"}\", 0)\n    def func_begin(self, name):\n        ctype = get_c_type(name)\n        self.emit(\"PyObject*\", 0)\n        self.emit(\"ast2obj_%s(void* _o)\" % (name), 0)\n        self.emit(\"{\", 0)\n        self.emit(\"%s o = (%s)_o;\" % (ctype, ctype), 1)\n        self.emit(\"PyObject *result = NULL, *value = NULL;\", 1)\n        self.emit('if (!o) {', 1)\n        self.emit(\"Py_INCREF(Py_None);\", 2)\n        self.emit('return Py_None;', 2)\n        self.emit(\"}\", 1)\n        self.emit('', 0)\ndef main(srcfile, dump_module=False):\n    argv0 = sys.argv[0]\n    components = argv0.split(os.sep)\n    argv0 = os.sep.join(components[-2:])\n    auto_gen_msg = common_msg % argv0\n    mod = asdl.parse(srcfile)\n    if dump_module:\n        print('Parsed Module:')\n        print(mod)\n    if not asdl.check(mod):\n        sys.exit(1)\n    if INC_DIR:\n        p = \"%s/%s-ast.h\" % (INC_DIR, mod.name)\n        f = open(p, \"w\")\n        f.write(auto_gen_msg)\n        f.write('\n        c = ChainOfVisitors(TypeDefVisitor(f),\n                            StructVisitor(f),\n                            PrototypeVisitor(f),\n                            )\n        c.visit(mod)\n        f.write(\"PyObject* Ta3AST_mod2obj(mod_ty t);\\n\")\n        f.write(\"mod_ty Ta3AST_obj2mod(PyObject* ast, PyArena* arena, int mode);\\n\")\n        f.write(\"int Ta3AST_Check(PyObject* obj);\\n\")\n        f.close()\n    if SRC_DIR:\n        p = os.path.join(SRC_DIR, str(mod.name) + \"-ast.c\")\n        f = open(p, \"w\")\n        f.write(auto_gen_msg)\n        f.write('\n        f.write('\\n')\n        f.write('\n        f.write('\n        f.write('\\n')\n        f.write(\"static PyTypeObject AST_type;\\n\")\n        v = ChainOfVisitors(\n            PyTypesDeclareVisitor(f),\n            PyTypesVisitor(f),\n            Obj2ModPrototypeVisitor(f),\n            FunctionVisitor(f),\n            ObjVisitor(f),\n            Obj2ModVisitor(f),\n            ASTModuleVisitor(f),\n            PartingShots(f),\n            )\n        v.visit(mod)\n        f.close()",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2019-19275",
        "description": "[{'lang': 'en', 'value': 'typed_ast 1.3.0 and 1.3.1 has an ast_for_arguments out-of-bounds read. An attacker with the ability to cause a Python interpreter to parse Python source (but not necessarily execute it) may be able to crash the interpreter process. This could be a concern, for example, in a web-based service that parses (but does not execute) Python code. (This issue also affected certain Python 3.8.0-alpha prereleases.)'}]",
        "cwe_number": 125
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-165",
      "code": "def login():\n    from flask_login import current_user\n    redirect_url = request.args.get(\"redirect\", request.script_root + url_for(\"index\"))\n    permissions = sorted(\n        filter(\n            lambda x: x is not None and isinstance(x, OctoPrintPermission),\n            map(\n                lambda x: getattr(Permissions, x.strip()),\n                request.args.get(\"permissions\", \"\").split(\",\"),\n            ),\n        ),\n        key=lambda x: x.get_name(),\n    )\n    if not permissions:\n        permissions = [Permissions.STATUS, Permissions.SETTINGS_READ]\n    user_id = request.args.get(\"user_id\", \"\")\n    if (not user_id or current_user.get_id() == user_id) and has_permissions(\n        *permissions\n    ):\n        return redirect(redirect_url)\n    render_kwargs = {\n        \"theming\": [],\n        \"redirect_url\": redirect_url,\n        \"permission_names\": map(lambda x: x.get_name(), permissions),\n        \"user_id\": user_id,\n        \"logged_in\": not current_user.is_anonymous,\n    }\n    try:\n        additional_assets = _add_additional_assets(\"octoprint.theming.login\")\n        additional_assets += _add_additional_assets(\"octoprint.plugin.forcelogin.theming\")\n        additional_assets += _add_additional_assets(\"octoprint.plugin.loginui.theming\")\n        render_kwargs.update({\"theming\": additional_assets})\n    except Exception:\n        _logger.exception(\"Error processing theming CSS, ignoring\")\n    return render_template(\"login.jinja2\", **render_kwargs)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-1430",
        "description": "[{'lang': 'en', 'value': 'Cross-site Scripting (XSS) - DOM in GitHub repository octoprint/octoprint prior to 1.8.0.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-166",
      "code": "    def exec_command(self, cmd, tmp_path, become_user=None, sudoable=False, executable='/bin/sh', in_data=None):\n        ''' run a command on the chroot '''\n        if sudoable and self.runner.become and self.runner.become_method not in self.become_methods_supported:\n            raise errors.AnsibleError(\"Internal Error: this module does not support running commands via %s\" % self.runner.become_method)\n        if in_data:\n            raise errors.AnsibleError(\"Internal Error: this module does not support optimized module pipelining\")\n        if executable:\n            local_cmd = [self.chroot_cmd, self.chroot, executable, '-c', cmd]\n        else:\n            local_cmd = '%s \"%s\" %s' % (self.chroot_cmd, self.chroot, cmd)\n        vvv(\"EXEC %s\" % (local_cmd), host=self.chroot)\n        p = subprocess.Popen(local_cmd, shell=isinstance(local_cmd, basestring),\n                             cwd=self.runner.basedir,\n                             stdin=subprocess.PIPE,\n                             stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        stdout, stderr = p.communicate()\n        return (p.returncode, '', stdout, stderr)\n    def put_file(self, in_path, out_path):\n        ''' transfer a file from local to chroot '''\n        if not out_path.startswith(os.path.sep):\n            out_path = os.path.join(os.path.sep, out_path)\n        normpath = os.path.normpath(out_path)\n        out_path = os.path.join(self.chroot, normpath[1:])\n        vvv(\"PUT %s TO %s\" % (in_path, out_path), host=self.chroot)\n        if not os.path.exists(in_path):\n            raise errors.AnsibleFileNotFound(\"file or module does not exist: %s\" % in_path)\n        try:\n            shutil.copyfile(in_path, out_path)\n        except shutil.Error:\n            traceback.print_exc()\n            raise errors.AnsibleError(\"failed to copy: %s and %s are the same\" % (in_path, out_path))\n        except IOError:\n            traceback.print_exc()\n            raise errors.AnsibleError(\"failed to transfer file to %s\" % out_path)\n    def fetch_file(self, in_path, out_path):\n        ''' fetch a file from chroot to local '''\n        if not in_path.startswith(os.path.sep):\n            in_path = os.path.join(os.path.sep, in_path)\n        normpath = os.path.normpath(in_path)\n        in_path = os.path.join(self.chroot, normpath[1:])\n        vvv(\"FETCH %s TO %s\" % (in_path, out_path), host=self.chroot)\n        if not os.path.exists(in_path):\n            raise errors.AnsibleFileNotFound(\"file or module does not exist: %s\" % in_path)\n        try:\n            shutil.copyfile(in_path, out_path)\n        except shutil.Error:\n            traceback.print_exc()\n            raise errors.AnsibleError(\"failed to copy: %s and %s are the same\" % (in_path, out_path))\n        except IOError:\n            traceback.print_exc()\n            raise errors.AnsibleError(\"failed to transfer file to %s\" % out_path)\n    def close(self):\n        ''' terminate the connection; nothing to do here '''\n        pass",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2015-6240",
        "description": "[{'lang': 'en', 'value': 'The chroot, jail, and zone connection plugins in ansible before 1.9.2 allow local users to escape a restricted environment via a symlink attack.'}]",
        "cwe_number": 59
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-167",
      "code": "def _process_shipping_data_for_order(\n    checkout_info: \"CheckoutInfo\",\n    base_shipping_price: Money,\n    shipping_price: TaxedMoney,\n    manager: \"PluginsManager\",\n    lines: Iterable[\"CheckoutLineInfo\"],\n) -> dict[str, Any]:\n    \"\"\"Fetch, process and return shipping data from checkout.\"\"\"\n    delivery_method_info = checkout_info.delivery_method_info\n    shipping_address = delivery_method_info.shipping_address\n    if (\n        delivery_method_info.store_as_customer_address\n        and checkout_info.user\n        and shipping_address\n    ):\n        store_user_address(\n            checkout_info.user, shipping_address, AddressType.SHIPPING, manager=manager\n        )\n        if checkout_info.user.addresses.filter(pk=shipping_address.pk).exists():\n            shipping_address = shipping_address.get_copy()\n    shipping_method = delivery_method_info.delivery_method\n    tax_class = getattr(shipping_method, \"tax_class\", None)\n    result: dict[str, Any] = {\n        \"shipping_address\": shipping_address,\n        \"base_shipping_price\": base_shipping_price,\n        \"shipping_price\": shipping_price,\n        \"weight\": checkout_info.checkout.get_total_weight(lines),\n        **get_shipping_tax_class_kwargs_for_order(tax_class),\n    }\n    result.update(delivery_method_info.delivery_method_order_field)\n    result.update(delivery_method_info.delivery_method_name)\n    return result\ndef _process_user_data_for_order(checkout_info: \"CheckoutInfo\", manager):\n    \"\"\"Fetch, process and return shipping data from checkout.\"\"\"\n    billing_address = checkout_info.billing_address\n    if checkout_info.user and billing_address:\n        store_user_address(\n            checkout_info.user, billing_address, AddressType.BILLING, manager=manager\n        )\n        if checkout_info.user.addresses.filter(pk=billing_address.pk).exists():\n            billing_address = billing_address.get_copy()\n    return {\n        \"user\": checkout_info.user,\n        \"user_email\": checkout_info.get_customer_email(),\n        \"billing_address\": billing_address,\n        \"customer_note\": checkout_info.checkout.note,\n    }",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-29888",
        "description": "[{'lang': 'en', 'value': 'Saleor is an e-commerce platform that serves high-volume companies. When using `Pickup: Local stock only` click-and-collect as a delivery method in specific conditions the customer could overwrite the warehouse address with its own, which exposes its address as click-and-collect address. This issue has been patched in versions: `3.14.61`, `3.15.37`, `3.16.34`, `3.17.32`, `3.18.28`, `3.19.15`.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-168",
      "code": "def preprocess_input_exprs_arg_string(input_exprs_str):\n  \"\"\"Parses input arg into dictionary that maps input key to python expression.\n  Parses input string in the format of 'input_key=<python expression>' into a\n  dictionary that maps each input_key to its python expression.\n  Args:\n    input_exprs_str: A string that specifies python expression for input keys.\n    Each input is separated by semicolon. For each input key:\n        'input_key=<python expression>'\n  Returns:\n    A dictionary that maps input keys to their values.\n  Raises:\n    RuntimeError: An error when the given input string is in a bad format.\n  \"\"\"\n  input_dict = {}\n  for input_raw in filter(bool, input_exprs_str.split(';')):\n    if '=' not in input_exprs_str:\n      raise RuntimeError('--input_exprs \"%s\" format is incorrect. Please follow'\n                         '\"<input_key>=<python expression>\"' % input_exprs_str)\n    input_key, expr = input_raw.split('=', 1)\n    input_dict[input_key] = eval(expr)\n  return input_dict\ndef load_inputs_from_input_arg_string(inputs_str, input_exprs_str,\n                                      input_examples_str):\n  \"\"\"Parses input arg strings and create inputs feed_dict.\n  Parses '--inputs' string for inputs to be loaded from file, and parses\n  '--input_exprs' string for inputs to be evaluated from python expression.\n  '--input_examples' string for inputs to be created from tf.example feature\n  dictionary list.\n  Args:\n    inputs_str: A string that specified where to load inputs. Each input is\n        separated by semicolon.\n        * For each input key:\n            '<input_key>=<filename>' or\n            '<input_key>=<filename>[<variable_name>]'\n        * The optional 'variable_name' key will be set to None if not specified.\n        * File specified by 'filename' will be loaded using numpy.load. Inputs\n            can be loaded from only .npy, .npz or pickle files.\n        * The \"[variable_name]\" key is optional depending on the input file type\n            as descripted in more details below.\n        When loading from a npy file, which always contains a numpy ndarray, the\n        content will be directly assigned to the specified input tensor. If a\n        variable_name is specified, it will be ignored and a warning will be\n        issued.\n        When loading from a npz zip file, user can specify which variable within\n        the zip file to load for the input tensor inside the square brackets. If\n        nothing is specified, this function will check that only one file is\n        included in the zip and load it for the specified input tensor.\n        When loading from a pickle file, if no variable_name is specified in the\n        square brackets, whatever that is inside the pickle file will be passed\n        to the specified input tensor, else SavedModel CLI will assume a\n        dictionary is stored in the pickle file and the value corresponding to\n        the variable_name will be used.\n    input_exprs_str: A string that specifies python expressions for inputs.\n        * In the format of: '<input_key>=<python expression>'.\n        * numpy module is available as np.\n    input_examples_str: A string that specifies tf.Example with dictionary.\n        * In the format of: '<input_key>=<[{feature:value list}]>'\n  Returns:\n    A dictionary that maps input tensor keys to numpy ndarrays.\n  Raises:\n    RuntimeError: An error when a key is specified, but the input file contains\n        multiple numpy ndarrays, none of which matches the given key.\n    RuntimeError: An error when no key is specified, but the input file contains\n        more than one numpy ndarrays.\n  \"\"\"\n  tensor_key_feed_dict = {}\n  inputs = preprocess_inputs_arg_string(inputs_str)\n  input_exprs = preprocess_input_exprs_arg_string(input_exprs_str)\n  input_examples = preprocess_input_examples_arg_string(input_examples_str)\n  for input_tensor_key, (filename, variable_name) in inputs.items():\n    data = np.load(file_io.FileIO(filename, mode='rb'), allow_pickle=True)\n    if variable_name:\n      if isinstance(data, np.ndarray):\n        logging.warn(\n            'Input file %s contains a single ndarray. Name key \\\"%s\\\" ignored.'\n            % (filename, variable_name))\n        tensor_key_feed_dict[input_tensor_key] = data\n      else:\n        if variable_name in data:\n          tensor_key_feed_dict[input_tensor_key] = data[variable_name]\n        else:\n          raise RuntimeError(\n              'Input file %s does not contain variable with name \\\"%s\\\".' %\n              (filename, variable_name))\n    else:\n      if isinstance(data, np.lib.npyio.NpzFile):\n        variable_name_list = data.files\n        if len(variable_name_list) != 1:\n          raise RuntimeError(\n              'Input file %s contains more than one ndarrays. Please specify '\n              'the name of ndarray to use.' % filename)\n        tensor_key_feed_dict[input_tensor_key] = data[variable_name_list[0]]\n      else:\n        tensor_key_feed_dict[input_tensor_key] = data\n  for input_tensor_key, py_expr_evaluated in input_exprs.items():\n    if input_tensor_key in tensor_key_feed_dict:\n      logging.warn(\n          'input_key %s has been specified with both --inputs and --input_exprs'\n          ' options. Value in --input_exprs will be used.' % input_tensor_key)\n    tensor_key_feed_dict[input_tensor_key] = py_expr_evaluated\n  for input_tensor_key, example in input_examples.items():\n    if input_tensor_key in tensor_key_feed_dict:\n      logging.warn(\n          'input_key %s has been specified in multiple options. Value in '\n          '--input_examples will be used.' % input_tensor_key)\n    tensor_key_feed_dict[input_tensor_key] = example\n  return tensor_key_feed_dict\ndef add_run_subparser(subparsers):\n  \"\"\"Add parser for `run`.\"\"\"\n  run_msg = ('Usage example:\\n'\n             'To run input tensors from files through a MetaGraphDef and save'\n             ' the output tensors to files:\\n'\n             '$saved_model_cli show --dir /tmp/saved_model --tag_set serve \\\\\\n'\n             '   --signature_def serving_default \\\\\\n'\n             '   --inputs input1_key=/tmp/124.npz[x],input2_key=/tmp/123.npy '\n             '\\\\\\n'\n             '   --input_exprs \\'input3_key=np.ones(2)\\' \\\\\\n'\n             '   --input_examples '\n             '\\'input4_key=[{\"id\":[26],\"weights\":[0.5, 0.5]}]\\' \\\\\\n'\n             '   --outdir=/out\\n\\n'\n             'For more information about input file format, please see:\\n'\n             'https://www.tensorflow.org/guide/saved_model_cli\\n')\n  parser_run = subparsers.add_parser(\n      'run', description=run_msg, formatter_class=argparse.RawTextHelpFormatter)\n  parser_run.add_argument(\n      '--dir',\n      type=str,\n      required=True,\n      help='directory containing the SavedModel to execute')\n  parser_run.add_argument(\n      '--tag_set',\n      type=str,\n      required=True,\n      help='tag-set of graph in SavedModel to load, separated by \\',\\'')\n  parser_run.add_argument(\n      '--signature_def',\n      type=str,\n      required=True,\n      metavar='SIGNATURE_DEF_KEY',\n      help='key of SignatureDef to run')\n  msg = ('Loading inputs from files, in the format of \\'<input_key>=<filename>,'\n         ' or \\'<input_key>=<filename>[<variable_name>]\\', separated by \\';\\'.'\n         ' The file format can only be from .npy, .npz or pickle.')\n  parser_run.add_argument('--inputs', type=str, default='', help=msg)\n  msg = ('Specifying inputs by python expressions, in the format of'\n         ' \"<input_key>=\\'<python expression>\\'\", separated by \\';\\'. '\n         'numpy module is available as \\'np\\'. '\n         'Will override duplicate input keys from --inputs option.')\n  parser_run.add_argument('--input_exprs', type=str, default='', help=msg)\n  msg = (\n      'Specifying tf.Example inputs as list of dictionaries. For example: '\n      '<input_key>=[{feature0:value_list,feature1:value_list}]. Use \";\" to '\n      'separate input keys. Will override duplicate input keys from --inputs '\n      'and --input_exprs option.')\n  parser_run.add_argument('--input_examples', type=str, default='', help=msg)\n  parser_run.add_argument(\n      '--outdir',\n      type=str,\n      default=None,\n      help='if specified, output tensor(s) will be saved to given directory')\n  parser_run.add_argument(\n      '--overwrite',\n      action='store_true',\n      help='if set, output file will be overwritten if it already exists.')\n  parser_run.add_argument(\n      '--tf_debug',\n      action='store_true',\n      help='if set, will use TensorFlow Debugger (tfdbg) to watch the '\n           'intermediate Tensors and runtime GraphDefs while running the '\n           'SavedModel.')\n  parser_run.add_argument(\n      '--worker',\n      type=str,\n      default=None,\n      help='if specified, a Session will be run on the worker. '\n           'Valid worker specification is a bns or gRPC path.')\n  parser_run.add_argument(\n      '--init_tpu',\n      action='store_true',\n      default=None,\n      help='if specified, tpu.initialize_system will be called on the Session. '\n           'This option should be only used if the worker is a TPU job.')\n  parser_run.set_defaults(func=run)\n  def testInputPreProcessFormats(self):\n    input_str = 'input1=/path/file.txt[ab3];input2=file2'\n    input_expr_str = 'input3=np.zeros([2,2]);input4=[4,5]'\n    input_dict = saved_model_cli.preprocess_inputs_arg_string(input_str)\n    input_expr_dict = saved_model_cli.preprocess_input_exprs_arg_string(\n        input_expr_str)\n    self.assertTrue(input_dict['input1'] == ('/path/file.txt', 'ab3'))\n    self.assertTrue(input_dict['input2'] == ('file2', None))\n    print(input_expr_dict['input3'])\n    self.assertAllClose(input_expr_dict['input3'], np.zeros([2, 2]))\n    self.assertAllClose(input_expr_dict['input4'], [4, 5])\n    self.assertTrue(len(input_dict) == 2)\n    self.assertTrue(len(input_expr_dict) == 2)\n  def testInputPreProcessErrorBadFormat(self):\n    input_str = 'inputx=file[[v1]v2'\n    with self.assertRaises(RuntimeError):\n      saved_model_cli.preprocess_inputs_arg_string(input_str)\n    input_str = 'inputx:file'\n    with self.assertRaises(RuntimeError):\n      saved_model_cli.preprocess_inputs_arg_string(input_str)\n    input_str = 'inputx:np.zeros((5))'\n    with self.assertRaises(RuntimeError):\n      saved_model_cli.preprocess_input_exprs_arg_string(input_str)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-41228",
        "description": "[{'lang': 'en', 'value': \"TensorFlow is an open source platform for machine learning. In affected versions TensorFlow's `saved_model_cli` tool is vulnerable to a code injection as it calls `eval` on user supplied strings. This can be used by attackers to run arbitrary code on the plaform where the CLI tool runs. However, given that the tool is always run manually, the impact of this is not severe. We have patched this by adding a `safe` flag which defaults to `True` and an explicit warning for users. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.\"}]",
        "cwe_number": 94
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-169",
      "code": "def parse_options_header(value):\n    \"\"\"\n    Parses a Content-Type header into a value in the following format:\n        (content_type, {parameters})\n    \"\"\"\n    if not value:\n        return (b'', {})\n    if isinstance(value, str):\n        value = value.encode('latin-1')\n    if b';' not in value:\n        return (value.lower().strip(), {})\n    ctype, rest = value.split(b';', 1)\n    options = {}\n    for match in OPTION_RE.finditer(rest):\n        key = match.group(1).lower()\n        value = match.group(2)\n        if value[0] == QUOTE and value[-1] == QUOTE:\n            value = value[1:-1]\n            value = value.replace(b'\\\\\\\\', b'\\\\').replace(b'\\\\\"', b'\"')\n        if key == b'filename':\n            if value[1:3] == b':\\\\' or value[:2] == b'\\\\\\\\':\n                value = value.split(b'\\\\')[-1]\n        options[key] = value\n    return ctype, options",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-24762",
        "description": "[{'lang': 'en', 'value': \"`python-multipart` is a streaming multipart parser for Python. When using form data, `python-multipart` uses a Regular Expression to parse the HTTP `Content-Type` header, including options. An attacker could send a custom-made `Content-Type` option that is very difficult for the RegEx to process, consuming CPU resources and stalling indefinitely (minutes or more) while holding the main event loop. This means that process can't handle any more requests, leading to regular expression denial of service. This vulnerability has been patched in version 0.0.7.\"}]",
        "cwe_number": 1333
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-170",
      "code": "def set_session_tracks(display_obj):\n    \"\"\"Save igv tracks as a session object. This way it's easy to verify that a user is requesting one of these files from remote_static view endpoint\n    Args:\n        display_obj(dict): A display object containing case name, list of genes, lucus and tracks\n    \"\"\"\n    session_tracks = list(display_obj.get(\"reference_track\", {}).values())\n    for key, track_items in display_obj.items():\n        if key not in [\"tracks\", \"custom_tracks\", \"sample_tracks\"]:\n            continue\n        for track_item in track_items:\n            session_tracks += list(track_item.values())\n    session[\"igv_tracks\"] = session_tracks\ndef make_igv_tracks(case_obj, variant_id, chrom=None, start=None, stop=None):\n    \"\"\"Create a dictionary containing the required tracks for displaying IGV tracks for case or a group of cases\n    Args:\n        institute_id(str): institute _id\n        case_obj(scout.models.Case)\n        variant_id(str): _id of a variant\n        chrom(str/None): requested chromosome [1-22], X, Y, [M-MT]\n        start(int/None): start of the genomic interval to be displayed\n        stop(int/None): stop of the genomic interval to be displayed\n    Returns:\n        display_obj(dict): A display object containing case name, list of genes, lucus and tracks\n    \"\"\"\n    display_obj = {}\n    variant_obj = store.variant(document_id=variant_id)\n    if variant_obj:\n        start = start or variant_obj[\"position\"]\n        stop = stop or variant_obj[\"end\"]\n        chromosome = chrom or variant_obj.get(\"chromosome\")\n        chromosome = chromosome.replace(\"MT\", \"M\")\n        display_obj[\"locus\"] = \"chr{0}:{1}-{2}\".format(chromosome, start, stop)\n    else:\n        chromosome = \"All\"\n    if \"38\" in str(case_obj.get(\"genome_build\", \"37\")) or chromosome == \"M\":\n        build = \"38\"\n    else:\n        build = \"37\"\n    set_common_tracks(display_obj, build)\n    grouped_cases = []\n    for group in case_obj.get(\"group\", []):\n        group_cases = list(store.cases(group=group))\n        for case in group_cases:\n            case_append_alignments(case)\n            grouped_cases.append(case)\n    if not grouped_cases:\n        case_append_alignments(case_obj)\n        grouped_cases.append(case_obj)\n    set_sample_tracks(display_obj, grouped_cases, chromosome)\n    if chrom != \"M\":\n        set_case_specific_tracks(display_obj, case_obj)\n    set_cloud_public_tracks(display_obj, build)\n    display_obj[\"display_center_guide\"] = True\n    return display_obj\ndef remote_cors(remote_url):\n    \"\"\"Proxy a remote URL.\n    Useful to e.g. eliminate CORS issues when the remote site does not\n        communicate CORS headers well, as in cloud tracks on figshare for IGV.js.\n    Based on code from answers to this thread:\n        https://stackoverflow.com/questions/6656363/proxying-to-another-web-service-with-flask/\n    \"\"\"\n    resp = requests.request(\n        method=request.method,\n        url=remote_url,\n        headers={key: value for (key, value) in request.headers if key != \"Host\"},\n        data=request.get_data(),\n        cookies=request.cookies,\n        allow_redirects=True,\n    )\n    excluded_headers = [\n        \"content-encoding\",\n        \"content-length\",\n        \"transfer-encoding\",\n        \"connection\",\n    ]\n    headers = [\n        (name, value)\n        for (name, value) in resp.raw.headers.items()\n        if name.lower() not in excluded_headers\n    ]\n    response = Response(resp.content, resp.status_code, headers)\n    return response\ndef remote_static():\n    \"\"\"Stream *large* static files with special requirements.\"\"\"\n    file_path = request.args.get(\"file\") or \".\"\n    if current_user.is_authenticated is False or file_path not in session.get(\"igv_tracks\", []):\n        LOG.warning(f\"{file_path} not in {session.get('igv_tracks', [])}\")\n        return abort(403)\n    range_header = request.headers.get(\"Range\", None)\n    if not range_header and (file_path.endswith(\".bam\") or file_path.endswith(\".cram\")):\n        return abort(500)\n    new_resp = send_file_partial(file_path)\n    return new_resp",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-1592",
        "description": "[{'lang': 'en', 'value': 'Server-Side Request Forgery in scout in GitHub repository clinical-genomics/scout prior to v4.42. An attacker could make the application perform arbitrary requests to fishing steal cookie, request to private area, or lead to xss...'}]",
        "cwe_number": 918
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-171",
      "code": "def get_jinja_env():\n    jinja_env = Environment(extensions=settings.DOCXTEMPLATE_JINJA_EXTENSIONS)\n    jinja_env.filters.update(get_jinja_filters())\n    return jinja_env",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-37301",
        "description": "[{'lang': 'en', 'value': 'Document Merge Service is a document template merge service providing an API to manage templates and merge them with given data. Versions 6.5.1 and prior are vulnerable to remote code execution via server-side template injection which, when executed as root, can result in full takeover of the affected system. As of time of publication, no patched version exists, nor have any known workarounds been disclosed.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-172",
      "code": "  def test_scatter_ops_even_partition(self, op):\n    v = variables_lib.Variable(array_ops.zeros((30, 1)))\n    sparse_delta = ops.IndexedSlices(\n        values=constant_op.constant([[0.], [1.], [2.], [3.], [4.]]),\n        indices=constant_op.constant([0, 10, 12, 21, 22]))\n    v0 = variables_lib.Variable(array_ops.zeros((10, 1)))\n    v1 = variables_lib.Variable(array_ops.zeros((10, 1)))\n    v2 = variables_lib.Variable(array_ops.zeros((10, 1)))\n    sv = sharded_variable.ShardedVariable([v0, v1, v2])\n    getattr(v, op)(sparse_delta, name='scatter_v')\n    getattr(sv, op)(sparse_delta, name='scatter_sv')\n    self.assertAllEqual(v, ops.convert_to_tensor(sv))\n    @def_function.function\n    def func():\n      getattr(v, op)(sparse_delta, name='scatter_v')\n      getattr(sv, op)(sparse_delta, name='scatter_sv')\n    func()\n    self.assertAllEqual(v, ops.convert_to_tensor(sv))",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-37642",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an end-to-end open source platform for machine learning. In affected versions the implementation of `tf.raw_ops.ResourceScatterDiv` is vulnerable to a division by 0 error. The [implementation](https://github.com/tensorflow/tensorflow/blob/8d72537c6abf5a44103b57b9c2e22c14f5f49698/tensorflow/core/kernels/resource_variable_ops.cc#L865) uses a common class for all binary operations but fails to treat the division by 0 case separately. We have patched the issue in GitHub commit 4aacb30888638da75023e6601149415b39763d76. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.'}]",
        "cwe_number": 369
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-173",
      "code": "    def read_config(self, config, **kwargs):\n        self.enable_registration = strtobool(\n            str(config.get(\"enable_registration\", False))\n        )\n        if \"disable_registration\" in config:\n            self.enable_registration = not strtobool(\n                str(config[\"disable_registration\"])\n            )\n        self.account_validity = AccountValidityConfig(\n            config.get(\"account_validity\") or {}, config\n        )\n        self.registrations_require_3pid = config.get(\"registrations_require_3pid\", [])\n        self.allowed_local_3pids = config.get(\"allowed_local_3pids\", [])\n        self.enable_3pid_lookup = config.get(\"enable_3pid_lookup\", True)\n        self.registration_shared_secret = config.get(\"registration_shared_secret\")\n        self.bcrypt_rounds = config.get(\"bcrypt_rounds\", 12)\n        self.trusted_third_party_id_servers = config.get(\n            \"trusted_third_party_id_servers\", [\"matrix.org\", \"vector.im\"]\n        )\n        account_threepid_delegates = config.get(\"account_threepid_delegates\") or {}\n        self.account_threepid_delegate_email = account_threepid_delegates.get(\"email\")\n        self.account_threepid_delegate_msisdn = account_threepid_delegates.get(\"msisdn\")\n        self.default_identity_server = config.get(\"default_identity_server\")\n        self.allow_guest_access = config.get(\"allow_guest_access\", False)\n        if config.get(\"invite_3pid_guest\", False):\n            raise ConfigError(\"invite_3pid_guest is no longer supported\")\n        self.auto_join_rooms = config.get(\"auto_join_rooms\", [])\n        for room_alias in self.auto_join_rooms:\n            if not RoomAlias.is_valid(room_alias):\n                raise ConfigError(\"Invalid auto_join_rooms entry %s\" % (room_alias,))\n        self.autocreate_auto_join_rooms = config.get(\"autocreate_auto_join_rooms\", True)\n        self.autocreate_auto_join_rooms_federated = config.get(\n            \"autocreate_auto_join_rooms_federated\", True\n        )\n        self.autocreate_auto_join_room_preset = (\n            config.get(\"autocreate_auto_join_room_preset\")\n            or RoomCreationPreset.PUBLIC_CHAT\n        )\n        self.auto_join_room_requires_invite = self.autocreate_auto_join_room_preset in {\n            RoomCreationPreset.PRIVATE_CHAT,\n            RoomCreationPreset.TRUSTED_PRIVATE_CHAT,\n        }\n        mxid_localpart = config.get(\"auto_join_mxid_localpart\")\n        self.auto_join_user_id = None\n        if mxid_localpart:\n            self.auto_join_user_id = UserID(\n                mxid_localpart, self.server_name\n            ).to_string()\n        if self.autocreate_auto_join_rooms:\n            if self.autocreate_auto_join_room_preset not in {\n                RoomCreationPreset.PUBLIC_CHAT,\n                RoomCreationPreset.PRIVATE_CHAT,\n                RoomCreationPreset.TRUSTED_PRIVATE_CHAT,\n            }:\n                raise ConfigError(\"Invalid value for autocreate_auto_join_room_preset\")\n            if self.auto_join_room_requires_invite:\n                if not mxid_localpart:\n                    raise ConfigError(\n                        \"The configuration option `auto_join_mxid_localpart` is required if \"\n                        \"`autocreate_auto_join_room_preset` is set to private_chat or trusted_private_chat, such that \"\n                        \"Synapse knows who to send invitations from. Please \"\n                        \"configure `auto_join_mxid_localpart`.\"\n                    )\n        self.auto_join_rooms_for_guests = config.get(\"auto_join_rooms_for_guests\", True)\n        self.enable_set_displayname = config.get(\"enable_set_displayname\", True)\n        self.enable_set_avatar_url = config.get(\"enable_set_avatar_url\", True)\n        self.enable_3pid_changes = config.get(\"enable_3pid_changes\", True)\n        self.disable_msisdn_registration = config.get(\n            \"disable_msisdn_registration\", False\n        )\n        session_lifetime = config.get(\"session_lifetime\")\n        if session_lifetime is not None:\n            session_lifetime = self.parse_duration(session_lifetime)\n        self.session_lifetime = session_lifetime\n        self.fallback_success_template = self.read_templates(\n            [\"auth_success.html\"], autoescape=True\n        )[0]\n    def read_templates(\n        self,\n        filenames: List[str],\n        custom_template_directory: Optional[str] = None,\n        autoescape: bool = False,\n    ) -> List[jinja2.Template]:\n        \"\"\"Load a list of template files from disk using the given variables.\n        This function will attempt to load the given templates from the default Synapse\n        template directory. If `custom_template_directory` is supplied, that directory\n        is tried first.\n        Files read are treated as Jinja templates. These templates are not rendered yet.\n        Args:\n            filenames: A list of template filenames to read.\n            custom_template_directory: A directory to try to look for the templates\n                before using the default Synapse template directory instead.\n            autoescape: Whether to autoescape variables before inserting them into the\n                template.\n        Raises:\n            ConfigError: if the file's path is incorrect or otherwise cannot be read.\n        Returns:\n            A list of jinja2 templates.\n        \"\"\"\n        templates = []\n        search_directories = [self.default_template_dir]\n        if custom_template_directory:\n            if not self.path_exists(custom_template_directory):\n                raise ConfigError(\n                    \"Configured template directory does not exist: %s\"\n                    % (custom_template_directory,)\n                )\n            search_directories.insert(0, custom_template_directory)\n        loader = jinja2.FileSystemLoader(search_directories)\n        env = jinja2.Environment(loader=loader, autoescape=autoescape)\n        env.filters.update(\n            {\n                \"format_ts\": _format_ts_filter,\n                \"mxc_to_http\": _create_mxc_to_http_filter(self.public_baseurl),\n            }\n        )\n        for filename in filenames:\n            template = env.get_template(filename)\n            templates.append(template)\n        return templates\n    def read_config(self, config, **kwargs):\n        consent_config = config.get(\"user_consent\")\n        self.terms_template = self.read_templates([\"terms.html\"], autoescape=True)[0]\n        if consent_config is None:\n            return\n        self.user_consent_version = str(consent_config[\"version\"])\n        self.user_consent_template_dir = self.abspath(consent_config[\"template_dir\"])\n        if not path.isdir(self.user_consent_template_dir):\n            raise ConfigError(\n                \"Could not find template directory '%s'\"\n                % (self.user_consent_template_dir,)\n            )\n        self.user_consent_server_notice_content = consent_config.get(\n            \"server_notice_content\"\n        )\n        self.block_events_without_consent_error = consent_config.get(\n            \"block_events_error\"\n        )\n        self.user_consent_server_notice_to_guests = bool(\n            consent_config.get(\"send_server_notice_to_guests\", False)\n        )\n        self.user_consent_at_registration = bool(\n            consent_config.get(\"require_at_registration\", False)\n        )\n        self.user_consent_policy_name = consent_config.get(\n            \"policy_name\", \"Privacy Policy\"\n        )\ndef safe_text(raw_text: str) -> jinja2.Markup:\n    \"\"\"\n    Process text: treat it as HTML but escape any tags (ie. just escape the\n    HTML) then linkify it.\n    \"\"\"\n    return jinja2.Markup(\n        bleach.linkify(bleach.clean(raw_text, tags=[], attributes={}, strip=False))\n    )",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-21332",
        "description": "[{'lang': 'en', 'value': 'Synapse is a Matrix reference homeserver written in python (pypi package matrix-synapse). Matrix is an ecosystem for open federated Instant Messaging and VoIP. In Synapse before version 1.27.0, the password reset endpoint served via Synapse was vulnerable to cross-site scripting (XSS) attacks. The impact depends on the configuration of the domain that Synapse is deployed on, but may allow access to cookies and other browser data, CSRF vulnerabilities, and access to other resources served on the same domain or parent domains. This is fixed in version 1.27.0.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-174",
      "code": "def parse_url(url):\n    \"\"\"\n    Given a url, return a parsed :class:`.Url` namedtuple. Best-effort is\n    performed to parse incomplete urls. Fields not provided will be None.\n    This parser is RFC 3986 compliant.\n    :param str url: URL to parse into a :class:`.Url` namedtuple.\n    Partly backwards-compatible with :mod:`urlparse`.\n    Example::\n        >>> parse_url('http://google.com/mail/')\n        Url(scheme='http', host='google.com', port=None, path='/mail/', ...)\n        >>> parse_url('google.com:80')\n        Url(scheme=None, host='google.com', port=80, path=None, ...)\n        >>> parse_url('/foo?bar')\n        Url(scheme=None, host=None, port=None, path='/foo', query='bar', ...)\n    \"\"\"\n    if not url:\n        return Url()\n    is_string = not isinstance(url, six.binary_type)\n    if not is_string:\n        url = url.decode(\"utf-8\")\n    if not SCHEME_REGEX.search(url):\n        url = \"//\" + url\n    try:\n        iri_ref = rfc3986.IRIReference.from_string(url, encoding=\"utf-8\")\n    except (ValueError, RFC3986Exception):\n        six.raise_from(LocationParseError(url), None)\n    def idna_encode(name):\n        if name and any([ord(x) > 128 for x in name]):\n            try:\n                import idna\n            except ImportError:\n                raise LocationParseError(\"Unable to parse URL without the 'idna' module\")\n            try:\n                return idna.encode(name.lower(), strict=True, std3_rules=True)\n            except idna.IDNAError:\n                raise LocationParseError(u\"Name '%s' is not a valid IDNA label\" % name)\n        return name\n    has_authority = iri_ref.authority is not None\n    uri_ref = iri_ref.encode(idna_encoder=idna_encode)\n    if has_authority and uri_ref.authority is None:\n        raise LocationParseError(url)\n    if uri_ref.scheme is None or uri_ref.scheme.lower() in NORMALIZABLE_SCHEMES:\n        uri_ref = uri_ref.normalize()\n    validator = Validator()\n    try:\n        validator.check_validity_of(\n            *validator.COMPONENT_NAMES\n        ).validate(uri_ref)\n    except ValidationError:\n        six.raise_from(LocationParseError(url), None)\n    path = uri_ref.path\n    if not path:\n        if (uri_ref.query is not None\n                or uri_ref.fragment is not None):\n            path = \"\"\n        else:\n            path = None\n    def to_input_type(x):\n        if x is None:\n            return None\n        elif not is_string and not isinstance(x, six.binary_type):\n            return x.encode('utf-8')\n        return x\n    return Url(\n        scheme=to_input_type(uri_ref.scheme),\n        auth=to_input_type(uri_ref.userinfo),\n        host=to_input_type(uri_ref.host),\n        port=int(uri_ref.port) if uri_ref.port is not None else None,\n        path=to_input_type(path),\n        query=to_input_type(uri_ref.query),\n        fragment=to_input_type(uri_ref.fragment)\n    )\ndef get_host(url):\n    \"\"\"\n    Deprecated. Use :func:`parse_url` instead.\n    \"\"\"\n    p = parse_url(url)\n    return p.scheme or 'http', p.hostname, p.port",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2020-7212",
        "description": "[{'lang': 'en', 'value': 'The _encode_invalid_chars function in util/url.py in the urllib3 library 1.25.2 through 1.25.7 for Python allows a denial of service (CPU consumption) because of an inefficient algorithm. The percent_encodings array contains all matches of percent encodings. It is not deduplicated. For a URL of length N, the size of percent_encodings may be up to O(N). The next step (normalize existing percent-encoded bytes) also takes up to O(N) for each step, so the total time is O(N^2). If percent_encodings were deduplicated, the time to compute _encode_invalid_chars would be O(kN), where k is at most 484 ((10+6*2)^2).'}]",
        "cwe_number": 400
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-175",
      "code": "    def _setup_master(self):\n        Router.max_message_size = self.config['max_message_size']\n        if self.config['profiling']:\n            enable_profiling()\n        self.broker = Broker(activate_compat=False)\n        self.router = Router(self.broker)\n        self.router.debug = self.config.get('debug', False)\n        self.router.undirectional = self.config['unidirectional']\n        self.router.add_handler(\n            fn=self._on_shutdown_msg,\n            handle=SHUTDOWN,\n            policy=has_parent_authority,\n        )\n        self.master = Context(self.router, 0, 'master')\n        parent_id = self.config['parent_ids'][0]\n        if parent_id == 0:\n            self.parent = self.master\n        else:\n            self.parent = Context(self.router, parent_id, 'parent')\n        in_fd = self.config.get('in_fd', 100)\n        in_fp = os.fdopen(os.dup(in_fd), 'rb', 0)\n        os.close(in_fd)\n        out_fp = os.fdopen(os.dup(self.config.get('out_fd', 1)), 'wb', 0)\n        self.stream = MitogenProtocol.build_stream(self.router, parent_id)\n        self.stream.accept(in_fp, out_fp)\n        self.stream.name = 'parent'\n        self.stream.receive_side.keep_alive = False\n        listen(self.stream, 'disconnect', self._on_parent_disconnect)\n        listen(self.broker, 'exit', self._on_broker_exit)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2019-15149",
        "description": "[{'lang': 'en', 'value': 'core.py in Mitogen before 0.2.8 has a typo that drops the unidirectional-routing protection mechanism in the case of a child that is initiated by another child. The Ansible extension is unaffected. NOTE: the vendor disputes this issue because it is exploitable only in conjunction with hypothetical other factors, i.e., an affected use case within a library caller, and a bug in the message receiver policy code that led to reliance on this extra protection mechanism'}]",
        "cwe_number": 254
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-176",
      "code": "    def create_app(\n        blocks: gradio.Blocks, app_kwargs: Dict[str, Any] | None = None\n    ) -> App:\n        app_kwargs = app_kwargs or {}\n        app_kwargs.setdefault(\"default_response_class\", ORJSONResponse)\n        app = App(**app_kwargs)\n        app.configure_app(blocks)\n        if not wasm_utils.IS_WASM:\n            app.add_middleware(\n                CORSMiddleware,\n                allow_origins=[\"*\"],\n                allow_methods=[\"*\"],\n                allow_headers=[\"*\"],\n            )\n        @app.get(\"/user\")\n        @app.get(\"/user/\")\n        def get_current_user(request: fastapi.Request) -> Optional[str]:\n            token = request.cookies.get(\n                f\"access-token-{app.cookie_id}\"\n            ) or request.cookies.get(f\"access-token-unsecure-{app.cookie_id}\")\n            return app.tokens.get(token)\n        @app.get(\"/login_check\")\n        @app.get(\"/login_check/\")\n        def login_check(user: str = Depends(get_current_user)):\n            if app.auth is None or user is not None:\n                return\n            raise HTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Not authenticated\"\n            )\n        @app.get(\"/token\")\n        @app.get(\"/token/\")\n        def get_token(request: fastapi.Request) -> dict:\n            token = request.cookies.get(f\"access-token-{app.cookie_id}\")\n            return {\"token\": token, \"user\": app.tokens.get(token)}\n        @app.get(\"/app_id\")\n        @app.get(\"/app_id/\")\n        def app_id(request: fastapi.Request) -> dict:\n            return {\"app_id\": app.get_blocks().app_id}\n        @app.get(\"/dev/reload\", dependencies=[Depends(login_check)])\n        async def notify_changes(\n            request: fastapi.Request,\n        ):\n            async def reload_checker(request: fastapi.Request):\n                heartbeat_rate = 15\n                check_rate = 0.05\n                last_heartbeat = time.perf_counter()\n                while True:\n                    if await request.is_disconnected():\n                        return\n                    if app.change_event and app.change_event.is_set():\n                        app.change_event.clear()\n                        yield \"\"\"data: CHANGE\\n\\n\"\"\"\n                    await asyncio.sleep(check_rate)\n                    if time.perf_counter() - last_heartbeat > heartbeat_rate:\n                        yield \"\"\"data: HEARTBEAT\\n\\n\"\"\"\n                        last_heartbeat = time.time()\n            return StreamingResponse(\n                reload_checker(request),\n                media_type=\"text/event-stream\",\n            )\n        @app.post(\"/login\")\n        @app.post(\"/login/\")\n        def login(form_data: OAuth2PasswordRequestForm = Depends()):\n            username, password = form_data.username.strip(), form_data.password\n            if app.auth is None:\n                return RedirectResponse(url=\"/\", status_code=status.HTTP_302_FOUND)\n            if (\n                not callable(app.auth)\n                and username in app.auth\n                and app.auth[username] == password\n            ) or (callable(app.auth) and app.auth.__call__(username, password)):\n                token = secrets.token_urlsafe(16)\n                app.tokens[token] = username\n                response = JSONResponse(content={\"success\": True})\n                response.set_cookie(\n                    key=f\"access-token-{app.cookie_id}\",\n                    value=token,\n                    httponly=True,\n                    samesite=\"none\",\n                    secure=True,\n                )\n                response.set_cookie(\n                    key=f\"access-token-unsecure-{app.cookie_id}\",\n                    value=token,\n                    httponly=True,\n                )\n                return response\n            else:\n                raise HTTPException(status_code=400, detail=\"Incorrect credentials.\")\n        if app.blocks is not None and app.blocks.expects_oauth:\n            attach_oauth(app)\n        @app.head(\"/\", response_class=HTMLResponse)\n        @app.get(\"/\", response_class=HTMLResponse)\n        def main(request: fastapi.Request, user: str = Depends(get_current_user)):\n            mimetypes.add_type(\"application/javascript\", \".js\")\n            blocks = app.get_blocks()\n            root_path = (\n                request.scope.get(\"root_path\")\n                or request.headers.get(\"X-Direct-Url\")\n                or \"\"\n            )\n            if app.auth is None or user is not None:\n                config = app.get_blocks().config\n                config[\"root\"] = route_utils.strip_url(root_path)\n            else:\n                config = {\n                    \"auth_required\": True,\n                    \"auth_message\": blocks.auth_message,\n                    \"space_id\": app.get_blocks().space_id,\n                    \"root\": route_utils.strip_url(root_path),\n                }\n            try:\n                template = (\n                    \"frontend/share.html\" if blocks.share else \"frontend/index.html\"\n                )\n                return templates.TemplateResponse(\n                    template,\n                    {\"request\": request, \"config\": config},\n                )\n            except TemplateNotFound as err:\n                if blocks.share:\n                    raise ValueError(\n                        \"Did you install Gradio from source files? Share mode only \"\n                        \"works when Gradio is installed through the pip package.\"\n                    ) from err\n                else:\n                    raise ValueError(\n                        \"Did you install Gradio from source files? You need to build \"\n                        \"the frontend by running /scripts/build_frontend.sh\"\n                    ) from err\n        @app.get(\"/info/\", dependencies=[Depends(login_check)])\n        @app.get(\"/info\", dependencies=[Depends(login_check)])\n        def api_info(serialize: bool = True):\n            return app.get_blocks().get_api_info()\n        @app.get(\"/config/\", dependencies=[Depends(login_check)])\n        @app.get(\"/config\", dependencies=[Depends(login_check)])\n        def get_config(request: fastapi.Request):\n            root_path = (\n                request.scope.get(\"root_path\")\n                or request.headers.get(\"X-Direct-Url\")\n                or \"\"\n            )\n            config = app.get_blocks().config\n            config[\"root\"] = route_utils.strip_url(root_path)\n            return config\n        @app.get(\"/static/{path:path}\")\n        def static_resource(path: str):\n            static_file = safe_join(STATIC_PATH_LIB, path)\n            return FileResponse(static_file)\n        @app.get(\"/custom_component/{id}/{type}/{file_name}\")\n        def custom_component_path(id: str, type: str, file_name: str):\n            config = app.get_blocks().config\n            components = config[\"components\"]\n            location = next(\n                (item for item in components if item[\"component_class_id\"] == id), None\n            )\n            if location is None:\n                raise HTTPException(status_code=404, detail=\"Component not found.\")\n            component_instance = app.get_blocks().get_component(location[\"id\"])\n            module_name = component_instance.__class__.__module__\n            module_path = sys.modules[module_name].__file__\n            if module_path is None or component_instance is None:\n                raise HTTPException(status_code=404, detail=\"Component not found.\")\n            return FileResponse(\n                safe_join(\n                    str(Path(module_path).parent),\n                    f\"{component_instance.__class__.TEMPLATE_DIR}/{type}/{file_name}\",\n                )\n            )\n        @app.get(\"/assets/{path:path}\")\n        def build_resource(path: str):\n            build_file = safe_join(BUILD_PATH_LIB, path)\n            return FileResponse(build_file)\n        @app.get(\"/favicon.ico\")\n        async def favicon():\n            blocks = app.get_blocks()\n            if blocks.favicon_path is None:\n                return static_resource(\"img/logo.svg\")\n            else:\n                return FileResponse(blocks.favicon_path)\n        @app.head(\"/proxy={url_path:path}\", dependencies=[Depends(login_check)])\n        @app.get(\"/proxy={url_path:path}\", dependencies=[Depends(login_check)])\n        async def reverse_proxy(url_path: str):\n            try:\n                rp_req = app.build_proxy_request(url_path)\n            except PermissionError as err:\n                raise HTTPException(status_code=400, detail=str(err)) from err\n            rp_resp = await client.send(rp_req, stream=True)\n            return StreamingResponse(\n                rp_resp.aiter_raw(),\n                status_code=rp_resp.status_code,\n                headers=rp_resp.headers,\n                background=BackgroundTask(rp_resp.aclose),\n            )\n        @app.head(\"/file={path_or_url:path}\", dependencies=[Depends(login_check)])\n        @app.get(\"/file={path_or_url:path}\", dependencies=[Depends(login_check)])\n        async def file(path_or_url: str, request: fastapi.Request):\n            blocks = app.get_blocks()\n            if utils.validate_url(path_or_url):\n                return RedirectResponse(\n                    url=path_or_url, status_code=status.HTTP_302_FOUND\n                )\n            abs_path = utils.abspath(path_or_url)\n            in_blocklist = any(\n                utils.is_in_or_equal(abs_path, blocked_path)\n                for blocked_path in blocks.blocked_paths\n            )\n            is_dir = abs_path.is_dir()\n            if in_blocklist or is_dir:\n                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")\n            created_by_app = str(abs_path) in set().union(*blocks.temp_file_sets)\n            in_allowlist = any(\n                utils.is_in_or_equal(abs_path, allowed_path)\n                for allowed_path in blocks.allowed_paths\n            )\n            was_uploaded = utils.is_in_or_equal(abs_path, app.uploaded_file_dir)\n            is_cached_example = utils.is_in_or_equal(\n                abs_path, utils.abspath(CACHED_FOLDER)\n            )\n            if not (\n                created_by_app or in_allowlist or was_uploaded or is_cached_example\n            ):\n                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")\n            if not abs_path.exists():\n                raise HTTPException(404, f\"File not found: {path_or_url}.\")\n            range_val = request.headers.get(\"Range\", \"\").strip()\n            if range_val.startswith(\"bytes=\") and \"-\" in range_val:\n                range_val = range_val[6:]\n                start, end = range_val.split(\"-\")\n                if start.isnumeric() and end.isnumeric():\n                    start = int(start)\n                    end = int(end)\n                    response = ranged_response.RangedFileResponse(\n                        abs_path,\n                        ranged_response.OpenRange(start, end),\n                        dict(request.headers),\n                        stat_result=os.stat(abs_path),\n                    )\n                    return response\n            return FileResponse(abs_path, headers={\"Accept-Ranges\": \"bytes\"})\n        @app.get(\n            \"/stream/{session_hash}/{run}/{component_id}\",\n            dependencies=[Depends(login_check)],\n        )\n        async def stream(\n            session_hash: str, run: int, component_id: int, request: fastapi.Request\n        ):\n            stream: list = (\n                app.get_blocks()\n                .pending_streams[session_hash]\n                .get(run, {})\n                .get(component_id, None)\n            )\n            if stream is None:\n                raise HTTPException(404, \"Stream not found.\")\n            def stream_wrapper():\n                check_stream_rate = 0.01\n                max_wait_time = 120\n                wait_time = 0\n                while True:\n                    if len(stream) == 0:\n                        if wait_time > max_wait_time:\n                            return\n                        wait_time += check_stream_rate\n                        time.sleep(check_stream_rate)\n                        continue\n                    wait_time = 0\n                    next_stream = stream.pop(0)\n                    if next_stream is None:\n                        return\n                    yield next_stream\n            return StreamingResponse(stream_wrapper())\n        @app.get(\"/file/{path:path}\", dependencies=[Depends(login_check)])\n        async def file_deprecated(path: str, request: fastapi.Request):\n            return await file(path, request)\n        @app.post(\"/reset/\")\n        @app.post(\"/reset\")\n        async def reset_iterator(body: ResetBody):\n            if body.event_id not in app.iterators:\n                return {\"success\": False}\n            async with app.lock:\n                del app.iterators[body.event_id]\n                app.iterators_to_reset.add(body.event_id)\n                await app.get_blocks()._queue.clean_event(body.event_id)\n            return {\"success\": True}\n        @app.post(\"/run/{api_name}\", dependencies=[Depends(login_check)])\n        @app.post(\"/run/{api_name}/\", dependencies=[Depends(login_check)])\n        @app.post(\"/api/{api_name}\", dependencies=[Depends(login_check)])\n        @app.post(\"/api/{api_name}/\", dependencies=[Depends(login_check)])\n        async def predict(\n            api_name: str,\n            body: PredictBody,\n            request: fastapi.Request,\n            username: str = Depends(get_current_user),\n        ):\n            fn_index_inferred = route_utils.infer_fn_index(\n                app=app, api_name=api_name, body=body\n            )\n            if not app.get_blocks().api_open and app.get_blocks().queue_enabled_for_fn(\n                fn_index_inferred\n            ):\n                raise HTTPException(\n                    detail=\"This API endpoint does not accept direct HTTP POST requests. Please join the queue to use this API.\",\n                    status_code=status.HTTP_404_NOT_FOUND,\n                )\n            gr_request = route_utils.compile_gr_request(\n                app,\n                body,\n                fn_index_inferred=fn_index_inferred,\n                username=username,\n                request=request,\n            )\n            try:\n                output = await route_utils.call_process_api(\n                    app=app,\n                    body=body,\n                    gr_request=gr_request,\n                    fn_index_inferred=fn_index_inferred,\n                )\n            except BaseException as error:\n                show_error = app.get_blocks().show_error or isinstance(error, Error)\n                traceback.print_exc()\n                return JSONResponse(\n                    content={\"error\": str(error) if show_error else None},\n                    status_code=500,\n                )\n            return output\n        @app.get(\"/queue/join\", dependencies=[Depends(login_check)])\n        async def queue_join(\n            fn_index: int,\n            session_hash: str,\n            request: fastapi.Request,\n            username: str = Depends(get_current_user),\n            data: Optional[str] = None,\n        ):\n            blocks = app.get_blocks()\n            if blocks._queue.server_app is None:\n                blocks._queue.set_server_app(app)\n            event = Event(session_hash, fn_index, request, username)\n            if data is not None:\n                input_data = json.loads(data)\n                event.data = PredictBody(\n                    session_hash=session_hash,\n                    fn_index=fn_index,\n                    data=input_data,\n                    request=request,\n                )\n            if blocks.dependencies[event.fn_index].get(\"every\", 0):\n                await cancel_tasks({f\"{event.session_hash}_{event.fn_index}\"})\n                await blocks._queue.reset_iterators(event._id)\n                blocks._queue.continuous_tasks.append(event)\n                task = run_coro_in_background(\n                    blocks._queue.process_events, [event], False\n                )\n                set_task_name(task, event.session_hash, event.fn_index, batch=False)\n                app._asyncio_tasks.append(task)\n            else:\n                rank = blocks._queue.push(event)\n                if rank is None:\n                    event.send_message(\"queue_full\", final=True)\n                else:\n                    estimation = blocks._queue.get_estimation()\n                    await blocks._queue.send_estimation(event, estimation, rank)\n            async def sse_stream(request: fastapi.Request):\n                try:\n                    last_heartbeat = time.perf_counter()\n                    while True:\n                        if await request.is_disconnected():\n                            await blocks._queue.clean_event(event)\n                        if not event.alive:\n                            return\n                        heartbeat_rate = 15\n                        check_rate = 0.05\n                        message = None\n                        try:\n                            message = event.message_queue.get_nowait()\n                            if message is None:\n                                return\n                        except EmptyQueue:\n                            await asyncio.sleep(check_rate)\n                            if time.perf_counter() - last_heartbeat > heartbeat_rate:\n                                message = {\"msg\": \"heartbeat\"}\n                                last_heartbeat = time.perf_counter()\n                        if message:\n                            yield f\"data: {json.dumps(message)}\\n\\n\"\n                except asyncio.CancelledError as e:\n                    await blocks._queue.clean_event(event)\n                    raise e\n            return StreamingResponse(\n                sse_stream(request),\n                media_type=\"text/event-stream\",\n            )\n        @app.post(\"/queue/data\", dependencies=[Depends(login_check)])\n        async def queue_data(\n            body: PredictBody,\n            request: fastapi.Request,\n            username: str = Depends(get_current_user),\n        ):\n            blocks = app.get_blocks()\n            blocks._queue.attach_data(body)\n        @app.post(\"/component_server\", dependencies=[Depends(login_check)])\n        @app.post(\"/component_server/\", dependencies=[Depends(login_check)])\n        def component_server(body: ComponentServerBody):\n            state = app.state_holder[body.session_hash]\n            component_id = body.component_id\n            block: Block\n            if component_id in state:\n                block = state[component_id]\n            else:\n                block = app.get_blocks().blocks[component_id]\n            fn = getattr(block, body.fn_name)\n            return fn(body.data)\n        @app.get(\n            \"/queue/status\",\n            dependencies=[Depends(login_check)],\n            response_model=Estimation,\n        )\n        async def get_queue_status():\n            return app.get_blocks()._queue.get_estimation()\n        @app.get(\"/upload_progress\")\n        def get_upload_progress(upload_id: str, request: fastapi.Request):\n            async def sse_stream(request: fastapi.Request):\n                last_heartbeat = time.perf_counter()\n                is_done = False\n                while True:\n                    if await request.is_disconnected():\n                        file_upload_statuses.stop_tracking(upload_id)\n                        return\n                    if is_done:\n                        file_upload_statuses.stop_tracking(upload_id)\n                        return\n                    heartbeat_rate = 15\n                    check_rate = 0.05\n                    message = None\n                    try:\n                        if update := file_upload_statuses.status(upload_id).popleft():\n                            if update.is_done:\n                                message = {\"msg\": \"done\"}\n                                is_done = True\n                            else:\n                                message = {\n                                    \"msg\": \"update\",\n                                    \"orig_name\": update.filename,\n                                    \"chunk_size\": update.chunk_size,\n                                }\n                        else:\n                            await asyncio.sleep(check_rate)\n                            if time.perf_counter() - last_heartbeat > heartbeat_rate:\n                                message = {\"msg\": \"heartbeat\"}\n                                last_heartbeat = time.perf_counter()\n                        if message:\n                            yield f\"data: {json.dumps(message)}\\n\\n\"\n                    except IndexError:\n                        if not file_upload_statuses.is_tracked(upload_id):\n                            return\n                        continue\n            return StreamingResponse(\n                sse_stream(request),\n                media_type=\"text/event-stream\",\n            )\n        @app.post(\"/upload\", dependencies=[Depends(login_check)])\n        async def upload_file(\n            request: fastapi.Request,\n            bg_tasks: BackgroundTasks,\n            upload_id: Optional[str] = None,\n        ):\n            content_type_header = request.headers.get(\"Content-Type\")\n            content_type: bytes\n            content_type, _ = parse_options_header(content_type_header)\n            if content_type != b\"multipart/form-data\":\n                raise HTTPException(status_code=400, detail=\"Invalid content type.\")\n            try:\n                if upload_id:\n                    file_upload_statuses.track(upload_id)\n                multipart_parser = GradioMultiPartParser(\n                    request.headers,\n                    request.stream(),\n                    max_files=1000,\n                    max_fields=1000,\n                    upload_id=upload_id if upload_id else None,\n                    upload_progress=file_upload_statuses if upload_id else None,\n                )\n                form = await multipart_parser.parse()\n            except MultiPartException as exc:\n                raise HTTPException(status_code=400, detail=exc.message) from exc\n            output_files = []\n            files_to_copy = []\n            locations: list[str] = []\n            for temp_file in form.getlist(\"files\"):\n                assert isinstance(temp_file, GradioUploadFile)\n                if temp_file.filename:\n                    file_name = Path(temp_file.filename).name\n                    name = client_utils.strip_invalid_filename_characters(file_name)\n                else:\n                    name = f\"tmp{secrets.token_hex(5)}\"\n                directory = Path(app.uploaded_file_dir) / temp_file.sha.hexdigest()\n                directory.mkdir(exist_ok=True, parents=True)\n                dest = (directory / name).resolve()\n                temp_file.file.close()\n                try:\n                    os.rename(temp_file.file.name, dest)\n                except OSError:\n                    files_to_copy.append(temp_file.file.name)\n                    locations.append(str(dest))\n                output_files.append(dest)\n            if files_to_copy:\n                bg_tasks.add_task(\n                    move_uploaded_files_to_cache, files_to_copy, locations\n                )\n            return output_files\n        @app.on_event(\"startup\")\n        @app.get(\"/startup-events\")\n        async def startup_events():\n            if not app.startup_events_triggered:\n                app.get_blocks().startup_events()\n                app.startup_events_triggered = True\n                return True\n            return False\n        @app.get(\"/theme.css\", response_class=PlainTextResponse)\n        def theme_css():\n            return PlainTextResponse(app.get_blocks().theme_css, media_type=\"text/css\")\n        @app.get(\"/robots.txt\", response_class=PlainTextResponse)\n        def robots_txt():\n            if app.get_blocks().share:\n                return \"User-agent: *\\nDisallow: /\"\n            else:\n                return \"User-agent: *\\nDisallow: \"\n        return app\n    async def process_events(self, events: list[Event], batch: bool) -> None:\n        awake_events: list[Event] = []\n        try:\n            for event in events:\n                if not event.data:\n                    self.awaiting_data_events[event._id] = event\n                    client_awake = await event.get_data()\n                    del self.awaiting_data_events[event._id]\n                    if not client_awake:\n                        await self.clean_event(event)\n                        continue\n                event.send_message(\"process_starts\")\n                awake_events.append(event)\n            if not awake_events:\n                return\n            begin_time = time.time()\n            try:\n                response = await self.call_prediction(awake_events, batch)\n                err = None\n            except Exception as e:\n                traceback.print_exc()\n                response = None\n                err = e\n                for event in awake_events:\n                    event.send_message(\n                        \"process_completed\",\n                        {\n                            \"output\": {\n                                \"error\": None\n                                if len(e.args) and e.args[0] is None\n                                else str(e)\n                            },\n                            \"success\": False,\n                        },\n                        final=True,\n                    )\n            if response and response.get(\"is_generating\", False):\n                old_response = response\n                old_err = err\n                while response and response.get(\"is_generating\", False):\n                    old_response = response\n                    old_err = err\n                    for event in awake_events:\n                        event.send_message(\n                            \"process_generating\",\n                            {\n                                \"output\": old_response,\n                                \"success\": old_response is not None,\n                            },\n                        )\n                    awake_events = [event for event in awake_events if event.alive]\n                    if not awake_events:\n                        return\n                    try:\n                        response = await self.call_prediction(awake_events, batch)\n                        err = None\n                    except Exception as e:\n                        response = None\n                        err = e\n                for event in awake_events:\n                    if response is None:\n                        relevant_response = err\n                    else:\n                        relevant_response = old_response or old_err\n                    event.send_message(\n                        \"process_completed\",\n                        {\n                            \"output\": {\"error\": str(relevant_response)}\n                            if isinstance(relevant_response, Exception)\n                            else relevant_response,\n                            \"success\": relevant_response\n                            and not isinstance(relevant_response, Exception),\n                        },\n                        final=True,\n                    )\n            elif response:\n                output = copy.deepcopy(response)\n                for e, event in enumerate(awake_events):\n                    if batch and \"data\" in output:\n                        output[\"data\"] = list(zip(*response.get(\"data\")))[e]\n                    event.send_message(\n                        \"process_completed\",\n                        {\n                            \"output\": output,\n                            \"success\": response is not None,\n                        },\n                        final=True,\n                    )\n            end_time = time.time()\n            if response is not None:\n                self.update_estimation(end_time - begin_time)\n        except Exception as e:\n            traceback.print_exc()\n        finally:\n            try:\n                self.active_jobs[self.active_jobs.index(events)] = None\n            except ValueError:\n                pass\n            for event in events:\n                await self.reset_iterators(event._id)\n    def __init__(\n        self,\n        src: str,\n        hf_token: str | None = None,\n        max_workers: int = 40,\n        serialize: bool = True,\n        output_dir: str | Path = DEFAULT_TEMP_DIR,\n        verbose: bool = True,\n        auth: tuple[str, str] | None = None,\n    ):\n        \"\"\"\n        Parameters:\n            src: Either the name of the Hugging Face Space to load, (e.g. \"abidlabs/whisper-large-v2\") or the full URL (including \"http\" or \"https\") of the hosted Gradio app to load (e.g. \"http://mydomain.com/app\" or \"https://bec81a83-5b5c-471e.gradio.live/\").\n            hf_token: The Hugging Face token to use to access private Spaces. Automatically fetched if you are logged in via the Hugging Face Hub CLI. Obtain from: https://huggingface.co/settings/token\n            max_workers: The maximum number of thread workers that can be used to make requests to the remote Gradio app simultaneously.\n            serialize: Whether the client should serialize the inputs and deserialize the outputs of the remote API. If set to False, the client will pass the inputs and outputs as-is, without serializing/deserializing them. E.g. you if you set this to False, you'd submit an image in base64 format instead of a filepath, and you'd get back an image in base64 format from the remote API instead of a filepath.\n            output_dir: The directory to save files that are downloaded from the remote API. If None, reads from the GRADIO_TEMP_DIR environment variable. Defaults to a temporary directory on your machine.\n            verbose: Whether the client should print statements to the console.\n        \"\"\"\n        self.verbose = verbose\n        self.hf_token = hf_token\n        self.serialize = serialize\n        self.headers = build_hf_headers(\n            token=hf_token,\n            library_name=\"gradio_client\",\n            library_version=utils.__version__,\n        )\n        self.space_id = None\n        self.cookies: dict[str, str] = {}\n        self.output_dir = (\n            str(output_dir) if isinstance(output_dir, Path) else output_dir\n        )\n        if src.startswith(\"http://\") or src.startswith(\"https://\"):\n            _src = src if src.endswith(\"/\") else src + \"/\"\n        else:\n            _src = self._space_name_to_src(src)\n            if _src is None:\n                raise ValueError(\n                    f\"Could not find Space: {src}. If it is a private Space, please provide an hf_token.\"\n                )\n            self.space_id = src\n        self.src = _src\n        state = self._get_space_state()\n        if state == SpaceStage.BUILDING:\n            if self.verbose:\n                print(\"Space is still building. Please wait...\")\n            while self._get_space_state() == SpaceStage.BUILDING:\n                time.sleep(2)\n                pass\n        if state in utils.INVALID_RUNTIME:\n            raise ValueError(\n                f\"The current space is in the invalid state: {state}. \"\n                \"Please contact the owner to fix this.\"\n            )\n        if self.verbose:\n            print(f\"Loaded as API: {self.src} \u2714\")\n        self.api_url = urllib.parse.urljoin(self.src, utils.API_URL)\n        self.sse_url = urllib.parse.urljoin(self.src, utils.SSE_URL)\n        self.sse_data_url = urllib.parse.urljoin(self.src, utils.SSE_DATA_URL)\n        self.ws_url = urllib.parse.urljoin(\n            self.src.replace(\"http\", \"ws\", 1), utils.WS_URL\n        )\n        self.upload_url = urllib.parse.urljoin(self.src, utils.UPLOAD_URL)\n        self.reset_url = urllib.parse.urljoin(self.src, utils.RESET_URL)\n        if auth is not None:\n            self._login(auth)\n        self.config = self._get_config()\n        self.app_version = version.parse(self.config.get(\"version\", \"2.0\"))\n        self._info = self._get_api_info()\n        self.session_hash = str(uuid.uuid4())\n        protocol = self.config.get(\"protocol\")\n        endpoint_class = Endpoint if protocol == \"sse\" else EndpointV3Compatibility\n        self.endpoints = [\n            endpoint_class(self, fn_index, dependency)\n            for fn_index, dependency in enumerate(self.config[\"dependencies\"])\n        ]\n        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)\n        threading.Thread(target=self._telemetry_thread).start()\n    def submit(\n        self,\n        *args,\n        api_name: str | None = None,\n        fn_index: int | None = None,\n        result_callbacks: Callable | list[Callable] | None = None,\n    ) -> Job:\n        \"\"\"\n        Creates and returns a Job object which calls the Gradio API in a background thread. The job can be used to retrieve the status and result of the remote API call.\n        Parameters:\n            args: The arguments to pass to the remote API. The order of the arguments must match the order of the inputs in the Gradio app.\n            api_name: The name of the API endpoint to call starting with a leading slash, e.g. \"/predict\". Does not need to be provided if the Gradio app has only one named API endpoint.\n            fn_index: As an alternative to api_name, this parameter takes the index of the API endpoint to call, e.g. 0. Both api_name and fn_index can be provided, but if they conflict, api_name will take precedence.\n            result_callbacks: A callback function, or list of callback functions, to be called when the result is ready. If a list of functions is provided, they will be called in order. The return values from the remote API are provided as separate parameters into the callback. If None, no callback will be called.\n        Returns:\n            A Job object that can be used to retrieve the status and result of the remote API call.\n        Example:\n            from gradio_client import Client\n            client = Client(src=\"gradio/calculator\")\n            job = client.submit(5, \"add\", 4, api_name=\"/predict\")\n            job.status()\n            >> <Status.STARTING: 'STARTING'>\n            job.result()\n            >> 9.0\n        \"\"\"\n        inferred_fn_index = self._infer_fn_index(api_name, fn_index)\n        helper = None\n        if self.endpoints[inferred_fn_index].protocol in (\"ws\", \"sse\"):\n            helper = self.new_helper(inferred_fn_index)\n        end_to_end_fn = self.endpoints[inferred_fn_index].make_end_to_end_fn(helper)\n        future = self.executor.submit(end_to_end_fn, *args)\n        job = Job(\n            future, communicator=helper, verbose=self.verbose, space_id=self.space_id\n        )\n        if result_callbacks:\n            if isinstance(result_callbacks, Callable):\n                result_callbacks = [result_callbacks]\n            def create_fn(callback) -> Callable:\n                def fn(future):\n                    if isinstance(future.result(), tuple):\n                        callback(*future.result())\n                    else:\n                        callback(future.result())\n                return fn\n            for callback in result_callbacks:\n                job.add_done_callback(create_fn(callback))\n        return job\nasync def stream_sse(\n    client: httpx.AsyncClient,\n    data: dict,\n    hash_data: dict,\n    helper: Communicator,\n    sse_url: str,\n    sse_data_url: str,\n    headers: dict[str, str],\n    cookies: dict[str, str] | None = None,\n) -> dict[str, Any]:\n    try:\n        async with client.stream(\n            \"GET\",\n            sse_url,\n            params=hash_data,\n            cookies=cookies,\n            headers=headers,\n        ) as response:\n            async for line in response.aiter_text():\n                if line.startswith(\"data:\"):\n                    resp = json.loads(line[5:])\n                    with helper.lock:\n                        has_progress = \"progress_data\" in resp\n                        status_update = StatusUpdate(\n                            code=Status.msg_to_status(resp[\"msg\"]),\n                            queue_size=resp.get(\"queue_size\"),\n                            rank=resp.get(\"rank\", None),\n                            success=resp.get(\"success\"),\n                            time=datetime.now(),\n                            eta=resp.get(\"rank_eta\"),\n                            progress_data=ProgressUnit.from_msg(resp[\"progress_data\"])\n                            if has_progress\n                            else None,\n                        )\n                        output = resp.get(\"output\", {}).get(\"data\", [])\n                        if output and status_update.code != Status.FINISHED:\n                            try:\n                                result = helper.prediction_processor(*output)\n                            except Exception as e:\n                                result = [e]\n                            helper.job.outputs.append(result)\n                        helper.job.latest_status = status_update\n                    if resp[\"msg\"] == \"queue_full\":\n                        raise QueueError(\"Queue is full! Please try again.\")\n                    elif resp[\"msg\"] == \"send_data\":\n                        event_id = resp[\"event_id\"]\n                        helper.event_id = event_id\n                        req = await client.post(\n                            sse_data_url,\n                            json={\"event_id\": event_id, **data, **hash_data},\n                            cookies=cookies,\n                            headers=headers,\n                        )\n                        req.raise_for_status()\n                    elif resp[\"msg\"] == \"process_completed\":\n                        return resp[\"output\"]\n                else:\n                    raise ValueError(f\"Unexpected message: {line}\")\n        raise ValueError(\"Did not receive process_completed message.\")\n    except asyncio.CancelledError:\n        raise\n    def get_config_file(self):\n        config = {\n            \"version\": routes.VERSION,\n            \"mode\": self.mode,\n            \"app_id\": self.app_id,\n            \"dev_mode\": self.dev_mode,\n            \"analytics_enabled\": self.analytics_enabled,\n            \"components\": [],\n            \"css\": self.css,\n            \"js\": self.js,\n            \"head\": self.head,\n            \"title\": self.title or \"Gradio\",\n            \"space_id\": self.space_id,\n            \"enable_queue\": True,\n            \"show_error\": getattr(self, \"show_error\", False),\n            \"show_api\": self.show_api,\n            \"is_colab\": utils.colab_check(),\n            \"stylesheets\": self.stylesheets,\n            \"theme\": self.theme.name,\n            \"protocol\": \"sse\",\n        }\n        def get_layout(block):\n            if not isinstance(block, BlockContext):\n                return {\"id\": block._id}\n            children_layout = []\n            for child in block.children:\n                children_layout.append(get_layout(child))\n            return {\"id\": block._id, \"children\": children_layout}\n        config[\"layout\"] = get_layout(self)\n        for _id, block in self.blocks.items():\n            props = block.get_config() if hasattr(block, \"get_config\") else {}\n            block_config = {\n                \"id\": _id,\n                \"type\": block.get_block_name(),\n                \"props\": utils.delete_none(props),\n            }\n            block_config[\"skip_api\"] = block.skip_api\n            block_config[\"component_class_id\"] = getattr(\n                block, \"component_class_id\", None\n            )\n            if not block.skip_api:\n                block_config[\"api_info\"] = block.api_info()\n                block_config[\"example_inputs\"] = block.example_inputs()\n            config[\"components\"].append(block_config)\n        config[\"dependencies\"] = self.dependencies\n        return config",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-0964",
        "description": "[{'lang': 'en', 'value': 'A local file include could be remotely triggered in Gradio due to a vulnerable user-supplied JSON value in an API request.'}]",
        "cwe_number": 22
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-177",
      "code": "def main(argv=sys.argv):\n    \"\"\"[summary]\n    Keyword Arguments:\n        argv {[type]} -- [description] (default: {sys.argv})\n    Raises:\n        UserError: [description]\n        UserError: [description]\n        UserError: [description]\n    \"\"\"\n    parser = argparse.ArgumentParser(argv[0])\n    parser.add_argument('-c', '--command', action='store', dest='command', default='add',\n                        help=\"valid commands are add,delete,update,\"\n                             \"regstatus,cvstatus,status,reglist,cvlist,reactivate,\"\n                             \"regdelete,\"\n                             \"bulkinfo. defaults to add\")\n    parser.add_argument('-t', '--targethost', action='store',\n                        dest='agent_ip', help=\"the IP address of the host to provision\")\n    parser.add_argument('-tp', '--targetport', action='store',\n                        dest='agent_port', help=\"the Port of the host to provision\")\n    parser.add_argument('-r', '--registrarhost', action='store',\n                        dest='registrar_ip', help=\"the IP address of the registrar where to retrieve the agents data from.\")\n    parser.add_argument('-rp', '--registrarport', action=\"store\",\n                        dest='registrar_port', help=\"the port of the registrar.\")\n    parser.add_argument('--cv_targethost', action='store', default=None, dest='cv_agent_ip',\n                        help='the IP address of the host to provision that the verifier will use (optional).  Use only if different than argument to option -t/--targethost')\n    parser.add_argument('-v', '--cv', action='store', dest='verifier_ip',\n                        help=\"the IP address of the cloud verifier\")\n    parser.add_argument('-vp', '--cvport', action='store', dest='verifier_port',\n                        help=\"the port of the cloud verifier\")\n    parser.add_argument('-vi', '--cvid', action='store', dest='verifier_id',\n                        help=\"the unique identifier of a cloud verifier\")\n    parser.add_argument('-nvc', '--no-verifier-check', action='store_false', dest='verifier_check', default=True,\n                        help='Disable the check to confirm if the agent is being processed by the specified verifier. Use only with -c/--command delete or reactivate')\n    parser.add_argument('-u', '--uuid', action='store',\n                        dest='agent_uuid', help=\"UUID for the agent to provision\")\n    parser.add_argument('-f', '--file', action='store', default=None,\n                        help='Deliver the specified plaintext to the provisioned agent')\n    parser.add_argument('--cert', action='store', dest='ca_dir', default=None,\n                        help='Create and deliver a certificate using a CA created by ca-util. Pass in the CA directory or use \"default\" to use the standard dir')\n    parser.add_argument('-k', '--key', action='store', dest='keyfile',\n                        help='an intermedia key file produced by user_data_encrypt')\n    parser.add_argument('-p', '--payload', action='store', default=None,\n                        help='Specify the encrypted payload to deliver with encrypted keys specified by -k')\n    parser.add_argument('--include', action='store', dest='incl_dir', default=None,\n                        help=\"Include additional files in provided directory in certificate zip file.  Must be specified with --cert\")\n    parser.add_argument('--allowlist', action='store', dest='allowlist',\n                        default=None, help=\"Specify the file path of an allowlist\")\n    parser.add_argument('--signature-verification-key', '--sign_verification_key', action='append', dest='ima_sign_verification_keys',\n                        default=[], help=\"Specify an IMA file signature verification key\")\n    parser.add_argument('--signature-verification-key-sig', action='append', dest='ima_sign_verification_key_sigs',\n                        default=[], help=\"Specify the GPG signature file for an IMA file signature verification key; pair this option with --signature-verification-key\")\n    parser.add_argument('--signature-verification-key-sig-key', action='append', dest='ima_sign_verification_key_sig_keys',\n                        default=[], help=\"Specify the GPG public key file use to validate the --signature-verification-key-sig; pair this option with --signature-verification-key\")\n    parser.add_argument('--signature-verification-key-url', action='append', dest='ima_sign_verification_key_urls',\n                        default=[], help=\"Specify the URL for a remote IMA file signature verification key\")\n    parser.add_argument('--signature-verification-key-sig-url', action='append',\n                        dest='ima_sign_verification_key_sig_urls',\n                        default=[], help=\"Specify the URL for the remote GPG signature of a remote IMA file signature verification key; pair this option with --signature-verification-key-url\")\n    parser.add_argument('--signature-verification-key-sig-url-key', action='append',\n                        dest='ima_sign_verification_key_sig_url_keys',\n                        default=[], help=\"Specify the GPG public key file used to validate the --signature-verification-key-sig-url; pair this option with --signature-verification-key-url\")\n    parser.add_argument('--mb_refstate', action='store', dest='mb_refstate',\n                        default=None, help=\"Specify the location of a measure boot reference state (intended state)\")\n    parser.add_argument('--allowlist-checksum', action='store', dest='allowlist_checksum',\n                        default=None, help=\"Specify the SHA2 checksum of an allowlist\")\n    parser.add_argument('--allowlist-sig', action='store', dest='allowlist_sig',\n                        default=None, help=\"Specify the GPG signature file of an allowlist\")\n    parser.add_argument('--allowlist-sig-key', action='store', dest='allowlist_sig_key',\n                        default=None, help=\"Specify the GPG public key file used to validate the --allowlist-sig or --allowlist-sig-url\")\n    parser.add_argument('--allowlist-url', action='store', dest='allowlist_url',\n                        default=None, help=\"Specify the URL of a remote allowlist\")\n    parser.add_argument('--allowlist-sig-url', action='store', dest='allowlist_sig_url',\n                        default=None, help=\"Specify the URL of the remote GPG signature file of an allowlist\")\n    parser.add_argument('--exclude', action='store', dest='ima_exclude',\n                        default=None, help=\"Specify the location of an IMA exclude list\")\n    parser.add_argument('--tpm_policy', action='store', dest='tpm_policy', default=None,\n                        help=\"Specify a TPM policy in JSON format. e.g., {\\\"15\\\":\\\"0000000000000000000000000000000000000000\\\"}\")\n    parser.add_argument('--vtpm_policy', action='store', dest='vtpm_policy',\n                        default=None, help=\"Specify a vTPM policy in JSON format\")\n    parser.add_argument('--verify', action='store_true', default=False,\n                        help='Block on cryptographically checked key derivation confirmation from the agent once it has been provisioned')\n    parser.add_argument('--allowlist-name', help='The name of allowlist to operate with')\n    parser.add_argument('--supported-version', default=None, action=\"store\", dest='supported_version', help='API version that is supported by the agent. Detected automatically by default')\n    args = parser.parse_args(argv[1:])\n    if( args.allowlist and args.allowlist_url):\n        parser.error(\"--allowlist and --allowlist-url cannot be specified at the same time\")\n    if( args.allowlist_url and not (args.allowlist_sig or args.allowlist_sig_url or args.allowlist_checksum)):\n        parser.error(\"--allowlist-url must have either --allowlist-sig, --allowlist-sig-url or --allowlist-checksum to verifier integrity\")\n    if( args.allowlist_sig and not (args.allowlist_url or args.allowlist)):\n        parser.error(\"--allowlist-sig must have either --allowlist or --allowlist-url\")\n    if( args.allowlist_sig_url and not (args.allowlist_url or args.allowlist)):\n        parser.error(\"--allowlist-sig-url must have either --allowlist or --allowlist-url\")\n    if( args.allowlist_checksum and not (args.allowlist_url or args.allowlist)):\n        parser.error(\"--allowlist-checksum must have either --allowlist or --allowlist-url\")\n    if( args.allowlist_sig and not args.allowlist_sig_key):\n        parser.error(\"--allowlist-sig must also have --allowlist-sig-key\")\n    if( args.allowlist_sig_url and not args.allowlist_sig_key):\n        parser.error(\"--allowlist-sig-url must also have --allowlist-sig-key\")\n    if( args.allowlist_sig_key and not (args.allowlist_sig or args.allowlist_sig_url)):\n        parser.error(\"--allowlist-sig-key must have either --allowlist-sig or --allowlist-sig-url\")\n    mytenant = Tenant()\n    if args.agent_uuid is not None:\n        mytenant.agent_uuid = args.agent_uuid\n        if mytenant.agent_uuid.startswith('-----BEGIN PUBLIC KEY-----'):\n            mytenant.agent_uuid = hashlib.sha256(\n                mytenant.agent_uuid).hexdigest()\n    else:\n        logger.warning(\"Using default UUID d432fbb3-d2f1-4a97-9ef7-75bd81c00000\")\n        mytenant.agent_uuid = \"d432fbb3-d2f1-4a97-9ef7-75bd81c00000\"\n    if config.STUB_VTPM and config.TPM_CANNED_VALUES is not None:\n        jsonIn = config.TPM_CANNED_VALUES\n        if \"add_vtpm_to_group\" in jsonIn:\n            mytenant.agent_uuid = jsonIn['add_vtpm_to_group']['retout']\n        else:\n            raise UserError(\"Command %s not found in canned JSON!\" %\n                            (\"add_vtpm_to_group\"))\n    if args.verifier_id is not None:\n        mytenant.verifier_id = args.verifier_id\n    if args.verifier_ip is not None:\n        mytenant.verifier_ip = args.verifier_ip\n    if args.verifier_port is not None:\n        mytenant.verifier_port = args.verifier_port\n    if args.registrar_ip is not None:\n        mytenant.registrar_ip = args.registrar_ip\n    if args.registrar_port is not None:\n        mytenant.registrar_port = args.registrar_port\n    if args.command in ['add', 'update']:\n        delete_tmp_files = logger.level > logging.DEBUG\n        if args.allowlist_url:\n            logger.info(\"Downloading Allowlist from %s\", args.allowlist_url)\n            response = requests.get(args.allowlist_url, allow_redirects=False)\n            if response.status_code == 200:\n                args.allowlist = write_to_namedtempfile(response.content, delete_tmp_files)\n                logger.debug(\"Allowlist temporarily saved in %s\" % args.allowlist)\n            else:\n                raise Exception(f\"Downloading allowlist ({args.allowlist_url}) failed with status code {response.status_code}!\")\n        if args.allowlist_sig_url:\n            logger.info(\"Downloading Allowlist signature from %s\", args.allowlist_sig_url)\n            response = requests.get(args.allowlist_sig_url, allow_redirects=False)\n            if response.status_code == 200:\n                args.allowlist_sig = write_to_namedtempfile(response.content, delete_tmp_files)\n                logger.debug(\"Allowlist signature temporarily saved in %s\", args.allowlist_sig)\n            else:\n                raise Exception(f\"Downloading allowlist signature ({args.allowlist_sig_url}) failed with status code {response.status_code}!\")\n        for i, key_file in enumerate(args.ima_sign_verification_keys):\n            if len(args.ima_sign_verification_key_sigs) <= i:\n                break\n            keysig_file = args.ima_sign_verification_key_sigs[i]\n            if len(args.ima_sign_verification_key_sig_keys) == 0:\n                raise UserError(\"A gpg key is missing for key signature file '%s'\" % keysig_file)\n            gpg_key_file = args.ima_sign_verification_key_sig_keys[i]\n            gpg.gpg_verify_filesignature(gpg_key_file, key_file, keysig_file, \"IMA file signing key\")\n            logger.info(\"Signature verification on %s was successful\" % key_file)\n        for i, key_url in enumerate(args.ima_sign_verification_key_urls):\n            logger.info(\"Downloading key from %s\", key_url)\n            response = requests.get(key_url, allow_redirects=False)\n            if response.status_code == 200:\n                key_file = write_to_namedtempfile(response.content, delete_tmp_files)\n                args.ima_sign_verification_keys.append(key_file)\n                logger.debug(\"Key temporarily saved in %s\" % key_file)\n            else:\n                raise Exception(f\"Downloading key ({key_url}) failed with status code {response.status_code}!\")\n            if len(args.ima_sign_verification_key_sig_urls) <= i:\n                continue\n            keysig_url = args.ima_sign_verification_key_sig_urls[i]\n            if len(args.ima_sign_verification_key_sig_url_keys) == 0:\n                raise UserError(\"A gpg key is missing for key signature URL '%s'\" % keysig_url)\n            logger.info(\"Downloading key signature from %s\" % keysig_url)\n            response = requests.get(keysig_url, allow_redirects=False)\n            if response.status_code == 200:\n                keysig_file = write_to_namedtempfile(response.content, delete_tmp_files)\n                logger.debug(\"Key signature temporarily saved in %s\" % keysig_file)\n            else:\n                raise Exception(f\"Downloading key signature ({key_url}) failed with status code {response.status_code}!\")\n            gpg_key_file = args.ima_sign_verification_key_sig_url_keys[i]\n            gpg.gpg_verify_filesignature(gpg_key_file, key_file, keysig_file, \"IMA file signing key\")\n            logger.info(\"Signature verification on %s was successful\" % key_url)\n    if args.command == 'add':\n        mytenant.init_add(vars(args))\n        mytenant.preloop()\n        mytenant.do_cv()\n        mytenant.do_quote()\n        if args.verify:\n            mytenant.do_verify()\n    elif args.command == 'update':\n        mytenant.init_add(vars(args))\n        mytenant.do_cvdelete(args.verifier_check)\n        mytenant.preloop()\n        mytenant.do_cv()\n        mytenant.do_quote()\n        if args.verify:\n            mytenant.do_verify()\n    elif args.command == 'delete':\n        mytenant.do_cvdelete(args.verifier_check)\n    elif args.command == 'status':\n        mytenant.do_status()\n    elif args.command == 'cvstatus':\n        mytenant.do_cvstatus()\n    elif args.command == 'bulkinfo':\n        mytenant.do_cvbulkinfo()\n    elif args.command == 'cvlist':\n        mytenant.do_cvlist()\n    elif args.command == 'reactivate':\n        mytenant.do_cvreactivate(args.verifier_check)\n    elif args.command == 'regstatus':\n        mytenant.do_regstatus()\n    elif args.command == 'reglist':\n        mytenant.do_reglist()\n    elif args.command == 'regdelete':\n        mytenant.do_regdelete()\n    elif args.command == 'addallowlist':\n        mytenant.do_add_allowlist(vars(args))\n    elif args.command == 'showallowlist':\n        mytenant.do_show_allowlist(args.allowlist_name)\n    elif args.command == 'deleteallowlist':\n        mytenant.do_delete_allowlist(args.allowlist_name)\n    else:\n        raise UserError(\"Invalid command specified: %s\" % (args.command))\n    def do_POST(self):\n        \"\"\"This method handles the POST requests to add agents to the Registrar Server.\n        Currently, only agents resources are available for POSTing, i.e. /agents. All other POST uri's\n        will return errors. POST requests require an an agent_id identifying the agent to add, and json\n        block sent in the body with 2 entries: ek and aik.\n        \"\"\"\n        session = SessionManager().make_session(engine)\n        rest_params = web_util.get_restful_params(self.path)\n        if rest_params is None:\n            web_util.echo_json_response(\n                self, 405, \"Not Implemented: Use /agents/ interface\")\n            return\n        if not rest_params[\"api_version\"]:\n            web_util.echo_json_response(self, 400, \"API Version not supported\")\n            return\n        if \"agents\" not in rest_params:\n            web_util.echo_json_response(self, 400, \"uri not supported\")\n            logger.warning('POST agent returning 400 response. uri not supported: %s', self.path)\n            return\n        agent_id = rest_params[\"agents\"]\n        if agent_id is None:\n            web_util.echo_json_response(self, 400, \"agent id not found in uri\")\n            logger.warning('POST agent returning 400 response. agent id not found in uri %s', self.path)\n            return\n        try:\n            content_length = int(self.headers.get('Content-Length', 0))\n            if content_length == 0:\n                web_util.echo_json_response(\n                    self, 400, \"Expected non zero content length\")\n                logger.warning('POST for %s returning 400 response. Expected non zero content length.', agent_id)\n                return\n            post_body = self.rfile.read(content_length)\n            json_body = json.loads(post_body)\n            ekcert = json_body['ekcert']\n            aik_tpm = json_body['aik_tpm']\n            initialize_tpm = tpm()\n            if ekcert is None or ekcert == 'emulator':\n                logger.warning('Agent %s did not submit an ekcert' % agent_id)\n                ek_tpm = json_body['ek_tpm']\n            else:\n                if 'ek_tpm' in json_body:\n                    logger.warning('Overriding ek_tpm for agent %s from ekcert' % agent_id)\n                ek509 = load_der_x509_certificate(\n                    base64.b64decode(ekcert),\n                    backend=default_backend(),\n                )\n                ek_tpm = base64.b64encode(\n                    tpm2_objects.ek_low_tpm2b_public_from_pubkey(\n                        ek509.public_key(),\n                    )\n                )\n            aik_attrs = tpm2_objects.get_tpm2b_public_object_attributes(\n                base64.b64decode(aik_tpm),\n            )\n            if aik_attrs != tpm2_objects.AK_EXPECTED_ATTRS:\n                web_util.echo_json_response(\n                    self, 400, \"Invalid AK attributes\")\n                logger.warning(\n                    \"Agent %s submitted AIK with invalid attributes! %s (provided) != %s (expected)\",\n                    agent_id,\n                    tpm2_objects.object_attributes_description(aik_attrs),\n                    tpm2_objects.object_attributes_description(tpm2_objects.AK_EXPECTED_ATTRS),\n                )\n                return\n            (blob, key) = initialize_tpm.encryptAIK(\n                agent_id,\n                base64.b64decode(ek_tpm),\n                base64.b64decode(aik_tpm),\n            )\n            regcount = 1\n            try:\n                agent = session.query(RegistrarMain).filter_by(\n                    agent_id=agent_id).first()\n            except NoResultFound:\n                agent = None\n            except SQLAlchemyError as e:\n                logger.error('SQLAlchemy Error: %s', e)\n                raise\n            if agent is not None:\n                regcount = agent.regcount\n                if agent.ek_tpm != ek_tpm or agent.ekcert != ekcert:\n                    logger.warning('WARNING: Overwriting previous registration for this UUID with new ek-ekcert pair!')\n                    regcount += 1\n                logger.info('Overwriting previous registration for this UUID.')\n                try:\n                    session.query(RegistrarMain).filter_by(\n                        agent_id=agent_id).delete()\n                    session.commit()\n                except SQLAlchemyError as e:\n                    logger.error('SQLAlchemy Error: %s', e)\n                    raise\n            contact_ip = json_body.get('ip', None)\n            contact_port = json_body.get('port', None)\n            if contact_ip is not None:\n                try:\n                    ipaddress.ip_address(contact_ip)\n                except ValueError:\n                    logger.warning(f\"Contact ip for agent {agent_id} is not a valid ip got: {contact_ip}.\")\n                    contact_ip = None\n            if contact_port is not None:\n                try:\n                    contact_port = int(contact_port)\n                    if contact_port < 1 or contact_port > 65535:\n                        logger.warning(f\"Contact port for agent {agent_id} is not a number between 1 and got: {contact_port}.\")\n                        contact_port = None\n                except ValueError:\n                    logger.warning(f\"Contact port for agent {agent_id} is not a valid number got: {contact_port}.\")\n                    contact_port = None\n            mtls_cert = json_body.get('mtls_cert', None)\n            if mtls_cert is None:\n                logger.warning(f\"Agent {agent_id} did not sent a mTLS certificate. Most operations will not work!\")\n            d = {}\n            d['agent_id'] = agent_id\n            d['ek_tpm'] = ek_tpm\n            d['aik_tpm'] = aik_tpm\n            d['ekcert'] = ekcert\n            d['ip'] = contact_ip\n            d['mtls_cert'] = mtls_cert\n            d['port'] = contact_port\n            d['virtual'] = int(ekcert == 'virtual')\n            d['active'] = int(False)\n            d['key'] = key\n            d['provider_keys'] = {}\n            d['regcount'] = regcount\n            try:\n                session.add(RegistrarMain(**d))\n                session.commit()\n            except SQLAlchemyError as e:\n                logger.error('SQLAlchemy Error: %s', e)\n                raise\n            response = {\n                'blob': blob,\n            }\n            web_util.echo_json_response(self, 200, \"Success\", response)\n            logger.info('POST returning key blob for agent_id: %s', agent_id)\n        except Exception as e:\n            web_util.echo_json_response(self, 400, \"Error: %s\" % e)\n            logger.warning(\"POST for %s returning 400 response. Error: %s\", agent_id, e)\n            logger.exception(e)\n    def do_PUT(self):\n        \"\"\"This method handles the PUT requests to add agents to the Registrar Server.\n        Currently, only agents resources are available for PUTing, i.e. /agents. All other PUT uri's\n        will return errors.\n        \"\"\"\n        session = SessionManager().make_session(engine)\n        rest_params = web_util.get_restful_params(self.path)\n        if rest_params is None:\n            web_util.echo_json_response(\n                self, 405, \"Not Implemented: Use /agents/ interface\")\n            return\n        if not rest_params[\"api_version\"]:\n            web_util.echo_json_response(self, 400, \"API Version not supported\")\n            return\n        if \"agents\" not in rest_params:\n            web_util.echo_json_response(self, 400, \"uri not supported\")\n            logger.warning('PUT agent returning 400 response. uri not supported: %s', self.path)\n            return\n        agent_id = rest_params[\"agents\"]\n        if agent_id is None:\n            web_util.echo_json_response(self, 400, \"agent id not found in uri\")\n            logger.warning('PUT agent returning 400 response. agent id not found in uri %s', self.path)\n            return\n        try:\n            content_length = int(self.headers.get('Content-Length', 0))\n            if content_length == 0:\n                web_util.echo_json_response(\n                    self, 400, \"Expected non zero content length\")\n                logger.warning('PUT for %s returning 400 response. Expected non zero content length.', agent_id)\n                return\n            post_body = self.rfile.read(content_length)\n            json_body = json.loads(post_body)\n            auth_tag = json_body['auth_tag']\n            try:\n                agent = session.query(RegistrarMain).filter_by(\n                    agent_id=agent_id).first()\n            except NoResultFound as e:\n                raise Exception(\n                    \"attempting to activate agent before requesting \"\n                    \"registrar for %s\" % agent_id) from e\n            except SQLAlchemyError as e:\n                logger.error('SQLAlchemy Error: %s', e)\n                raise\n            if config.STUB_TPM:\n                try:\n                    session.query(RegistrarMain).filter(RegistrarMain.agent_id == agent_id).update(\n                        {'active': True})\n                    session.commit()\n                except SQLAlchemyError as e:\n                    logger.error('SQLAlchemy Error: %s', e)\n                    raise\n            else:\n                if engine.dialect.name == \"mysql\":\n                    agent.key = agent.key.encode('utf-8')\n                ex_mac = crypto.do_hmac(agent.key, agent_id)\n                if ex_mac == auth_tag:\n                    try:\n                        session.query(RegistrarMain).filter(RegistrarMain.agent_id == agent_id).update(\n                            {'active': True})\n                        session.commit()\n                    except SQLAlchemyError as e:\n                        logger.error('SQLAlchemy Error: %s', e)\n                        raise\n                else:\n                    raise Exception(\n                        f\"Auth tag {auth_tag} does not match expected value {ex_mac}\")\n            web_util.echo_json_response(self, 200, \"Success\")\n            logger.info('PUT activated: %s', agent_id)\n        except Exception as e:\n            web_util.echo_json_response(self, 400, \"Error: %s\" % e)\n            logger.warning(\"PUT for %s returning 400 response. Error: %s\", agent_id, e)\n            logger.exception(e)\n            return\ndef main():\n    for ML in [config.MEASUREDBOOT_ML, config.IMA_ML]:\n        if not os.access(ML, os.F_OK):\n            logger.warning(\"Measurement list path %s not accessible by agent. Any attempt to instruct it to access this path - via \\\"keylime_tenant\\\" CLI - will result in agent process dying\", ML)\n    if config.get('cloud_agent', 'agent_uuid') == 'dmidecode':\n        if os.getuid() != 0:\n            raise RuntimeError('agent_uuid is configured to use dmidecode, '\n                               'but current process is not running as root.')\n        cmd = ['which', 'dmidecode']\n        ret = cmd_exec.run(cmd, raiseOnError=False)\n        if ret['code'] != 0:\n            raise RuntimeError('agent_uuid is configured to use dmidecode, '\n                               'but it\\'s is not found on the system.')\n    instance_tpm = tpm()\n    registrar_ip = config.get('cloud_agent', 'registrar_ip')\n    registrar_port = config.get('cloud_agent', 'registrar_port')\n    contact_ip = os.getenv(\"KEYLIME_AGENT_CONTACT_IP\", None)\n    if contact_ip is None and config.has_option('cloud_agent', 'agent_contact_ip'):\n        contact_ip = config.get('cloud_agent', 'agent_contact_ip')\n    contact_port = os.getenv(\"KEYLIME_AGENT_CONTACT_PORT\", None)\n    if contact_port is None and config.has_option('cloud_agent', 'agent_contact_port'):\n        contact_port = config.get('cloud_agent', 'agent_contact_port', fallback=\"invalid\")\n    secure_mount.mount()\n    config.ch_dir(config.WORK_DIR, logger)\n    (ekcert, ek_tpm, aik_tpm) = instance_tpm.tpm_init(self_activate=False, config_pw=config.get(\n        'cloud_agent', 'tpm_ownerpassword'))\n    virtual_agent = instance_tpm.is_vtpm()\n    kernel_version = tuple(platform.release().split(\"-\")[0].split(\".\"))\n    if tuple(map(int,kernel_version)) < (5, 10, 0) and instance_tpm.defaults[\"hash\"] != algorithms.Hash.SHA1:\n        logger.warning(\"IMA attestation only works on kernel versions <5.10 with SHA1 as hash algorithm. \"\n                       \"Even if ascii_runtime_measurements shows \\\"%s\\\" as the \"\n                       \"algorithm, it might be just padding zeros\", (instance_tpm.defaults[\"hash\"]))\n    if ekcert is None:\n        if virtual_agent:\n            ekcert = 'virtual'\n        elif instance_tpm.is_emulator():\n            ekcert = 'emulator'\n    try:\n        agent_uuid = config.get('cloud_agent', 'agent_uuid')\n    except configparser.NoOptionError:\n        agent_uuid = None\n    if agent_uuid == 'hash_ek':\n        ek_pubkey = pubkey_from_tpm2b_public(base64.b64decode(ek_tpm))\n        ek_pubkey_pem = ek_pubkey.public_bytes(encoding=serialization.Encoding.PEM,\n                                               format=serialization.PublicFormat.SubjectPublicKeyInfo)\n        agent_uuid = hashlib.sha256(ek_pubkey_pem).hexdigest()\n    elif agent_uuid == 'generate' or agent_uuid is None:\n        agent_uuid = str(uuid.uuid4())\n    elif agent_uuid == 'dmidecode':\n        cmd = ['dmidecode', '-s', 'system-uuid']\n        ret = cmd_exec.run(cmd)\n        sys_uuid = ret['retout'][0].decode('utf-8')\n        agent_uuid = sys_uuid.strip()\n        try:\n            uuid.UUID(agent_uuid)\n        except ValueError as e:\n            raise RuntimeError(\"The UUID returned from dmidecode is invalid: %s\" % e)\n    elif agent_uuid == 'hostname':\n        agent_uuid = socket.getfqdn()\n    elif agent_uuid == 'environment':\n        agent_uuid = os.getenv(\"KEYLIME_AGENT_UUID\", None)\n        if agent_uuid is None:\n            raise RuntimeError(\"Env variable KEYLIME_AGENT_UUID is empty, but agent_uuid is set to 'environment'\")\n    if config.STUB_VTPM and config.TPM_CANNED_VALUES is not None:\n        jsonIn = config.TPM_CANNED_VALUES\n        if \"add_vtpm_to_group\" in jsonIn:\n            agent_uuid = jsonIn['add_vtpm_to_group']['retout']\n        else:\n            raise Exception(\"Command %s not found in canned json!\" %\n                            (\"add_vtpm_to_group\"))\n    logger.info(\"Agent UUID: %s\", agent_uuid)\n    serveraddr = (config.get('cloud_agent', 'cloudagent_ip'),\n                  config.getint('cloud_agent', 'cloudagent_port'))\n    keylime_ca = config.get('cloud_agent', 'keylime_ca')\n    if keylime_ca == \"default\":\n        keylime_ca = os.path.join(config.WORK_DIR, 'cv_ca', 'cacert.crt')\n    server = CloudAgentHTTPServer(serveraddr, Handler, agent_uuid)\n    context = web_util.generate_mtls_context(server.mtls_cert_path, server.rsakey_path, keylime_ca, logger=logger)\n    server.socket = context.wrap_socket(server.socket, server_side=True)\n    serverthread = threading.Thread(target=server.serve_forever, daemon=True)\n    mtls_cert = server.mtls_cert.public_bytes(serialization.Encoding.PEM)\n    keyblob = registrar_client.doRegisterAgent(\n        registrar_ip, registrar_port, agent_uuid, ek_tpm, ekcert, aik_tpm, mtls_cert, contact_ip, contact_port)\n    if keyblob is None:\n        instance_tpm.flush_keys()\n        raise Exception(\"Registration failed\")\n    key = instance_tpm.activate_identity(keyblob)\n    if key is None:\n        instance_tpm.flush_keys()\n        raise Exception(\"Activation failed\")\n    retval = registrar_client.doActivateAgent(\n        registrar_ip, registrar_port, agent_uuid, key)\n    if not retval:\n        instance_tpm.flush_keys()\n        raise Exception(\"Registration failed on activate\")\n    revocation_process = multiprocessing.Process(target=revocation_listener, daemon=True)\n    revocation_process.start()\n    logger.info(\"Starting Cloud Agent on %s:%s with API version %s. Use <Ctrl-C> to stop\", serveraddr[0], serveraddr[1], keylime_api_version.current_version())\n    serverthread.start()\n    def shutdown_handler(*_):\n        logger.info(\"TERM Signal received, shutting down...\")\n        logger.debug(\"Stopping revocation notifier...\")\n        revocation_process.terminate()\n        logger.debug(\"Shutting down HTTP server...\")\n        server.shutdown()\n        server.server_close()\n        serverthread.join()\n        logger.debug(\"...HTTP server stopped\")\n        revocation_process.join()\n        logger.debug(\"... revocation notifier stopped\")\n        instance_tpm.flush_keys()\n        logger.debug(\"Flushed keys successfully\")\n        sys.exit(0)\n    signal.signal(signal.SIGTERM, shutdown_handler)\n    signal.signal(signal.SIGQUIT, shutdown_handler)\n    signal.signal(signal.SIGINT, shutdown_handler)\n    serverthread.join()\n    def post(self):\n        \"\"\"This method handles the POST requests to add agents to the Cloud Verifier.\n        Currently, only agents resources are available for POSTing, i.e. /agents. All other POST uri's will return errors.\n        agents requests require a json block sent in the body\n        \"\"\"\n        session = get_session()\n        try:\n            rest_params = web_util.get_restful_params(self.request.uri)\n            if rest_params is None:\n                web_util.echo_json_response(\n                    self, 405, \"Not Implemented: Use /agents/ interface\")\n                return\n            if not rest_params[\"api_version\"]:\n                web_util.echo_json_response(self, 400, \"API Version not supported\")\n                return\n            if \"agents\" not in rest_params:\n                web_util.echo_json_response(self, 400, \"uri not supported\")\n                logger.warning('POST returning 400 response. uri not supported: %s', self.request.path)\n                return\n            agent_id = rest_params[\"agents\"]\n            if agent_id is not None:\n                content_length = len(self.request.body)\n                if content_length == 0:\n                    web_util.echo_json_response(\n                        self, 400, \"Expected non zero content length\")\n                    logger.warning('POST returning 400 response. Expected non zero content length.')\n                else:\n                    json_body = json.loads(self.request.body)\n                    agent_data = {}\n                    agent_data['v'] = json_body['v']\n                    agent_data['ip'] = json_body['cloudagent_ip']\n                    agent_data['port'] = int(json_body['cloudagent_port'])\n                    agent_data['operational_state'] = states.START\n                    agent_data['public_key'] = \"\"\n                    agent_data['tpm_policy'] = json_body['tpm_policy']\n                    agent_data['vtpm_policy'] = json_body['vtpm_policy']\n                    agent_data['meta_data'] = json_body['metadata']\n                    agent_data['allowlist'] = json_body['allowlist']\n                    agent_data['mb_refstate'] = json_body['mb_refstate']\n                    agent_data['ima_sign_verification_keys'] = json_body['ima_sign_verification_keys']\n                    agent_data['revocation_key'] = json_body['revocation_key']\n                    agent_data['accept_tpm_hash_algs'] = json_body['accept_tpm_hash_algs']\n                    agent_data['accept_tpm_encryption_algs'] = json_body['accept_tpm_encryption_algs']\n                    agent_data['accept_tpm_signing_algs'] = json_body['accept_tpm_signing_algs']\n                    agent_data['supported_version'] = json_body['supported_version']\n                    agent_data['hash_alg'] = \"\"\n                    agent_data['enc_alg'] = \"\"\n                    agent_data['sign_alg'] = \"\"\n                    agent_data['agent_id'] = agent_id\n                    agent_data['boottime'] = 0\n                    agent_data['ima_pcrs'] = []\n                    agent_data['pcr10'] = None\n                    agent_data['next_ima_ml_entry'] = 0\n                    agent_data['learned_ima_keyrings'] = {}\n                    agent_data['verifier_id'] = config.get('cloud_verifier', 'cloudverifier_id', fallback=cloud_verifier_common.DEFAULT_VERIFIER_ID)\n                    agent_data['verifier_ip'] = config.get('cloud_verifier', 'cloudverifier_ip')\n                    agent_data['verifier_port'] = config.get('cloud_verifier', 'cloudverifier_port')\n                    registrar_client.init_client_tls('cloud_verifier')\n                    registrar_data = registrar_client.getData(config.get(\"cloud_verifier\", \"registrar_ip\"),\n                                                              config.get(\"cloud_verifier\", \"registrar_port\"), agent_id)\n                    if registrar_data is None:\n                        web_util.echo_json_response(self, 400,\n                                                    f\"Data for agent {agent_id} could not be found in registrar!\")\n                        logger.warning(f\"Data for agent {agent_id} could not be found in registrar!\")\n                        return\n                    agent_data['mtls_cert'] = registrar_data.get('mtls_cert', None)\n                    agent_data['ak_tpm'] = registrar_data['aik_tpm']\n                    if registrar_data.get('mtls_cert', None) is None and agent_data['supported_version'] != \"1.0\":\n                        web_util.echo_json_response(self, 400, \"mTLS certificate for agent is required!\")\n                        return\n                    is_valid, err_msg = cloud_verifier_common.validate_agent_data(agent_data)\n                    if not is_valid:\n                        web_util.echo_json_response(self, 400, err_msg)\n                        logger.warning(err_msg)\n                        return\n                    try:\n                        new_agent_count = session.query(\n                            VerfierMain).filter_by(agent_id=agent_id).count()\n                    except SQLAlchemyError as e:\n                        logger.error('SQLAlchemy Error: %s', e)\n                        raise e\n                    if new_agent_count > 0:\n                        web_util.echo_json_response(\n                            self, 409, \"Agent of uuid %s already exists\" % (agent_id))\n                        logger.warning(\"Agent of uuid %s already exists\", agent_id)\n                    else:\n                        try:\n                            session.add(VerfierMain(**agent_data))\n                            session.commit()\n                        except SQLAlchemyError as e:\n                            logger.error('SQLAlchemy Error: %s', e)\n                            raise e\n                        for key in list(exclude_db.keys()):\n                            agent_data[key] = exclude_db[key]\n                        mtls_cert = registrar_data.get('mtls_cert', None)\n                        agent_data['ssl_context'] = None\n                        if mtls_cert:\n                            agent_data['ssl_context'] = web_util.generate_agent_mtls_context(mtls_cert,\n                                                                                             self.mtls_options)\n                        if agent_data['ssl_context'] is None:\n                            logger.warning('Connecting to agent without mTLS: %s', agent_id)\n                        asyncio.ensure_future(\n                            process_agent(agent_data, states.GET_QUOTE))\n                        web_util.echo_json_response(self, 200, \"Success\")\n                        logger.info('POST returning 200 response for adding agent id: %s', agent_id)\n            else:\n                web_util.echo_json_response(self, 400, \"uri not supported\")\n                logger.warning(\"POST returning 400 response. uri not supported\")\n        except Exception as e:\n            web_util.echo_json_response(self, 400, \"Exception error: %s\" % e)\n            logger.warning(\"POST returning 400 response. Exception error: %s\", e)\n            logger.exception(e)\n    def put(self):\n        \"\"\"This method handles the PUT requests to add agents to the Cloud Verifier.\n        Currently, only agents resources are available for PUTing, i.e. /agents. All other PUT uri's will return errors.\n        agents requests require a json block sent in the body\n        \"\"\"\n        session = get_session()\n        try:\n            rest_params = web_util.get_restful_params(self.request.uri)\n            if rest_params is None:\n                web_util.echo_json_response(\n                    self, 405, \"Not Implemented: Use /agents/ interface\")\n                return\n            if not rest_params[\"api_version\"]:\n                web_util.echo_json_response(self, 400, \"API Version not supported\")\n                return\n            if \"agents\" not in rest_params:\n                web_util.echo_json_response(self, 400, \"uri not supported\")\n                logger.warning('PUT returning 400 response. uri not supported: %s', self.request.path)\n                return\n            agent_id = rest_params[\"agents\"]\n            if agent_id is None:\n                web_util.echo_json_response(self, 400, \"uri not supported\")\n                logger.warning(\"PUT returning 400 response. uri not supported\")\n            try:\n                verifier_id = config.get('cloud_verifier', 'cloudverifier_id', fallback=cloud_verifier_common.DEFAULT_VERIFIER_ID)\n                agent = session.query(VerfierMain).filter_by(\n                    agent_id=agent_id, verifier_id=verifier_id).one()\n            except SQLAlchemyError as e:\n                logger.error('SQLAlchemy Error: %s', e)\n                raise e\n            if agent is None:\n                web_util.echo_json_response(self, 404, \"agent id not found\")\n                logger.info('PUT returning 404 response. agent id: %s not found.', agent_id)\n                return\n            if \"reactivate\" in rest_params:\n                if not isinstance(agent, dict):\n                    agent = _from_db_obj(agent)\n                if agent[\"mtls_cert\"]:\n                    agent['ssl_context'] = web_util.generate_agent_mtls_context(agent[\"mtls_cert\"], self.mtls_options)\n                agent[\"operational_state\"] = states.START\n                asyncio.ensure_future(\n                    process_agent(agent, states.GET_QUOTE))\n                web_util.echo_json_response(self, 200, \"Success\")\n                logger.info('PUT returning 200 response for agent id: %s', agent_id)\n            elif \"stop\" in rest_params:\n                logger.debug(\"Stopping polling on %s\", agent_id)\n                try:\n                    session.query(VerfierMain).filter(VerfierMain.agent_id == agent_id).update(\n                        {'operational_state': states.TENANT_FAILED})\n                    session.commit()\n                except SQLAlchemyError as e:\n                    logger.error('SQLAlchemy Error: %s', e)\n                web_util.echo_json_response(self, 200, \"Success\")\n                logger.info('PUT returning 200 response for agent id: %s', agent_id)\n            else:\n                web_util.echo_json_response(self, 400, \"uri not supported\")\n                logger.warning(\"PUT returning 400 response. uri not supported\")\n        except Exception as e:\n            web_util.echo_json_response(self, 400, \"Exception error: %s\" % e)\n            logger.warning(\"PUT returning 400 response. Exception error: %s\", e)\n            logger.exception(e)\n    async def get(self):\n        \"\"\"This method handles the GET requests to retrieve status on agents from the WebApp.\n        Currently, only the web app is available for GETing, i.e. /agents. All other GET uri's\n        will return errors.\n        \"\"\"\n        rest_params = web_util.get_restful_params(self.request.uri)\n        if rest_params is None:\n            web_util.echo_json_response(\n                self, 405, \"Not Implemented: Use /agents/ or /logs/ interface\")\n            return\n        if \"logs\" in rest_params and rest_params[\"logs\"] == \"tenant\":\n            offset = 0\n            if \"pos\" in rest_params and rest_params[\"pos\"] is not None and rest_params[\"pos\"].isdigit():\n                offset = int(rest_params[\"pos\"])\n            with open(keylime_logging.LOGSTREAM, encoding=\"utf-8\") as f:\n                logValue = f.readlines()\n                web_util.echo_json_response(self, 200, \"Success\", {\n                                          'log': logValue[offset:]})\n            return\n        if \"agents\" not in rest_params:\n            web_util.echo_json_response(self, 400, \"uri not supported\")\n            logger.warning('GET returning 400 response. uri not supported: %s', self.request.path)\n            return\n        agent_id = rest_params[\"agents\"]\n        if agent_id is not None:\n            agents = await self.get_agent_state(agent_id)\n            agents[\"id\"] = agent_id\n            web_util.echo_json_response(self, 200, \"Success\", agents)\n            return\n        try:\n            get_agents = RequestsClient(registrar_base_tls_url, tls_enabled)\n            response = get_agents.get(\n                (f'/v{api_version}/agents/'),\n                cert=cert,\n                verify=False\n            )\n        except Exception as e:\n            logger.error(\"Status command response: %s:%s Unexpected response from Registrar.\",\n                tenant_templ.registrar_ip, tenant_templ.registrar_port)\n            logger.exception(e)\n            web_util.echo_json_response(\n                self, 500, \"Unexpected response from Registrar\", str(e))\n            return\n        response_body = response.json()\n        if response.status_code != 200:\n            logger.error(\"Status command response: %d Unexpected response from Registrar.\", response.status_code)\n            keylime_logging.log_http_response(\n                logger, logging.ERROR, response_body)\n            return None\n        if (\"results\" not in response_body) or (\"uuids\" not in response_body[\"results\"]):\n            logger.critical(\"Error: unexpected http response body from Registrar: %s\", response.status_code)\n            return None\n        agent_list = response_body[\"results\"][\"uuids\"]\n        web_util.echo_json_response(self, 200, \"Success\", {\n                                  'uuids': agent_list})\n    def post(self):\n        \"\"\"This method handles the POST requests to add agents to the Cloud Verifier.\n        Currently, only agents resources are available for POSTing, i.e. /agents. All other POST uri's will return errors.\n        agents requests require a yaml block sent in the body\n        \"\"\"\n        rest_params = web_util.get_restful_params(self.request.uri)\n        if rest_params is None:\n            web_util.echo_json_response(\n                self, 405, \"Not Implemented: Use /agents/ interface\")\n            return\n        if \"agents\" not in rest_params:\n            web_util.echo_json_response(self, 400, \"uri not supported\")\n            logger.warning('POST returning 400 response. uri not supported: %s', self.request.path)\n            return\n        agent_id = rest_params[\"agents\"]\n        if self.get_argument(\"ptype\", Agent_Init_Types.FILE, True) == Agent_Init_Types.FILE:\n            keyfile = None\n            payload = None\n            data = {'data': parse_data_uri(\n                self.get_argument(\"file_data\", None, True))}\n            ca_dir = None\n            incl_dir = None\n            ca_dir_pw = None\n        elif self.get_argument(\"ptype\", Agent_Init_Types.FILE, True) == Agent_Init_Types.KEYFILE:\n            keyfile = {'data': parse_data_uri(\n                self.get_argument(\"keyfile_data\", None, True)), }\n            payload = {'data': parse_data_uri(\n                self.get_argument(\"file_data\", None, True))}\n            data = None\n            ca_dir = None\n            incl_dir = None\n            ca_dir_pw = None\n        elif self.get_argument(\"ptype\", Agent_Init_Types.FILE, True) == Agent_Init_Types.CA_DIR:\n            keyfile = None\n            payload = None\n            data = None\n            incl_dir = {\n                'data': parse_data_uri(self.get_argument(\"include_dir_data\", None, True)),\n                'name': self.get_argument(\"include_dir_name\", \"\", True).splitlines()\n            }\n            ca_dir = self.get_argument(\"ca_dir\", 'default', True)\n            if ca_dir == \"\":\n                ca_dir = 'default'\n            ca_dir_pw = self.get_argument(\"ca_dir_pw\", 'default', True)\n            if ca_dir_pw == \"\":\n                ca_dir_pw = 'default'\n        else:\n            web_util.echo_json_response(self, 400, \"invalid payload type chosen\")\n            logger.warning('POST returning 400 response. malformed query')\n            return\n        tpm_policy = self.get_argument(\"tpm_policy\", \"\", True)\n        if tpm_policy == \"\":\n            tpm_policy = None\n        vtpm_policy = self.get_argument(\"vtpm_policy\", \"\", True)\n        if vtpm_policy == \"\":\n            vtpm_policy = None\n        allowlist = None\n        a_list_data = self.get_argument(\"a_list_data\", None, True)\n        if a_list_data != \"\":\n            allowlist_str = parse_data_uri(a_list_data)\n            if allowlist_str is not None:\n                allowlist = allowlist_str[0].splitlines()\n        ima_exclude = None\n        e_list_data = self.get_argument(\"e_list_data\", None, True)\n        if e_list_data != \"\":\n            ima_exclude_str = parse_data_uri(e_list_data)\n            if ima_exclude_str is not None:\n                ima_exclude = ima_exclude_str[0].splitlines()\n        args = {\n            'agent_ip': self.get_argument(\"agent_ip\", None, True),\n            'file': data,\n            'keyfile': keyfile,\n            'payload': payload,\n            'ca_dir': ca_dir,\n            'incl_dir': incl_dir,\n            'ca_dir_pw': ca_dir_pw,\n            'tpm_policy': tpm_policy,\n            'vtpm_policy': vtpm_policy,\n            'allowlist': allowlist,\n            'ima_exclude': ima_exclude,\n        }\n        try:\n            mytenant = tenant.Tenant()\n            mytenant.agent_uuid = agent_id\n            mytenant.init_add(args)\n            mytenant.preloop()\n            mytenant.do_cv()\n            mytenant.do_quote()\n        except Exception as e:\n            logger.exception(e)\n            logger.warning('POST returning 500 response. Tenant error: %s', e)\n            web_util.echo_json_response(self, 500, \"Request failure\", str(e))\n            return\n        web_util.echo_json_response(self, 200, \"Success\")",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-23949",
        "description": "[{'lang': 'en', 'value': 'In Keylime before 6.3.0, unsanitized UUIDs can be passed by a rogue agent and can lead to log spoofing on the verifier and registrar.'}]",
        "cwe_number": 290
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-178",
      "code": "    def get_additional_permissions(self):\n        return [\n            {\n                \"key\": \"LIST\",\n                \"name\": \"List plugins\",\n                \"description\": gettext(\"Allows to list installed plugins.\"),\n                \"default_groups\": [READONLY_GROUP, USER_GROUP, ADMIN_GROUP],\n                \"roles\": [\"manage\"],\n            },\n            {\n                \"key\": \"MANAGE\",\n                \"name\": \"Manage plugins\",\n                \"description\": gettext(\n                    \"Allows to enable, disable and uninstall installed plugins.\"\n                ),\n                \"default_groups\": [ADMIN_GROUP],\n                \"roles\": [\"manage\"],\n            },\n            {\n                \"key\": \"INSTALL\",\n                \"name\": \"Install new plugins\",\n                \"description\": gettext(\n                    'Allows to install new plugins. Includes the \"Manage plugins\" permission.'\n                ),\n                \"default_groups\": [ADMIN_GROUP],\n                \"roles\": [\"install\"],\n                \"permissions\": [\"PLUGIN_PLUGINMANAGER_MANAGE\"],\n                \"dangerous\": True,\n            },\n        ]",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-3068",
        "description": "[{'lang': 'en', 'value': 'Improper Privilege Management in GitHub repository octoprint/octoprint prior to 1.8.3.'}]",
        "cwe_number": 269
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-179",
      "code": "def main():\n    try:\n        print(\"---------\")\n        print(\"Panthera(P.)uncia [v0.20]\")\n        print(\"A.R.P. Syndicate [https://arpsyndicate.io]\")\n        print(\"---------\")\n        if len(sys.argv) < 3:\n            sys.exit(\n                \"usage: puncia <mode:subdomain/exploit/enrich/bulk/storekey> <query:domain/eoidentifier/jsonfile/apikey> [output_file/output_directory]\\nrefer: https://github.com/ARPSyndicate/puncia\n            )\n        mode = sys.argv[1]\n        query = sys.argv[2]\n        output_file = sys.argv[3] if len(sys.argv) == 4 else None\n        akey = read_key()\n        if mode not in API_URLS and mode != \"bulk\" and mode != \"storekey\":\n            sys.exit(\"Invalid Mode\")\n        if mode == \"bulk\":\n            if not os.path.isfile(query):\n                sys.exit(\"jsonfile as query input required for bulk mode\")\n            if output_file:\n                os.makedirs(output_file + \"/subdomain/\", exist_ok=True)\n                os.makedirs(output_file + \"/exploit/\", exist_ok=True)\n                os.makedirs(output_file + \"/enrich/\", exist_ok=True)\n            else:\n                sys.exit(\"Bulk Mode requires an Output Directory\")\n            with open(query, \"r\") as f:\n                input_file = json.load(f)\n            if \"subdomain\" in input_file:\n                for bulk_query in input_file[\"subdomain\"]:\n                    try:\n                        query_api(\n                            \"subdomain\",\n                            bulk_query,\n                            output_file + \"/subdomain/\" + bulk_query + \".json\",\n                            akey=akey,\n                        )\n                    except Exception as ne:\n                        sys.exit(f\"Error: {str(ne)}\")\n                        continue\n            if \"exploit\" in input_file:\n                for bulk_query in input_file[\"exploit\"]:\n                    try:\n                        query_api(\n                            \"exploit\",\n                            bulk_query,\n                            output_file + \"/exploit/\" + bulk_query + \".json\",\n                            akey=akey,\n                        )\n                    except Exception as ne:\n                        sys.exit(f\"Error: {str(ne)}\")\n            if \"enrich\" in input_file:\n                for bulk_query in input_file[\"enrich\"]:\n                    try:\n                        query_api(\n                            \"enrich\",\n                            bulk_query,\n                            output_file + \"/enrich/\" + bulk_query + \".json\",\n                            akey=akey,\n                        )\n                    except Exception as ne:\n                        sys.exit(f\"Error: {str(ne)}\")\n        elif mode == \"storekey\":\n            store_key(query)\n            print(\"Successful!\")\n        else:\n            query_api(mode, query, output_file, akey=akey)\n    except Exception as e:\n        sys.exit(f\"Error: {str(e)}\")",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-41124",
        "description": "[{'lang': 'en', 'value': 'Puncia is the Official CLI utility for Subdomain Center & Exploit Observer. `API_URLS` is utilizing HTTP instead of HTTPS for communication that can lead to issues like Eavesdropping, Data Tampering, Unauthorized Data Access & MITM Attacks. This issue has been addressed in release version 0.21 by using https rather than http connections. All users are advised to upgrade. There is no known workarounds for this vulnerability.\\n'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-180",
      "code": "    def emit(self, s, depth, reflow=True):\n        if reflow:\n            lines = reflow_lines(s, depth)\n        else:\n            lines = [s]\n        for line in lines:\n            line = (\" \" * TABSIZE * depth) + line + \"\\n\"\n            self.file.write(line)\n    def visitField(self, field, name, sum=None, prod=None, depth=0):\n        ctype = get_c_type(field.type)\n        if field.opt:\n            check = \"exists_not_none(obj, &PyId_%s)\" % (field.name,)\n        else:\n            check = \"_PyObject_HasAttrId(obj, &PyId_%s)\" % (field.name,)\n        self.emit(\"if (%s) {\" % (check,), depth, reflow=False)\n        self.emit(\"int res;\", depth+1)\n        if field.seq:\n            self.emit(\"Py_ssize_t len;\", depth+1)\n            self.emit(\"Py_ssize_t i;\", depth+1)\n        self.emit(\"tmp = _PyObject_GetAttrId(obj, &PyId_%s);\" % field.name, depth+1)\n        self.emit(\"if (tmp == NULL) goto failed;\", depth+1)\n        if field.seq:\n            self.emit(\"if (!PyList_Check(tmp)) {\", depth+1)\n            self.emit(\"PyErr_Format(PyExc_TypeError, \\\"%s field \\\\\\\"%s\\\\\\\" must \"\n                      \"be a list, not a %%.200s\\\", tmp->ob_type->tp_name);\" %\n                      (name, field.name),\n                      depth+2, reflow=False)\n            self.emit(\"goto failed;\", depth+2)\n            self.emit(\"}\", depth+1)\n            self.emit(\"len = PyList_GET_SIZE(tmp);\", depth+1)\n            if self.isSimpleType(field):\n                self.emit(\"%s = _Ta3_asdl_int_seq_new(len, arena);\" % field.name, depth+1)\n            else:\n                self.emit(\"%s = _Ta3_asdl_seq_new(len, arena);\" % field.name, depth+1)\n            self.emit(\"if (%s == NULL) goto failed;\" % field.name, depth+1)\n            self.emit(\"for (i = 0; i < len; i++) {\", depth+1)\n            self.emit(\"%s value;\" % ctype, depth+2)\n            self.emit(\"res = obj2ast_%s(PyList_GET_ITEM(tmp, i), &value, arena);\" %\n                      field.type, depth+2, reflow=False)\n            self.emit(\"if (res != 0) goto failed;\", depth+2)\n            self.emit(\"if (len != PyList_GET_SIZE(tmp)) {\", depth+2)\n            self.emit(\"PyErr_SetString(PyExc_RuntimeError, \\\"%s field \\\\\\\"%s\\\\\\\" \"\n                      \"changed size during iteration\\\");\" %\n                      (name, field.name),\n                      depth+3, reflow=False)\n            self.emit(\"goto failed;\", depth+3)\n            self.emit(\"}\", depth+2)\n            self.emit(\"asdl_seq_SET(%s, i, value);\" % field.name, depth+2)\n            self.emit(\"}\", depth+1)\n        else:\n            self.emit(\"res = obj2ast_%s(tmp, &%s, arena);\" %\n                      (field.type, field.name), depth+1)\n            self.emit(\"if (res != 0) goto failed;\", depth+1)\n        self.emit(\"Py_CLEAR(tmp);\", depth+1)\n        self.emit(\"} else {\", depth)\n        if not field.opt:\n            message = \"required field \\\\\\\"%s\\\\\\\" missing from %s\" % (field.name, name)\n            format = \"PyErr_SetString(PyExc_TypeError, \\\"%s\\\");\"\n            self.emit(format % message, depth+1, reflow=False)\n            self.emit(\"return 1;\", depth+1)\n        else:\n            if self.isNumeric(field):\n                self.emit(\"%s = 0;\" % field.name, depth+1)\n            elif not self.isSimpleType(field):\n                self.emit(\"%s = NULL;\" % field.name, depth+1)\n            else:\n                raise TypeError(\"could not determine the default value for %s\" % field.name)\n        self.emit(\"}\", depth)\n    def visitModule(self, mod):\n        self.emit(\"\"\"\ntypedef struct {\n    PyObject_HEAD\n    PyObject *dict;\n} AST_object;\nstatic void\nast_dealloc(AST_object *self)\n{\n    Py_CLEAR(self->dict);\n    Py_TYPE(self)->tp_free(self);\n}\nstatic int\nast_traverse(AST_object *self, visitproc visit, void *arg)\n{\n    Py_VISIT(self->dict);\n    return 0;\n}\nstatic void\nast_clear(AST_object *self)\n{\n    Py_CLEAR(self->dict);\n}\nstatic int\nast_type_init(PyObject *self, PyObject *args, PyObject *kw)\n{\n    _Py_IDENTIFIER(_fields);\n    Py_ssize_t i, numfields = 0;\n    int res = -1;\n    PyObject *key, *value, *fields;\n    fields = _PyObject_GetAttrId((PyObject*)Py_TYPE(self), &PyId__fields);\n    if (!fields)\n        PyErr_Clear();\n    if (fields) {\n        numfields = PySequence_Size(fields);\n        if (numfields == -1)\n            goto cleanup;\n    }\n    res = 0; /* if no error occurs, this stays 0 to the end */\n    if (PyTuple_GET_SIZE(args) > 0) {\n        if (numfields != PyTuple_GET_SIZE(args)) {\n            PyErr_Format(PyExc_TypeError, \"%.400s constructor takes %s\"\n                         \"%zd positional argument%s\",\n                         Py_TYPE(self)->tp_name,\n                         numfields == 0 ? \"\" : \"either 0 or \",\n                         numfields, numfields == 1 ? \"\" : \"s\");\n            res = -1;\n            goto cleanup;\n        }\n        for (i = 0; i < PyTuple_GET_SIZE(args); i++) {\n            /* cannot be reached when fields is NULL */\n            PyObject *name = PySequence_GetItem(fields, i);\n            if (!name) {\n                res = -1;\n                goto cleanup;\n            }\n            res = PyObject_SetAttr(self, name, PyTuple_GET_ITEM(args, i));\n            Py_DECREF(name);\n            if (res < 0)\n                goto cleanup;\n        }\n    }\n    if (kw) {\n        i = 0;  /* needed by PyDict_Next */\n        while (PyDict_Next(kw, &i, &key, &value)) {\n            res = PyObject_SetAttr(self, key, value);\n            if (res < 0)\n                goto cleanup;\n        }\n    }\n  cleanup:\n    Py_XDECREF(fields);\n    return res;\n}\n/* Pickling support */\nstatic PyObject *\nast_type_reduce(PyObject *self, PyObject *unused)\n{\n    PyObject *res;\n    _Py_IDENTIFIER(__dict__);\n    PyObject *dict = _PyObject_GetAttrId(self, &PyId___dict__);\n    if (dict == NULL) {\n        if (PyErr_ExceptionMatches(PyExc_AttributeError))\n            PyErr_Clear();\n        else\n            return NULL;\n    }\n    if (dict) {\n        res = Py_BuildValue(\"O()O\", Py_TYPE(self), dict);\n        Py_DECREF(dict);\n        return res;\n    }\n    return Py_BuildValue(\"O()\", Py_TYPE(self));\n}\nstatic PyMethodDef ast_type_methods[] = {\n    {\"__reduce__\", ast_type_reduce, METH_NOARGS, NULL},\n    {NULL}\n};\nstatic PyGetSetDef ast_type_getsets[] = {\n    {\"__dict__\", PyObject_GenericGetDict, PyObject_GenericSetDict},\n    {NULL}\n};\nstatic PyTypeObject AST_type = {\n    PyVarObject_HEAD_INIT(NULL, 0)\n    \"_ast3.AST\",\n    sizeof(AST_object),\n    0,\n    (destructor)ast_dealloc, /* tp_dealloc */\n    0,                       /* tp_print */\n    0,                       /* tp_getattr */\n    0,                       /* tp_setattr */\n    0,                       /* tp_reserved */\n    0,                       /* tp_repr */\n    0,                       /* tp_as_number */\n    0,                       /* tp_as_sequence */\n    0,                       /* tp_as_mapping */\n    0,                       /* tp_hash */\n    0,                       /* tp_call */\n    0,                       /* tp_str */\n    PyObject_GenericGetAttr, /* tp_getattro */\n    PyObject_GenericSetAttr, /* tp_setattro */\n    0,                       /* tp_as_buffer */\n    Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE | Py_TPFLAGS_HAVE_GC, /* tp_flags */\n    0,                       /* tp_doc */\n    (traverseproc)ast_traverse, /* tp_traverse */\n    (inquiry)ast_clear,      /* tp_clear */\n    0,                       /* tp_richcompare */\n    0,                       /* tp_weaklistoffset */\n    0,                       /* tp_iter */\n    0,                       /* tp_iternext */\n    ast_type_methods,        /* tp_methods */\n    0,                       /* tp_members */\n    ast_type_getsets,        /* tp_getset */\n    0,                       /* tp_base */\n    0,                       /* tp_dict */\n    0,                       /* tp_descr_get */\n    0,                       /* tp_descr_set */\n    offsetof(AST_object, dict),/* tp_dictoffset */\n    (initproc)ast_type_init, /* tp_init */\n    PyType_GenericAlloc,     /* tp_alloc */\n    PyType_GenericNew,       /* tp_new */\n    PyObject_GC_Del,         /* tp_free */\n};\nstatic PyTypeObject* make_type(char *type, PyTypeObject* base, char**fields, int num_fields)\n{\n    PyObject *fnames, *result;\n    int i;\n    fnames = PyTuple_New(num_fields);\n    if (!fnames) return NULL;\n    for (i = 0; i < num_fields; i++) {\n        PyObject *field = PyUnicode_FromString(fields[i]);\n        if (!field) {\n            Py_DECREF(fnames);\n            return NULL;\n        }\n        PyTuple_SET_ITEM(fnames, i, field);\n    }\n    result = PyObject_CallFunction((PyObject*)&PyType_Type, \"s(O){sOss}\",\n                    type, base, \"_fields\", fnames, \"__module__\", \"_ast3\");\n    Py_DECREF(fnames);\n    return (PyTypeObject*)result;\n}\nstatic int add_attributes(PyTypeObject* type, char**attrs, int num_fields)\n{\n    int i, result;\n    _Py_IDENTIFIER(_attributes);\n    PyObject *s, *l = PyTuple_New(num_fields);\n    if (!l)\n        return 0;\n    for (i = 0; i < num_fields; i++) {\n        s = PyUnicode_FromString(attrs[i]);\n        if (!s) {\n            Py_DECREF(l);\n            return 0;\n        }\n        PyTuple_SET_ITEM(l, i, s);\n    }\n    result = _PyObject_SetAttrId((PyObject*)type, &PyId__attributes, l) >= 0;\n    Py_DECREF(l);\n    return result;\n}\n/* Conversion AST -> Python */\nstatic PyObject* ast2obj_list(asdl_seq *seq, PyObject* (*func)(void*))\n{\n    Py_ssize_t i, n = asdl_seq_LEN(seq);\n    PyObject *result = PyList_New(n);\n    PyObject *value;\n    if (!result)\n        return NULL;\n    for (i = 0; i < n; i++) {\n        value = func(asdl_seq_GET(seq, i));\n        if (!value) {\n            Py_DECREF(result);\n            return NULL;\n        }\n        PyList_SET_ITEM(result, i, value);\n    }\n    return result;\n}\nstatic PyObject* ast2obj_object(void *o)\n{\n    if (!o)\n        o = Py_None;\n    Py_INCREF((PyObject*)o);\n    return (PyObject*)o;\n}\nstatic PyObject* ast2obj_int(long b)\n{\n    return PyLong_FromLong(b);\n}\n/* Conversion Python -> AST */\nstatic int obj2ast_singleton(PyObject *obj, PyObject** out, PyArena* arena)\n{\n    if (obj != Py_None && obj != Py_True && obj != Py_False) {\n        PyErr_SetString(PyExc_ValueError,\n                        \"AST singleton must be True, False, or None\");\n        return 1;\n    }\n    *out = obj;\n    return 0;\n}\nstatic int obj2ast_object(PyObject* obj, PyObject** out, PyArena* arena)\n{\n    if (obj == Py_None)\n        obj = NULL;\n    if (obj) {\n        if (PyArena_AddPyObject(arena, obj) < 0) {\n            *out = NULL;\n            return -1;\n        }\n        Py_INCREF(obj);\n    }\n    *out = obj;\n    return 0;\n}\nstatic int obj2ast_constant(PyObject* obj, PyObject** out, PyArena* arena)\n{\n    if (obj) {\n        if (PyArena_AddPyObject(arena, obj) < 0) {\n            *out = NULL;\n            return -1;\n        }\n        Py_INCREF(obj);\n    }\n    *out = obj;\n    return 0;\n}\nstatic int obj2ast_identifier(PyObject* obj, PyObject** out, PyArena* arena)\n{\n    if (!PyUnicode_CheckExact(obj) && obj != Py_None) {\n        PyErr_SetString(PyExc_TypeError, \"AST identifier must be of type str\");\n        return 1;\n    }\n    return obj2ast_object(obj, out, arena);\n}\nstatic int obj2ast_string(PyObject* obj, PyObject** out, PyArena* arena)\n{\n    if (!PyUnicode_CheckExact(obj) && !PyBytes_CheckExact(obj)) {\n        PyErr_SetString(PyExc_TypeError, \"AST string must be of type str\");\n        return 1;\n    }\n    return obj2ast_object(obj, out, arena);\n}\nstatic int obj2ast_bytes(PyObject* obj, PyObject** out, PyArena* arena)\n{\n    if (!PyBytes_CheckExact(obj)) {\n        PyErr_SetString(PyExc_TypeError, \"AST bytes must be of type bytes\");\n        return 1;\n    }\n    return obj2ast_object(obj, out, arena);\n}\nstatic int obj2ast_int(PyObject* obj, int* out, PyArena* arena)\n{\n    int i;\n    if (!PyLong_Check(obj)) {\n        PyErr_Format(PyExc_ValueError, \"invalid integer value: %R\", obj);\n        return 1;\n    }\n    i = _PyLong_AsInt(obj);\n    if (i == -1 && PyErr_Occurred())\n        return 1;\n    *out = i;\n    return 0;\n}\nstatic int add_ast_fields(void)\n{\n    PyObject *empty_tuple, *d;\n    if (PyType_Ready(&AST_type) < 0)\n        return -1;\n    d = AST_type.tp_dict;\n    empty_tuple = PyTuple_New(0);\n    if (!empty_tuple ||\n        PyDict_SetItemString(d, \"_fields\", empty_tuple) < 0 ||\n        PyDict_SetItemString(d, \"_attributes\", empty_tuple) < 0) {\n        Py_XDECREF(empty_tuple);\n        return -1;\n    }\n    Py_DECREF(empty_tuple);\n    return 0;\n}\nstatic int exists_not_none(PyObject *obj, _Py_Identifier *id)\n{\n    int isnone;\n    PyObject *attr = _PyObject_GetAttrId(obj, id);\n    if (!attr) {\n        PyErr_Clear();\n        return 0;\n    }\n    isnone = attr == Py_None;\n    Py_DECREF(attr);\n    return !isnone;\n}\n\"\"\", 0, reflow=False)\n        self.emit(\"static int init_types(void)\",0)\n        self.emit(\"{\", 0)\n        self.emit(\"static int initialized;\", 1)\n        self.emit(\"if (initialized) return 1;\", 1)\n        self.emit(\"if (add_ast_fields() < 0) return 0;\", 1)\n        for dfn in mod.dfns:\n            self.visit(dfn)\n        self.emit(\"initialized = 1;\", 1)\n        self.emit(\"return 1;\", 1);\n        self.emit(\"}\", 0)\n    def func_begin(self, name):\n        ctype = get_c_type(name)\n        self.emit(\"PyObject*\", 0)\n        self.emit(\"ast2obj_%s(void* _o)\" % (name), 0)\n        self.emit(\"{\", 0)\n        self.emit(\"%s o = (%s)_o;\" % (ctype, ctype), 1)\n        self.emit(\"PyObject *result = NULL, *value = NULL;\", 1)\n        self.emit('if (!o) {', 1)\n        self.emit(\"Py_INCREF(Py_None);\", 2)\n        self.emit('return Py_None;', 2)\n        self.emit(\"}\", 1)\n        self.emit('', 0)\ndef main(srcfile, dump_module=False):\n    argv0 = sys.argv[0]\n    components = argv0.split(os.sep)\n    argv0 = os.sep.join(components[-2:])\n    auto_gen_msg = common_msg % argv0\n    mod = asdl.parse(srcfile)\n    if dump_module:\n        print('Parsed Module:')\n        print(mod)\n    if not asdl.check(mod):\n        sys.exit(1)\n    if INC_DIR:\n        p = \"%s/%s-ast.h\" % (INC_DIR, mod.name)\n        f = open(p, \"w\")\n        f.write(auto_gen_msg)\n        f.write('\n        c = ChainOfVisitors(TypeDefVisitor(f),\n                            StructVisitor(f),\n                            PrototypeVisitor(f),\n                            )\n        c.visit(mod)\n        f.write(\"PyObject* Ta3AST_mod2obj(mod_ty t);\\n\")\n        f.write(\"mod_ty Ta3AST_obj2mod(PyObject* ast, PyArena* arena, int mode);\\n\")\n        f.write(\"int Ta3AST_Check(PyObject* obj);\\n\")\n        f.close()\n    if SRC_DIR:\n        p = os.path.join(SRC_DIR, str(mod.name) + \"-ast.c\")\n        f = open(p, \"w\")\n        f.write(auto_gen_msg)\n        f.write('\n        f.write('\\n')\n        f.write('\n        f.write('\n        f.write('\\n')\n        f.write(\"static PyTypeObject AST_type;\\n\")\n        v = ChainOfVisitors(\n            PyTypesDeclareVisitor(f),\n            PyTypesVisitor(f),\n            Obj2ModPrototypeVisitor(f),\n            FunctionVisitor(f),\n            ObjVisitor(f),\n            Obj2ModVisitor(f),\n            ASTModuleVisitor(f),\n            PartingShots(f),\n            )\n        v.visit(mod)\n        f.close()",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2019-19274",
        "description": "[{'lang': 'en', 'value': 'typed_ast 1.3.0 and 1.3.1 has a handle_keywordonly_args out-of-bounds read. An attacker with the ability to cause a Python interpreter to parse Python source (but not necessarily execute it) may be able to crash the interpreter process. This could be a concern, for example, in a web-based service that parses (but does not execute) Python code. (This issue also affected certain Python 3.8.0-alpha prereleases.)'}]",
        "cwe_number": 125
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-181",
      "code": "    def _load_from(self, data: bytes) -> None:\n        if data.strip() == b'':\n            data = XMP_EMPTY\n        def basic_parser(xml):\n            return parse(BytesIO(xml))\n        def strip_illegal_bytes_parser(xml):\n            return parse(BytesIO(re_xml_illegal_bytes.sub(b'', xml)))\n        def recovery_parser(xml):\n            parser = XMLParser(recover=True)\n            return parse(BytesIO(xml), parser)\n        def replace_with_empty_xmp(_xml=None):\n            log.warning(\"Error occurred parsing XMP, replacing with empty XMP.\")\n            return basic_parser(XMP_EMPTY)\n        if self.overwrite_invalid_xml:\n            parsers: Iterable[Callable] = [\n                basic_parser,\n                strip_illegal_bytes_parser,\n                recovery_parser,\n                replace_with_empty_xmp,\n            ]\n        else:\n            parsers = [basic_parser]\n        for parser in parsers:\n            try:\n                self._xmp = parser(data)\n            except (XMLSyntaxError if self.overwrite_invalid_xml else NeverRaise) as e:\n                if str(e).startswith(\"Start tag expected, '<' not found\") or str(\n                    e\n                ).startswith(\"Document is empty\"):\n                    self._xmp = replace_with_empty_xmp()\n                    break\n            else:\n                break\n        try:\n            pis = self._xmp.xpath('/processing-instruction()')\n            for pi in pis:\n                etree.strip_tags(self._xmp, pi.tag)\n            self._get_rdf_root()\n        except (Exception if self.overwrite_invalid_xml else NeverRaise) as e:\n            log.warning(\"Error occurred parsing XMP\", exc_info=e)\n            self._xmp = replace_with_empty_xmp()\n        return",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-29421",
        "description": "[{'lang': 'en', 'value': 'models/metadata.py in the pikepdf package 1.3.0 through 2.9.2 for Python allows XXE when parsing XMP metadata entries.'}]",
        "cwe_number": 611
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-182",
      "code": "def process_quote_response(agent, json_response, agentAttestState) -> Failure:\n    \"\"\"Validates the response from the Cloud agent.\n    This method invokes an Registrar Server call to register, and then check the quote.\n    \"\"\"\n    failure = Failure(Component.QUOTE_VALIDATION)\n    received_public_key = None\n    quote = None\n    try:\n        received_public_key = json_response.get(\"pubkey\", None)\n        quote = json_response[\"quote\"]\n        ima_measurement_list = json_response.get(\"ima_measurement_list\", None)\n        ima_measurement_list_entry = json_response.get(\"ima_measurement_list_entry\", 0)\n        mb_measurement_list = json_response.get(\"mb_measurement_list\", None)\n        boottime = json_response.get(\"boottime\", 0)\n        logger.debug(\"received quote:      %s\", quote)\n        logger.debug(\"for nonce:           %s\", agent['nonce'])\n        logger.debug(\"received public key: %s\", received_public_key)\n        logger.debug(\"received ima_measurement_list    %s\", (ima_measurement_list is not None))\n        logger.debug(\"received ima_measurement_list_entry: %d\", ima_measurement_list_entry)\n        logger.debug(\"received boottime: %s\", boottime)\n        logger.debug(\"received boot log    %s\", (mb_measurement_list is not None))\n    except Exception as e:\n        failure.add_event(\"invalid_data\", {\"message\": \"parsing agents get quote respone failed\", \"data\": e}, False)\n        return failure\n    if not isinstance(ima_measurement_list_entry, int):\n        raise Exception(\"ima_measurement_list_entry parameter must be an integer\")\n    if not isinstance(boottime, int):\n        raise Exception(\"boottime parameter must be an integer\")\n    if received_public_key is None:\n        if agent.get('public_key', \"\") == \"\" or agent.get('b64_encrypted_V', \"\") == \"\":\n            logger.error(\"agent did not provide public key and no key or encrypted_v was cached at CV\")\n            failure.add_event(\"no_pubkey\", \"agent did not provide public key and no key or encrypted_v was cached at CV\", False)\n            return failure\n        agent['provide_V'] = False\n        received_public_key = agent['public_key']\n    hash_alg = json_response.get('hash_alg')\n    enc_alg = json_response.get('enc_alg')\n    sign_alg = json_response.get('sign_alg')\n    agent['hash_alg'] = hash_alg\n    agent['enc_alg'] = enc_alg\n    agent['sign_alg'] = sign_alg\n    if not algorithms.is_accepted(hash_alg, agent['accept_tpm_hash_algs'])\\\n            or not algorithms.Hash.is_recognized(hash_alg):\n        logger.error(f\"TPM Quote is using an unaccepted hash algorithm: {hash_alg}\")\n        failure.add_event(\"invalid_hash_alg\",\n                          {\"message\": f\"TPM Quote is using an unaccepted hash algorithm: {hash_alg}\", \"data\": hash_alg},\n                          False)\n        return failure\n    if not algorithms.is_accepted(enc_alg, agent['accept_tpm_encryption_algs']):\n        logger.error(f\"TPM Quote is using an unaccepted encryption algorithm: {enc_alg}\")\n        failure.add_event(\"invalid_enc_alg\",\n                          {\"message\": f\"TPM Quote is using an unaccepted encryption algorithm: {enc_alg}\", \"data\": enc_alg},\n                          False)\n        return failure\n    if not algorithms.is_accepted(sign_alg, agent['accept_tpm_signing_algs']):\n        logger.error(f\"TPM Quote is using an unaccepted signing algorithm: {sign_alg}\")\n        failure.add_event(\"invalid_sign_alg\",\n                          {\"message\": f\"TPM Quote is using an unaccepted signing algorithm: {sign_alg}\", \"data\": {sign_alg}},\n                          False)\n        return failure\n    if ima_measurement_list_entry == 0:\n        agentAttestState.reset_ima_attestation()\n    elif ima_measurement_list_entry != agentAttestState.get_next_ima_ml_entry():\n        logger.error(\"Agent did not respond with requested next IMA measurement list entry \"\n                     f\"{agentAttestState.get_next_ima_ml_entry()} but started at {ima_measurement_list_entry}\")\n        failure.add_event(\"invalid_ima_entry_nb\",\n                          {\"message\": \"Agent did not respond with requested next IMA measurement list entry\",\n                           \"got\": ima_measurement_list_entry, \"expected\": agentAttestState.get_next_ima_ml_entry()},\n                          False)\n    elif not agentAttestState.is_expected_boottime(boottime):\n        agentAttestState.reset_ima_attestation()\n        return failure\n    agentAttestState.set_boottime(boottime)\n    ima_keyrings = agentAttestState.get_ima_keyrings()\n    tenant_keyring = ima_file_signatures.ImaKeyring.from_string(agent['ima_sign_verification_keys'])\n    ima_keyrings.set_tenant_keyring(tenant_keyring)\n    quote_validation_failure = get_tpm_instance().check_quote(\n        agentAttestState,\n        agent['nonce'],\n        received_public_key,\n        quote,\n        agent['ak_tpm'],\n        agent['tpm_policy'],\n        ima_measurement_list,\n        agent['allowlist'],\n        algorithms.Hash(hash_alg),\n        ima_keyrings,\n        mb_measurement_list,\n        agent['mb_refstate'])\n    failure.merge(quote_validation_failure)\n    if not failure:\n        agent['first_verified'] = True\n        if received_public_key != agent.get('public_key', \"\"):\n            agent['public_key'] = received_public_key\n            agent['b64_encrypted_V'] = \"\"\n            agent['provide_V'] = True\n    return failure\n    def validate_tpm_quote(self, public_key, quote, hash_alg):\n        \"\"\" Validate TPM Quote received from the Agent\n        Arguments:\n            public_key {[type]} -- [description]\n            quote {[type]} -- [description]\n            hash_alg {bool} -- [description]\n        Raises:\n            UserError: [description]\n        Returns:\n            [type] -- [description]\n        \"\"\"\n        registrar_client.init_client_tls('tenant')\n        if self.registrar_data is None:\n            logger.warning(\"AIK not found in registrar, quote not validated\")\n            return False\n        failure = self.tpm_instance.check_quote(AgentAttestState(self.agent_uuid), self.nonce, public_key, quote, self.registrar_data['aik_tpm'], hash_alg=hash_alg)\n        if failure:\n            if self.registrar_data['regcount'] > 1:\n                logger.error(\"WARNING: This UUID had more than one ek-ekcert registered to it! This might indicate that your system is misconfigured or a malicious host is present. Run 'regdelete' for this agent and restart\")\n                sys.exit()\n            return False\n        if self.registrar_data['regcount'] > 1:\n            logger.warning(\"WARNING: This UUID had more than one ek-ekcert registered to it! This might indicate that your system is misconfigured. Run 'regdelete' for this agent and restart\")\n        if not config.STUB_TPM and (not config.getboolean('tenant', 'require_ek_cert') and config.get('tenant', 'ek_check_script') == \"\"):\n            logger.warning(\n                \"DANGER: EK cert checking is disabled and no additional checks on EKs have been specified with ek_check_script option. Keylime is not secure!!\")\n        if not self.check_ek(self.registrar_data['ekcert']):\n            return False\n        if 'provider_keys' in self.registrar_data:\n            if not self.check_ek(self.registrar_data['provider_keys']['ekcert']):\n                return False\n        script = config.get('tenant', 'ek_check_script')\n        if not script:\n            return True\n        if script[0] != '/':\n            script = os.path.join(config.WORK_DIR, script)\n        logger.info(\"Checking EK with script %s\", script)\n        env = os.environ.copy()\n        env['AGENT_UUID'] = self.agent_uuid\n        env['EK'] = tpm2_objects.pubkey_from_tpm2b_public(\n            base64.b64decode(self.registrar_data['ek_tpm']),\n            ).public_bytes(\n                crypto_serialization.Encoding.PEM,\n                crypto_serialization.PublicFormat.SubjectPublicKeyInfo,\n            )\n        env['EK_TPM'] = self.registrar_data['ek_tpm']\n        if self.registrar_data['ekcert'] is not None:\n            env['EK_CERT'] = self.registrar_data['ekcert']\n        else:\n            env['EK_CERT'] = \"\"\n        env['PROVKEYS'] = json.dumps(self.registrar_data.get('provider_keys', {}))\n        proc = subprocess.Popen(script, env=env, shell=True,\n                                cwd=config.WORK_DIR, stdout=subprocess.PIPE,\n                                stderr=subprocess.STDOUT)\n        retval = proc.wait()\n        if retval != 0:\n            raise UserError(\"External check script failed to validate EK\")\n        logger.debug(\"External check script successfully to validated EK\")\n        while True:\n            line = proc.stdout.readline().decode()\n            if line == \"\":\n                break\n            logger.debug(\"ek_check output: %s\", line.strip())\n        return True\n    def create_quote(self, nonce, data=None, pcrmask=tpm_abstract.AbstractTPM.EMPTYMASK, hash_alg=None):\n        if hash_alg is None:\n            hash_alg = self.defaults['hash']\n        quote = \"\"\n        with tempfile.NamedTemporaryFile() as quotepath, \\\n                tempfile.NamedTemporaryFile() as sigpath, \\\n                tempfile.NamedTemporaryFile() as pcrpath:\n            keyhandle = self.get_tpm_metadata('aik_handle')\n            aik_pw = self.get_tpm_metadata('aik_pw')\n            if pcrmask is None:\n                pcrmask = tpm_abstract.AbstractTPM.EMPTYMASK\n            if data is not None:\n                pcrmask = \"0x%X\" % (int(pcrmask, 0) + (1 << config.TPM_DATA_PCR))\n            pcrlist = self.__pcr_mask_to_list(pcrmask)\n            with self.tpmutilLock:\n                if data is not None:\n                    self.__run([\"tpm2_pcrreset\", str(config.TPM_DATA_PCR)], lock=False)\n                    self.extendPCR(pcrval=config.TPM_DATA_PCR, hashval=self.hashdigest(data), lock=False)\n                nonce = bytes(nonce, encoding=\"utf8\").hex()\n                if self.tools_version == \"3.2\":\n                    command = [\"tpm2_quote\", \"-k\", hex(keyhandle), \"-L\", \"%s:%s\" % (hash_alg, pcrlist), \"-q\", nonce, \"-m\", quotepath.name, \"-s\", sigpath.name, \"-p\", pcrpath.name, \"-G\", hash_alg, \"-P\", aik_pw]\n                elif self.tools_version in [\"4.0\", \"4.2\"]:\n                    command = [\"tpm2_quote\", \"-c\", keyhandle, \"-l\", \"%s:%s\" % (hash_alg, pcrlist), \"-q\", nonce, \"-m\", quotepath.name, \"-s\", sigpath.name, \"-o\", pcrpath.name, \"-g\", hash_alg, \"-p\", aik_pw]\n                retDict = self.__run(command, lock=False, outputpaths=[quotepath.name, sigpath.name, pcrpath.name])\n                quoteraw = retDict['fileouts'][quotepath.name]\n                quote_b64encode = base64.b64encode(zlib.compress(quoteraw))\n                sigraw = retDict['fileouts'][sigpath.name]\n                sigraw_b64encode = base64.b64encode(zlib.compress(sigraw))\n                pcrraw = retDict['fileouts'][pcrpath.name]\n                pcrraw_b64encode = base64.b64encode(zlib.compress(pcrraw))\n                quote = quote_b64encode.decode('utf-8') + \":\" + sigraw_b64encode.decode('utf-8') + \":\" + pcrraw_b64encode.decode('utf-8')\n        return 'r' + quote\n    def _tpm2_checkquote(self, aikTpmFromRegistrar, quote, nonce, hash_alg):\n        \"\"\"Write the files from data returned from tpm2_quote for running tpm2_checkquote\n        :param aikTpmFromRegistrar: AIK used to generate the quote and is needed for verifying it now.\n        :param quote: quote data in the format 'r<b64-compressed-quoteblob>:<b64-compressed-sigblob>:<b64-compressed-pcrblob>\n        :param nonce: nonce that was used to create the quote\n        :param hash_alg: the hash algorithm that was used\n        :returns: Returns the 'retout' from running tpm2_checkquote and True in case of success, None and False in case of error.\n        This function throws an Exception on bad input.\n        \"\"\"\n        aikFromRegistrar = tpm2_objects.pubkey_from_tpm2b_public(\n            base64.b64decode(aikTpmFromRegistrar),\n            ).public_bytes(\n                crypto_serialization.Encoding.PEM,\n                crypto_serialization.PublicFormat.SubjectPublicKeyInfo,\n            )\n        if quote[0] != 'r':\n            raise Exception(\"Invalid quote type %s\" % quote[0])\n        quote = quote[1:]\n        quote_tokens = quote.split(\":\")\n        if len(quote_tokens) < 3:\n            raise Exception(\"Quote is not compound! %s\" % quote)\n        quoteblob = zlib.decompress(base64.b64decode(quote_tokens[0]))\n        sigblob = zlib.decompress(base64.b64decode(quote_tokens[1]))\n        pcrblob = zlib.decompress(base64.b64decode(quote_tokens[2]))\n        qfd = sfd = pfd = afd = -1\n        quoteFile = None\n        aikFile = None\n        sigFile = None\n        pcrFile = None\n        try:\n            qfd, qtemp = tempfile.mkstemp()\n            quoteFile = open(qtemp, \"wb\")\n            quoteFile.write(quoteblob)\n            quoteFile.close()\n            sfd, stemp = tempfile.mkstemp()\n            sigFile = open(stemp, \"wb\")\n            sigFile.write(sigblob)\n            sigFile.close()\n            pfd, ptemp = tempfile.mkstemp()\n            pcrFile = open(ptemp, \"wb\")\n            pcrFile.write(pcrblob)\n            pcrFile.close()\n            afd, atemp = tempfile.mkstemp()\n            aikFile = open(atemp, \"wb\")\n            aikFile.write(aikFromRegistrar)\n            aikFile.close()\n            retDict = self.__tpm2_checkquote(aikFile.name, nonce, quoteFile.name, sigFile.name, pcrFile.name, hash_alg)\n            retout = retDict['retout']\n            reterr = retDict['reterr']\n            code = retDict['code']\n        except Exception as e:\n            logger.error(\"Error verifying quote: \" + str(e))\n            logger.exception(e)\n            return None, False\n        finally:\n            for fd in [qfd, sfd, pfd, afd]:\n                if fd >= 0:\n                    os.close(fd)\n            for fi in [aikFile, quoteFile, sigFile, pcrFile]:\n                if fi is not None:\n                    os.remove(fi.name)\n        if len(retout) < 1 or code != tpm_abstract.AbstractTPM.EXIT_SUCESS:\n            logger.error(\"Failed to validate signature, output: %s\" % reterr)\n            return None, False\n        return retout, True\n    def check_quote(self, agentAttestState, nonce, data, quote, aikTpmFromRegistrar, tpm_policy={},\n                    ima_measurement_list=None, allowlist={}, hash_alg=None, ima_keyrings=None,\n                    mb_measurement_list=None, mb_refstate=None) -> Failure:\n        failure = Failure(Component.QUOTE_VALIDATION)\n        if hash_alg is None:\n            hash_alg = self.defaults['hash']\n        retout, success = self._tpm2_checkquote(aikTpmFromRegistrar, quote, nonce, hash_alg)\n        if not success:\n            failure.add_event(\"quote_validation\", {\"message\": \"Quote validation using tpm2-tools\", \"data\": retout}, False)\n            return failure\n        pcrs = []\n        jsonout = config.yaml_to_dict(retout, logger=logger)\n        if jsonout is None:\n            failure.add_event(\"quote_validation\", {\"message\": \"YAML parsing failed for quote validation using tpm2-tools.\",\n                                                    \"data\": retout}, False)\n            return failure\n        if \"pcrs\" in jsonout:\n            if hash_alg in jsonout[\"pcrs\"]:\n                alg_size = hash_alg.get_size() // 4\n                for pcrval, hashval in jsonout[\"pcrs\"][hash_alg].items():\n                    pcrs.append(\"PCR \" + str(pcrval) + \" \" + '{0:0{1}x}'.format(hashval, alg_size))\n        if len(pcrs) == 0:\n            pcrs = None\n        return self.check_pcrs(agentAttestState, tpm_policy, pcrs, data, False, ima_measurement_list, allowlist,\n                               ima_keyrings, mb_measurement_list, mb_refstate, hash_alg)\n    def create_quote(self, nonce, data=None, pcrmask=EMPTYMASK, hash_alg=None):\n        pass\n    def check_quote(self, agentAttestState, nonce, data, quote, aikTpmFromRegistrar, tpm_policy={}, ima_measurement_list=None, allowlist={}, hash_alg=None, ima_keyrings=None, mb_measurement_list=None, mb_refstate=None):\n        pass",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-23951",
        "description": "[{'lang': 'en', 'value': 'In Keylime before 6.3.0, quote responses from the agent can contain possibly untrusted ZIP data which can lead to zip bombs.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-183",
      "code": "    def create_option(self, name, value, label, selected, index, subindex=None, attrs=None):\n        result = super(TagFormWidget, self).create_option(\n            name=name, value=value, label=label, selected=selected,\n            index=index, subindex=subindex, attrs=attrs\n        )\n        result['attrs']['data-color'] = self.queryset.get(pk=value).color\n        return result",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2018-16407",
        "description": "[{'lang': 'en', 'value': 'An issue was discovered in Mayan EDMS before 3.0.3. The Tags app has XSS because tag label values are mishandled.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-184",
      "code": "def is_local_uri(uri, is_tracking_or_registry_uri=True):\n    \"\"\"\n    Returns true if the specified URI is a local file path (/foo or file:/foo).\n    :param uri: The URI.\n    :param is_tracking_uri: Whether or not the specified URI is an MLflow Tracking or MLflow\n                            Model Registry URI. Examples of other URIs are MLflow artifact URIs,\n                            filesystem paths, etc.\n    \"\"\"\n    if uri == \"databricks\" and is_tracking_or_registry_uri:\n        return False\n    if is_windows() and uri.startswith(\"\\\\\\\\\"):\n        return False\n    parsed_uri = urllib.parse.urlparse(uri)\n    if parsed_uri.hostname and not (\n        parsed_uri.hostname == \".\"\n        or parsed_uri.hostname.startswith(\"localhost\")\n        or parsed_uri.hostname.startswith(\"127.0.0.1\")\n    ):\n        return False\n    scheme = parsed_uri.scheme\n    if scheme == \"\" or scheme == \"file\":\n        return True\n    if is_windows() and len(scheme) == 1 and scheme.lower() == pathlib.Path(uri).drive.lower()[0]:\n        return True\n    return False\ndef is_file_uri(uri):\n    return urllib.parse.urlparse(uri).scheme == \"file\"",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-6977",
        "description": "[{'lang': 'en', 'value': 'This vulnerability enables malicious users to read sensitive files on the server.'}]",
        "cwe_number": 29
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-185",
      "code": "  def testSimple(self):\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.float32)\n        pi = array_ops.placeholder(dtypes.int64)\n        gi = array_ops.placeholder(dtypes.int64)\n        v = 2. * (array_ops.zeros([128, 128]) + x)\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea([dtypes.float32])\n        stage = stager.put(pi, [v], [0])\n        k, y = stager.get(gi)\n        y = math_ops.reduce_max(math_ops.matmul(y, y))\n    G.finalize()\n    with self.session(graph=G) as sess:\n      sess.run(stage, feed_dict={x: -1, pi: 0})\n      for i in range(10):\n        _, yval = sess.run([stage, y], feed_dict={x: i, pi: i + 1, gi: i})\n        self.assertAllClose(4 * (i - 1) * (i - 1) * 128, yval, rtol=1e-4)\n  def testMultiple(self):\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.float32)\n        pi = array_ops.placeholder(dtypes.int64)\n        gi = array_ops.placeholder(dtypes.int64)\n        v = 2. * (array_ops.zeros([128, 128]) + x)\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea([dtypes.float32, dtypes.float32])\n        stage = stager.put(pi, [x, v], [0, 1])\n        k, (z, y) = stager.get(gi)\n        y = math_ops.reduce_max(z * math_ops.matmul(y, y))\n    G.finalize()\n    with self.session(graph=G) as sess:\n      sess.run(stage, feed_dict={x: -1, pi: 0})\n      for i in range(10):\n        _, yval = sess.run([stage, y], feed_dict={x: i, pi: i + 1, gi: i})\n        self.assertAllClose(\n            4 * (i - 1) * (i - 1) * (i - 1) * 128, yval, rtol=1e-4)\n  def testDictionary(self):\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.float32)\n        pi = array_ops.placeholder(dtypes.int64)\n        gi = array_ops.placeholder(dtypes.int64)\n        v = 2. * (array_ops.zeros([128, 128]) + x)\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea(\n            [dtypes.float32, dtypes.float32],\n            shapes=[[], [128, 128]],\n            names=['x', 'v'])\n        stage = stager.put(pi, {'x': x, 'v': v})\n        key, ret = stager.get(gi)\n        z = ret['x']\n        y = ret['v']\n        y = math_ops.reduce_max(z * math_ops.matmul(y, y))\n    G.finalize()\n    with self.session(graph=G) as sess:\n      sess.run(stage, feed_dict={x: -1, pi: 0})\n      for i in range(10):\n        _, yval = sess.run([stage, y], feed_dict={x: i, pi: i + 1, gi: i})\n        self.assertAllClose(\n            4 * (i - 1) * (i - 1) * (i - 1) * 128, yval, rtol=1e-4)\n  def testColocation(self):\n    gpu_dev = test.gpu_device_name()\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.float32)\n        v = 2. * (array_ops.zeros([128, 128]) + x)\n      with ops.device(gpu_dev):\n        stager = data_flow_ops.MapStagingArea([dtypes.float32])\n        y = stager.put(1, [v], [0])\n        expected_name = gpu_dev if 'gpu' not in gpu_dev else '/device:GPU:0'\n        self.assertEqual(y.device, expected_name)\n      with ops.device('/cpu:0'):\n        _, x = stager.get(1)\n        y = stager.peek(1)[0]\n        _, z = stager.get()\n        self.assertEqual(x[0].device, '/device:CPU:0')\n        self.assertEqual(y.device, '/device:CPU:0')\n        self.assertEqual(z[0].device, '/device:CPU:0')\n    G.finalize()\n  def testPeek(self):\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.int32, name='x')\n        pi = array_ops.placeholder(dtypes.int64)\n        gi = array_ops.placeholder(dtypes.int64)\n        p = array_ops.placeholder(dtypes.int32, name='p')\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea(\n            [\n                dtypes.int32,\n            ], shapes=[[]])\n        stage = stager.put(pi, [x], [0])\n        peek = stager.peek(gi)\n        size = stager.size()\n    G.finalize()\n    n = 10\n    with self.session(graph=G) as sess:\n      for i in range(n):\n        sess.run(stage, feed_dict={x: i, pi: i})\n      for i in range(n):\n        self.assertTrue(sess.run(peek, feed_dict={gi: i})[0] == i)\n      self.assertTrue(sess.run(size) == 10)\n  def testSizeAndClear(self):\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.float32, name='x')\n        pi = array_ops.placeholder(dtypes.int64)\n        gi = array_ops.placeholder(dtypes.int64)\n        v = 2. * (array_ops.zeros([128, 128]) + x)\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea(\n            [dtypes.float32, dtypes.float32],\n            shapes=[[], [128, 128]],\n            names=['x', 'v'])\n        stage = stager.put(pi, {'x': x, 'v': v})\n        size = stager.size()\n        clear = stager.clear()\n    G.finalize()\n    with self.session(graph=G) as sess:\n      sess.run(stage, feed_dict={x: -1, pi: 3})\n      self.assertEqual(sess.run(size), 1)\n      sess.run(stage, feed_dict={x: -1, pi: 1})\n      self.assertEqual(sess.run(size), 2)\n      sess.run(clear)\n      self.assertEqual(sess.run(size), 0)\n  def testCapacity(self):\n    capacity = 3\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.int32, name='x')\n        pi = array_ops.placeholder(dtypes.int64, name='pi')\n        gi = array_ops.placeholder(dtypes.int64, name='gi')\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea(\n            [\n                dtypes.int32,\n            ], capacity=capacity, shapes=[[]])\n      stage = stager.put(pi, [x], [0])\n      get = stager.get()\n      size = stager.size()\n    G.finalize()\n    from six.moves import queue as Queue\n    import threading\n    queue = Queue.Queue()\n    n = 8\n    with self.session(graph=G) as sess:\n      def thread_run():\n        for i in range(n):\n          sess.run(stage, feed_dict={x: i, pi: i})\n          queue.put(0)\n      t = threading.Thread(target=thread_run)\n      t.daemon = True\n      t.start()\n      try:\n        for i in range(n):\n          queue.get(timeout=TIMEOUT)\n      except Queue.Empty:\n        pass\n      if not i == capacity:\n        self.fail(\"Expected to timeout on iteration '{}' \"\n                  \"but instead timed out on iteration '{}' \"\n                  \"Staging Area size is '{}' and configured \"\n                  \"capacity is '{}'.\".format(capacity, i, sess.run(size),\n                                             capacity))\n      self.assertTrue(sess.run(size) == capacity)\n      for i in range(n):\n        sess.run(get)\n      self.assertTrue(sess.run(size) == 0)\n  def testMemoryLimit(self):\n    memory_limit = 512 * 1024\n    chunk = 200 * 1024\n    capacity = memory_limit // chunk\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.uint8, name='x')\n        pi = array_ops.placeholder(dtypes.int64, name='pi')\n        gi = array_ops.placeholder(dtypes.int64, name='gi')\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea(\n            [dtypes.uint8], memory_limit=memory_limit, shapes=[[]])\n        stage = stager.put(pi, [x], [0])\n        get = stager.get()\n        size = stager.size()\n    G.finalize()\n    from six.moves import queue as Queue\n    import threading\n    import numpy as np\n    queue = Queue.Queue()\n    n = 8\n    with self.session(graph=G) as sess:\n      def thread_run():\n        for i in range(n):\n          data = np.full(chunk, i, dtype=np.uint8)\n          sess.run(stage, feed_dict={x: data, pi: i})\n          queue.put(0)\n      t = threading.Thread(target=thread_run)\n      t.daemon = True\n      t.start()\n      try:\n        for i in range(n):\n          queue.get(timeout=TIMEOUT)\n      except Queue.Empty:\n        pass\n      if not i == capacity:\n        self.fail(\"Expected to timeout on iteration '{}' \"\n                  \"but instead timed out on iteration '{}' \"\n                  \"Staging Area size is '{}' and configured \"\n                  \"capacity is '{}'.\".format(capacity, i, sess.run(size),\n                                             capacity))\n      self.assertTrue(sess.run(size) == capacity)\n      for i in range(n):\n        sess.run(get)\n      self.assertTrue(sess.run(size) == 0)\n  def testOrdering(self):\n    import six\n    import random\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.int32, name='x')\n        pi = array_ops.placeholder(dtypes.int64, name='pi')\n        gi = array_ops.placeholder(dtypes.int64, name='gi')\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea(\n            [\n                dtypes.int32,\n            ], shapes=[[]], ordered=True)\n        stage = stager.put(pi, [x], [0])\n        get = stager.get()\n        size = stager.size()\n    G.finalize()\n    n = 10\n    with self.session(graph=G) as sess:\n      keys = list(reversed(six.moves.range(n)))\n      for i in keys:\n        sess.run(stage, feed_dict={pi: i, x: i})\n      self.assertTrue(sess.run(size) == n)\n      for i, k in enumerate(reversed(keys)):\n        get_key, values = sess.run(get)\n        self.assertTrue(i == k == get_key == values)\n      self.assertTrue(sess.run(size) == 0)\n  def testPartialDictInsert(self):\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.float32)\n        f = array_ops.placeholder(dtypes.float32)\n        v = array_ops.placeholder(dtypes.float32)\n        pi = array_ops.placeholder(dtypes.int64)\n        gi = array_ops.placeholder(dtypes.int64)\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea(\n            [dtypes.float32, dtypes.float32, dtypes.float32],\n            names=['x', 'v', 'f'])\n        stage_xf = stager.put(pi, {'x': x, 'f': f})\n        stage_v = stager.put(pi, {'v': v})\n        key, ret = stager.get(gi)\n        size = stager.size()\n        isize = stager.incomplete_size()\n    G.finalize()\n    with self.session(graph=G) as sess:\n      self.assertTrue(sess.run([size, isize]) == [0, 0])\n      sess.run(stage_xf, feed_dict={pi: 0, x: 1, f: 2})\n      self.assertTrue(sess.run([size, isize]) == [0, 1])\n      sess.run(stage_xf, feed_dict={pi: 1, x: 1, f: 2})\n      self.assertTrue(sess.run([size, isize]) == [0, 2])\n      sess.run(stage_v, feed_dict={pi: 0, v: 1})\n      self.assertTrue(sess.run([size, isize]) == [1, 1])\n      self.assertTrue(\n          sess.run([key, ret], feed_dict={\n              gi: 0\n          }) == [0, {\n              'x': 1,\n              'f': 2,\n              'v': 1\n          }])\n      self.assertTrue(sess.run([size, isize]) == [0, 1])\n      sess.run(stage_v, feed_dict={pi: 1, v: 3})\n      self.assertTrue(\n          sess.run([key, ret], feed_dict={\n              gi: 1\n          }) == [1, {\n              'x': 1,\n              'f': 2,\n              'v': 3\n          }])\n  def testPartialIndexInsert(self):\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.float32)\n        f = array_ops.placeholder(dtypes.float32)\n        v = array_ops.placeholder(dtypes.float32)\n        pi = array_ops.placeholder(dtypes.int64)\n        gi = array_ops.placeholder(dtypes.int64)\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea(\n            [dtypes.float32, dtypes.float32, dtypes.float32])\n        stage_xf = stager.put(pi, [x, f], [0, 2])\n        stage_v = stager.put(pi, [v], [1])\n        key, ret = stager.get(gi)\n        size = stager.size()\n        isize = stager.incomplete_size()\n    G.finalize()\n    with self.session(graph=G) as sess:\n      self.assertTrue(sess.run([size, isize]) == [0, 0])\n      sess.run(stage_xf, feed_dict={pi: 0, x: 1, f: 2})\n      self.assertTrue(sess.run([size, isize]) == [0, 1])\n      sess.run(stage_xf, feed_dict={pi: 1, x: 1, f: 2})\n      self.assertTrue(sess.run([size, isize]) == [0, 2])\n      sess.run(stage_v, feed_dict={pi: 0, v: 1})\n      self.assertTrue(sess.run([size, isize]) == [1, 1])\n      self.assertTrue(sess.run([key, ret], feed_dict={gi: 0}) == [0, [1, 1, 2]])\n      self.assertTrue(sess.run([size, isize]) == [0, 1])\n      sess.run(stage_v, feed_dict={pi: 1, v: 3})\n      self.assertTrue(sess.run([key, ret], feed_dict={gi: 1}) == [1, [1, 3, 2]])\n  def testPartialDictGetsAndPeeks(self):\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.float32)\n        f = array_ops.placeholder(dtypes.float32)\n        v = array_ops.placeholder(dtypes.float32)\n        pi = array_ops.placeholder(dtypes.int64)\n        pei = array_ops.placeholder(dtypes.int64)\n        gi = array_ops.placeholder(dtypes.int64)\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea(\n            [dtypes.float32, dtypes.float32, dtypes.float32],\n            names=['x', 'v', 'f'])\n        stage_xf = stager.put(pi, {'x': x, 'f': f})\n        stage_v = stager.put(pi, {'v': v})\n        peek_xf = stager.peek(pei, ['x', 'f'])\n        peek_v = stager.peek(pei, ['v'])\n        key_xf, get_xf = stager.get(gi, ['x', 'f'])\n        key_v, get_v = stager.get(gi, ['v'])\n        pop_key_xf, pop_xf = stager.get(indices=['x', 'f'])\n        pop_key_v, pop_v = stager.get(pi, ['v'])\n        size = stager.size()\n        isize = stager.incomplete_size()\n    G.finalize()\n    with self.session(graph=G) as sess:\n      self.assertTrue(sess.run([size, isize]) == [0, 0])\n      sess.run(stage_xf, feed_dict={pi: 0, x: 1, f: 2})\n      self.assertTrue(sess.run([size, isize]) == [0, 1])\n      sess.run(stage_xf, feed_dict={pi: 1, x: 1, f: 2})\n      self.assertTrue(sess.run([size, isize]) == [0, 2])\n      sess.run(stage_v, feed_dict={pi: 0, v: 1})\n      self.assertTrue(sess.run([size, isize]) == [1, 1])\n      self.assertTrue(sess.run(peek_xf, feed_dict={pei: 0}) == {'x': 1, 'f': 2})\n      self.assertTrue(sess.run(peek_v, feed_dict={pei: 0}) == {'v': 1})\n      self.assertTrue(sess.run([size, isize]) == [1, 1])\n      self.assertTrue(\n          sess.run([key_xf, get_xf], feed_dict={\n              gi: 0\n          }) == [0, {\n              'x': 1,\n              'f': 2\n          }])\n      self.assertTrue(sess.run([size, isize]) == [1, 1])\n      with self.assertRaises(errors.InvalidArgumentError) as cm:\n        sess.run([key_xf, get_xf], feed_dict={gi: 0})\n      exc_str = (\"Tensor at index '0' for key '0' \" 'has already been removed.')\n      self.assertTrue(exc_str in cm.exception.message)\n      self.assertTrue(\n          sess.run([key_v, get_v], feed_dict={\n              gi: 0\n          }) == [0, {\n              'v': 1\n          }])\n      self.assertTrue(sess.run([size, isize]) == [0, 1])\n      sess.run(stage_v, feed_dict={pi: 1, v: 1})\n      self.assertTrue(sess.run([size, isize]) == [1, 0])\n      self.assertTrue(sess.run([pop_key_xf, pop_xf]) == [1, {'x': 1, 'f': 2}])\n      self.assertTrue(sess.run([size, isize]) == [1, 0])\n      self.assertTrue(\n          sess.run([pop_key_v, pop_v], feed_dict={\n              pi: 1\n          }) == [1, {\n              'v': 1\n          }])\n      self.assertTrue(sess.run([size, isize]) == [0, 0])\n  def testPartialIndexGets(self):\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.float32)\n        f = array_ops.placeholder(dtypes.float32)\n        v = array_ops.placeholder(dtypes.float32)\n        pi = array_ops.placeholder(dtypes.int64)\n        pei = array_ops.placeholder(dtypes.int64)\n        gi = array_ops.placeholder(dtypes.int64)\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea(\n            [dtypes.float32, dtypes.float32, dtypes.float32])\n        stage_xvf = stager.put(pi, [x, v, f], [0, 1, 2])\n        key_xf, get_xf = stager.get(gi, [0, 2])\n        key_v, get_v = stager.get(gi, [1])\n        size = stager.size()\n        isize = stager.incomplete_size()\n    G.finalize()\n    with self.session(graph=G) as sess:\n      sess.run(stage_xvf, feed_dict={pi: 0, x: 1, f: 2, v: 3})\n      self.assertTrue(sess.run([size, isize]) == [1, 0])\n      self.assertTrue(\n          sess.run([key_xf, get_xf], feed_dict={\n              gi: 0\n          }) == [0, [1, 2]])\n      self.assertTrue(sess.run([size, isize]) == [1, 0])\n      self.assertTrue(sess.run([key_v, get_v], feed_dict={gi: 0}) == [0, [3]])\n      self.assertTrue(sess.run([size, isize]) == [0, 0])\nif __name__ == '__main__':\n  test.main()",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-21734",
        "description": "[{'lang': 'en', 'value': 'Tensorflow is an Open Source Machine Learning Framework. The implementation of `MapStage` is vulnerable a `CHECK`-fail if the key tensor is not a scalar. The fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.'}]",
        "cwe_number": 843
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-186",
      "code": "def get_parser():\n    parser = configargparse.ArgumentParser(\n        prog='rdiffweb',\n        description='Web interface to browse and restore rdiff-backup repositories.',\n        default_config_files=['/etc/rdiffweb/rdw.conf', '/etc/rdiffweb/rdw.conf.d/*.conf'],\n        add_env_var_help=True,\n        auto_env_var_prefix='RDIFFWEB_',\n        config_file_parser_class=ConfigFileParser,\n        conflict_handler='resolve',\n    )\n    parser.add_argument(\n        '-f', '--config', is_config_file=True, metavar='FILE', help='location of Rdiffweb configuration file'\n    )\n    parser.add(\n        '--database-uri',\n        '--sqlitedb-file',\n        '--sqlitedbfile',\n        metavar='URI',\n        help=\"\"\"Location of the database used for persistence. SQLite and PostgreSQL\n            database are supported officially. To use a SQLite database you may\n            define the location using a file path or a URI.\n            e.g.: /srv/rdiffweb/file.db or sqlite:///srv/rdiffweb/file.db`.\n            To use PostgreSQL server you must provide\n            a URI similar to postgresql://user:pass@10.255.1.34/dbname and you\n            must install required dependencies.\n            By default, Rdiffweb uses a SQLite embedded database located at\n            /etc/rdiffweb/rdw.db.\"\"\",\n        default='/etc/rdiffweb/rdw.db',\n    )\n    parser.add_argument(\n        '-d',\n        '--debug',\n        action='store_true',\n        help='enable rdiffweb debug mode - change the log level to DEBUG, print exception stack trace to the web interface and show SQL query in logs',\n    )\n    parser.add_argument(\n        '--admin-user',\n        '--adminuser',\n        metavar='USERNAME',\n        help='administrator username. The administrator user get created on startup if the database is empty.',\n        default='admin',\n    )\n    parser.add_argument(\n        '--admin-password',\n        metavar='USERNAME',\n        help=\"\"\"administrator encrypted password as SSHA. Read online\n            documentation to know more about how to encrypt your password\n            into SSHA or use http://projects.marsching.org/weave4j/util/genpassword.php\n            When defined, administrator password cannot be updated using the web interface.\n            When undefined, default administrator password is `admin123` and\n            it can be updated using the web interface.\"\"\",\n    )\n    parser.add_argument(\n        '--default-theme',\n        '--defaulttheme',\n        help='define the default theme. Either: default, blue or orange. Define the CSS file to be loaded in the web interface. You may manually edit a CSS file to customize it. The location is similar to `/usr/local/lib/python3.9/dist-packages/rdiffweb/static/`',\n        choices=['default', 'blue', 'orange'],\n        default='default',\n    )\n    parser.add_argument(\n        '--environment',\n        choices=['development', 'production'],\n        help='define the type of environment: development, production. This is used to limit the information shown to the user when an error occur.',\n        default='production',\n    )\n    parser.add_argument(\n        '--email-encryption',\n        '--emailencryption',\n        choices=['none', 'ssl', 'starttls'],\n        help='type of encryption to be used when establishing communication with SMTP server. Default: none',\n        default='none',\n    )\n    parser.add_argument(\n        '--email-host',\n        '--emailhost',\n        metavar='HOST',\n        help='SMTP server used to send email in the form <host>:<port>. If the port is not provided, default to standard port 25 or 465 is used. e.g.: smtp.gmail.com:587',\n    )\n    parser.add_argument(\n        '--email-sender',\n        '--emailsender',\n        metavar='EMAIL',\n        help='email addres used for the `from:` field when sending email.',\n    )\n    parser.add_argument(\n        '--email-notification-time',\n        '--emailnotificationtime',\n        metavar='TIME',\n        help='time when the email notifcation should be sent for inactive backups. e.g.: 22:00 Default value: 23:00',\n        default='23:00',\n    )\n    parser.add_argument(\n        '--email-username',\n        '--emailusername',\n        metavar='USERNAME',\n        help='username used for authentication with the SMTP server.',\n    )\n    parser.add_argument(\n        '--email-password',\n        '--emailpassword',\n        metavar='PASSWORD',\n        help='password used for authentication with the SMTP server.',\n    )\n    parser.add_argument(\n        '--email-send-changed-notification',\n        '--emailsendchangednotification',\n        help='True to send notification when sensitive information get change in user profile.',\n        action='store_true',\n        default=False,\n    )\n    parser.add_argument(\n        '--favicon',\n        help='location of an icon to be used as a favicon displayed in web browser.',\n        default=pkg_resources.resource_filename('rdiffweb', 'static/favicon.ico'),\n    )\n    parser.add_argument(\n        '--footer-name', '--footername', help=argparse.SUPPRESS, default='rdiffweb'\n    )\n    parser.add_argument(\n        '--footer-url', '--footerurl', help=argparse.SUPPRESS, default='https://rdiffweb.org/'\n    )\n    parser.add_argument(\n        '--header-logo',\n        '--headerlogo',\n        help='location of an image (preferably a .png) to be used as a replacement for the rdiffweb logo.',\n    )\n    parser.add_argument(\n        '--header-name',\n        '--headername',\n        help='application name displayed in the title bar and header menu.',\n        default='rdiffweb',\n    )\n    parser.add_argument(\n        '--ldap-add-missing-user',\n        '--addmissinguser',\n        action='store_true',\n        help='enable creation of users from LDAP when the credential are valid.',\n        default=False,\n    )\n    parser.add_argument(\n        '--ldap-add-user-default-role',\n        help='default role used when creating users from LDAP. This parameter is only useful when `--ldap-add-missing-user` is enabled.',\n        default='user',\n        choices=['admin', 'maintainer', 'user'],\n    )\n    parser.add_argument(\n        '--ldap-add-user-default-userroot',\n        help='default user root directory used when creating users from LDAP. LDAP attributes may be used to define the default location. e.g.: `/backups/{uid[0]}/`. This parameter is only useful when `--ldap-add-missing-user` is enabled.',\n        default='',\n    )\n    parser.add_argument(\n        '--ldap-uri',\n        '--ldapuri',\n        help='URL to the LDAP server used to validate user credentials. e.g.: ldap://localhost:389',\n    )\n    parser.add_argument(\n        '--ldap-base-dn',\n        '--ldapbasedn',\n        metavar='DN',\n        help='DN of the branch of the directory where all searches should start from. e.g.: dc=my,dc=domain',\n        default=\"\",\n    )\n    parser.add_argument(\n        '--ldap-scope',\n        '--ldapscope',\n        help='scope of the search. Can be either base, onelevel or subtree',\n        choices=['base', 'onelevel', 'subtree'],\n        default=\"subtree\",\n    )\n    parser.add_argument('--ldap-tls', '--ldaptls', action='store_true', help='enable TLS')\n    parser.add_argument(\n        '--ldap-username-attribute',\n        '--ldapattribute',\n        metavar='ATTRIBUTE',\n        help=\"The attribute to search username. If no attributes are provided, the default is to use `uid`. It's a good idea to choose an attribute that will be unique across all entries in the subtree you will be using.\",\n        default='uid',\n    )\n    parser.add_argument(\n        '--ldap-filter',\n        '--ldapfilter',\n        help=\"search filter to limit LDAP lookup. If not provided, defaults to (objectClass=*), which searches for all objects in the tree.\",\n        default='(objectClass=*)',\n    )\n    parser.add_argument(\n        '--ldap-required-group',\n        '--ldaprequiredgroup',\n        metavar='GROUPNAME',\n        help=\"name of the group of which the user must be a member to access rdiffweb. Should be used with ldap-group-attribute and ldap-group-attribute-is-dn.\",\n    )\n    parser.add_argument(\n        '--ldap-group-attribute',\n        '--ldapgroupattribute',\n        metavar='ATTRIBUTE',\n        help=\"name of the attribute defining the groups of which the user is a member. Should be used with ldap-required-group and ldap-group-attribute-is-dn.\",\n        default='member',\n    )\n    parser.add_argument(\n        '--ldap-group-attribute-is-dn',\n        '--ldapgroupattributeisdn',\n        help=\"True if the content of the attribute `ldap-group-attribute` is a DN.\",\n        action='store_true',\n    )\n    parser.add_argument(\n        '--ldap-bind-dn',\n        '--ldapbinddn',\n        metavar='DN',\n        help=\"optional DN used to bind to the server when searching for entries. If not provided, will use an anonymous bind.\",\n        default=\"\",\n    )\n    parser.add_argument(\n        '--ldap-bind-password',\n        '--ldapbindpassword',\n        metavar='PASSWORD',\n        help=\"password to use in conjunction with LdapBindDn. Note that the bind password is probably sensitive data, and should be properly protected. You should only use the LdapBindDn and LdapBindPassword if you absolutely need them to search the directory.\",\n        default=\"\",\n    )\n    parser.add_argument(\n        '--ldap-version',\n        '--ldapversion',\n        '--ldapprotocolversion',\n        help=\"version of LDAP in use either 2 or 3. Default to 3.\",\n        default=3,\n        type=int,\n        choices=[2, 3],\n    )\n    parser.add_argument(\n        '--ldap-network-timeout',\n        '--ldapnetworktimeout',\n        metavar='SECONDS',\n        help=\"timeout in seconds value used for LDAP connection\",\n        default=100,\n        type=int,\n    )\n    parser.add_argument(\n        '--ldap-timeout',\n        '--ldaptimeout',\n        metavar='SECONDS',\n        help=\"timeout in seconds value used for LDAP request\",\n        default=300,\n        type=int,\n    )\n    parser.add_argument(\n        '--ldap-encoding',\n        '--ldapencoding',\n        metavar='ENCODING',\n        help=\"encoding used by your LDAP server.\",\n        default=\"utf-8\",\n    )\n    parser.add_argument(\n        '--log-access-file', '--logaccessfile', metavar='FILE', help='location of Rdiffweb log access file.'\n    )\n    parser.add_argument(\n        '--log-file',\n        '--logfile',\n        metavar='FILE',\n        help='location of Rdiffweb log file. Print log to the console if not define in config file.',\n    )\n    parser.add_argument(\n        '--log-level',\n        '--loglevel',\n        help='Define the log level.',\n        choices=['ERROR', 'WARN', 'INFO', 'DEBUG'],\n        default='INFO',\n    )\n    parser.add_argument(\n        '--max-depth',\n        '--maxdepth',\n        metavar='DEPTH',\n        help=\"define the maximum folder depthness to search into the user's root directory to find repositories. This is commonly used if you repositories are organised with multiple sub-folder.\",\n        type=int,\n        default=3,\n    )\n    parser.add('--quota-set-cmd', '--quotasetcmd', metavar='COMMAND', help=\"command line to set the user's quota.\")\n    parser.add('--quota-get-cmd', '--quotagetcmd', metavar='COMMAND', help=\"command line to get the user's quota.\")\n    parser.add(\n        '--quota-used-cmd', '--quotausedcmd', metavar='COMMAND', help=\"Command line to get user's quota disk usage.\"\n    )\n    parser.add(\n        '--remove-older-time',\n        '--removeoldertime',\n        metavar='TIME',\n        help=\"Time when to execute the remove older scheduled job. e.g.: 22:30\",\n        default='23:00',\n    )\n    parser.add('--server-host', '--serverhost', metavar='IP', default='127.0.0.1', help='IP address to listen to')\n    parser.add(\n        '--server-port',\n        '--serverport',\n        metavar='PORT',\n        help='port to listen to for HTTP request',\n        default='8080',\n        type=int,\n    )\n    parser.add(\n        '--session-dir',\n        '--sessiondir',\n        metavar='FOLDER',\n        help='location where to store user session information. When undefined, the user sessions are kept in memory.',\n    )\n    parser.add(\n        '--rate-limit',\n        metavar='LIMIT',\n        type=int,\n        default=10,\n        help='maximum number of requests per minute that can be made by an IP address for an unauthenticated connection. When this limit is reached, an HTTP 429 message is returned to the user. This security measure is used to limit brute force attacks on the login page and the RESTful API.',\n    )\n    parser.add(\n        '--ssl-certificate',\n        '--sslcertificate',\n        metavar='CERT',\n        help='location of the SSL Certification to enable HTTPS (not recommended)',\n    )\n    parser.add(\n        '--ssl-private-key',\n        '--sslprivatekey',\n        metavar='KEY',\n        help='location of the SSL Private Key to enable HTTPS (not recommended)',\n    )\n    parser.add(\n        '--tempdir',\n        metavar='FOLDER',\n        help='alternate temporary folder to be used when restoring files. Might be useful if the default location has limited disk space. Default to TEMPDIR environment or `/tmp`.',\n    )\n    parser.add(\n        '--disable-ssh-keys',\n        action='store_true',\n        help='used to hide SSH Key management to avoid users to add or remove SSH Key using the web application',\n        default=False,\n    )\n    parser.add_argument('--version', action='version', version='%(prog)s ' + VERSION)\n    flags = ['--welcome-msg'] + ['--welcome-msg-' + i for i in ['ca', 'en', 'es', 'fr', 'ru']] + ['--welcomemsg']\n    parser.add_argument(\n        *flags,\n        metavar='HTML',\n        help='replace the welcome message displayed in the login page for default locale or for a specific locale',\n        action=LocaleAction\n    )\n    return parser\ndef parse_args(args=None, config_file_contents=None):\n    args = sys.argv[1:] if args is None else args\n    return get_parser().parse_args(args, config_file_contents=config_file_contents)\nclass LocaleAction(argparse.Action):\n    def __init__(self, option_strings, dest, nargs=None, **kwargs):\n        super(LocaleAction, self).__init__(option_strings, dest, **kwargs)\n    def set_password(self, password, old_password=None):\n        \"\"\"\n        Change the user's password. Raise a ValueError if the username or\n        the password are invalid.\n        \"\"\"\n        assert isinstance(password, str)\n        assert old_password is None or isinstance(old_password, str)\n        if not password:\n            raise ValueError(\"password can't be empty\")\n        if self.username == self._store._admin_user and self._store._admin_password:\n            raise ValueError(_(\"can't update admin-password defined in configuration file\"))\n        if old_password and not check_password(old_password, self.hash_password):\n            raise ValueError(_(\"Wrong password\"))\n        logger.info(\"updating user password [%s]\", self.username)\n        self.hash_password = hash_password(password)\n        self._store.bus.publish('user_password_changed', self)\n    def _set_user_root(self, value):\n        \"\"\"\n        Used to take care of updating the user_root.\n        When user_root get update, we also want to update the repository list\n        to reflect the filesystem.\n        \"\"\"\n        self._set_attr('user_root', value)\n        self.refresh_repos()\n    def validate_role(self, field):\n        currentuser = cherrypy.request.currentuser\n        if self.username.data == currentuser.username and self.role.data != currentuser.role:\n            raise ValueError(_('Cannot edit your own role.'))\n    def populate_obj(self, userobj):\n        if self.password.data:\n            userobj.set_password(self.password.data, old_password=None)\n        userobj.role = self.role.data\n        userobj.email = self.email.data or ''\n        userobj.user_root = self.user_root.data\n        if not userobj.valid_user_root():\n            flash(_(\"User's root directory %s is not accessible!\") % userobj.user_root, level='error')\n            logger.warning(\"user's root directory %s is not accessible\" % userobj.user_root)\n        new_quota = self.disk_quota.data or 0\n        old_quota = humanfriendly.parse_size(humanfriendly.format_size(self.disk_quota.object_data or 0, binary=True))\n        if old_quota != new_quota:\n            userobj.disk_quota = new_quota\n            if userobj.disk_quota != new_quota:\n                flash(_(\"Setting user's quota is not supported\"), level='warning')\nclass PrefsGeneralPanelProvider(Controller):\n    panel_id = 'general'\n    panel_name = _('Profile')\n    def _handle_set_password(self, action, form):\n        \"\"\"\n        Called when changing user password.\n        \"\"\"\n        assert self.app.currentuser\n        assert action == 'set_password'\n        assert form\n        if not form.validate():\n            flash(form.error_message, level='error')\n            return\n        try:\n            self.app.currentuser.set_password(form.new.data, old_password=form.current.data)\n            flash(_(\"Password updated successfully.\"), level='success')\n        except ValueError as e:\n            flash(str(e), level='warning')",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-3175",
        "description": "[{'lang': 'en', 'value': 'Missing Custom Error Page in GitHub repository ikus060/rdiffweb prior to 2.4.2.'}]",
        "cwe_number": 755
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-187",
      "code": "def send_nscript(title, msg, gtype, force=False, test=None):\n    \"\"\"Run user's notification script\"\"\"\n    if test:\n        script = test.get(\"nscript_script\")\n        nscript_parameters = test.get(\"nscript_parameters\")\n    else:\n        script = sabnzbd.cfg.nscript_script()\n        nscript_parameters = sabnzbd.cfg.nscript_parameters()\n    nscript_parameters = nscript_parameters.split()\n    if not script:\n        return T(\"Cannot send, missing required data\")\n    title = \"SABnzbd: \" + T(NOTIFICATION.get(gtype, \"other\"))\n    if force or check_classes(gtype, \"nscript\"):\n        script_path = make_script_path(script)\n        if script_path:\n            ret = -1\n            output = None\n            try:\n                p = build_and_run_command([script_path, gtype, title, msg] + nscript_parameters, env=create_env())\n                output = p.stdout.read()\n                ret = p.wait()\n            except:\n                logging.info(\"Failed script %s, Traceback: \", script, exc_info=True)\n            if ret:\n                logging.error(T('Script returned exit code %s and output \"%s\"'), ret, output)\n                return T('Script returned exit code %s and output \"%s\"') % (ret, output)\n            else:\n                logging.info(\"Successfully executed notification script %s\", script_path)\n        else:\n            return T('Notification script \"%s\" does not exist') % script_path\n    return \"\"",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-34237",
        "description": "[{'lang': 'en', 'value': 'SABnzbd is an open source automated Usenet download tool. A design flaw was discovered in SABnzbd that could allow remote code execution. Manipulating the Parameters setting in the Notification Script functionality allows code execution with the privileges of the SABnzbd process. Exploiting the vulnerabilities requires access to the web interface. Remote exploitation is possible if users[exposed their setup to the internet or other untrusted networks without setting a username/password. By default SABnzbd is only accessible from `localhost`, with no authentication required for the web interface. This issue has been patched in commits `e3a722` and `422b4f` which have been included in the 4.0.2 release. Users are advised to upgrade. Users unable to upgrade should ensure that a username and password have been set if their instance is web accessible.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-188",
      "code": "def _read_from_sections(user, collection_url, permission):\n    \"\"\"Get regex sections.\"\"\"\n    filename = os.path.expanduser(config.get(\"rights\", \"file\"))\n    rights_type = config.get(\"rights\", \"type\").lower()\n    regex = ConfigParser({\"login\": user, \"path\": collection_url})\n    if rights_type in DEFINED_RIGHTS:\n        log.LOGGER.debug(\"Rights type '%s'\" % rights_type)\n        regex.readfp(StringIO(DEFINED_RIGHTS[rights_type]))\n    elif rights_type == \"from_file\":\n        log.LOGGER.debug(\"Reading rights from file %s\" % filename)\n        if not regex.read(filename):\n            log.LOGGER.error(\"File '%s' not found for rights\" % filename)\n            return False\n    else:\n        log.LOGGER.error(\"Unknown rights type '%s'\" % rights_type)\n        return False\n    for section in regex.sections():\n        re_user = regex.get(section, \"user\")\n        re_collection = regex.get(section, \"collection\")\n        log.LOGGER.debug(\n            \"Test if '%s:%s' matches against '%s:%s' from section '%s'\" % (\n                user, collection_url, re_user, re_collection, section))\n        user_match = re.match(re_user, user)\n        if user_match:\n            re_collection = re_collection.format(*user_match.groups())\n            if re.match(re_collection, collection_url):\n                log.LOGGER.debug(\"Section '%s' matches\" % section)\n                if permission in regex.get(section, \"permission\"):\n                    return True\n            else:\n                log.LOGGER.debug(\"Section '%s' does not match\" % section)\n    return False",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2015-8748",
        "description": "[{'lang': 'en', 'value': 'Radicale before 1.1 allows remote authenticated users to bypass owner_write and owner_only limitations via regex metacharacters in the user name, as demonstrated by \".*\".'}]",
        "cwe_number": 264
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-189",
      "code": "try:\n  import yaml\nexcept ImportError:\n  yaml = None\ndef model_from_config(config, custom_objects=None):\n  \"\"\"Instantiates a Keras model from its config.\n  Usage:\n  ```\n  tf.keras.Model().from_config(model.get_config())\n  tf.keras.Sequential().from_config(model.get_config())\n  ```\n  Args:\n      config: Configuration dictionary.\n      custom_objects: Optional dictionary mapping names\n          (strings) to custom classes or functions to be\n          considered during deserialization.\n  Returns:\n      A Keras model instance (uncompiled).\n  Raises:\n      TypeError: if `config` is not a dictionary.\n  \"\"\"\n  if isinstance(config, list):\n    raise TypeError('`model_from_config` expects a dictionary, not a list. '\n                    'Maybe you meant to use '\n                    '`Sequential.from_config(config)`?')\n  from tensorflow.python.keras.layers import deserialize\n  return deserialize(config, custom_objects=custom_objects)\ndef model_from_yaml(yaml_string, custom_objects=None):\n  \"\"\"Parses a yaml model configuration file and returns a model instance.\n  Usage:\n  >>> model = tf.keras.Sequential([\n  ...     tf.keras.layers.Dense(5, input_shape=(3,)),\n  ...     tf.keras.layers.Softmax()])\n  >>> try:\n  ...   import yaml\n  ...   config = model.to_yaml()\n  ...   loaded_model = tf.keras.models.model_from_yaml(config)\n  ... except ImportError:\n  ...   pass\n  Args:\n      yaml_string: YAML string or open file encoding a model configuration.\n      custom_objects: Optional dictionary mapping names\n          (strings) to custom classes or functions to be\n          considered during deserialization.\n  Returns:\n      A Keras model instance (uncompiled).\n  Raises:\n      ImportError: if yaml module is not found.\n  \"\"\"\n  if yaml is None:\n    raise ImportError('Requires yaml module installed (`pip install pyyaml`).')\n  try:\n    config = yaml.unsafe_load(yaml_string)\n  except AttributeError:\n    config = yaml.load(yaml_string)\n  from tensorflow.python.keras.layers import deserialize\n  return deserialize(config, custom_objects=custom_objects)\n  def to_json(self, **kwargs):\n    \"\"\"Returns a JSON string containing the network configuration.\n    To load a network from a JSON save file, use\n    `keras.models.model_from_json(json_string, custom_objects={})`.\n    Args:\n        **kwargs: Additional keyword arguments\n            to be passed to `json.dumps()`.\n    Returns:\n        A JSON string.\n    \"\"\"\n    model_config = self._updated_config()\n    return json.dumps(\n        model_config, default=json_utils.get_json_type, **kwargs)\n  def to_yaml(self, **kwargs):\n    \"\"\"Returns a yaml string containing the network configuration.\n    To load a network from a yaml save file, use\n    `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n    `custom_objects` should be a dictionary mapping\n    the names of custom losses / layers / etc to the corresponding\n    functions / classes.\n    Args:\n        **kwargs: Additional keyword arguments\n            to be passed to `yaml.dump()`.\n    Returns:\n        A YAML string.\n    Raises:\n        ImportError: if yaml module is not found.\n    \"\"\"\n    if yaml is None:\n      raise ImportError(\n          'Requires yaml module installed (`pip install pyyaml`).')\n    return yaml.dump(self._updated_config(), **kwargs)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-37678",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an end-to-end open source platform for machine learning. In affected versions TensorFlow and Keras can be tricked to perform arbitrary code execution when deserializing a Keras model from YAML format. The [implementation](https://github.com/tensorflow/tensorflow/blob/460e000de3a83278fb00b61a16d161b1964f15f4/tensorflow/python/keras/saving/model_config.py#L66-L104) uses `yaml.unsafe_load` which can perform arbitrary code execution on the input. Given that YAML format support requires a significant amount of work, we have removed it for now. We have patched the issue in GitHub commit 23d6383eb6c14084a8fc3bdf164043b974818012. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.'}]",
        "cwe_number": 502
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-190",
      "code": "def register_views(app: Flask) -> None:\n    @app.after_request\n    def add_header(r: Response) -> Response:\n        \"\"\"\n        Disable cache for all auth server requests\n        \"\"\"\n        r.headers[\"Cache-Control\"] = \"no-cache, no-store, must-revalidate\"\n        r.headers[\"Pragma\"] = \"no-cache\"\n        r.headers[\"Expires\"] = \"0\"\n        r.headers[\"Cache-Control\"] = \"public, max-age=0\"\n        return r\n    @app.route(\"/login/server-config\", methods=[\"GET\"])\n    def server_config() -> Response:\n        return jsonify(\n            {\n                \"CLOUD\": app.config.get(\"CLOUD\"),\n                \"CLOUD_URL\": app.config.get(\"CLOUD_URL\"),\n                \"GITHUB_URL\": app.config.get(\"GITHUB_URL\"),\n                \"DOCUMENTATION_URL\": app.config.get(\"DOCUMENTATION_URL\"),\n                \"VIDEOS_URL\": app.config.get(\"VIDEOS_URL\"),\n            }\n        )\n    def is_authenticated(request: Request) -> bool:\n        if not app.config[\"AUTH_ENABLED\"]:\n            return True\n        cookie_token = request.cookies.get(\"auth_token\")\n        username = request.cookies.get(\"auth_username\")\n        token_creation_limit = datetime.datetime.utcnow() - datetime.timedelta(\n            hours=app.config[\"TOKEN_DURATION_HOURS\"]\n        )\n        return db.session.query(\n            db.session.query(Token)\n            .join(User)\n            .filter(\n                Token.token == cookie_token,\n                User.username == username,\n                Token.created > token_creation_limit,\n            )\n            .exists()\n        ).scalar()\n    def serve_static_or_dev(path: PathType) -> Response:\n        file_path = os.path.join(app.config[\"STATIC_DIR\"], path)\n        if os.path.isfile(file_path):\n            return send_from_directory(app.config[\"STATIC_DIR\"], path)\n        else:\n            return send_from_directory(app.config[\"STATIC_DIR\"], \"index.html\")\n    @app.route(\"/login\", defaults={\"path\": \"\"}, methods=[\"GET\"])\n    @app.route(\"/login/<path:path>\", methods=[\"GET\"])\n    def login_static(path: PathType) -> Response:\n        if is_authenticated(request) and path == \"\":\n            return handle_login(redirect_type=\"server\")\n        return serve_static_or_dev(path)\n    @app.route(\"/login/admin\", methods=[\"GET\"])\n    def login_admin() -> Tuple[str, int] | Response:\n        if not is_authenticated(request):\n            return \"\", 401\n        return serve_static_or_dev(\"/admin\")\n    @app.route(\"/auth\", methods=[\"GET\"])\n    def index() -> Tuple[Literal[\"\"], Literal[200]] | Tuple[Literal[\"\"], Literal[401]]:\n        if is_authenticated(request):\n            return \"\", 200\n        else:\n            return \"\", 401\n    @app.route(\"/login/clear\", methods=[\"GET\"])\n    def logout() -> Response | None:\n        resp = redirect_response(\"/\")\n        resp.set_cookie(\"auth_token\", \"\")\n        resp.set_cookie(\"auth_username\", \"\")\n        return resp\n    def redirect_response(url: str, redirect_type: str = \"server\") -> Response:\n        if redirect_type == \"client\":\n            return jsonify({\"redirect\": url})\n        elif redirect_type == \"server\":\n            return redirect(url)\n    @app.route(\"/login/submit\", methods=[\"POST\"])\n    def login() -> Response | Tuple[Response, Literal[401]] | None:\n        return handle_login()\n    @app.route(\"/login\", methods=[\"POST\"])\n    def login_post() -> Response | Tuple[Response, Literal[401]] | None:\n        return handle_login(redirect_type=\"server\")\n    def handle_login(\n        redirect_type: str = \"client\",\n    ) -> Response | Tuple[Response, Literal[401]] | None:\n        request_args = request.args.copy()\n        redirect_url = request_args.pop(\"redirect_url\", \"/\")\n        query_args = \"&\".join(\n            [arg + \"=\" + value for arg, value in request_args.items()]\n        )\n        if query_args:\n            redirect_url += \"?\" + query_args\n        if is_authenticated(request):\n            return redirect_response(redirect_url, redirect_type)\n        if request.method == \"POST\":\n            token_creation_limit = datetime.datetime.utcnow() - datetime.timedelta(\n                hours=app.config[\"TOKEN_DURATION_HOURS\"]\n            )\n            Token.query.filter(Token.created < token_creation_limit).delete()\n            username = request.form.get(\"username\")\n            password = request.form.get(\"password\")\n            token = request.form.get(\"token\")\n            user = User.query.filter(User.username == username).first()\n            invalid_login_msg = \"Username password combination does not exist.\"\n            if user is None:\n                return jsonify({\"error\": invalid_login_msg}), 401\n            else:\n                if password is not None:\n                    can_login = check_password_hash(user.password_hash, password)\n                elif token is not None and user.token_hash is not None:\n                    can_login = check_password_hash(user.token_hash, token)\n                else:\n                    can_login = False\n                if can_login:\n                    token = Token(user=user.uuid, token=str(secrets.token_hex(16)))\n                    db.session.add(token)\n                    db.session.commit()\n                    resp = redirect_response(redirect_url, redirect_type)\n                    resp.set_cookie(\"auth_token\", token.token)\n                    resp.set_cookie(\"auth_username\", username)\n                    return resp\n                else:\n                    return jsonify({\"error\": invalid_login_msg}), 401\n    @app.route(\"/login/users\", methods=[\"DELETE\"])\n    def delete_user() -> Union[\n        Tuple[Literal[\"\"], Literal[401]],\n        Tuple[Response, Literal[500]],\n        Tuple[Response, Literal[405]],\n        Literal[\"\"],\n    ]:\n        if not is_authenticated(request):\n            return \"\", 401\n        self_username = request.cookies.get(\"auth_username\")\n        if \"username\" in request.form:\n            to_delete_username = request.form.get(\"username\")\n            user = User.query.filter(User.username == to_delete_username).first()\n            if user is not None:\n                if user.is_admin:\n                    return jsonify({\"error\": \"Admins cannot be deleted.\"}), 500\n                elif self_username == to_delete_username:\n                    return jsonify({\"error\": \"Deleting own user is not allowed.\"}), 405\n                else:\n                    db.session.delete(user)\n                    db.session.commit()\n                    return \"\"\n            else:\n                return jsonify({\"error\": \"User does not exist.\"}), 500\n        else:\n            return jsonify({\"error\": \"No username supplied.\"}), 500\n    @app.route(\"/login/users\", methods=[\"POST\"])\n    def add_user() -> Union[\n        Tuple[Literal[\"\"], Literal[401]],\n        Tuple[Response, Literal[409]],\n        Tuple[Response, Literal[400]],\n        Literal[\"\"],\n    ]:\n        if not is_authenticated(request):\n            return \"\", 401\n        if \"username\" in request.form:\n            username = request.form.get(\"username\")\n            password = request.form.get(\"password\")\n            if username == app.config.get(\"ORCHEST_CLOUD_RESERVED_USER\"):\n                return jsonify({\"error\": \"User is reserved.\"}), 409\n            if len(password) == 0:\n                return jsonify({\"error\": \"Password cannot be empty.\"}), 400\n            user = User.query.filter(User.username == username).first()\n            if user is not None:\n                return jsonify({\"error\": \"User already exists.\"}), 409\n            user = User(\n                username=username,\n                password_hash=generate_password_hash(password),\n                uuid=str(uuid.uuid4()),\n            )\n            db.session.add(user)\n            db.session.commit()\n            return \"\"\n        else:\n            return jsonify({\"error\": \"No username supplied.\"}), 400\n    @app.route(\"/login/users\", methods=[\"GET\"])\n    def get_users() -> Tuple[Literal[\"\"], Literal[401]] | Tuple[Response, Literal[200]]:\n        if not is_authenticated(request):\n            return \"\", 401\n        data_json: Dict[\n            Literal[\"users\"],\n            List[Dict[Literal[\"username\"], str]],\n        ] = {\"users\": []}\n        users = User.query.all()\n        for user in users:\n            if user.username != app.config.get(\"ORCHEST_CLOUD_RESERVED_USER\"):\n                data_json[\"users\"].append({\"username\": user.username})\n        return jsonify(data_json), 200\n    @app.route(\"/auth/service\", methods=[\"GET\"])\n    def auth_service() -> Union[\n        Tuple[Literal[\"\"], Literal[200]],\n        Tuple[Literal[\"\"], Literal[401]],\n    ]:\n        global _auth_cache, _auth_cache_age\n        if is_authenticated(request):\n            return \"\", 200\n        original_uri = request.headers.get(\"X-Original-URI\")\n        if original_uri is None:\n            return \"\", 401\n        try:\n            components = original_uri.split(\"/\")[1].split(\"_\")[-2].split(\"-\")\n            session_uuid_prefix = components[-1]\n            project_uuid_prefix = components[-2]\n        except Exception:\n            app.logger.error(\"Failed to parse X-Original-URI: %s\" % original_uri)\n            return \"\", 401\n        auth_check = get_auth_cache(\n            project_uuid_prefix, session_uuid_prefix, _auth_cache, _auth_cache_age\n        )\n        if auth_check[\"status\"] == \"available\":\n            if auth_check[\"requires_authentication\"] is False:\n                return \"\", 200\n            else:\n                return \"\", 401\n        else:\n            base_url = \"http://%s/api/services/\" % (app.config[\"ORCHEST_API_ADDRESS\"])\n            service_url = \"%s?project_uuid_prefix=%s&session_uuid_prefix=%s\" % (\n                base_url,\n                project_uuid_prefix,\n                session_uuid_prefix,\n            )\n            try:\n                r = requests.get(service_url)\n                services = r.json().get(\"services\", [])\n                if len(services) == 0:\n                    raise Exception(\"No services found\")\n                if len(services) > 1:\n                    raise Exception(\n                        \"Filtered /api/services endpoint \"\n                        \"should always return a single service\"\n                    )\n                if services[0][\"service\"][\"requires_authentication\"] is False:\n                    set_auth_cache(\n                        project_uuid_prefix, session_uuid_prefix, False, _auth_cache\n                    )\n                    return \"\", 200\n                else:\n                    set_auth_cache(\n                        project_uuid_prefix, session_uuid_prefix, True, _auth_cache\n                    )\n                    raise Exception(\"'requires_authentication' is not set to False\")\n            except Exception as e:\n                app.logger.error(e)\n                return \"\", 401",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-39268",
        "description": "[{'lang': 'en', 'value': \"### Impact In a CSRF attack, an innocent end user is tricked by an attacker into submitting a web request that they did not intend. This may cause actions to be performed on the website that can include inadvertent client or server data leakage, change of session state, or manipulation of an end user's account. ### Patch Upgrade to v2022.09.10 to patch this vulnerability. ### Workarounds Rebuild and redeploy the Orchest `auth-server` with this commit: https://github.com/orchest/orchest/commit/c2587a963cca742c4a2503bce4cfb4161bf64c2d ### References https://en.wikipedia.org/wiki/Cross-site_request_forgery https://cwe.mitre.org/data/definitions/352.html ### For more information If you have any questions or comments about this advisory: * Open an issue in https://github.com/orchest/orchest * Email us at rick@orchest.io\"}]",
        "cwe_number": 352
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-191",
      "code": "  def testGroupInitialization(self):\n    group_size = 2\n    group_key = 100\n    @def_function.function\n    def f():\n      with ops.device('CPU:0'):\n        _collective_ops.initialize_communicator(\n            group_key=group_key, rank=0, group_size=group_size)\n      with ops.device('CPU:1'):\n        _collective_ops.initialize_communicator(\n            group_key=group_key, rank=1, group_size=group_size)\n    self.evaluate(f())\n  def testAllReduceV3(self, device, communication):\n    group_size = 2\n    group_key = 101\n    dev0 = '/device:%s:0' % device\n    dev1 = '/device:%s:1' % device\n    @def_function.function\n    def run_all_reduce_2devices():\n      collectives = []\n      with ops.device(dev0):\n        group_handle0 = _collective_ops.initialize_communicator(\n            group_key=group_key,\n            rank=0,\n            group_size=group_size,\n            communication_hint=communication)\n        collectives.append(\n            _collective_ops.all_reduce_v3(\n                group_handle0, [1.0], reduction='Add'))\n      with ops.device(dev1):\n        group_handle1 = _collective_ops.initialize_communicator(\n            group_key=group_key,\n            rank=1,\n            group_size=group_size,\n            communication_hint=communication)\n        collectives.append(\n            _collective_ops.all_reduce_v3(\n                group_handle1, [2.0], reduction='Add'))\n      return collectives\n    for result in run_all_reduce_2devices():\n      self.assertAllClose(result, [3.], rtol=1e-5, atol=1e-5)\n  def testAllToAllV3(self, device, communication):\n    group_size = 2\n    group_key = 104\n    dev0 = '/device:%s:0' % device\n    dev1 = '/device:%s:1' % device\n    @def_function.function\n    def run_all_to_all_2devices():\n      collectives = []\n      with ops.device(dev0):\n        group_handle0 = _collective_ops.initialize_communicator(\n            group_key=group_key,\n            rank=0,\n            group_size=group_size,\n            communication_hint=communication)\n        collectives.append(\n            _collective_ops.all_to_all_v3(group_handle0, [1.0, 3.0]))\n      with ops.device(dev1):\n        group_handle1 = _collective_ops.initialize_communicator(\n            group_key=group_key,\n            rank=1,\n            group_size=group_size,\n            communication_hint=communication)\n        collectives.append(\n            _collective_ops.all_to_all_v3(group_handle1, [2.0, 4.0]))\n      return collectives\n    result = run_all_to_all_2devices()\n    self.assertAllClose(result[0], [1.0, 2.0], rtol=1e-5, atol=1e-5)\n    self.assertAllClose(result[1], [3.0, 4.0], rtol=1e-5, atol=1e-5)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-41220",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. In affected versions the async implementation of `CollectiveReduceV2` suffers from a memory leak and a use after free. This occurs due to the asynchronous computation and the fact that objects that have been `std::move()`d from are still accessed. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, as this version is the only one that is also affected.'}]",
        "cwe_number": 416
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-192",
      "code": "    def __init__(self):\n        self.reqparse = reqparse.RequestParser()\n        super(AuthenticatedService, self).__init__()\n        self.auth_dict = dict()\n        if current_user.is_authenticated():\n            roles_marshal = []\n            for role in current_user.roles:\n                roles_marshal.append(marshal(role.__dict__, ROLE_FIELDS))\n            roles_marshal.append({\"name\": current_user.role})\n            for role in RBACRole.roles[current_user.role].get_parents():\n                roles_marshal.append({\"name\": role.name})\n            self.auth_dict = {\n                \"authenticated\": True,\n                \"user\": current_user.email,\n                \"roles\": roles_marshal\n            }\n        else:\n            if app.config.get('FRONTED_BY_NGINX'):\n                url = \"https://{}:{}{}\".format(app.config.get('FQDN'), app.config.get('NGINX_PORT'), '/login')\n            else:\n                url = \"http://{}:{}{}\".format(app.config.get('FQDN'), app.config.get('API_PORT'), '/login')\n            self.auth_dict = {\n                \"authenticated\": False,\n                \"user\": None,\n                \"url\": url\n            }\n    def _deny_hook(self, resource=None):\n        app = self.get_app()\n        if current_user.is_authenticated():\n            status = 403\n        else:\n            status = 401\n        if app.config.get('FRONTED_BY_NGINX'):\n                url = \"https://{}:{}{}\".format(app.config.get('FQDN'), app.config.get('NGINX_PORT'), '/login')\n        else:\n                url = \"http://{}:{}{}\".format(app.config.get('FQDN'), app.config.get('API_PORT'), '/login')\n        if current_user.is_authenticated():\n            auth_dict = {\n                \"authenticated\": True,\n                \"user\": current_user.email,\n                \"roles\": current_user.role,\n            }\n        else:\n            auth_dict = {\n                \"authenticated\": False,\n                \"user\": None,\n                \"url\": url\n            }\n        return Response(response=json.dumps({\"auth\": auth_dict}), status=status, mimetype=\"application/json\")\n    def _consumer(self, auth):\n        auth.process_response()\n        errors = auth.get_errors()\n        if not errors:\n            if auth.is_authenticated():\n                return True\n            else:\n                return False\n        else:\n            current_app.logger.error('Error processing %s' % (', '.join(errors)))\n            return False\n    def get(self):\n        if not current_user.is_authenticated():\n            return \"Must be logged in to log out\", 200\n        logout_user()\n        return \"Logged Out\", 200",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2017-7266",
        "description": "[{'lang': 'en', 'value': 'Netflix Security Monkey before 0.8.0 has an Open Redirect. The logout functionality accepted the \"next\" parameter which then redirects to any domain irrespective of the Host header.'}]",
        "cwe_number": 601
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-193",
      "code": "    def __init__(self, pidfile=None):\n        if not pidfile:\n            self.pidfile = \"/tmp/%s.pid\" % self.__class__.__name__.lower()\n        else:\n            self.pidfile = pidfile\n    def become_daemon(self, root_dir='/'):\n        if os.fork() != 0:\n            os._exit(0)\n        os.setsid()\n        os.chdir(root_dir)\n        os.umask(0)\n        if os.fork() != 0:\n            os._exit(0)\n        sys.stdin.close()\n        sys.__stdin__ = sys.stdin\n        sys.stdout.close()\n        sys.stdout = sys.__stdout__ = _NullDevice()\n        sys.stderr.close()\n        sys.stderr = sys.__stderr__ = _NullDevice()\n        for fd in range(1024):\n            try:\n                os.close(fd)\n            except OSError:\n                pass\n    def process_command_line(self, argv, verbose=1):\n        usage = \"usage:  %s  start | stop | restart | status | debug \" \\\n                \"(run as non-daemon)\" % os.path.basename(argv[0])\n        if len(argv) < 2:\n            print usage\n            raise SystemExit\n        else:\n            operation = argv[1]\n        pid = self.get_pid()\n        if operation == 'status':\n            if self.is_process_running():\n                print \"Server process %s is running.\" % pid\n            else:\n                print \"Server is not running.\"\n        elif operation == 'start':\n            if self.is_process_running():\n                print \"Server process %s is already running.\" % pid\n                raise SystemExit\n            else:\n                if verbose:\n                    print \"Starting server process.\"\n                self.daemon_start()\n        elif operation == 'stop':\n            if self.is_process_running():\n                self.daemon_stop()\n                if verbose:\n                    print \"Server process %s stopped.\" % pid\n            else:\n                print \"Server process %s is not running.\" % pid\n                raise SystemExit\n        elif operation == 'restart':\n            self.daemon_stop()\n            if verbose:\n                print \"Restarting server process.\"\n            self.daemon_start()\n        elif operation == 'debug':\n            self.daemon_start(0)\n        else:\n            print \"Unknown operation:\", operation\n            raise SystemExit\nclass _NullDevice:\n    def write(self, s):\n        pass",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2011-2765",
        "description": "[{'lang': 'en', 'value': 'pyro before 3.15 unsafely handles pid files in temporary directory locations and opening the pid file as root. An attacker can use this flaw to overwrite arbitrary files via symlinks.'}]",
        "cwe_number": 59
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-194",
      "code": "def main():\n    global logger\n    result = 0\n    parser = argparse.ArgumentParser(description='Keycloak REST client',\n                    prog=prog_name,\n                    epilog=verbose_help.format(prog_name=prog_name),\n                    formatter_class=argparse.RawDescriptionHelpFormatter)\n    parser.add_argument('-v', '--verbose', action='store_true',\n                        help='be chatty')\n    parser.add_argument('-d', '--debug', action='store_true',\n                        help='turn on debug info')\n    parser.add_argument('--show-traceback', action='store_true',\n                        help='exceptions print traceback in addition to '\n                             'error message')\n    parser.add_argument('--log-file',\n                        default='/tmp/{prog_name}.log'.format(\n                            prog_name=prog_name),\n                        help='log file pathname')\n    parser.add_argument('--permit-insecure-transport',  action='store_true',\n                        help='Normally secure transport such as TLS '\n                        'is required, defeat this check')\n    parser.add_argument('--tls-verify', action=TlsVerifyAction,\n                        default=True,\n                        help='TLS certificate verification for requests to'\n                        ' the server. May be one of case insenstive '\n                        '[true, yes, on] to enable,'\n                        '[false, no, off] to disable.'\n                        'Or the pathname to a OpenSSL CA bundle to use.'\n                        ' Default is True.')\n    group = parser.add_argument_group('Server')\n    group.add_argument('-s', '--server',\n                       required=True,\n                       help='DNS name or IP address of Keycloak server')\n    group.add_argument('-a', '--auth-role',\n                       choices=AUTH_ROLES,\n                       default='root-admin',\n                       help='authenticating as what type of user (default: root-admin)')\n    group.add_argument('-u', '--admin-username',\n                       default='admin',\n                       help='admin user name (default: admin)')\n    group.add_argument('-p', '--admin-password',\n                       required=True,\n                       help='admin password')\n    group.add_argument('--admin-realm',\n                       default='master',\n                       help='realm admin belongs to')\n    cmd_parsers = parser.add_subparsers(help='available commands')\n    realm_parser = cmd_parsers.add_parser('realm',\n                                          help='realm operations')\n    sub_parser = realm_parser.add_subparsers(help='realm commands')\n    cmd_parser = sub_parser.add_parser('server_info',\n                                       help='dump server info')\n    cmd_parser.set_defaults(func=do_server_info)\n    cmd_parser = sub_parser.add_parser('list',\n                                       help='list realm names')\n    cmd_parser.set_defaults(func=do_list_realms)\n    cmd_parser = sub_parser.add_parser('create',\n                                       help='create new realm')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.set_defaults(func=do_create_realm)\n    cmd_parser = sub_parser.add_parser('delete',\n                                       help='delete existing realm')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.set_defaults(func=do_delete_realm)\n    cmd_parser = sub_parser.add_parser('metadata',\n                                       help='retrieve realm metadata')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.set_defaults(func=do_get_realm_metadata)\n    client_parser = cmd_parsers.add_parser('client',\n                                           help='client operations')\n    sub_parser = client_parser.add_subparsers(help='client commands')\n    cmd_parser = sub_parser.add_parser('list',\n                                       help='list client names')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.set_defaults(func=do_list_clients)\n    cmd_parser = sub_parser.add_parser('create',\n                                       help='create new client')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.add_argument('-m', '--metadata', type=argparse.FileType('rb'),\n                            required=True,\n                            help='SP metadata file or stdin')\n    cmd_parser.set_defaults(func=do_create_client)\n    cmd_parser = sub_parser.add_parser('register',\n                                       help='register new client')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.add_argument('-m', '--metadata', type=argparse.FileType('rb'),\n                            required=True,\n                            help='SP metadata file or stdin')\n    cmd_parser.add_argument('--initial-access-token', required=True,\n                            help='realm initial access token for '\n                            'client registeration')\n    cmd_parser.set_defaults(func=do_register_client)\n    cmd_parser = sub_parser.add_parser('delete',\n                                       help='delete existing client')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.add_argument('-c', '--client-name', required=True,\n                            help='client name')\n    cmd_parser.set_defaults(func=do_delete_client)\n    cmd_parser = sub_parser.add_parser('test',\n                                       help='experimental test used during '\n                                       'development')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.add_argument('-c', '--client-name', required=True,\n                            help='client name')\n    cmd_parser.set_defaults(func=do_client_test)\n    options = parser.parse_args()\n    configure_logging(options)\n    if options.permit_insecure_transport:\n        os.environ['OAUTHLIB_INSECURE_TRANSPORT'] = '1'\n    try:\n        anonymous_conn = KeycloakAnonymousConnection(options.server,\n                                                     options.tls_verify)\n        admin_conn = KeycloakAdminConnection(options.server,\n                                             options.auth_role,\n                                             options.admin_realm,\n                                             ADMIN_CLIENT_ID,\n                                             options.admin_username,\n                                             options.admin_password,\n                                             options.tls_verify)\n    except Exception as e:\n        if options.show_traceback:\n            traceback.print_exc()\n        print(six.text_type(e), file=sys.stderr)\n        result = 1\n        return result\n    try:\n        if options.func == do_register_client:\n            conn = admin_conn\n        else:\n            conn = admin_conn\n        result = options.func(options, conn)\n    except Exception as e:\n        if options.show_traceback:\n            traceback.print_exc()\n        print(six.text_type(e), file=sys.stderr)\n        result = 2\n        return result\n    return result",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2017-15112",
        "description": "[{'lang': 'en', 'value': 'keycloak-httpd-client-install versions before 0.8 allow users to insecurely pass password through command line, leaking it via command history and process info to other local users.'}]",
        "cwe_number": 200
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-195",
      "code": "def publish(request, user_id=None):\n    initial = None\n    if user_id:\n        user_to = get_object_or_404(User, pk=user_id)\n        initial = {'users': [user_to.st.nickname]}\n    user = request.user\n    tform = TopicForPrivateForm(\n        user=user, data=post_data(request))\n    cform = CommentForm(\n        user=user, data=post_data(request))\n    tpform = TopicPrivateManyForm(\n        user=user, data=post_data(request), initial=initial)\n    if (is_post(request) and\n            all([tform.is_valid(), cform.is_valid(), tpform.is_valid()]) and\n            not request.is_limited()):\n        if not user.st.update_post_hash(tform.get_topic_hash()):\n            return redirect(\n                request.POST.get('next', None) or\n                tform.category.get_absolute_url())\n        topic = tform.save()\n        cform.topic = topic\n        comment = cform.save()\n        comment_posted(comment=comment, mentions=None)\n        tpform.topic = topic\n        tpform.save_m2m()\n        TopicNotification.bulk_create(\n            users=tpform.get_users(), comment=comment)\n        return redirect(topic.get_absolute_url())\n    return render(\n        request=request,\n        template_name='spirit/topic/private/publish.html',\n        context={\n            'tform': tform,\n            'cform': cform,\n            'tpform': tpform})\ndef create_access(request, topic_id):\n    topic_private = TopicPrivate.objects.for_create_or_404(topic_id, request.user)\n    form = TopicPrivateInviteForm(\n        topic=topic_private.topic,\n        data=post_data(request))\n    if form.is_valid():\n        form.save()\n        notify_access(user=form.get_user(), topic_private=topic_private)\n    else:\n        messages.error(request, utils.render_form_errors(form))\n    return redirect(request.POST.get('next', topic_private.get_absolute_url()))\ndef delete_access(request, pk):\n    topic_private = TopicPrivate.objects.for_delete_or_404(pk, request.user)\n    if request.method == 'POST':\n        topic_private.delete()\n        if request.user.pk == topic_private.user_id:\n            return redirect(reverse(\"spirit:topic:private:index\"))\n        return redirect(request.POST.get('next', topic_private.get_absolute_url()))\n    return render(\n        request=request,\n        template_name='spirit/topic/private/delete.html',\n        context={'topic_private': topic_private})\ndef join_in(request, topic_id):\n    topic = get_object_or_404(\n        Topic,\n        pk=topic_id,\n        user=request.user,\n        category_id=settings.ST_TOPIC_PRIVATE_CATEGORY_PK)\n    form = TopicPrivateJoinForm(\n        topic=topic,\n        user=request.user,\n        data=post_data(request))\n    if is_post(request) and form.is_valid():\n        topic_private = form.save()\n        notify_access(user=form.get_user(), topic_private=topic_private)\n        return redirect(request.POST.get('next', topic.get_absolute_url()))\n    return render(\n        request=request,\n        template_name='spirit/topic/private/join.html',\n        context={\n            'topic': topic,\n            'form': form})\ndef publish(request, topic_id, pk=None):\n    initial = None\n    if pk:\n        comment = get_object_or_404(\n            Comment.objects.for_access(user=request.user), pk=pk)\n        quote = markdown.quotify(comment.comment, comment.user.st.nickname)\n        initial = {'comment': quote}\n    user = request.user\n    topic = get_object_or_404(\n        Topic.objects.opened().for_access(user),\n        pk=topic_id)\n    form = CommentForm(\n        user=user,\n        topic=topic,\n        data=post_data(request),\n        initial=initial)\n    if is_post(request) and not request.is_limited() and form.is_valid():\n        if not user.st.update_post_hash(form.get_comment_hash()):\n            return redirect(\n                request.POST.get('next', None) or\n                Comment\n                .get_last_for_topic(topic_id)\n                .get_absolute_url())\n        comment = form.save()\n        comment_posted(comment=comment, mentions=form.mentions)\n        return redirect(request.POST.get('next', comment.get_absolute_url()))\n    return render(\n        request=request,\n        template_name='spirit/comment/publish.html',\n        context={\n            'form': form,\n            'topic': topic})\ndef update(request, pk):\n    comment = Comment.objects.for_update_or_404(pk, request.user)\n    form = CommentForm(data=post_data(request), instance=comment)\n    if is_post(request) and form.is_valid():\n        pre_comment_update(comment=Comment.objects.get(pk=comment.pk))\n        comment = form.save()\n        post_comment_update(comment=comment)\n        return redirect(request.POST.get('next', comment.get_absolute_url()))\n    return render(\n        request=request,\n        template_name='spirit/comment/update.html',\n        context={'form': form})\ndef delete(request, pk, remove=True):\n    comment = get_object_or_404(Comment, pk=pk)\n    if is_post(request):\n        (Comment.objects\n         .filter(pk=pk)\n         .update(is_removed=remove))\n        return redirect(request.GET.get('next', comment.get_absolute_url()))\n    return render(\n        request=request,\n        template_name='spirit/comment/moderate.html',\n        context={'comment': comment})\ndef move(request, topic_id):\n    topic = get_object_or_404(Topic, pk=topic_id)\n    form = CommentMoveForm(topic=topic, data=request.POST)\n    if form.is_valid():\n        comments = form.save()\n        for comment in comments:\n            comment_posted(comment=comment, mentions=None)\n            topic.decrease_comment_count()\n            post_comment_move(comment=comment, topic=topic)\n    else:\n        messages.error(request, render_form_errors(form))\n    return redirect(request.POST.get('next', topic.get_absolute_url()))\ndef publish(request, category_id=None):\n    if category_id:\n        get_object_or_404(\n            Category.objects.visible(),\n            pk=category_id)\n    user = request.user\n    form = TopicForm(\n        user=user,\n        data=post_data(request),\n        initial={'category': category_id})\n    cform = CommentForm(\n        user=user,\n        data=post_data(request))\n    if (is_post(request) and\n            all([form.is_valid(), cform.is_valid()]) and\n            not request.is_limited()):\n        if not user.st.update_post_hash(form.get_topic_hash()):\n            return redirect(\n                request.POST.get('next', None) or\n                form.get_category().get_absolute_url())\n        topic = form.save()\n        cform.topic = topic\n        comment = cform.save()\n        comment_posted(comment=comment, mentions=cform.mentions)\n        return redirect(topic.get_absolute_url())\n    return render(\n        request=request,\n        template_name='spirit/topic/publish.html',\n        context={'form': form, 'cform': cform})\ndef update(request, pk):\n    topic = Topic.objects.for_update_or_404(pk, request.user)\n    category_id = topic.category_id\n    form = TopicForm(\n        user=request.user,\n        data=post_data(request),\n        instance=topic)\n    if is_post(request) and form.is_valid():\n        topic = form.save()\n        if topic.category_id != category_id:\n            Comment.create_moderation_action(\n                user=request.user, topic=topic, action=Comment.MOVED)\n        return redirect(request.POST.get('next', topic.get_absolute_url()))\n    return render(\n        request=request,\n        template_name='spirit/topic/update.html',\n        context={'form': form})\ndef _moderate(request, pk, field_name, to_value, action=None, message=None):\n    topic = get_object_or_404(Topic, pk=pk)\n    if is_post(request):\n        count = (\n            Topic.objects\n            .filter(pk=pk)\n            .exclude(**{field_name: to_value})\n            .update(**{\n                field_name: to_value,\n                'reindex_at': timezone.now()}))\n        if count and action is not None:\n            Comment.create_moderation_action(\n                user=request.user,\n                topic=topic,\n                action=action)\n        if message is not None:\n            messages.info(request, message)\n        return redirect(request.POST.get(\n            'next', topic.get_absolute_url()))\n    return render(\n        request=request,\n        template_name='spirit/topic/moderate.html',\n        context={'topic': topic})\ndef custom_login(request, **kwargs):\n    if request.user.is_authenticated:\n        return redirect(request.GET.get('next', request.user.st.get_absolute_url()))\n    if request.method == \"POST\" and request.is_limited():\n        return redirect(request.get_full_path())\n    return _login_view(request, authentication_form=LoginForm, **kwargs)\ndef custom_logout(request, **kwargs):\n    if not request.user.is_authenticated:\n        return redirect(request.GET.get('next', reverse(settings.LOGIN_URL)))\n    if request.method == 'POST':\n        return _logout_view(request, **kwargs)\n    return render(request, 'spirit/user/auth/logout.html')\ndef register(request, registration_form=RegistrationForm):\n    if request.user.is_authenticated:\n        return redirect(request.GET.get('next', reverse('spirit:user:update')))\n    form = registration_form(data=post_data(request))\n    if (is_post(request) and\n            not request.is_limited() and\n            form.is_valid()):\n        user = form.save()\n        send_activation_email(request, user)\n        messages.info(\n            request, _(\n                \"We have sent you an email to %(email)s \"\n                \"so you can activate your account!\") % {'email': form.get_email()})\n        return redirect(reverse(settings.LOGIN_URL))\n    return render(\n        request=request,\n        template_name='spirit/user/auth/register.html',\n        context={'form': form})\ndef resend_activation_email(request):\n    if request.user.is_authenticated:\n        return redirect(request.GET.get('next', reverse('spirit:user:update')))\n    form = ResendActivationForm(data=post_data(request))\n    if is_post(request):\n        if not request.is_limited() and form.is_valid():\n            user = form.get_user()\n            send_activation_email(request, user)\n        messages.info(\n            request, _(\n                \"If you don't receive an email, please make sure you've entered \"\n                \"the address you registered with, and check your spam folder.\"))\n        return redirect(reverse(settings.LOGIN_URL))\n    return render(\n        request=request,\n        template_name='spirit/user/auth/activation_resend.html',\n        context={'form': form})\ndef create(request, comment_id):\n    comment = get_object_or_404(\n        Comment.objects.exclude(user=request.user),\n        pk=comment_id)\n    form = LikeForm(\n        user=request.user,\n        comment=comment,\n        data=post_data(request))\n    if is_post(request) and form.is_valid():\n        like = form.save()\n        like.comment.increase_likes_count()\n        if is_ajax(request):\n            return json_response({'url_delete': like.get_delete_url()})\n        return redirect(request.POST.get('next', comment.get_absolute_url()))\n    return render(\n        request=request,\n        template_name='spirit/comment/like/create.html',\n        context={\n            'form': form,\n            'comment': comment})\ndef delete(request, pk):\n    like = get_object_or_404(CommentLike, pk=pk, user=request.user)\n    if is_post(request):\n        like.delete()\n        like.comment.decrease_likes_count()\n        if is_ajax(request):\n            url = reverse(\n                'spirit:comment:like:create',\n                kwargs={'comment_id': like.comment.pk})\n            return json_response({'url_create': url, })\n        return redirect(request.POST.get('next', like.comment.get_absolute_url()))\n    return render(\n        request=request,\n        template_name='spirit/comment/like/delete.html',\n        context={'like': like})\ndef close_or_open(request, pk, close=True):\n    poll = get_object_or_404(\n        CommentPoll,\n        pk=pk,\n        comment__user=request.user\n    )\n    if close:\n        close_at = timezone.now()\n    else:\n        close_at = None\n    (CommentPoll.objects\n     .filter(pk=poll.pk)\n     .update(close_at=close_at))\n    return redirect(request.GET.get('next', poll.get_absolute_url()))\ndef vote(request, pk):\n    poll = get_object_or_404(\n        CommentPoll.objects.unremoved(),\n        pk=pk\n    )\n    if not request.user.is_authenticated:\n        return redirect_to_login(next=poll.get_absolute_url())\n    form = PollVoteManyForm(user=request.user, poll=poll, data=request.POST)\n    if form.is_valid():\n        CommentPollChoice.decrease_vote_count(poll=poll, voter=request.user)\n        form.save_m2m()\n        CommentPollChoice.increase_vote_count(poll=poll, voter=request.user)\n        return redirect(request.POST.get('next', poll.get_absolute_url()))\n    messages.error(request, utils.render_form_errors(form))\n    return redirect(request.POST.get('next', poll.get_absolute_url()))\ndef create(request, comment_id):\n    comment = get_object_or_404(Comment, pk=comment_id)\n    form = FlagForm(\n        user=request.user,\n        comment=comment,\n        data=post_data(request))\n    if is_post(request) and form.is_valid():\n        form.save()\n        return redirect(request.POST.get('next', comment.get_absolute_url()))\n    return render(\n        request=request,\n        template_name='spirit/comment/flag/create.html',\n        context={\n            'form': form,\n            'comment': comment})\ndef create(request, topic_id):\n    topic = get_object_or_404(\n        Topic.objects.for_access(request.user),\n        pk=topic_id)\n    form = NotificationCreationForm(\n        user=request.user,\n        topic=topic,\n        data=request.POST)\n    if form.is_valid():\n        form.save()\n    else:\n        messages.error(request, utils.render_form_errors(form))\n    return redirect(request.POST.get('next', topic.get_absolute_url()))\ndef update(request, pk):\n    notification = get_object_or_404(TopicNotification, pk=pk, user=request.user)\n    form = NotificationForm(data=request.POST, instance=notification)\n    if form.is_valid():\n        form.save()\n    else:\n        messages.error(request, utils.render_form_errors(form))\n    return redirect(request.POST.get(\n        'next', notification.topic.get_absolute_url()))\ndef mark_all_as_read(request):\n    (TopicNotification.objects\n        .for_access(request.user)\n        .filter(is_read=False)\n        .update(is_read=True))\n    return redirect(request.POST.get(\n        'next', reverse('spirit:topic:notification:index')))\ndef edit(request, user_id):\n    user = get_object_or_404(User, pk=user_id)\n    uform = UserForm(data=post_data(request), instance=user)\n    form = UserProfileForm(data=post_data(request), instance=user.st)\n    if is_post(request) and all([uform.is_valid(), form.is_valid()]):\n        uform.save()\n        form.save()\n        messages.info(request, _(\"This profile has been updated!\"))\n        return redirect(request.GET.get(\"next\", request.get_full_path()))\n    return render(\n        request=request,\n        template_name='spirit/user/admin/edit.html',\n        context={'form': form, 'uform': uform})\ndef config_basic(request):\n    form = BasicConfigForm(data=post_data(request))\n    if is_post(request) and form.is_valid():\n        form.save()\n        messages.info(request, _(\"Settings updated!\"))\n        return redirect(request.GET.get(\"next\", request.get_full_path()))\n    return render(\n        request=request,\n        template_name='spirit/admin/config_basic.html',\n        context={'form': form})\ndef guest_only(view_func):\n    @wraps(view_func)\n    def wrapper(request, *args, **kwargs):\n        if request.user.is_authenticated:\n            return redirect(request.GET.get('next', request.user.st.get_absolute_url()))\n        return view_func(request, *args, **kwargs)\n    return wrapper\ndef create(request, topic_id):\n    topic = get_object_or_404(Topic, pk=topic_id)\n    form = FavoriteForm(user=request.user, topic=topic, data=request.POST)\n    if form.is_valid():\n        form.save()\n    else:\n        messages.error(request, utils.render_form_errors(form))\n    return redirect(request.POST.get('next', topic.get_absolute_url()))\ndef delete(request, pk):\n    favorite = get_object_or_404(TopicFavorite, pk=pk, user=request.user)\n    favorite.delete()\n    return redirect(request.POST.get('next', favorite.topic.get_absolute_url()))",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-0869",
        "description": "[{'lang': 'en', 'value': 'Multiple Open Redirect in GitHub repository nitely/spirit prior to 0.12.3.'}]",
        "cwe_number": 601
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-196",
      "code": "def check_docs():\n    data = request.get_json()\n    if data[\"docs\"].split(\"/\")[0] == \"local\":\n        return {\"status\": \"exists\"}\n    vectorstore = \"vectors/\" + data[\"docs\"]\n    base_path = \"https://raw.githubusercontent.com/arc53/DocsHUB/main/\"\n    if os.path.exists(vectorstore) or data[\"docs\"] == \"default\":\n        return {\"status\": \"exists\"}\n    else:\n        r = requests.get(base_path + vectorstore + \"index.faiss\")\n        if r.status_code != 200:\n            return {\"status\": \"null\"}\n        else:\n            if not os.path.exists(vectorstore):\n                os.makedirs(vectorstore)\n            with open(vectorstore + \"index.faiss\", \"wb\") as f:\n                f.write(r.content)\n            r = requests.get(base_path + vectorstore + \"index.pkl\")\n            with open(vectorstore + \"index.pkl\", \"wb\") as f:\n                f.write(r.content)\n        return {\"status\": \"loaded\"}",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-31451",
        "description": "[{'lang': 'en', 'value': 'DocsGPT is a GPT-powered chat for documentation. DocsGPT is vulnerable to unauthenticated limited file write in routes.py. This vulnerability is fixed in 0.8.1.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-197",
      "code": "    def run(self):\n        os.chdir(self.config.project_target_path)\n        shutil.copyfile(DOCS_INDEX_FILE_PATH, \"index.html\")\n        port = self.args.port\n        if self.args.browser:\n            webbrowser.open_new_tab(f\"http://localhost:{port}\")\n        with socketserver.TCPServer((\"\", port), SimpleHTTPRequestHandler) as httpd:\n            click.echo(f\"Serving docs at {port}\")\n            click.echo(f\"To access from your browser, navigate to: http://localhost:{port}\")\n            click.echo(\"\\n\\n\")\n            click.echo(\"Press Ctrl+C to exit.\")\n            httpd.serve_forever()",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-36105",
        "description": "[{'lang': 'en', 'value': 'dbt enables data analysts and engineers to transform their data using the same practices that software engineers use to build applications. Prior to versions 1.6.15, 1.7.15, and 1.8.1, Binding to `INADDR_ANY (0.0.0.0)` or `IN6ADDR_ANY (::)` exposes an application on all network interfaces, increasing the risk of unauthorized access. As stated in the Python docs, a special form for address is accepted instead of a host address: `\\'\\'` represents `INADDR_ANY`, equivalent to `\"0.0.0.0\"`. On systems with IPv6, \\'\\' represents `IN6ADDR_ANY`, which is equivalent to `\"::\"`. A user who serves docs on an unsecured public network, may unknowingly be hosting an unsecured (http) web site for any remote user/system to access on the same network. The issue has has been mitigated in dbt-core v1.6.15, dbt-core v1.7.15, and dbt-core v1.8.1 by binding to localhost explicitly by default in `dbt docs serve`.\\n'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-198",
      "code": "    def serialize_key_and_certificates_to_pkcs12(\n        self,\n        name: bytes | None,\n        key: PKCS12PrivateKeyTypes | None,\n        cert: x509.Certificate | None,\n        cas: list[_PKCS12CATypes] | None,\n        encryption_algorithm: serialization.KeySerializationEncryption,\n    ) -> bytes:\n        password = None\n        if name is not None:\n            utils._check_bytes(\"name\", name)\n        if isinstance(encryption_algorithm, serialization.NoEncryption):\n            nid_cert = -1\n            nid_key = -1\n            pkcs12_iter = 0\n            mac_iter = 0\n            mac_alg = self._ffi.NULL\n        elif isinstance(\n            encryption_algorithm, serialization.BestAvailableEncryption\n        ):\n            if rust_openssl.CRYPTOGRAPHY_OPENSSL_300_OR_GREATER:\n                nid_cert = self._lib.NID_aes_256_cbc\n                nid_key = self._lib.NID_aes_256_cbc\n            else:\n                nid_cert = self._lib.NID_pbe_WithSHA1And3_Key_TripleDES_CBC\n                nid_key = self._lib.NID_pbe_WithSHA1And3_Key_TripleDES_CBC\n            pkcs12_iter = 20000\n            mac_iter = 1\n            mac_alg = self._ffi.NULL\n            password = encryption_algorithm.password\n        elif (\n            isinstance(\n                encryption_algorithm, serialization._KeySerializationEncryption\n            )\n            and encryption_algorithm._format\n            is serialization.PrivateFormat.PKCS12\n        ):\n            nid_cert = 0\n            nid_key = 0\n            pkcs12_iter = 20000\n            mac_iter = 1\n            password = encryption_algorithm.password\n            keycertalg = encryption_algorithm._key_cert_algorithm\n            if keycertalg is PBES.PBESv1SHA1And3KeyTripleDESCBC:\n                nid_cert = self._lib.NID_pbe_WithSHA1And3_Key_TripleDES_CBC\n                nid_key = self._lib.NID_pbe_WithSHA1And3_Key_TripleDES_CBC\n            elif keycertalg is PBES.PBESv2SHA256AndAES256CBC:\n                if not rust_openssl.CRYPTOGRAPHY_OPENSSL_300_OR_GREATER:\n                    raise UnsupportedAlgorithm(\n                        \"PBESv2 is not supported by this version of OpenSSL\"\n                    )\n                nid_cert = self._lib.NID_aes_256_cbc\n                nid_key = self._lib.NID_aes_256_cbc\n            else:\n                assert keycertalg is None\n            if encryption_algorithm._hmac_hash is not None:\n                if not self._lib.Cryptography_HAS_PKCS12_SET_MAC:\n                    raise UnsupportedAlgorithm(\n                        \"Setting MAC algorithm is not supported by this \"\n                        \"version of OpenSSL.\"\n                    )\n                mac_alg = self._evp_md_non_null_from_algorithm(\n                    encryption_algorithm._hmac_hash\n                )\n                self.openssl_assert(mac_alg != self._ffi.NULL)\n            else:\n                mac_alg = self._ffi.NULL\n            if encryption_algorithm._kdf_rounds is not None:\n                pkcs12_iter = encryption_algorithm._kdf_rounds\n        else:\n            raise ValueError(\"Unsupported key encryption type\")\n        if cas is None or len(cas) == 0:\n            sk_x509 = self._ffi.NULL\n        else:\n            sk_x509 = self._lib.sk_X509_new_null()\n            sk_x509 = self._ffi.gc(sk_x509, self._lib.sk_X509_free)\n            ossl_cas = []\n            for ca in cas:\n                if isinstance(ca, PKCS12Certificate):\n                    ca_alias = ca.friendly_name\n                    ossl_ca = self._cert2ossl(ca.certificate)\n                    if ca_alias is None:\n                        res = self._lib.X509_alias_set1(\n                            ossl_ca, self._ffi.NULL, -1\n                        )\n                    else:\n                        res = self._lib.X509_alias_set1(\n                            ossl_ca, ca_alias, len(ca_alias)\n                        )\n                    self.openssl_assert(res == 1)\n                else:\n                    ossl_ca = self._cert2ossl(ca)\n                ossl_cas.append(ossl_ca)\n                res = self._lib.sk_X509_push(sk_x509, ossl_ca)\n                backend.openssl_assert(res >= 1)\n        with self._zeroed_null_terminated_buf(password) as password_buf:\n            with self._zeroed_null_terminated_buf(name) as name_buf:\n                ossl_cert = self._cert2ossl(cert) if cert else self._ffi.NULL\n                ossl_pkey = (\n                    self._key2ossl(key) if key is not None else self._ffi.NULL\n                )\n                p12 = self._lib.PKCS12_create(\n                    password_buf,\n                    name_buf,\n                    ossl_pkey,\n                    ossl_cert,\n                    sk_x509,\n                    nid_key,\n                    nid_cert,\n                    pkcs12_iter,\n                    mac_iter,\n                    0,\n                )\n            if (\n                self._lib.Cryptography_HAS_PKCS12_SET_MAC\n                and mac_alg != self._ffi.NULL\n            ):\n                self._lib.PKCS12_set_mac(\n                    p12,\n                    password_buf,\n                    -1,\n                    self._ffi.NULL,\n                    0,\n                    mac_iter,\n                    mac_alg,\n                )\n        self.openssl_assert(p12 != self._ffi.NULL)\n        p12 = self._ffi.gc(p12, self._lib.PKCS12_free)\n        bio = self._create_mem_bio_gc()\n        res = self._lib.i2d_PKCS12_bio(bio, p12)\n        self.openssl_assert(res > 0)\n        return self._read_mem_bio(bio)\n    def poly1305_supported(self) -> bool:\n        if self._fips_enabled:\n            return False\n        return True\n    def pkcs7_supported(self) -> bool:\n        return not rust_openssl.CRYPTOGRAPHY_IS_BORINGSSL",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-26130",
        "description": "[{'lang': 'en', 'value': 'cryptography is a package designed to expose cryptographic primitives and recipes to Python developers. Starting in version 38.0.0 and prior to version 42.0.4, if `pkcs12.serialize_key_and_certificates` is called with both a certificate whose public key did not match the provided private key and an `encryption_algorithm` with `hmac_hash` set (via `PrivateFormat.PKCS12.encryption_builder().hmac_hash(...)`, then a NULL pointer dereference would occur, crashing the Python process. This has been resolved in version 42.0.4, the first version in which a `ValueError` is properly raised.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-199",
      "code": "    def mint_token(self, preview=False):\n        decimals = int(self.token_dec.value())\n        mint_baton_vout = 2 if self.token_baton_to_e.text() != '' and not self.token_fixed_supply_cb.isChecked() else None\n        init_mint_qty = self.token_qty_e.get_amount()\n        if init_mint_qty is None:\n            self.show_message(_(\"Invalid token quantity entered.\"))\n            return\n        if init_mint_qty > (2 ** 64) - 1:\n            maxqty = format_satoshis_plain_nofloat((2 ** 64) - 1, decimals)\n            self.show_message(_(\"Token output quantity is too large. Maximum %s.\")%(maxqty,))\n            return\n        outputs = []\n        try:\n            token_id_hex = self.token_id_e.text()\n            token_type = self.wallet.token_types[token_id_hex]['class']\n            slp_op_return_msg = buildMintOpReturnOutput_V1(token_id_hex, mint_baton_vout, init_mint_qty, token_type)\n            outputs.append(slp_op_return_msg)\n        except OPReturnTooLarge:\n            self.show_message(_(\"Optional string text causiing OP_RETURN greater than 223 bytes.\"))\n            return\n        except Exception as e:\n            traceback.print_exc(file=sys.stdout)\n            self.show_message(str(e))\n            return\n        try:\n            addr = self.parse_address(self.token_pay_to_e.text())\n            outputs.append((TYPE_ADDRESS, addr, 546))\n        except:\n            self.show_message(_(\"Enter a Mint Receiver Address in SLP address format.\"))\n            return\n        if not self.token_fixed_supply_cb.isChecked():\n            try:\n                addr = self.parse_address(self.token_baton_to_e.text())\n                outputs.append((TYPE_ADDRESS, addr, 546))\n            except:\n                self.show_message(_(\"Enter a Baton Address in SLP address format.\"))\n                return\n        self.main_window.token_type_combo.setCurrentIndex(0)\n        assert self.main_window.slp_token_id == None\n        coins = self.main_window.get_coins()\n        fee = None\n        try:\n            baton_input = self.main_window.wallet.get_slp_token_baton(self.token_id_e.text())\n        except SlpNoMintingBatonFound as e:\n            self.show_message(_(\"No baton exists for this token.\"))\n            return\n        desired_fee_rate = 1.0\n        try:\n            tx = self.main_window.wallet.make_unsigned_transaction(coins, outputs, self.main_window.config, fee, None)\n            desired_fee_rate = tx.get_fee() / tx.estimated_size()\n        except NotEnoughFunds:\n            self.show_message(_(\"Insufficient funds\"))\n            return\n        except ExcessiveFee:\n            self.show_message(_(\"Your fee is too high.  Max is 50 sat/byte.\"))\n            return\n        except BaseException as e:\n            traceback.print_exc(file=sys.stdout)\n            self.show_message(str(e))\n            return\n        try:\n            baton_utxo = self.main_window.wallet.get_slp_token_baton(self.token_id_e.text())\n        except SlpNoMintingBatonFound:\n            self.show_message(_(\"There is no minting baton found for this token.\"))\n            return\n        tx.add_inputs([baton_utxo])\n        for txin in tx._inputs:\n            self.main_window.wallet.add_input_info(txin)\n        def tx_adjust_change_amount_based_on_baton_amount(tx, desired_fee_rate):\n            ''' adjust change amount (based on amount added from baton) '''\n            if len(tx._outputs) not in (3,4):\n                self.print_error(f\"Unkown tx shape, not adjusting fee!\")\n                return\n            chg = tx._outputs[-1]\n            assert len(chg) == 3, \"Expected tx output to be of length 3\"\n            if not self.main_window.wallet.is_mine(chg[1]):\n                self.print_error(f\"Unkown change address {chg[1]}, not adjusting fee!\")\n                return\n            chg_amt = chg[2]\n            if chg_amt <= 546:\n                self.print_error(\"Could not determine change output, not adjusting fee!\")\n                return\n            curr_fee, curr_size = tx.get_fee(), tx.estimated_size()\n            fee_rate = curr_fee / curr_size\n            diff = math.ceil((fee_rate - desired_fee_rate) * curr_size)\n            if diff > 0:\n                tx._outputs[-1] = (chg[0], chg[1], chg[2] + diff)\n                self.print_error(f\"Added {diff} sats to change to maintain fee rate of {desired_fee_rate:0.2f}, new fee: {tx.get_fee()}\")\n        tx_adjust_change_amount_based_on_baton_amount(tx, desired_fee_rate)\n        if preview:\n            show_transaction(tx, self.main_window, None, False, self)\n            return\n        msg = []\n        if self.main_window.wallet.has_password():\n            msg.append(\"\")\n            msg.append(_(\"Enter your password to proceed\"))\n            password = self.main_window.password_dialog('\\n'.join(msg))\n            if not password:\n                return\n        else:\n            password = None\n        tx_desc = None\n        def sign_done(success):\n            if success:\n                if not tx.is_complete():\n                    show_transaction(tx, self.main_window, None, False, self)\n                    self.main_window.do_clear()\n                else:\n                    self.main_window.broadcast_transaction(tx, tx_desc)\n        self.main_window.sign_tx_with_password(tx, sign_done, password)\n        self.mint_button.setDisabled(True)\n        self.close()",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2020-11014",
        "description": "[{'lang': 'en', 'value': 'Electron-Cash-SLP before version 3.6.2 has a vulnerability. All token creators that use the \"Mint Tool\" feature of the Electron Cash SLP Edition are at risk of sending the minting authority baton to the wrong SLP address. Sending the mint baton to the wrong address will give another party the ability to issue new tokens or permanently destroy future minting capability. This is fixed version 3.6.2.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-200",
      "code": "    def prepare(\n        self,\n        *,\n        conn: SnowflakeConnection,\n        authenticator: str,\n        service_name: str | None,\n        account: str,\n        user: str,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Web Browser based Authentication.\"\"\"\n        logger.debug(\"authenticating by Web Browser\")\n        socket_connection = self._socket(socket.AF_INET, socket.SOCK_STREAM)\n        try:\n            try:\n                socket_connection.bind(\n                    (\n                        os.getenv(\"SF_AUTH_SOCKET_ADDR\", \"localhost\"),\n                        int(os.getenv(\"SF_AUTH_SOCKET_PORT\", 0)),\n                    )\n                )\n            except socket.gaierror as ex:\n                if ex.args[0] == socket.EAI_NONAME:\n                    raise OperationalError(\n                        msg=\"localhost is not found. Ensure /etc/hosts has \"\n                        \"localhost entry.\",\n                        errno=ER_NO_HOSTNAME_FOUND,\n                    )\n                else:\n                    raise ex\n            socket_connection.listen(0)\n            callback_port = socket_connection.getsockname()[1]\n            print(\n                \"Initiating login request with your identity provider. A \"\n                \"browser window should have opened for you to complete the \"\n                \"login. If you can't see it, check existing browser windows, \"\n                \"or your OS settings. Press CTRL+C to abort and try again...\"\n            )\n            logger.debug(\"step 1: query GS to obtain SSO url\")\n            sso_url = self._get_sso_url(\n                conn, authenticator, service_name, account, callback_port, user\n            )\n            logger.debug(\"step 2: open a browser\")\n            print(f\"Going to open: {sso_url} to authenticate...\")\n            if not self._webbrowser.open_new(sso_url):\n                print(\n                    \"We were unable to open a browser window for you, \"\n                    \"please open the url above manually then paste the \"\n                    \"URL you are redirected to into the terminal.\"\n                )\n                url = input(\"Enter the URL the SSO URL redirected you to: \")\n                self._process_get_url(url)\n                if not self._token:\n                    self._handle_failure(\n                        conn=conn,\n                        ret={\n                            \"code\": ER_UNABLE_TO_OPEN_BROWSER,\n                            \"message\": (\n                                \"Unable to open a browser in this environment and \"\n                                \"SSO URL contained no token\"\n                            ),\n                        },\n                    )\n                    return\n            else:\n                logger.debug(\"step 3: accept SAML token\")\n                self._receive_saml_token(conn, socket_connection)\n        finally:\n            socket_connection.close()\n    def warn(\n        self,\n        msg: str,\n        path_name: str | None = None,\n        func_name: str | None = None,\n        *args: Any,\n        **kwargs: Any,\n    ) -> None:\n        warnings.warn(\n            \"The 'warn' method is deprecated, \" \"use 'warning' instead\",\n            DeprecationWarning,\n            2,\n        )\n        self.warning(msg, path_name, func_name, *args, **kwargs)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-34233",
        "description": "[{'lang': 'en', 'value': 'The Snowflake Connector for Python provides an interface for developing Python applications that can connect to Snowflake and perform all standard operations. Versions prior to 3.0.2 are vulnerable to command injection via single sign-on(SSO) browser URL authentication. In order to exploit the potential for command injection, an attacker would need to be successful in (1) establishing a malicious resource and (2) redirecting users to utilize the resource. The attacker could set up a malicious, publicly accessible server which responds to the SSO URL with an attack payload. If the attacker then tricked a user into visiting the maliciously crafted connection URL, the user\u2019s local machine would render the malicious payload, leading to a remote code execution. This attack scenario can be mitigated through URL whitelisting as well as common anti-phishing resources. Version 3.0.2 contains a patch for this issue.'}]",
        "cwe_number": 77
      },
      "cwe_types": [],
      "severity": "high"
    }
  ]
}
