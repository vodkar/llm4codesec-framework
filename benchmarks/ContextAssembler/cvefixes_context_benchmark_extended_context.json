{
  "metadata": {
    "name": "CVEFixes-with-Context-Benchmark-Extended-Context",
    "task_type": "binary",
    "total_samples": 300,
    "cwe_distribution": {
      "CWE-79": 26,
      "CWE-601": 10,
      "CWE-22": 19,
      "CWE-918": 11,
      "CWE-416": 1,
      "CWE-89": 8,
      "CWE-295": 1,
      "CWE-502": 5,
      "CWE-29": 5,
      "CWE-352": 6,
      "CWE-200": 10,
      "CWE-835": 1,
      "CWE-611": 4,
      "CWE-770": 4,
      "CWE-617": 11,
      "CWE-670": 3,
      "CWE-94": 5,
      "CWE-1336": 1,
      "CWE-674": 4,
      "CWE-125": 3,
      "CWE-1333": 2,
      "CWE-19": 1,
      "CWE-255": 2,
      "CWE-77": 5,
      "CWE-755": 2,
      "CWE-346": 1,
      "CWE-20": 14,
      "CWE-620": 1,
      "CWE-78": 9,
      "CWE-59": 3,
      "CWE-287": 3,
      "CWE-74": 5,
      "CWE-190": 3,
      "CWE-362": 3,
      "CWE-73": 2,
      "CWE-613": 3,
      "CWE-93": 1,
      "CWE-522": 1,
      "CWE-264": 5,
      "CWE-119": 2,
      "CWE-307": 1,
      "CWE-400": 5,
      "CWE-863": 3,
      "CWE-131": 2,
      "CWE-134": 1,
      "CWE-203": 3,
      "CWE-311": 1,
      "CWE-1284": 1,
      "CWE-521": 2,
      "CWE-732": 2,
      "CWE-476": 4,
      "CWE-434": 1,
      "CWE-367": 2,
      "CWE-23": 1,
      "CWE-116": 1,
      "CWE-444": 1,
      "CWE-36": 1,
      "CWE-667": 2,
      "CWE-269": 1,
      "CWE-312": 1,
      "CWE-532": 1,
      "CWE-1188": 1,
      "CWE-436": 1
    }
  },
  "samples": [
    {
      "id": "ContextAssembler-1",
      "code": "    def post(self, request, *args, **kwargs):\n        content = request.POST.get(\"content\", \"\")\n        lexer = request.POST.get(\"lexer\", highlight.LEXER_DEFAULT).strip()\n        filename = request.POST.get(\"filename\", \"\").strip()\n        expires = request.POST.get(\"expires\", \"\").strip()\n        response_format = request.POST.get(\"format\", \"default\").strip()\n        if not content.strip():\n            return HttpResponseBadRequest(\"No content given\")\n        if not lexer and not filename:\n            return HttpResponseBadRequest(\n                \"No lexer or filename given. Unable to \"\n                \"determine a highlight. Valid lexers are: %s\"\n                % \", \".join(highlight.LEXER_KEYS)\n            )\n        if lexer and lexer not in highlight.LEXER_KEYS:\n            return HttpResponseBadRequest(\n                'Invalid lexer \"%s\" given. Valid lexers are: %s'\n                % (lexer, \", \".join(highlight.LEXER_KEYS))\n            )\n        if not lexer and filename:\n            try:\n                lexer_cls = get_lexer_for_filename(filename)\n                lexer = lexer_cls.aliases[0]\n            except (ClassNotFound, IndexError):\n                lexer = config.PLAIN_CODE_SYMBOL\n        if expires:\n            expire_options = [str(i) for i in dict(config.EXPIRE_CHOICES)]\n            if expires not in expire_options:\n                return HttpResponseBadRequest(\n                    'Invalid expire choice \"{}\" given. Valid values are: {}'.format(\n                        expires, \", \".join(expire_options)\n                    )\n                )\n            expires, expire_type = get_expire_values(expires)\n        else:\n            expires = datetime.datetime.now() + datetime.timedelta(seconds=60 * 60 * 24)\n            expire_type = Snippet.EXPIRE_TIME\n        snippet = Snippet.objects.create(\n            content=content,\n            lexer=lexer,\n            expires=expires,\n            expire_type=expire_type,\n        )\n        formatter = getattr(self, f\"_format_{response_format}\", None)\n        if callable(formatter):\n            return HttpResponse(formatter(snippet))\n        return HttpResponse(self._format_default(snippet))\ndef handler500(request, template_name=\"dpaste/500.html\"):\n    context = {}\n    context.update(config.extra_template_context)\n    response = render(request, template_name, context, status=500)\n    add_never_cache_headers(response)\n    return response",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-49277",
        "description": "[{'lang': 'en', 'value': \"dpaste is an open source pastebin application written in Python using the Django framework. A security vulnerability has been identified in the expires parameter of the dpaste API, allowing for a POST Reflected XSS attack. This vulnerability can be exploited by an attacker to execute arbitrary JavaScript code in the context of a user's browser, potentially leading to unauthorized access, data theft, or other malicious activities. Users are strongly advised to upgrade to dpaste release v3.8 or later versions, as dpaste versions older than v3.8 are susceptible to the identified security vulnerability. No known workarounds have been identified, and applying the patch is the most effective way to remediate the vulnerability.\"}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-2",
      "code": "    def __init__(\n        self,\n        agent: IAgent,\n        ip_whitelist: Optional[IPSet] = None,\n        ip_blacklist: Optional[IPSet] = None,\n    ):\n        \"\"\"\n        Args:\n            agent: The Agent to wrap.\n            ip_whitelist: IP addresses to allow.\n            ip_blacklist: IP addresses to disallow.\n        \"\"\"\n        self._agent = agent\n        self._ip_whitelist = ip_whitelist\n        self._ip_blacklist = ip_blacklist\n    def request(\n        self,\n        method: bytes,\n        uri: bytes,\n        headers: Optional[Headers] = None,\n        bodyProducer: Optional[IBodyProducer] = None,\n    ) -> defer.Deferred:\n        h = urllib.parse.urlparse(uri.decode(\"ascii\"))\n        try:\n            ip_address = IPAddress(h.hostname)\n            if check_against_blacklist(\n                ip_address, self._ip_whitelist, self._ip_blacklist\n            ):\n                logger.info(\"Blocking access to %s due to blacklist\" % (ip_address,))\n                e = SynapseError(403, \"IP address blocked by IP blacklist entry\")\n                return defer.fail(Failure(e))\n        except Exception:\n            pass\n        return self._agent.request(\n            method, uri, headers=headers, bodyProducer=bodyProducer\n        )\n    def __init__(\n        self,\n        hs: \"HomeServer\",\n        treq_args: Dict[str, Any] = {},\n        ip_whitelist: Optional[IPSet] = None,\n        ip_blacklist: Optional[IPSet] = None,\n        http_proxy: Optional[bytes] = None,\n        https_proxy: Optional[bytes] = None,\n    ):\n        \"\"\"\n        Args:\n            hs\n            treq_args: Extra keyword arguments to be given to treq.request.\n            ip_blacklist: The IP addresses that are blacklisted that\n                we may not request.\n            ip_whitelist: The whitelisted IP addresses, that we can\n               request if it were otherwise caught in a blacklist.\n            http_proxy: proxy server to use for http connections. host[:port]\n            https_proxy: proxy server to use for https connections. host[:port]\n        \"\"\"\n        self.hs = hs\n        self._ip_whitelist = ip_whitelist\n        self._ip_blacklist = ip_blacklist\n        self._extra_treq_args = treq_args\n        self.user_agent = hs.version_string\n        self.clock = hs.get_clock()\n        if hs.config.user_agent_suffix:\n            self.user_agent = \"%s %s\" % (self.user_agent, hs.config.user_agent_suffix)\n        self._cooperator = Cooperator(scheduler=_make_scheduler(hs.get_reactor()))\n        self.user_agent = self.user_agent.encode(\"ascii\")\n        if self._ip_blacklist:\n            real_reactor = hs.get_reactor()\n            nameResolver = IPBlacklistingResolver(\n                real_reactor, self._ip_whitelist, self._ip_blacklist\n            )\n            @implementer(IReactorPluggableNameResolver)\n            class Reactor:\n                def __getattr__(_self, attr):\n                    if attr == \"nameResolver\":\n                        return nameResolver\n                    else:\n                        return getattr(real_reactor, attr)\n            self.reactor = Reactor()\n        else:\n            self.reactor = hs.get_reactor()\n        pool = HTTPConnectionPool(self.reactor)\n        pool.maxPersistentPerHost = max((100 * hs.config.caches.global_factor, 5))\n        pool.cachedConnectionTimeout = 2 * 60\n        self.agent = ProxyAgent(\n            self.reactor,\n            connectTimeout=15,\n            contextFactory=self.hs.get_http_client_context_factory(),\n            pool=pool,\n            http_proxy=http_proxy,\n            https_proxy=https_proxy,\n        )\n        if self._ip_blacklist:\n            self.agent = BlacklistingAgentWrapper(\n                self.agent,\n                ip_whitelist=self._ip_whitelist,\n                ip_blacklist=self._ip_blacklist,\n            )\n    async def request(\n        self,\n        method: str,\n        uri: str,\n        data: Optional[bytes] = None,\n        headers: Optional[Headers] = None,\n    ) -> IResponse:\n        \"\"\"\n        Args:\n            method: HTTP method to use.\n            uri: URI to query.\n            data: Data to send in the request body, if applicable.\n            headers: Request headers.\n        Returns:\n            Response object, once the headers have been read.\n        Raises:\n            RequestTimedOutError if the request times out before the headers are read\n        \"\"\"\n        outgoing_requests_counter.labels(method).inc()\n        logger.debug(\"Sending request %s %s\", method, redact_uri(uri))\n        with start_active_span(\n            \"outgoing-client-request\",\n            tags={\n                tags.SPAN_KIND: tags.SPAN_KIND_RPC_CLIENT,\n                tags.HTTP_METHOD: method,\n                tags.HTTP_URL: uri,\n            },\n            finish_on_close=True,\n        ):\n            try:\n                body_producer = None\n                if data is not None:\n                    body_producer = QuieterFileBodyProducer(\n                        BytesIO(data), cooperator=self._cooperator,\n                    )\n                request_deferred = treq.request(\n                    method,\n                    uri,\n                    agent=self.agent,\n                    data=body_producer,\n                    headers=headers,\n                    **self._extra_treq_args,\n                )\n                request_deferred = timeout_deferred(\n                    request_deferred, 60, self.hs.get_reactor(),\n                )\n                request_deferred.addErrback(_timeout_to_request_timed_out_error)\n                response = await make_deferred_yieldable(request_deferred)\n                incoming_responses_counter.labels(method, response.code).inc()\n                logger.info(\n                    \"Received response to %s %s: %s\",\n                    method,\n                    redact_uri(uri),\n                    response.code,\n                )\n                return response\n            except Exception as e:\n                incoming_responses_counter.labels(method, \"ERR\").inc()\n                logger.info(\n                    \"Error sending request to  %s %s: %s %s\",\n                    method,\n                    redact_uri(uri),\n                    type(e).__name__,\n                    e.args[0],\n                )\n                set_tag(tags.ERROR, True)\n                set_tag(\"error_reason\", e.args[0])\n                raise\n    def __init__(\n        self,\n        reactor: IReactorCore,\n        tls_client_options_factory: Optional[FederationPolicyForHTTPS],\n        user_agent: bytes,\n        _srv_resolver: Optional[SrvResolver] = None,\n        _well_known_resolver: Optional[WellKnownResolver] = None,\n    ):\n        self._reactor = reactor\n        self._clock = Clock(reactor)\n        self._pool = HTTPConnectionPool(reactor)\n        self._pool.retryAutomatically = False\n        self._pool.maxPersistentPerHost = 5\n        self._pool.cachedConnectionTimeout = 2 * 60\n        self._agent = Agent.usingEndpointFactory(\n            self._reactor,\n            MatrixHostnameEndpointFactory(\n                reactor, tls_client_options_factory, _srv_resolver\n            ),\n            pool=self._pool,\n        )\n        self.user_agent = user_agent\n        if _well_known_resolver is None:\n            _well_known_resolver = WellKnownResolver(\n                self._reactor,\n                agent=Agent(\n                    self._reactor,\n                    pool=self._pool,\n                    contextFactory=tls_client_options_factory,\n                ),\n                user_agent=self.user_agent,\n            )\n        self._well_known_resolver = _well_known_resolver\n    def request(\n        self,\n        method: bytes,\n        uri: bytes,\n        headers: Optional[Headers] = None,\n        bodyProducer: Optional[IBodyProducer] = None,\n    ) -> defer.Deferred:\n        \"\"\"\n        Args:\n            method: HTTP method: GET/POST/etc\n            uri: Absolute URI to be retrieved\n            headers:\n                HTTP headers to send with the request, or None to send no extra headers.\n            bodyProducer:\n                An object which can generate bytes to make up the\n                body of this request (for example, the properly encoded contents of\n                a file for a file upload).  Or None if the request is to have\n                no body.\n        Returns:\n            Deferred[twisted.web.iweb.IResponse]:\n                fires when the header of the response has been received (regardless of the\n                response status code). Fails if there is any problem which prevents that\n                response from being received (including problems that prevent the request\n                from being sent).\n        \"\"\"\n        parsed_uri = urllib.parse.urlparse(uri)\n        assert parsed_uri.hostname\n        delegated_server = None\n        if (\n            parsed_uri.scheme == b\"matrix\"\n            and not _is_ip_literal(parsed_uri.hostname)\n            and not parsed_uri.port\n        ):\n            well_known_result = yield defer.ensureDeferred(\n                self._well_known_resolver.get_well_known(parsed_uri.hostname)\n            )\n            delegated_server = well_known_result.delegated_server\n        if delegated_server:\n            uri = urllib.parse.urlunparse(\n                (\n                    parsed_uri.scheme,\n                    delegated_server,\n                    parsed_uri.path,\n                    parsed_uri.params,\n                    parsed_uri.query,\n                    parsed_uri.fragment,\n                )\n            )\n            parsed_uri = urllib.parse.urlparse(uri)\n        if headers is None:\n            headers = Headers()\n        else:\n            headers = headers.copy()\n        if not headers.hasHeader(b\"host\"):\n            headers.addRawHeader(b\"host\", parsed_uri.netloc)\n        if not headers.hasHeader(b\"user-agent\"):\n            headers.addRawHeader(b\"user-agent\", self.user_agent)\n        res = yield make_deferred_yieldable(\n            self._agent.request(method, uri, headers, bodyProducer)\n        )\n        return res\n    def __init__(self, hs: \"HomeServer\"):\n        super().__init__(hs)\n        self.hs = hs\n        self.store = hs.get_datastore()\n        self.storage = hs.get_storage()\n        self.state_store = self.storage.state\n        self.federation_client = hs.get_federation_client()\n        self.state_handler = hs.get_state_handler()\n        self._state_resolution_handler = hs.get_state_resolution_handler()\n        self.server_name = hs.hostname\n        self.keyring = hs.get_keyring()\n        self.action_generator = hs.get_action_generator()\n        self.is_mine_id = hs.is_mine_id\n        self.spam_checker = hs.get_spam_checker()\n        self.event_creation_handler = hs.get_event_creation_handler()\n        self._message_handler = hs.get_message_handler()\n        self._server_notices_mxid = hs.config.server_notices_mxid\n        self.config = hs.config\n        self.http_client = hs.get_simple_http_client()\n        self._instance_name = hs.get_instance_name()\n        self._replication = hs.get_replication_data_handler()\n        self._send_events = ReplicationFederationSendEventsRestServlet.make_client(hs)\n        self._clean_room_for_join_client = ReplicationCleanRoomRestServlet.make_client(\n            hs\n        )\n        if hs.config.worker_app:\n            self._user_device_resync = ReplicationUserDevicesResyncRestServlet.make_client(\n                hs\n            )\n            self._maybe_store_room_on_outlier_membership = ReplicationStoreRoomOnOutlierMembershipRestServlet.make_client(\n                hs\n            )\n        else:\n            self._device_list_updater = hs.get_device_handler().device_list_updater\n            self._maybe_store_room_on_outlier_membership = (\n                self.store.maybe_store_room_on_outlier_membership\n            )\n        self.room_queues = {}\n        self._room_pdu_linearizer = Linearizer(\"fed_room_pdu\")\n        self.third_party_event_rules = hs.get_third_party_event_rules()\n        self._ephemeral_messages_enabled = hs.config.enable_ephemeral_messages\n    def read_config(self, config, **kwargs):\n        self.federation_domain_whitelist = None\n        federation_domain_whitelist = config.get(\"federation_domain_whitelist\", None)\n        if federation_domain_whitelist is not None:\n            self.federation_domain_whitelist = {}\n            for domain in federation_domain_whitelist:\n                self.federation_domain_whitelist[domain] = True\n        self.federation_ip_range_blacklist = config.get(\n            \"federation_ip_range_blacklist\", []\n        )\n        try:\n            self.federation_ip_range_blacklist = IPSet(\n                self.federation_ip_range_blacklist\n            )\n            self.federation_ip_range_blacklist.update([\"0.0.0.0\", \"::\"])\n        except Exception as e:\n            raise ConfigError(\n                \"Invalid range(s) provided in federation_ip_range_blacklist: %s\" % e\n            )\n        federation_metrics_domains = config.get(\"federation_metrics_domains\") or []\n        validate_config(\n            _METRICS_FOR_DOMAINS_SCHEMA,\n            federation_metrics_domains,\n            (\"federation_metrics_domains\",),\n        )\n        self.federation_metrics_domains = set(federation_metrics_domains)\n    def generate_config_section(self, config_dir_path, server_name, **kwargs):\n        return \"\"\"\\\n        federation_ip_range_blacklist:\n          - '127.0.0.0/8'\n          - '10.0.0.0/8'\n          - '172.16.0.0/12'\n          - '192.168.0.0/16'\n          - '100.64.0.0/10'\n          - '169.254.0.0/16'\n          - '::1/128'\n          - 'fe80::/64'\n          - 'fc00::/7'\n        \"\"\"\n_METRICS_FOR_DOMAINS_SCHEMA = {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n    def __init__(self, hs, tls_client_options_factory):\n        self.hs = hs\n        self.signing_key = hs.signing_key\n        self.server_name = hs.hostname\n        real_reactor = hs.get_reactor()\n        nameResolver = IPBlacklistingResolver(\n            real_reactor, None, hs.config.federation_ip_range_blacklist\n        )\n        @implementer(IReactorPluggableNameResolver)\n        class Reactor:\n            def __getattr__(_self, attr):\n                if attr == \"nameResolver\":\n                    return nameResolver\n                else:\n                    return getattr(real_reactor, attr)\n        self.reactor = Reactor()\n        user_agent = hs.version_string\n        if hs.config.user_agent_suffix:\n            user_agent = \"%s %s\" % (user_agent, hs.config.user_agent_suffix)\n        user_agent = user_agent.encode(\"ascii\")\n        self.agent = MatrixFederationAgent(\n            self.reactor, tls_client_options_factory, user_agent\n        )\n        self.agent = BlacklistingAgentWrapper(\n            self.agent, ip_blacklist=hs.config.federation_ip_range_blacklist,\n        )\n        self.clock = hs.get_clock()\n        self._store = hs.get_datastore()\n        self.version_string_bytes = hs.version_string.encode(\"ascii\")\n        self.default_timeout = 60\n        def schedule(x):\n            self.reactor.callLater(_EPSILON, x)\n        self._cooperator = Cooperator(scheduler=schedule)\n    def __init__(self, hs, pusherdict):\n        self.hs = hs\n        self.store = self.hs.get_datastore()\n        self.storage = self.hs.get_storage()\n        self.clock = self.hs.get_clock()\n        self.state_handler = self.hs.get_state_handler()\n        self.user_id = pusherdict[\"user_name\"]\n        self.app_id = pusherdict[\"app_id\"]\n        self.app_display_name = pusherdict[\"app_display_name\"]\n        self.device_display_name = pusherdict[\"device_display_name\"]\n        self.pushkey = pusherdict[\"pushkey\"]\n        self.pushkey_ts = pusherdict[\"ts\"]\n        self.data = pusherdict[\"data\"]\n        self.last_stream_ordering = pusherdict[\"last_stream_ordering\"]\n        self.backoff_delay = HttpPusher.INITIAL_BACKOFF_SEC\n        self.failing_since = pusherdict[\"failing_since\"]\n        self.timed_call = None\n        self._is_processing = False\n        self._group_unread_count_by_room = hs.config.push_group_unread_count_by_room\n        self.max_stream_ordering = None\n        if \"data\" not in pusherdict:\n            raise PusherConfigException(\"No 'data' key for HTTP pusher\")\n        self.data = pusherdict[\"data\"]\n        self.name = \"%s/%s/%s\" % (\n            pusherdict[\"user_name\"],\n            pusherdict[\"app_id\"],\n            pusherdict[\"pushkey\"],\n        )\n        if self.data is None:\n            raise PusherConfigException(\"data can not be null for HTTP pusher\")\n        if \"url\" not in self.data:\n            raise PusherConfigException(\"'url' required in data for HTTP pusher\")\n        self.url = self.data[\"url\"]\n        self.http_client = hs.get_proxied_http_client()\n        self.data_minus_url = {}\n        self.data_minus_url.update(self.data)\n        del self.data_minus_url[\"url\"]\n    def __init__(self, hs):\n        self.hs = hs\n        self.auth = hs.get_auth()\n        self.client = hs.get_http_client()\n        self.clock = hs.get_clock()\n        self.server_name = hs.hostname\n        self.store = hs.get_datastore()\n        self.max_upload_size = hs.config.max_upload_size\n        self.max_image_pixels = hs.config.max_image_pixels\n        self.primary_base_path = hs.config.media_store_path\n        self.filepaths = MediaFilePaths(self.primary_base_path)\n        self.dynamic_thumbnails = hs.config.dynamic_thumbnails\n        self.thumbnail_requirements = hs.config.thumbnail_requirements\n        self.remote_media_linearizer = Linearizer(name=\"media_remote\")\n        self.recently_accessed_remotes = set()\n        self.recently_accessed_locals = set()\n        self.federation_domain_whitelist = hs.config.federation_domain_whitelist\n        storage_providers = []\n        for clz, provider_config, wrapper_config in hs.config.media_storage_providers:\n            backend = clz(hs, provider_config)\n            provider = StorageProviderWrapper(\n                backend,\n                store_local=wrapper_config.store_local,\n                store_remote=wrapper_config.store_remote,\n                store_synchronous=wrapper_config.store_synchronous,\n            )\n            storage_providers.append(provider)\n        self.media_storage = MediaStorage(\n            self.hs, self.primary_base_path, self.filepaths, storage_providers\n        )\n        self.clock.looping_call(\n            self._start_update_recently_accessed, UPDATE_RECENTLY_ACCESSED_TS\n        )\n    def __init__(self, hs):\n        super().__init__(hs)\n        self.http_client = SimpleHttpClient(hs)\n        self.blacklisting_http_client = SimpleHttpClient(\n            hs, ip_blacklist=hs.config.federation_ip_range_blacklist\n        )\n        self.federation_http_client = hs.get_http_client()\n        self.hs = hs\n    def get_simple_http_client(self) -> SimpleHttpClient:\n        return SimpleHttpClient(self)\n    def get_proxied_http_client(self) -> SimpleHttpClient:\n        return SimpleHttpClient(\n            self,\n            http_proxy=os.getenvb(b\"http_proxy\"),\n            https_proxy=os.getenvb(b\"HTTPS_PROXY\"),\n        )\n    def get_room_creation_handler(self) -> RoomCreationHandler:\n        return RoomCreationHandler(self)\n    def get_sendmail(self) -> sendmail:\n        return sendmail\n    def get_state_handler(self) -> StateHandler:\n        return StateHandler(self)\n    def get_state_resolution_handler(self) -> StateResolutionHandler:\n        return StateResolutionHandler(self)\n    def get_presence_handler(self) -> PresenceHandler:\n        return PresenceHandler(self)\n    def get_typing_handler(self):\n        if self.config.worker.writers.typing == self.get_instance_name():\n            return TypingWriterHandler(self)\n        else:\n            return FollowerTypingHandler(self)\n    def __init__(self, hs):\n        super().__init__(hs)\n        self.clock = hs.get_clock()\n        self.client = hs.get_http_client()\n        self.key_servers = self.config.key_servers\n    def __init__(self, hs):\n        self.server_name = hs.hostname\n        self.client = hs.get_http_client()",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-21273",
        "description": "[{'lang': 'en', 'value': 'Synapse is a Matrix reference homeserver written in python (pypi package matrix-synapse). Matrix is an ecosystem for open federated Instant Messaging and VoIP. In Synapse before version 1.25.0, requests to user provided domains were not restricted to external IP addresses when calculating the key validity for third-party invite events and sending push notifications. This could cause Synapse to make requests to internal infrastructure. The type of request was not controlled by the user, although limited modification of request bodies was possible. For the most thorough protection server administrators should remove the deprecated `federation_ip_range_blacklist` from their settings after upgrading to Synapse v1.25.0 which will result in Synapse using the improved default IP address restrictions. See the new `ip_range_blacklist` and `ip_range_whitelist` settings if more specific control is necessary.'}]",
        "cwe_number": 601
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-3",
      "code": "    def check_kb_exist(self, user_id, kb_ids):\n        kb_ids_str = ','.join(\"'{}'\".format(str(x)) for x in kb_ids)\n        query = \"SELECT kb_id FROM KnowledgeBase WHERE kb_id IN ({}) AND deleted = 0 AND user_id = %s\".format(kb_ids_str)\n        result = self.execute_query_(query, (user_id,), fetch=True)\n        debug_logger.info(\"check_kb_exist {}\".format(result))\n        valid_kb_ids = [kb_info[0] for kb_info in result]\n        unvalid_kb_ids = list(set(kb_ids) - set(valid_kb_ids))\n        return unvalid_kb_ids\n    def check_file_exist_by_name(self, user_id, kb_id, file_names):\n        results = []\n        batch_size = 100\n        for i in range(0, len(file_names), batch_size):\n            batch_file_names = file_names[i:i+batch_size]\n            file_names_str = ','.join(\"'{}'\".format(str(x).replace(\"'\", \"\\\\'\")) for x in batch_file_names)\n            query = \"\"\"\n                SELECT file_id, file_name, file_size, status FROM File\n                WHERE deleted = 0\n                AND file_name IN ({})\n                AND kb_id = %s\n                AND kb_id IN (SELECT kb_id FROM KnowledgeBase WHERE user_id = %s)\n            \"\"\".format(file_names_str)\n            batch_result = self.execute_query_(query, (kb_id, user_id), fetch=True)\n            debug_logger.info(\"check_file_exist_by_name batch {}: {}\".format(i//batch_size, batch_result))\n            results.extend(batch_result)\n        return results\n    def get_knowledge_base_name(self, kb_ids):\n        kb_ids_str = ','.join(\"'{}'\".format(str(x)) for x in kb_ids)\n        query = \"SELECT user_id, kb_id, kb_name FROM KnowledgeBase WHERE kb_id IN ({}) AND deleted = 0\".format(kb_ids_str)\n        return self.execute_query_(query, (), fetch=True)\n    def delete_knowledge_base(self, user_id, kb_ids):\n        kb_ids_str = ','.join(\"'{}'\".format(str(x)) for x in kb_ids)\n        query = \"UPDATE KnowledgeBase SET deleted = 1 WHERE user_id = %s AND kb_id IN ({})\".format(kb_ids_str)\n        self.execute_query_(query, (user_id,), commit=True)\n        query = \"\"\"UPDATE File SET deleted = 1 WHERE kb_id IN ({}) AND kb_id IN (SELECT kb_id FROM KnowledgeBase WHERE user_id = %s)\"\"\".format(kb_ids_str)\n        self.execute_query_(query, (user_id,), commit=True)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-25722",
        "description": "[{'lang': 'en', 'value': 'qanything_kernel/connector/database/mysql/mysql_client.py in qanything.ai QAnything before 1.2.0 allows SQL Injection.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-4",
      "code": "    def run(self):\n        os.chdir(self.config.project_target_path)\n        shutil.copyfile(DOCS_INDEX_FILE_PATH, \"index.html\")\n        port = self.args.port\n        if self.args.browser:\n            webbrowser.open_new_tab(f\"http://localhost:{port}\")\n        with socketserver.TCPServer((\"\", port), SimpleHTTPRequestHandler) as httpd:\n            click.echo(f\"Serving docs at {port}\")\n            click.echo(f\"To access from your browser, navigate to: http://localhost:{port}\")\n            click.echo(\"\\n\\n\")\n            click.echo(\"Press Ctrl+C to exit.\")\n            httpd.serve_forever()",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-36105",
        "description": "[{'lang': 'en', 'value': 'dbt enables data analysts and engineers to transform their data using the same practices that software engineers use to build applications. Prior to versions 1.6.15, 1.7.15, and 1.8.1, Binding to `INADDR_ANY (0.0.0.0)` or `IN6ADDR_ANY (::)` exposes an application on all network interfaces, increasing the risk of unauthorized access. As stated in the Python docs, a special form for address is accepted instead of a host address: `\\'\\'` represents `INADDR_ANY`, equivalent to `\"0.0.0.0\"`. On systems with IPv6, \\'\\' represents `IN6ADDR_ANY`, which is equivalent to `\"::\"`. A user who serves docs on an unsecured public network, may unknowingly be hosting an unsecured (http) web site for any remote user/system to access on the same network. The issue has has been mitigated in dbt-core v1.6.15, dbt-core v1.7.15, and dbt-core v1.8.1 by binding to localhost explicitly by default in `dbt docs serve`.\\n'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-5",
      "code": "def filefind(filename: str, path_dirs: Sequence[str] | str | None = None) -> str:\n    \"\"\"Find a file by looking through a sequence of paths.\n    This iterates through a sequence of paths looking for a file and returns\n    the full, absolute path of the first occurrence of the file.  If no set of\n    path dirs is given, the filename is tested as is, after running through\n    :func:`expandvars` and :func:`expanduser`.  Thus a simple call::\n        filefind(\"myfile.txt\")\n    will find the file in the current working dir, but::\n        filefind(\"~/myfile.txt\")\n    Will find the file in the users home directory.  This function does not\n    automatically try any paths, such as the cwd or the user's home directory.\n    Parameters\n    ----------\n    filename : str\n        The filename to look for.\n    path_dirs : str, None or sequence of str\n        The sequence of paths to look for the file in.  If None, the filename\n        need to be absolute or be in the cwd.  If a string, the string is\n        put into a sequence and the searched.  If a sequence, walk through\n        each element and join with ``filename``, calling :func:`expandvars`\n        and :func:`expanduser` before testing for existence.\n    Returns\n    -------\n    Raises :exc:`IOError` or returns absolute path to file.\n    \"\"\"\n    filename = filename.strip('\"').strip(\"'\")\n    if os.path.isabs(filename) and os.path.isfile(filename):\n        return filename\n    if path_dirs is None:\n        path_dirs = (\"\",)\n    elif isinstance(path_dirs, str):\n        path_dirs = (path_dirs,)\n    for path in path_dirs:\n        if path == \".\":\n            path = os.getcwd()\n        testname = expand_path(os.path.join(path, filename))\n        if os.path.isfile(testname):\n            return os.path.abspath(testname)\n    msg = f\"File {filename!r} does not exist in any of the search paths: {path_dirs!r}\"\n    raise OSError(msg)\ndef expand_path(s: str) -> str:\n    \"\"\"Expand $VARS and ~names in a string, like a shell\n    :Examples:\n       In [2]: os.environ['FOO']='test'\n       In [3]: expand_path('variable FOO is $FOO')\n       Out[3]: 'variable FOO is test'\n    \"\"\"\n    if os.name == \"nt\":\n        s = s.replace(\"$\\\\\", \"IPYTHON_TEMP\")\n    s = os.path.expandvars(os.path.expanduser(s))\n    if os.name == \"nt\":\n        s = s.replace(\"IPYTHON_TEMP\", \"$\\\\\")\n    return s",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-35178",
        "description": "[{'lang': 'en', 'value': 'The Jupyter Server provides the backend for Jupyter web applications. Jupyter Server on Windows has a vulnerability that lets unauthenticated attackers leak the NTLMv2 password hash of the Windows user running the Jupyter server. An attacker can crack this password to gain access to the Windows machine hosting the Jupyter server, or access other network-accessible machines or 3rd party services using that credential. Or an attacker perform an NTLM relay attack without cracking the credential to gain access to other network-accessible machines. This vulnerability is fixed in 2.14.1.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-6",
      "code": "    def download_compressed(self, url, zippath, unzippedpath):\n        \"\"\"Downloads a compressed file and extracts it\n        Args:\n            url (str): download link\n            zippath (str): path to download compressed file\n            unzippedpath (str): path to unzip compressed file\n        \"\"\"\n        response = requests.get(url, stream=True)\n        with open(zippath, \"wb\") as f:\n            f.write(response.raw.read())\n        shutil.unpack_archive(zippath, unzippedpath)\n        os.remove(zippath)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-23530",
        "description": "[{'lang': 'en', 'value': 'GuardDog is a CLI tool to identify malicious PyPI packages. Versions prior to v0.1.8 are vulnerable to arbitrary file write when scanning a specially-crafted remote PyPI package. Extracting files using shutil.unpack_archive() from a potentially malicious tarball without validating that the destination file path is within the intended destination directory can cause files outside the destination directory to be overwritten.  This issue is patched in version 0.1.8. Potential workarounds include using a safer module, like zipfile, and validating the location of the extracted files and discarding those with malicious paths.'}]",
        "cwe_number": 22
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-7",
      "code": "    def get_recipe_from_file(self, file):\n        recipe = Recipe.objects.create(name=file['title'], created_by=self.request.user, internal=True, space=self.request.space, )\n        recipe.description = ''\n        try:\n            if file['description'] != '':\n                recipe.description = file['description'].strip()\n        except Exception as e:\n            print(recipe.name, ': failed to parse recipe description ', str(e))\n        instructions = file['instructions']\n        if not instructions:\n            instructions = ''\n        step = Step.objects.create(instruction=instructions, space=self.request.space,)\n        try:\n            if file['url'] != '':\n                step.instruction += '\\n\\n' + _('Imported from') + ': ' + file['url']\n                step.save()\n        except Exception as e:\n            print(recipe.name, ': failed to import source url ', str(e))\n        try:\n            ingredient_parser = IngredientParser(self.request, True)\n            for ingredient in file['ingredients'].split('\\n'):\n                if len(ingredient.strip()) > 0:\n                    amount, unit, food, note = ingredient_parser.parse(food)\n                    f = ingredient_parser.get_food(ingredient)\n                    u = ingredient_parser.get_unit(unit)\n                    step.ingredients.add(Ingredient.objects.create(\n                        food=f, unit=u, amount=amount, note=note, original_text=ingredient, space=self.request.space,\n                    ))\n        except Exception as e:\n            print(recipe.name, ': failed to parse recipe ingredients ', str(e))\n        recipe.steps.add(step)\n        try:\n            if file['quantity'] != '':\n                for item in file['quantity'].split(' '):\n                    if item.isdigit():\n                        recipe.servings = int(item)\n                        break\n        except Exception as e:\n            print(recipe.name, ': failed to parse quantity ', str(e))\n        try:\n            if file['totalTime'] != '':\n                recipe.waiting_time = int(file['totalTime'])\n        except Exception as e:\n            print(recipe.name, ': failed to parse total times ', str(e))\n        try:\n            if file['preparationTime'] != '':\n                recipe.working_time = int(file['preparationTime'])\n        except Exception as e:\n            print(recipe.name, ': failed to parse prep time ', str(e))\n        try:\n            if file['cookingTime'] != '':\n                recipe.waiting_time = int(file['cookingTime'])\n        except Exception as e:\n            print(recipe.name, ': failed to parse cooking time ', str(e))\n        recipe.save()\n        try:\n            if file['keywords'] != '':\n                for keyword in file['keywords'].split(';'):\n                    k, created = Keyword.objects.get_or_create(name=keyword.strip(), space=self.request.space)\n                    recipe.keywords.add(k)\n            recipe.save()\n        except Exception as e:\n            print(recipe.name, ': failed to parse keywords ', str(e))\n        try:\n            if file['pictures'][0] != '':\n                image_file_name = file['pictures'][0].split('/')[-1]\n                for f in self.files:\n                    if '.rtk' in f['name']:\n                        import_zip = ZipFile(f['file'])\n                        self.import_recipe_image(recipe, BytesIO(import_zip.read(image_file_name)), filetype=get_filetype(image_file_name))\n            else:\n                if file['originalPicture'] != '':\n                    response = requests.get(file['originalPicture'])\n                    if imghdr.what(BytesIO(response.content)) is not None:\n                        self.import_recipe_image(recipe, BytesIO(response.content), filetype=get_filetype(file['originalPicture']))\n                    else:\n                        raise Exception(\"Original image failed to download.\")\n        except Exception as e:\n            print(recipe.name, ': failed to import image ', str(e))\n        return recipe\n    def get_file_from_recipe(self, recipe):\n        raise NotImplementedError('Method not implemented in storage integration')\n    def image(self, request, pk):\n        obj = self.get_object()\n        if obj.get_space() != request.space:\n            raise PermissionDenied(detail='You do not have the required permission to perform this action', code=403)\n        serializer = self.serializer_class(obj, data=request.data, partial=True)\n        if serializer.is_valid():\n            serializer.save()\n            image = None\n            filetype = \".jpeg\"\n            if 'image' in serializer.validated_data:\n                image = obj.image\n                filetype = mimetypes.guess_extension(serializer.validated_data['image'].content_type) or filetype\n            elif 'image_url' in serializer.validated_data:\n                try:\n                    response = requests.get(serializer.validated_data['image_url'])\n                    image = File(io.BytesIO(response.content))\n                    filetype = mimetypes.guess_extension(response.headers['content-type']) or filetype\n                except UnidentifiedImageError as e:\n                    print(e)\n                    pass\n                except MissingSchema as e:\n                    print(e)\n                    pass\n                except Exception as e:\n                    print(e)\n                    pass\n            if image is not None:\n                img = handle_image(request, image, filetype)\n                obj.image = File(img, name=f'{uuid.uuid4()}_{obj.pk}{filetype}')\n                obj.save()\n                return Response(serializer.data)\n        return Response(serializer.errors, 400)\ndef recipe_from_source(request):\n    \"\"\"\n    function to retrieve a recipe from a given url or source string\n    :param request: standard request with additional post parameters\n            - url: url to use for importing recipe\n            - data: if no url is given recipe is imported from provided source data\n            - (optional) bookmarklet: id of bookmarklet import to use, overrides URL and data attributes\n    :return: JsonResponse containing the parsed json, original html,json and images\n    \"\"\"\n    if request.method == 'GET':\n        return HttpResponse(status=405)\n    request_payload = json.loads(request.body.decode('utf-8'))\n    url = request_payload.get('url', None)\n    data = request_payload.get('data', None)\n    bookmarklet = request_payload.get('bookmarklet', None)\n    if bookmarklet := BookmarkletImport.objects.filter(pk=bookmarklet).first():\n        url = bookmarklet.url\n        data = bookmarklet.html\n        bookmarklet.delete()\n    external_request_headers = {\"User-Agent\": \"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7\"}\n    if not url and not data:\n        return JsonResponse({\n            'error': True,\n            'msg': _('Nothing to do.')\n        }, status=400)\n    if url:\n        try:\n            data = requests.get(url, headers=external_request_headers).content\n        except requests.exceptions.ConnectionError:\n            return JsonResponse({\n                'error': True,\n                'msg': _('Connection Refused.')\n            }, status=400)\n        except requests.exceptions.MissingSchema:\n            return JsonResponse({\n                'error': True,\n                'msg': _('Bad URL Schema.')\n            }, status=400)\n    recipe_json, recipe_tree, recipe_html, recipe_images = get_recipe_from_source(data, url, request)\n    if len(recipe_tree) == 0 and len(recipe_json) == 0:\n        return JsonResponse({\n            'error': True,\n            'msg': _('No usable data could be found.')\n        }, status=400)\n    else:\n        return JsonResponse({\n            'recipe_json': recipe_json,\n            'recipe_tree': recipe_tree,\n            'recipe_html': recipe_html,\n            'recipe_images': list(dict.fromkeys(recipe_images)),\n        })\ndef get_backup(request):\n    if not request.user.is_superuser:\n        return HttpResponse('', status=403)\n    def get_recipe_from_file(self, file):\n        recipe_xml = file\n        recipe = Recipe.objects.create(\n            name=recipe_xml.find('title').text.strip(),\n            created_by=self.request.user, internal=True, space=self.request.space)\n        if recipe_xml.find('preptime') is not None and recipe_xml.find('preptime').text is not None:\n            recipe.working_time = parse_time(recipe_xml.find('preptime').text.strip())\n        if recipe_xml.find('cooktime') is not None and recipe_xml.find('cooktime').text is not None:\n            recipe.waiting_time = parse_time(recipe_xml.find('cooktime').text.strip())\n        if recipe_xml.find('quantity') is not None and recipe_xml.find('quantity').text is not None:\n            recipe.servings = parse_servings(recipe_xml.find('quantity').text.strip())\n            recipe.servings_text = parse_servings_text(recipe_xml.find('quantity').text.strip())\n        if recipe_xml.find('url') is not None and recipe_xml.find('url').text is not None:\n            recipe.source_url = recipe_xml.find('url').text.strip()\n        if recipe_xml.find('description') is not None:\n            if len(recipe_xml.find('description')) > 0:\n                recipe.description = recipe_xml.find('description')[0].text[:512]\n        for step in recipe_xml.find('recipetext').getchildren():\n            step = Step.objects.create(\n                instruction=step.text.strip(), space=self.request.space,\n            )\n            recipe.steps.add(step)\n        ingredient_parser = IngredientParser(self.request, True)\n        for ingredient in recipe_xml.find('ingredient').getchildren():\n            if ingredient.text.strip() != '':\n                amount, unit, food, note = ingredient_parser.parse(ingredient.text.strip())\n                f = ingredient_parser.get_food(food)\n                u = ingredient_parser.get_unit(unit)\n                recipe.steps.first().ingredients.add(Ingredient.objects.create(\n                    food=f, unit=u, amount=amount, note=note, original_text=ingredient.text.strip(), space=self.request.space,\n                ))\n        if recipe_xml.find('imageurl') is not None:\n            try:\n                response = requests.get(recipe_xml.find('imageurl').text.strip())\n                self.import_recipe_image(recipe, BytesIO(response.content))\n            except Exception as e:\n                print('failed to import image ', str(e))\n        recipe.save()\n        return recipe\n    def get_file_from_recipe(self, recipe):\n        raise NotImplementedError('Method not implemented in storage integration')\n    def get_recipe_from_file(self, file):\n        recipe = Recipe.objects.create(\n            name=file['name'].strip(),\n            created_by=self.request.user, internal=True,\n            space=self.request.space)\n        try:\n            if file['recipeYield'] != '':\n                recipe.servings = int(file['recipeYield'])\n            if file['totalTime'] != '':\n                recipe.waiting_time = int(file['totalTime']) - int(file['timePrep'])\n            if file['prepTime'] != '':\n                recipe.working_time = int(file['timePrep'])\n            recipe.save()\n        except Exception as e:\n            print('failed to parse yield or time ', str(e))\n        ingredient_parser = IngredientParser(self.request, True)\n        ingredients_added = False\n        for s in file['recipeInstructions']:\n            step = Step.objects.create(\n                instruction=s['text'], space=self.request.space,\n            )\n            if not ingredients_added:\n                ingredients_added = True\n                for ingredient in file['recipeIngredient']:\n                    amount, unit, food, note = ingredient_parser.parse(ingredient)\n                    f = ingredient_parser.get_food(food)\n                    u = ingredient_parser.get_unit(unit)\n                    step.ingredients.add(Ingredient.objects.create(\n                        food=f, unit=u, amount=amount, note=note, original_text=ingredient, space=self.request.space,\n                    ))\n            recipe.steps.add(step)\n        if len(file['image']) > 0:\n            try:\n                response = requests.get(file['image'][0])\n                self.import_recipe_image(recipe, BytesIO(response.content))\n            except Exception as e:\n                print('failed to import image ', str(e))\n        return recipe\n    def get_file_from_recipe(self, recipe):\n        data = {\n            '@context': 'http://schema.org',\n            '@type': 'Recipe',\n            'creditText': '',\n            'isBasedOn': '',\n            'name': recipe.name,\n            'description': recipe.description,\n            'prepTime': str(recipe.working_time),\n            'totalTime': str(recipe.waiting_time + recipe.working_time),\n            'recipeYield': str(recipe.servings),\n            'image': [],\n            'recipeCategory': [],\n            'comment': [],\n            'recipeIngredient': [],\n            'recipeInstructions': [],\n        }\n        for s in recipe.steps.all():\n            data['recipeInstructions'].append({\n                '@type': 'HowToStep',\n                'text': s.instruction\n            })\n            for i in s.ingredients.all():\n                data['recipeIngredient'].append(f'{float(i.amount)} {i.unit} {i.food}')\n        return data\n    def get_recipe_from_file(self, file):\n        recipe_html = file.getvalue().decode(\"utf-8\")\n        recipe_json, recipe_tree, html_data, images = get_recipe_from_source(recipe_html, 'CookBookApp', self.request)\n        recipe = Recipe.objects.create(\n            name=recipe_json['name'].strip(),\n            created_by=self.request.user, internal=True,\n            space=self.request.space)\n        try:\n            recipe.servings = re.findall('([0-9])+', recipe_json['recipeYield'])[0]\n        except Exception as e:\n            pass\n        try:\n            recipe.working_time = iso_duration_to_minutes(recipe_json['prepTime'])\n            recipe.waiting_time = iso_duration_to_minutes(recipe_json['cookTime'])\n        except Exception:\n            pass\n        step = Step.objects.create(instruction=recipe_json['recipeInstructions'], space=self.request.space, )\n        if 'nutrition' in recipe_json:\n            step.instruction = step.instruction + '\\n\\n' + recipe_json['nutrition']\n        step.save()\n        recipe.steps.add(step)\n        ingredient_parser = IngredientParser(self.request, True)\n        for ingredient in recipe_json['recipeIngredient']:\n            f = ingredient_parser.get_food(ingredient['ingredient']['text'])\n            u = ingredient_parser.get_unit(ingredient['unit']['text'])\n            step.ingredients.add(Ingredient.objects.create(\n                food=f, unit=u, amount=ingredient['amount'], note=ingredient['note'],  space=self.request.space,\n            ))\n        if len(images) > 0:\n            try:\n                response = requests.get(images[0])\n                self.import_recipe_image(recipe, BytesIO(response.content))\n            except Exception as e:\n                print('failed to import image ', str(e))\n        recipe.save()\n        return recipe\n    def get_share_link(recipe):\n        url = recipe.storage.url + '/ocs/v2.php/apps/files_sharing/api/v1/shares?format=json&path=' + recipe.file_path\n        headers = {\n            \"OCS-APIRequest\": \"true\",\n            \"Content-Type\": \"application/json\"\n        }\n        r = requests.get(\n            url,\n            headers=headers,\n            auth=HTTPBasicAuth(\n                recipe.storage.username, recipe.storage.password\n            )\n        )\n        response_json = r.json()\n        for element in response_json['ocs']['data']:\n            if element['share_type'] == '3':\n                return element['url']\n        return Nextcloud.create_share_link(recipe)\n    def get_file(recipe):\n        if not recipe.link:\n            recipe.link = Dropbox.get_share_link(recipe)\n            recipe.save()\n        response = requests.get(recipe.link.replace('www.dropbox.', 'dl.dropboxusercontent.'))\n        return io.BytesIO(response.content)\n    def rename_file(recipe, new_name):\n        url = \"https://api.dropboxapi.com/2/files/move_v2\"\n        headers = {\n            \"Authorization\": \"Bearer \" + recipe.storage.token,\n            \"Content-Type\": \"application/json\"\n        }\n        data = {\n            \"from_path\": recipe.file_path,\n            \"to_path\": \"%s/%s%s\" % (\n                os.path.dirname(recipe.file_path),\n                new_name,\n                os.path.splitext(recipe.file_path)[1]\n            )\n        }\n        r = requests.post(url, headers=headers, data=json.dumps(data))\n        return r.json()",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-23071",
        "description": "[{'lang': 'en', 'value': 'In Recipes, versions 0.9.1 through 1.2.5 are vulnerable to Server Side Request Forgery (SSRF), in the \u201cImport Recipe\u201d functionality. When an attacker enters the localhost URL, a low privileged attacker can access/read the internal file system to access sensitive information.'}]",
        "cwe_number": 918
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-8",
      "code": "    def __init__(\n        self,\n        hs: \"HomeServer\",\n        treq_args: Dict[str, Any] = {},\n        ip_whitelist: Optional[IPSet] = None,\n        ip_blacklist: Optional[IPSet] = None,\n        http_proxy: Optional[bytes] = None,\n        https_proxy: Optional[bytes] = None,\n    ):\n        \"\"\"\n        Args:\n            hs\n            treq_args: Extra keyword arguments to be given to treq.request.\n            ip_blacklist: The IP addresses that are blacklisted that\n                we may not request.\n            ip_whitelist: The whitelisted IP addresses, that we can\n               request if it were otherwise caught in a blacklist.\n            http_proxy: proxy server to use for http connections. host[:port]\n            https_proxy: proxy server to use for https connections. host[:port]\n        \"\"\"\n        self.hs = hs\n        self._ip_whitelist = ip_whitelist\n        self._ip_blacklist = ip_blacklist\n        self._extra_treq_args = treq_args\n        self.user_agent = hs.version_string\n        self.clock = hs.get_clock()\n        if hs.config.user_agent_suffix:\n            self.user_agent = \"%s %s\" % (self.user_agent, hs.config.user_agent_suffix)\n        self._cooperator = Cooperator(scheduler=_make_scheduler(hs.get_reactor()))\n        self.user_agent = self.user_agent.encode(\"ascii\")\n        if self._ip_blacklist:\n            real_reactor = hs.get_reactor()\n            nameResolver = IPBlacklistingResolver(\n                real_reactor, self._ip_whitelist, self._ip_blacklist\n            )\n            @implementer(IReactorPluggableNameResolver)\n            class Reactor:\n                def __getattr__(_self, attr):\n                    if attr == \"nameResolver\":\n                        return nameResolver\n                    else:\n                        return getattr(real_reactor, attr)\n            self.reactor = Reactor()\n        else:\n            self.reactor = hs.get_reactor()\n        pool = HTTPConnectionPool(self.reactor)\n        pool.maxPersistentPerHost = max((100 * hs.config.caches.global_factor, 5))\n        pool.cachedConnectionTimeout = 2 * 60\n        self.agent = ProxyAgent(\n            self.reactor,\n            connectTimeout=15,\n            contextFactory=self.hs.get_http_client_context_factory(),\n            pool=pool,\n            http_proxy=http_proxy,\n            https_proxy=https_proxy,\n        )\n        if self._ip_blacklist:\n            self.agent = BlacklistingAgentWrapper(\n                self.agent,\n                ip_whitelist=self._ip_whitelist,\n                ip_blacklist=self._ip_blacklist,\n            )\n    def __init__(self, hs: \"HomeServer\"):\n        super().__init__(hs)\n        self.hs = hs\n        self.store = hs.get_datastore()\n        self.storage = hs.get_storage()\n        self.state_store = self.storage.state\n        self.federation_client = hs.get_federation_client()\n        self.state_handler = hs.get_state_handler()\n        self._state_resolution_handler = hs.get_state_resolution_handler()\n        self.server_name = hs.hostname\n        self.keyring = hs.get_keyring()\n        self.action_generator = hs.get_action_generator()\n        self.is_mine_id = hs.is_mine_id\n        self.spam_checker = hs.get_spam_checker()\n        self.event_creation_handler = hs.get_event_creation_handler()\n        self._message_handler = hs.get_message_handler()\n        self._server_notices_mxid = hs.config.server_notices_mxid\n        self.config = hs.config\n        self.http_client = hs.get_simple_http_client()\n        self._instance_name = hs.get_instance_name()\n        self._replication = hs.get_replication_data_handler()\n        self._send_events = ReplicationFederationSendEventsRestServlet.make_client(hs)\n        self._clean_room_for_join_client = ReplicationCleanRoomRestServlet.make_client(\n            hs\n        )\n        if hs.config.worker_app:\n            self._user_device_resync = ReplicationUserDevicesResyncRestServlet.make_client(\n                hs\n            )\n            self._maybe_store_room_on_outlier_membership = ReplicationStoreRoomOnOutlierMembershipRestServlet.make_client(\n                hs\n            )\n        else:\n            self._device_list_updater = hs.get_device_handler().device_list_updater\n            self._maybe_store_room_on_outlier_membership = (\n                self.store.maybe_store_room_on_outlier_membership\n            )\n        self.room_queues = {}\n        self._room_pdu_linearizer = Linearizer(\"fed_room_pdu\")\n        self.third_party_event_rules = hs.get_third_party_event_rules()\n        self._ephemeral_messages_enabled = hs.config.enable_ephemeral_messages\n    def read_config(self, config, **kwargs):\n        self.federation_domain_whitelist = None\n        federation_domain_whitelist = config.get(\"federation_domain_whitelist\", None)\n        if federation_domain_whitelist is not None:\n            self.federation_domain_whitelist = {}\n            for domain in federation_domain_whitelist:\n                self.federation_domain_whitelist[domain] = True\n        self.federation_ip_range_blacklist = config.get(\n            \"federation_ip_range_blacklist\", []\n        )\n        try:\n            self.federation_ip_range_blacklist = IPSet(\n                self.federation_ip_range_blacklist\n            )\n            self.federation_ip_range_blacklist.update([\"0.0.0.0\", \"::\"])\n        except Exception as e:\n            raise ConfigError(\n                \"Invalid range(s) provided in federation_ip_range_blacklist: %s\" % e\n            )\n        federation_metrics_domains = config.get(\"federation_metrics_domains\") or []\n        validate_config(\n            _METRICS_FOR_DOMAINS_SCHEMA,\n            federation_metrics_domains,\n            (\"federation_metrics_domains\",),\n        )\n        self.federation_metrics_domains = set(federation_metrics_domains)\n    def generate_config_section(self, config_dir_path, server_name, **kwargs):\n        return \"\"\"\\\n        federation_ip_range_blacklist:\n          - '127.0.0.0/8'\n          - '10.0.0.0/8'\n          - '172.16.0.0/12'\n          - '192.168.0.0/16'\n          - '100.64.0.0/10'\n          - '169.254.0.0/16'\n          - '::1/128'\n          - 'fe80::/64'\n          - 'fc00::/7'\n        \"\"\"\n    def __init__(self, hs, tls_client_options_factory):\n        self.hs = hs\n        self.signing_key = hs.signing_key\n        self.server_name = hs.hostname\n        real_reactor = hs.get_reactor()\n        nameResolver = IPBlacklistingResolver(\n            real_reactor, None, hs.config.federation_ip_range_blacklist\n        )\n        @implementer(IReactorPluggableNameResolver)\n        class Reactor:\n            def __getattr__(_self, attr):\n                if attr == \"nameResolver\":\n                    return nameResolver\n                else:\n                    return getattr(real_reactor, attr)\n        self.reactor = Reactor()\n        user_agent = hs.version_string\n        if hs.config.user_agent_suffix:\n            user_agent = \"%s %s\" % (user_agent, hs.config.user_agent_suffix)\n        user_agent = user_agent.encode(\"ascii\")\n        self.agent = MatrixFederationAgent(\n            self.reactor, tls_client_options_factory, user_agent\n        )\n        self.agent = BlacklistingAgentWrapper(\n            self.agent, ip_blacklist=hs.config.federation_ip_range_blacklist,\n        )\n        self.clock = hs.get_clock()\n        self._store = hs.get_datastore()\n        self.version_string_bytes = hs.version_string.encode(\"ascii\")\n        self.default_timeout = 60\n        def schedule(x):\n            self.reactor.callLater(_EPSILON, x)\n        self._cooperator = Cooperator(scheduler=schedule)\n    def __init__(self, hs, pusherdict):\n        self.hs = hs\n        self.store = self.hs.get_datastore()\n        self.storage = self.hs.get_storage()\n        self.clock = self.hs.get_clock()\n        self.state_handler = self.hs.get_state_handler()\n        self.user_id = pusherdict[\"user_name\"]\n        self.app_id = pusherdict[\"app_id\"]\n        self.app_display_name = pusherdict[\"app_display_name\"]\n        self.device_display_name = pusherdict[\"device_display_name\"]\n        self.pushkey = pusherdict[\"pushkey\"]\n        self.pushkey_ts = pusherdict[\"ts\"]\n        self.data = pusherdict[\"data\"]\n        self.last_stream_ordering = pusherdict[\"last_stream_ordering\"]\n        self.backoff_delay = HttpPusher.INITIAL_BACKOFF_SEC\n        self.failing_since = pusherdict[\"failing_since\"]\n        self.timed_call = None\n        self._is_processing = False\n        self._group_unread_count_by_room = hs.config.push_group_unread_count_by_room\n        self.max_stream_ordering = None\n        if \"data\" not in pusherdict:\n            raise PusherConfigException(\"No 'data' key for HTTP pusher\")\n        self.data = pusherdict[\"data\"]\n        self.name = \"%s/%s/%s\" % (\n            pusherdict[\"user_name\"],\n            pusherdict[\"app_id\"],\n            pusherdict[\"pushkey\"],\n        )\n        if self.data is None:\n            raise PusherConfigException(\"data can not be null for HTTP pusher\")\n        if \"url\" not in self.data:\n            raise PusherConfigException(\"'url' required in data for HTTP pusher\")\n        self.url = self.data[\"url\"]\n        self.http_client = hs.get_proxied_http_client()\n        self.data_minus_url = {}\n        self.data_minus_url.update(self.data)\n        del self.data_minus_url[\"url\"]\n    def __init__(self, hs):\n        self.hs = hs\n        self.auth = hs.get_auth()\n        self.client = hs.get_http_client()\n        self.clock = hs.get_clock()\n        self.server_name = hs.hostname\n        self.store = hs.get_datastore()\n        self.max_upload_size = hs.config.max_upload_size\n        self.max_image_pixels = hs.config.max_image_pixels\n        self.primary_base_path = hs.config.media_store_path\n        self.filepaths = MediaFilePaths(self.primary_base_path)\n        self.dynamic_thumbnails = hs.config.dynamic_thumbnails\n        self.thumbnail_requirements = hs.config.thumbnail_requirements\n        self.remote_media_linearizer = Linearizer(name=\"media_remote\")\n        self.recently_accessed_remotes = set()\n        self.recently_accessed_locals = set()\n        self.federation_domain_whitelist = hs.config.federation_domain_whitelist\n        storage_providers = []\n        for clz, provider_config, wrapper_config in hs.config.media_storage_providers:\n            backend = clz(hs, provider_config)\n            provider = StorageProviderWrapper(\n                backend,\n                store_local=wrapper_config.store_local,\n                store_remote=wrapper_config.store_remote,\n                store_synchronous=wrapper_config.store_synchronous,\n            )\n            storage_providers.append(provider)\n        self.media_storage = MediaStorage(\n            self.hs, self.primary_base_path, self.filepaths, storage_providers\n        )\n        self.clock.looping_call(\n            self._start_update_recently_accessed, UPDATE_RECENTLY_ACCESSED_TS\n        )\n    def __init__(\n        self,\n        reactor: IReactorCore,\n        tls_client_options_factory: Optional[FederationPolicyForHTTPS],\n        user_agent: bytes,\n        _srv_resolver: Optional[SrvResolver] = None,\n        _well_known_resolver: Optional[WellKnownResolver] = None,\n    ):\n        self._reactor = reactor\n        self._clock = Clock(reactor)\n        self._pool = HTTPConnectionPool(reactor)\n        self._pool.retryAutomatically = False\n        self._pool.maxPersistentPerHost = 5\n        self._pool.cachedConnectionTimeout = 2 * 60\n        self._agent = Agent.usingEndpointFactory(\n            self._reactor,\n            MatrixHostnameEndpointFactory(\n                reactor, tls_client_options_factory, _srv_resolver\n            ),\n            pool=self._pool,\n        )\n        self.user_agent = user_agent\n        if _well_known_resolver is None:\n            _well_known_resolver = WellKnownResolver(\n                self._reactor,\n                agent=Agent(\n                    self._reactor,\n                    pool=self._pool,\n                    contextFactory=tls_client_options_factory,\n                ),\n                user_agent=self.user_agent,\n            )\n        self._well_known_resolver = _well_known_resolver\n    def __init__(self, hs):\n        super().__init__(hs)\n        self.hs = hs\n        self.is_mine_id = hs.is_mine_id\n        self.http_client = hs.get_simple_http_client()\n        self._presence_enabled = hs.config.use_presence\n        self._user_to_num_current_syncs = {}\n        self.notifier = hs.get_notifier()\n        self.instance_id = hs.get_instance_id()\n        self.users_going_offline = {}\n        self._bump_active_client = ReplicationBumpPresenceActiveTime.make_client(hs)\n        self._set_state_client = ReplicationPresenceSetState.make_client(hs)\n        self._send_stop_syncing_loop = self.clock.looping_call(\n            self.send_stop_syncing, UPDATE_SYNCING_USERS_MS\n        )\n        hs.get_reactor().addSystemEventTrigger(\n            \"before\",\n            \"shutdown\",\n            run_as_background_process,\n            \"generic_presence.on_shutdown\",\n            self._on_shutdown,\n        )\n    def __init__(self, hs: \"HomeServer\"):\n        self.config = hs.config\n        self.http_client = hs.get_simple_http_client()\n        self.clock = hs.get_clock()\n        self._instance_name = hs.get_instance_name()\n        self._get_query_client = ReplicationGetQueryRestServlet.make_client(hs)\n        self._send_edu = ReplicationFederationSendEduRestServlet.make_client(hs)\n        self.edu_handlers = (\n            {}\n        )\n        self.query_handlers = {}\n        self._edu_type_to_instance = {}\n    def __init__(self, hs):\n        super().__init__(hs)\n        self.http_client = SimpleHttpClient(hs)\n        self.blacklisting_http_client = SimpleHttpClient(\n            hs, ip_blacklist=hs.config.federation_ip_range_blacklist\n        )\n        self.federation_http_client = hs.get_http_client()\n        self.hs = hs\n    def __init__(self, hs):\n        super().__init__(hs)\n        self.clock = hs.get_clock()\n        self.client = hs.get_http_client()\n        self.key_servers = self.config.key_servers\n    def get_http_client(self) -> MatrixFederationHttpClient:\n        tls_client_options_factory = context_factory.FederationPolicyForHTTPS(\n            self.config\n        )\n        return MatrixFederationHttpClient(self, tls_client_options_factory)\n    def __init__(self, hs):\n        self.server_name = hs.hostname\n        self.client = hs.get_http_client()",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-21273",
        "description": "[{'lang': 'en', 'value': 'Synapse is a Matrix reference homeserver written in python (pypi package matrix-synapse). Matrix is an ecosystem for open federated Instant Messaging and VoIP. In Synapse before version 1.25.0, requests to user provided domains were not restricted to external IP addresses when calculating the key validity for third-party invite events and sending push notifications. This could cause Synapse to make requests to internal infrastructure. The type of request was not controlled by the user, although limited modification of request bodies was possible. For the most thorough protection server administrators should remove the deprecated `federation_ip_range_blacklist` from their settings after upgrading to Synapse v1.25.0 which will result in Synapse using the improved default IP address restrictions. See the new `ip_range_blacklist` and `ip_range_whitelist` settings if more specific control is necessary.'}]",
        "cwe_number": 601
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-9",
      "code": "  def testGroupInitialization(self):\n    group_size = 2\n    group_key = 100\n    @def_function.function\n    def f():\n      with ops.device('CPU:0'):\n        _collective_ops.initialize_communicator(\n            group_key=group_key, rank=0, group_size=group_size)\n      with ops.device('CPU:1'):\n        _collective_ops.initialize_communicator(\n            group_key=group_key, rank=1, group_size=group_size)\n    self.evaluate(f())\n  def testAllReduceV3(self, device, communication):\n    group_size = 2\n    group_key = 101\n    dev0 = '/device:%s:0' % device\n    dev1 = '/device:%s:1' % device\n    @def_function.function\n    def run_all_reduce_2devices():\n      collectives = []\n      with ops.device(dev0):\n        group_handle0 = _collective_ops.initialize_communicator(\n            group_key=group_key,\n            rank=0,\n            group_size=group_size,\n            communication_hint=communication)\n        collectives.append(\n            _collective_ops.all_reduce_v3(\n                group_handle0, [1.0], reduction='Add'))\n      with ops.device(dev1):\n        group_handle1 = _collective_ops.initialize_communicator(\n            group_key=group_key,\n            rank=1,\n            group_size=group_size,\n            communication_hint=communication)\n        collectives.append(\n            _collective_ops.all_reduce_v3(\n                group_handle1, [2.0], reduction='Add'))\n      return collectives\n    for result in run_all_reduce_2devices():\n      self.assertAllClose(result, [3.], rtol=1e-5, atol=1e-5)\n  def testAllToAllV3(self, device, communication):\n    group_size = 2\n    group_key = 104\n    dev0 = '/device:%s:0' % device\n    dev1 = '/device:%s:1' % device\n    @def_function.function\n    def run_all_to_all_2devices():\n      collectives = []\n      with ops.device(dev0):\n        group_handle0 = _collective_ops.initialize_communicator(\n            group_key=group_key,\n            rank=0,\n            group_size=group_size,\n            communication_hint=communication)\n        collectives.append(\n            _collective_ops.all_to_all_v3(group_handle0, [1.0, 3.0]))\n      with ops.device(dev1):\n        group_handle1 = _collective_ops.initialize_communicator(\n            group_key=group_key,\n            rank=1,\n            group_size=group_size,\n            communication_hint=communication)\n        collectives.append(\n            _collective_ops.all_to_all_v3(group_handle1, [2.0, 4.0]))\n      return collectives\n    result = run_all_to_all_2devices()\n    self.assertAllClose(result[0], [1.0, 2.0], rtol=1e-5, atol=1e-5)\n    self.assertAllClose(result[1], [3.0, 4.0], rtol=1e-5, atol=1e-5)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-41220",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. In affected versions the async implementation of `CollectiveReduceV2` suffers from a memory leak and a use after free. This occurs due to the asynchronous computation and the fact that objects that have been `std::move()`d from are still accessed. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, as this version is the only one that is also affected.'}]",
        "cwe_number": 416
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-10",
      "code": "def execute_cmd(cmd, from_async=False):\n\t\"\"\"execute a request as python module\"\"\"\n\tfor hook in frappe.get_hooks(\"override_whitelisted_methods\", {}).get(cmd, []):\n\t\tcmd = hook\n\t\tbreak\n\tif run_server_script_api(cmd):\n\t\treturn None\n\ttry:\n\t\tmethod = get_attr(cmd)\n\texcept Exception as e:\n\t\tif frappe.local.conf.developer_mode:\n\t\t\traise e\n\t\telse:\n\t\t\tfrappe.respond_as_web_page(title='Invalid Method', html='Method not found',\n\t\t\tindicator_color='red', http_status_code=404)\n\t\treturn\n\tif from_async:\n\t\tmethod = method.queue\n\tis_whitelisted(method)\n\tis_valid_http_method(method)\n\treturn frappe.call(method, **frappe.form_dict)\ndef is_valid_http_method(method):\n\thttp_method = frappe.local.request.method\n\tif http_method not in frappe.allowed_http_methods_for_whitelisted_func[method]:\n\t\tfrappe.throw(_(\"Not permitted\"), frappe.PermissionError)\ndef upload_file():\n\tuser = None\n\tif frappe.session.user == 'Guest':\n\t\tif frappe.get_system_settings('allow_guests_to_upload_files'):\n\t\t\tignore_permissions = True\n\t\telse:\n\t\t\treturn\n\telse:\n\t\tuser = frappe.get_doc(\"User\", frappe.session.user)\n\t\tignore_permissions = False\n\tfiles = frappe.request.files\n\tis_private = frappe.form_dict.is_private\n\tdoctype = frappe.form_dict.doctype\n\tdocname = frappe.form_dict.docname\n\tfieldname = frappe.form_dict.fieldname\n\tfile_url = frappe.form_dict.file_url\n\tfolder = frappe.form_dict.folder or 'Home'\n\tmethod = frappe.form_dict.method\n\tcontent = None\n\tfilename = None\n\tif 'file' in files:\n\t\tfile = files['file']\n\t\tcontent = file.stream.read()\n\t\tfilename = file.filename\n\tfrappe.local.uploaded_file = content\n\tfrappe.local.uploaded_filename = filename\n\tif frappe.session.user == 'Guest' or (user and not user.has_desk_access()):\n\t\timport mimetypes\n\t\tfiletype = mimetypes.guess_type(filename)[0]\n\t\tif filetype not in ALLOWED_MIMETYPES:\n\t\t\tfrappe.throw(_(\"You can only upload JPG, PNG, PDF, or Microsoft documents.\"))\n\tif method:\n\t\tmethod = frappe.get_attr(method)\n\t\tis_whitelisted(method)\n\t\treturn method()\n\telse:\n\t\tret = frappe.get_doc({\n\t\t\t\"doctype\": \"File\",\n\t\t\t\"attached_to_doctype\": doctype,\n\t\t\t\"attached_to_name\": docname,\n\t\t\t\"attached_to_field\": fieldname,\n\t\t\t\"folder\": folder,\n\t\t\t\"file_name\": filename,\n\t\t\t\"file_url\": file_url,\n\t\t\t\"is_private\": cint(is_private),\n\t\t\t\"content\": content\n\t\t})\n\t\tret.save(ignore_permissions=ignore_permissions)\n\t\treturn ret\ndef get_attr(cmd):\n\t\"\"\"get method object from cmd\"\"\"\n\tif '.' in cmd:\n\t\tmethod = frappe.get_attr(cmd)\n\telse:\n\t\tmethod = globals()[cmd]\n\tfrappe.log(\"method:\" + cmd)\n\treturn method\ndef ping():\n\treturn \"pong\"\ndef whitelist(allow_guest=False, xss_safe=False, methods=None):\n\t\"\"\"\n\tDecorator for whitelisting a function and making it accessible via HTTP.\n\tStandard request will be `/api/method/[path.to.method]`\n\t:param allow_guest: Allow non logged-in user to access this method.\n\t:param methods: Allowed http method to access the method.\n\tUse as:\n\t\t@frappe.whitelist()\n\t\tdef myfunc(param1, param2):\n\t\t\tpass\n\t\"\"\"\n\tif not methods:\n\t\tmethods = ['GET', 'POST', 'PUT', 'DELETE']\n\tdef innerfn(fn):\n\t\tglobal whitelisted, guest_methods, xss_safe_methods, allowed_http_methods_for_whitelisted_func\n\t\twhitelisted.append(fn)\n\t\tallowed_http_methods_for_whitelisted_func[fn] = methods\n\t\tif allow_guest:\n\t\t\tguest_methods.append(fn)\n\t\t\tif xss_safe:\n\t\t\t\txss_safe_methods.append(fn)\n\t\treturn fn\n\treturn innerfn\ndef read_only():\n\tdef innfn(fn):\n\t\tdef wrapper_fn(*args, **kwargs):\n\t\t\tif conf.read_from_replica:\n\t\t\t\tconnect_replica()\n\t\t\ttry:\n\t\t\t\tretval = fn(*args, **get_newargs(fn, kwargs))\n\t\t\texcept:\n\t\t\t\traise\n\t\t\tfinally:\n\t\t\t\tif local and hasattr(local, 'primary_db'):\n\t\t\t\t\tlocal.db.close()\n\t\t\t\t\tlocal.db = local.primary_db\n\t\t\treturn retval\n\t\treturn wrapper_fn\n\treturn innfn\ndef validate_and_sanitize_search_inputs(fn, instance, args, kwargs):\n\tkwargs.update(dict(zip(fn.__code__.co_varnames, args)))\n\tsanitize_searchfield(kwargs['searchfield'])\n\tkwargs['start'] = cint(kwargs['start'])\n\tkwargs['page_len'] = cint(kwargs['page_len'])\n\tif kwargs['doctype'] and not frappe.db.exists('DocType', kwargs['doctype']):\n\t\treturn []\n\treturn fn(**kwargs)\n\tdef whitelist(f):\n\t\t\"\"\"Decorator: Whitelist method to be called remotely via REST API.\"\"\"\n\t\tf.whitelisted = True\n\t\treturn f\n\tdef is_whitelisted(self, method):\n\t\tfn = getattr(self, method, None)\n\t\tif not fn:\n\t\t\traise NotFound(\"Method {0} not found\".format(method))\n\t\telif not getattr(fn, \"whitelisted\", False):\n\t\t\traise Forbidden(\"Method {0} not whitelisted\".format(method))",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-23057",
        "description": "[{'lang': 'en', 'value': 'In ERPNext, versions v12.0.9--v13.0.3 are vulnerable to Stored Cross-Site-Scripting (XSS), due to user input not being validated properly. A low privileged attacker could inject arbitrary code into input fields when editing his profile.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-11",
      "code": "def search(request):\n\tcontext_dict = {}\n\tif 'q' in request.GET and request.GET['q'] != '':\n\t\tq = request.GET['q']\n\t\tcursor = connection.cursor()\n\t\tcursor.execute(\"SELECT id,title,artist,cover FROM recordstoreapp_record WHERE title like '%\" + q + \"%' or artist like '%\" + q + \"%' or label like '%\" + q + \"%' or cat_no like '%\" + q + \"%';\")\n\t\trec_list=cursor.fetchall()\n\t\ttotal=len(rec_list)\n\t\tpg=int(request.GET['page']) if 'page' in request.GET else 1\n\t\tub=min(pg*12, total)\n\t\tcontext_dict['rec_list'] = rec_list[(pg-1)*12:ub]\n\t\tmaxrange = int(total/12)\n\t\tif total%12 > 0:\n\t\t\tmaxrange = maxrange + 1\n\t\tif maxrange == 1:\n\t\t\tmaxrange = 0\n\t\tcontext_dict['range'] = range(1,maxrange+1)\n\t\tprint total\n\t\tcontext_dict['q'] = q\n\treturn render(request, 'search.html', context_dict)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2015-10056",
        "description": "[{'lang': 'en', 'value': 'A vulnerability was found in 2071174A vinylmap. It has been classified as critical. Affected is the function contact of the file recordstoreapp/views.py. The manipulation leads to sql injection. The name of the patch is b07b79a1e92cc62574ba0492cce000ef4a7bd25f. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-218400.'}]",
        "cwe_number": 89
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-12",
      "code": "    def init_handle(self):\n        \"\"\"\n        sets common options to curl handle.\n        \"\"\"\n        self.c.setopt(pycurl.FOLLOWLOCATION, 1)\n        self.c.setopt(pycurl.MAXREDIRS, 10)\n        self.c.setopt(pycurl.CONNECTTIMEOUT, 30)\n        self.c.setopt(pycurl.NOSIGNAL, 1)\n        self.c.setopt(pycurl.NOPROGRESS, 1)\n        if hasattr(pycurl, \"AUTOREFERER\"):\n            self.c.setopt(pycurl.AUTOREFERER, 1)\n        self.c.setopt(pycurl.SSL_VERIFYPEER, 0)\n        self.c.setopt(pycurl.LOW_SPEED_TIME, 60)\n        self.c.setopt(pycurl.LOW_SPEED_LIMIT, 5)\n        if hasattr(pycurl, \"USE_SSL\"):\n            self.c.setopt(pycurl.USE_SSL, pycurl.USESSL_TRY)\n        self.c.setopt(\n            pycurl.USERAGENT,\n            b\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36\",\n        )\n        if pycurl.version_info()[7]:\n            self.c.setopt(pycurl.ENCODING, b\"gzip, deflate\")\n        self.c.setopt(\n            pycurl.HTTPHEADER,\n            [\n                b\"Accept: */*\",\n                b\"Accept-Language: en-US,en\",\n                b\"Accept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7\",\n                b\"Connection: keep-alive\",\n                b\"Keep-Alive: 300\",\n                b\"Expect:\",\n            ],\n        )\n    def set_interface(self, options):\n        options = {\n            k: v.encode() if hasattr(v, \"encode\") else v for k, v in options.items()\n        }\n        interface, proxy, ipv6 = (\n            options[\"interface\"],\n            options[\"proxies\"],\n            options[\"ipv6\"],\n        )\n        if interface and interface.lower() != \"none\":\n            self.c.setopt(pycurl.INTERFACE, interface)\n        if proxy:\n            if proxy[\"type\"] == \"http\":\n                self.c.setopt(pycurl.PROXYTYPE, pycurl.PROXYTYPE_HTTP)\n            elif proxy[\"type\"] == \"https\":\n                self.c.setopt(pycurl.PROXYTYPE, pycurl.PROXYTYPE_HTTPS)\n                self.c.setopt(pycurl.PROXY_SSL_VERIFYPEER, 0)\n            elif proxy[\"type\"] == \"socks4\":\n                self.c.setopt(pycurl.PROXYTYPE, pycurl.PROXYTYPE_SOCKS4)\n            elif proxy[\"type\"] == \"socks5\":\n                self.c.setopt(pycurl.PROXYTYPE, pycurl.PROXYTYPE_SOCKS5)\n            self.c.setopt(pycurl.PROXY, proxy[\"host\"])\n            self.c.setopt(pycurl.PROXYPORT, int(proxy[\"port\"]))\n            if proxy[\"username\"]:\n                user = proxy[\"username\"]\n                pw = proxy[\"password\"]\n                self.c.setopt(pycurl.PROXYUSERPWD, f\"{user}:{pw}\".encode())\n        if ipv6:\n            self.c.setopt(pycurl.IPRESOLVE, pycurl.IPRESOLVE_WHATEVER)\n        else:\n            self.c.setopt(pycurl.IPRESOLVE, pycurl.IPRESOLVE_V4)\n        if \"auth\" in options:\n            self.c.setopt(pycurl.USERPWD, options[\"auth\"])\n        if \"timeout\" in options:\n            self.c.setopt(pycurl.LOW_SPEED_TIME, int(options[\"timeout\"]))\n    def add_cookies(self):\n        \"\"\"\n        put cookies from curl handle to cj.\n        \"\"\"\n        if self.cj:\n            self.cj.add_cookies(self.c.getinfo(pycurl.INFO_COOKIELIST))\n    def get_options(self):\n        \"\"\"\n        returns options needed for pycurl.\n        \"\"\"\n        return {\n            \"interface\": self.iface(),\n            \"proxies\": self.get_proxies(),\n            \"ipv6\": self.pyload.config.get(\"download\", \"ipv6\"),\n        }",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-0509",
        "description": "[{'lang': 'en', 'value': 'Improper Certificate Validation in GitHub repository pyload/pyload prior to 0.5.0b3.dev44.'}]",
        "cwe_number": 295
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-13",
      "code": "    def _load_yamlconfig(self, configfile):\n        yamlconfig = None\n        try:\n            if self._recent_pyyaml():\n                yamlconfig = yaml.load(open(configfile), Loader=yaml.FullLoader)\n            else:\n                yamlconfig = yaml.load(open(configfile))\n        except yaml.YAMLError as exc:\n            logger.error(\"Error in configuration file {0}:\".format(configfile))\n            if hasattr(exc, 'problem_mark'):\n                mark = exc.problem_mark\n                raise PystemonConfigException(\"error position: (%s:%s)\" % (mark.line + 1, mark.column + 1))\n        for includes in yamlconfig.get(\"includes\", []):\n            try:\n                logger.debug(\"loading include '{0}'\".format(includes))\n                yamlconfig.update(yaml.load(open(includes)))\n            except Exception as e:\n                raise PystemonConfigException(\"failed to load '{0}': {1}\".format(includes, e))\n        return yamlconfig",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-27213",
        "description": "[{'lang': 'en', 'value': 'config.py in pystemon before 2021-02-13 allows code execution via YAML deserialization because SafeLoader and safe_load are not used.'}]",
        "cwe_number": 502
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-14",
      "code": "def is_local_uri(uri, is_tracking_or_registry_uri=True):\n    \"\"\"\n    Returns true if the specified URI is a local file path (/foo or file:/foo).\n    :param uri: The URI.\n    :param is_tracking_uri: Whether or not the specified URI is an MLflow Tracking or MLflow\n                            Model Registry URI. Examples of other URIs are MLflow artifact URIs,\n                            filesystem paths, etc.\n    \"\"\"\n    if uri == \"databricks\" and is_tracking_or_registry_uri:\n        return False\n    if is_windows() and uri.startswith(\"\\\\\\\\\"):\n        return False\n    parsed_uri = urllib.parse.urlparse(uri)\n    if parsed_uri.hostname and not (\n        parsed_uri.hostname == \".\"\n        or parsed_uri.hostname.startswith(\"localhost\")\n        or parsed_uri.hostname.startswith(\"127.0.0.1\")\n    ):\n        return False\n    scheme = parsed_uri.scheme\n    if scheme == \"\" or scheme == \"file\":\n        return True\n    if is_windows() and len(scheme) == 1 and scheme.lower() == pathlib.Path(uri).drive.lower()[0]:\n        return True\n    return False",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-6977",
        "description": "[{'lang': 'en', 'value': 'This vulnerability enables malicious users to read sensitive files on the server.'}]",
        "cwe_number": 29
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-15",
      "code": "def call_with_ns(f, ns, arg=1):\n    td = Rtd()\n    this = ns.get('context', ns.get('here'))\n    td.this = this\n    request = ns.get('request', {})\n    td._push(request)\n    td._push(InstanceDict(td.this, td))\n    td._push(ns)\n    try:\n        if arg==2:\n            return f(None, td)\n        else:\n            return f(td)\n    finally:\n        td._pop(3)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2009-5145",
        "description": "[{'lang': 'en', 'value': 'Cross-site scripting (XSS) vulnerability in ZMI pages that use the manage_tabs_message in Zope 2.11.4, 2.11.2, 2.10.9, 2.10.7, 2.10.6, 2.10.5, 2.10.4, 2.10.2, 2.10.1, 2.12.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-16",
      "code": "def check_mounted(secdir):\n    whatsmounted = cmd_exec.run(\"mount\")['retout']\n    whatsmounted_converted = config.convert(whatsmounted)\n    for line in whatsmounted_converted:\n        tokens = line.split()\n        tmpfs = False\n        if len(tokens) < 3:\n            continue\n        if tokens[0] == 'tmpfs':\n            tmpfs = True\n        if tokens[2] == secdir:\n            if not tmpfs:\n                logger.error(\"secure storage location %s already mounted \"\n                             \"on wrong file system type: %s.  Unmount to\"\n                             \"continue.\", secdir, tokens[0])\n                raise Exception(\n                    f\"secure storage location {secdir} already mounted on \"\n                    f\"wrong file system type: {tokens[0]}.  Unmount to \"\n                    f\"continue.\")\n            logger.debug(\n                \"secure storage location %s already mounted on tmpfs\" % secdir)\n            return True\n    logger.debug(\"secure storage location %s not mounted \" % secdir)\n    return False\ndef mount():\n    secdir = config.WORK_DIR + \"/secure\"\n    if not config.MOUNT_SECURE:\n        secdir = config.WORK_DIR + \"/tmpfs-dev\"\n        if not os.path.isdir(secdir):\n            os.makedirs(secdir)\n        return secdir\n    if not check_mounted(secdir):\n        if not os.path.exists(secdir):\n            os.makedirs(secdir, 0o700)\n        config.chownroot(secdir, logger)\n        size = config.get('cloud_agent', 'secure_size')\n        logger.info(\"mounting secure storage location %s on tmpfs\" % secdir)\n        cmd = ('mount', '-t', 'tmpfs', '-o', 'size=%s,mode=0700' % size,\n               'tmpfs', secdir)\n        cmd_exec.run(cmd)\n    return secdir",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-23948",
        "description": "[{'lang': 'en', 'value': 'A flaw was found in Keylime before 6.3.0. The logic in the Keylime agent for checking for a secure mount can be fooled by previously created unprivileged mounts allowing secrets to be leaked to other processes on the host.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-17",
      "code": "def shutdown():\n    task = int(request.args.get(\"parameter\").strip())\n    showtext = {}\n    if task in (0, 1):\n        calibre_db.dispose()\n        ub.dispose()\n        if task == 0:\n            showtext['text'] = _(u'Server restarted, please reload page')\n        else:\n            showtext['text'] = _(u'Performing shutdown of server, please close window')\n        web_server.stop(task == 0)\n        return json.dumps(showtext)\n    if task == 2:\n        log.warning(\"reconnecting to calibre database\")\n        calibre_db.reconnect_db(config, ub.app_DB_path)\n        showtext['text'] = _(u'Reconnect successful')\n        return json.dumps(showtext)\n    showtext['text'] = _(u'Unknown command')\n    return json.dumps(showtext), 400\ndef delete_shelf(shelf_id):\n    cur_shelf = ub.session.query(ub.Shelf).filter(ub.Shelf.id == shelf_id).first()\n    try:\n        delete_shelf_helper(cur_shelf)\n    except InvalidRequestError:\n        ub.session.rollback()\n        log.error(\"Settings DB is not Writeable\")\n        flash(_(\"Settings DB is not Writeable\"), category=\"error\")\n    return redirect(url_for('web.index'))",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-4164",
        "description": "[{'lang': 'en', 'value': 'calibre-web is vulnerable to Cross-Site Request Forgery (CSRF)'}]",
        "cwe_number": 352
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-18",
      "code": "def main():\n    import gradio as gr\n    if gr.__version__ not in ['3.28.3','3.32.2']: assert False, \"\u8bf7\u7528 pip install -r requirements.txt \u5b89\u88c5\u4f9d\u8d56\"\n    from request_llm.bridge_all import predict\n    from toolbox import format_io, find_free_port, on_file_uploaded, on_report_generated, get_conf, ArgsGeneralWrapper, DummyWith\n    proxies, WEB_PORT, LLM_MODEL, CONCURRENT_COUNT, AUTHENTICATION, CHATBOT_HEIGHT, LAYOUT, API_KEY, AVAIL_LLM_MODELS = \\\n        get_conf('proxies', 'WEB_PORT', 'LLM_MODEL', 'CONCURRENT_COUNT', 'AUTHENTICATION', 'CHATBOT_HEIGHT', 'LAYOUT', 'API_KEY', 'AVAIL_LLM_MODELS')\n    PORT = find_free_port() if WEB_PORT <= 0 else WEB_PORT\n    if not AUTHENTICATION: AUTHENTICATION = None\n    from check_proxy import get_current_version\n    initial_prompt = \"Serve me as a writing and programming assistant.\"\n    title_html = f\"<h1 align=\\\"center\\\">ChatGPT \u5b66\u672f\u4f18\u5316 {get_current_version()}</h1>\"\n    description =  \"\"\"\u4ee3\u7801\u5f00\u6e90\u548c\u66f4\u65b0[\u5730\u5740\ud83d\ude80](https://github.com/binary-husky/chatgpt_academic)\uff0c\u611f\u8c22\u70ed\u60c5\u7684[\u5f00\u53d1\u8005\u4eec\u2764\ufe0f](https://github.com/binary-husky/chatgpt_academic/graphs/contributors)\"\"\"\n    import logging\n    os.makedirs(\"gpt_log\", exist_ok=True)\n    try:logging.basicConfig(filename=\"gpt_log/chat_secrets.log\", level=logging.INFO, encoding=\"utf-8\")\n    except:logging.basicConfig(filename=\"gpt_log/chat_secrets.log\", level=logging.INFO)\n    print(\"\u6240\u6709\u95ee\u8be2\u8bb0\u5f55\u5c06\u81ea\u52a8\u4fdd\u5b58\u5728\u672c\u5730\u76ee\u5f55./gpt_log/chat_secrets.log, \u8bf7\u6ce8\u610f\u81ea\u6211\u9690\u79c1\u4fdd\u62a4\u54e6\uff01\")\n    from core_functional import get_core_functions\n    functional = get_core_functions()\n    from crazy_functional import get_crazy_functions\n    crazy_fns = get_crazy_functions()\n    gr.Chatbot.postprocess = format_io\n    from theme import adjust_theme, advanced_css\n    set_theme = adjust_theme()\n    from check_proxy import check_proxy, auto_update, warm_up_modules\n    proxy_info = check_proxy(proxies)\n    gr_L1 = lambda: gr.Row().style()\n    gr_L2 = lambda scale: gr.Column(scale=scale)\n    if LAYOUT == \"TOP-DOWN\":\n        gr_L1 = lambda: DummyWith()\n        gr_L2 = lambda scale: gr.Row()\n        CHATBOT_HEIGHT /= 2\n    cancel_handles = []\n    with gr.Blocks(title=\"ChatGPT \u5b66\u672f\u4f18\u5316\", theme=set_theme, analytics_enabled=False, css=advanced_css) as demo:\n        gr.HTML(title_html)\n        cookies = gr.State({'api_key': API_KEY, 'llm_model': LLM_MODEL})\n        with gr_L1():\n            with gr_L2(scale=2):\n                chatbot = gr.Chatbot(label=f\"\u5f53\u524d\u6a21\u578b\uff1a{LLM_MODEL}\")\n                chatbot.style(height=CHATBOT_HEIGHT)\n                history = gr.State([])\n            with gr_L2(scale=1):\n                with gr.Accordion(\"\u8f93\u5165\u533a\", open=True) as area_input_primary:\n                    with gr.Row():\n                        txt = gr.Textbox(show_label=False, placeholder=\"Input question here.\").style(container=False)\n                    with gr.Row():\n                        submitBtn = gr.Button(\"\u63d0\u4ea4\", variant=\"primary\")\n                    with gr.Row():\n                        resetBtn = gr.Button(\"\u91cd\u7f6e\", variant=\"secondary\"); resetBtn.style(size=\"sm\")\n                        stopBtn = gr.Button(\"\u505c\u6b62\", variant=\"secondary\"); stopBtn.style(size=\"sm\")\n                        clearBtn = gr.Button(\"\u6e05\u9664\", variant=\"secondary\", visible=False); clearBtn.style(size=\"sm\")\n                    with gr.Row():\n                        status = gr.Markdown(f\"Tip: \u6309Enter\u63d0\u4ea4, \u6309Shift+Enter\u6362\u884c\u3002\u5f53\u524d\u6a21\u578b: {LLM_MODEL} \\n {proxy_info}\")\n                with gr.Accordion(\"\u57fa\u7840\u529f\u80fd\u533a\", open=True) as area_basic_fn:\n                    with gr.Row():\n                        for k in functional:\n                            if (\"Visible\" in functional[k]) and (not functional[k][\"Visible\"]): continue\n                            variant = functional[k][\"Color\"] if \"Color\" in functional[k] else \"secondary\"\n                            functional[k][\"Button\"] = gr.Button(k, variant=variant)\n                with gr.Accordion(\"\u51fd\u6570\u63d2\u4ef6\u533a\", open=True) as area_crazy_fn:\n                    with gr.Row():\n                        gr.Markdown(\"\u6ce8\u610f\uff1a\u4ee5\u4e0b\u201c\u7ea2\u989c\u8272\u201d\u6807\u8bc6\u7684\u51fd\u6570\u63d2\u4ef6\u9700\u4ece\u8f93\u5165\u533a\u8bfb\u53d6\u8def\u5f84\u4f5c\u4e3a\u53c2\u6570.\")\n                    with gr.Row():\n                        for k in crazy_fns:\n                            if not crazy_fns[k].get(\"AsButton\", True): continue\n                            variant = crazy_fns[k][\"Color\"] if \"Color\" in crazy_fns[k] else \"secondary\"\n                            crazy_fns[k][\"Button\"] = gr.Button(k, variant=variant)\n                            crazy_fns[k][\"Button\"].style(size=\"sm\")\n                    with gr.Row():\n                        with gr.Accordion(\"\u66f4\u591a\u51fd\u6570\u63d2\u4ef6\", open=True):\n                            dropdown_fn_list = [k for k in crazy_fns.keys() if not crazy_fns[k].get(\"AsButton\", True)]\n                            with gr.Row():\n                                dropdown = gr.Dropdown(dropdown_fn_list, value=r\"\u6253\u5f00\u63d2\u4ef6\u5217\u8868\", label=\"\").style(container=False)\n                            with gr.Row():\n                                plugin_advanced_arg = gr.Textbox(show_label=True, label=\"\u9ad8\u7ea7\u53c2\u6570\u8f93\u5165\u533a\", visible=False,\n                                                                 placeholder=\"\u8fd9\u91cc\u662f\u7279\u6b8a\u51fd\u6570\u63d2\u4ef6\u7684\u9ad8\u7ea7\u53c2\u6570\u8f93\u5165\u533a\").style(container=False)\n                            with gr.Row():\n                                switchy_bt = gr.Button(r\"\u8bf7\u5148\u4ece\u63d2\u4ef6\u5217\u8868\u4e2d\u9009\u62e9\", variant=\"secondary\")\n                    with gr.Row():\n                        with gr.Accordion(\"\u70b9\u51fb\u5c55\u5f00\u201c\u6587\u4ef6\u4e0a\u4f20\u533a\u201d\u3002\u4e0a\u4f20\u672c\u5730\u6587\u4ef6\u53ef\u4f9b\u7ea2\u8272\u51fd\u6570\u63d2\u4ef6\u8c03\u7528\u3002\", open=False) as area_file_up:\n                            file_upload = gr.Files(label=\"\u4efb\u4f55\u6587\u4ef6, \u4f46\u63a8\u8350\u4e0a\u4f20\u538b\u7f29\u6587\u4ef6(zip, tar)\", file_count=\"multiple\")\n                with gr.Accordion(\"\u66f4\u6362\u6a21\u578b & SysPrompt & \u4ea4\u4e92\u754c\u9762\u5e03\u5c40\", open=(LAYOUT == \"TOP-DOWN\")):\n                    system_prompt = gr.Textbox(show_label=True, placeholder=f\"System Prompt\", label=\"System prompt\", value=initial_prompt)\n                    top_p = gr.Slider(minimum=-0, maximum=1.0, value=1.0, step=0.01,interactive=True, label=\"Top-p (nucleus sampling)\",)\n                    temperature = gr.Slider(minimum=-0, maximum=2.0, value=1.0, step=0.01, interactive=True, label=\"Temperature\",)\n                    max_length_sl = gr.Slider(minimum=256, maximum=4096, value=512, step=1, interactive=True, label=\"Local LLM MaxLength\",)\n                    checkboxes = gr.CheckboxGroup([\"\u57fa\u7840\u529f\u80fd\u533a\", \"\u51fd\u6570\u63d2\u4ef6\u533a\", \"\u5e95\u90e8\u8f93\u5165\u533a\", \"\u8f93\u5165\u6e05\u9664\u952e\", \"\u63d2\u4ef6\u53c2\u6570\u533a\"], value=[\"\u57fa\u7840\u529f\u80fd\u533a\", \"\u51fd\u6570\u63d2\u4ef6\u533a\"], label=\"\u663e\u793a/\u9690\u85cf\u529f\u80fd\u533a\")\n                    md_dropdown = gr.Dropdown(AVAIL_LLM_MODELS, value=LLM_MODEL, label=\"\u66f4\u6362LLM\u6a21\u578b/\u8bf7\u6c42\u6e90\").style(container=False)\n                    gr.Markdown(description)\n                with gr.Accordion(\"\u5907\u9009\u8f93\u5165\u533a\", open=True, visible=False) as area_input_secondary:\n                    with gr.Row():\n                        txt2 = gr.Textbox(show_label=False, placeholder=\"Input question here.\", label=\"\u8f93\u5165\u533a2\").style(container=False)\n                    with gr.Row():\n                        submitBtn2 = gr.Button(\"\u63d0\u4ea4\", variant=\"primary\")\n                    with gr.Row():\n                        resetBtn2 = gr.Button(\"\u91cd\u7f6e\", variant=\"secondary\"); resetBtn2.style(size=\"sm\")\n                        stopBtn2 = gr.Button(\"\u505c\u6b62\", variant=\"secondary\"); stopBtn2.style(size=\"sm\")\n                        clearBtn2 = gr.Button(\"\u6e05\u9664\", variant=\"secondary\", visible=False); clearBtn2.style(size=\"sm\")\n        def fn_area_visibility(a):\n            ret = {}\n            ret.update({area_basic_fn: gr.update(visible=(\"\u57fa\u7840\u529f\u80fd\u533a\" in a))})\n            ret.update({area_crazy_fn: gr.update(visible=(\"\u51fd\u6570\u63d2\u4ef6\u533a\" in a))})\n            ret.update({area_input_primary: gr.update(visible=(\"\u5e95\u90e8\u8f93\u5165\u533a\" not in a))})\n            ret.update({area_input_secondary: gr.update(visible=(\"\u5e95\u90e8\u8f93\u5165\u533a\" in a))})\n            ret.update({clearBtn: gr.update(visible=(\"\u8f93\u5165\u6e05\u9664\u952e\" in a))})\n            ret.update({clearBtn2: gr.update(visible=(\"\u8f93\u5165\u6e05\u9664\u952e\" in a))})\n            ret.update({plugin_advanced_arg: gr.update(visible=(\"\u63d2\u4ef6\u53c2\u6570\u533a\" in a))})\n            if \"\u5e95\u90e8\u8f93\u5165\u533a\" in a: ret.update({txt: gr.update(value=\"\")})\n            return ret\n        checkboxes.select(fn_area_visibility, [checkboxes], [area_basic_fn, area_crazy_fn, area_input_primary, area_input_secondary, txt, txt2, clearBtn, clearBtn2, plugin_advanced_arg] )\n        input_combo = [cookies, max_length_sl, md_dropdown, txt, txt2, top_p, temperature, chatbot, history, system_prompt, plugin_advanced_arg]\n        output_combo = [cookies, chatbot, history, status]\n        predict_args = dict(fn=ArgsGeneralWrapper(predict), inputs=input_combo, outputs=output_combo)\n        cancel_handles.append(txt.submit(**predict_args))\n        cancel_handles.append(txt2.submit(**predict_args))\n        cancel_handles.append(submitBtn.click(**predict_args))\n        cancel_handles.append(submitBtn2.click(**predict_args))\n        resetBtn.click(lambda: ([], [], \"\u5df2\u91cd\u7f6e\"), None, [chatbot, history, status])\n        resetBtn2.click(lambda: ([], [], \"\u5df2\u91cd\u7f6e\"), None, [chatbot, history, status])\n        clearBtn.click(lambda: (\"\",\"\"), None, [txt, txt2])\n        clearBtn2.click(lambda: (\"\",\"\"), None, [txt, txt2])\n        for k in functional:\n            if (\"Visible\" in functional[k]) and (not functional[k][\"Visible\"]): continue\n            click_handle = functional[k][\"Button\"].click(fn=ArgsGeneralWrapper(predict), inputs=[*input_combo, gr.State(True), gr.State(k)], outputs=output_combo)\n            cancel_handles.append(click_handle)\n        file_upload.upload(on_file_uploaded, [file_upload, chatbot, txt, txt2, checkboxes], [chatbot, txt, txt2])\n        for k in crazy_fns:\n            if not crazy_fns[k].get(\"AsButton\", True): continue\n            click_handle = crazy_fns[k][\"Button\"].click(ArgsGeneralWrapper(crazy_fns[k][\"Function\"]), [*input_combo, gr.State(PORT)], output_combo)\n            click_handle.then(on_report_generated, [file_upload, chatbot], [file_upload, chatbot])\n            cancel_handles.append(click_handle)\n        def on_dropdown_changed(k):\n            variant = crazy_fns[k][\"Color\"] if \"Color\" in crazy_fns[k] else \"secondary\"\n            ret = {switchy_bt: gr.update(value=k, variant=variant)}\n            if crazy_fns[k].get(\"AdvancedArgs\", False):\n                ret.update({plugin_advanced_arg: gr.update(visible=True,  label=f\"\u63d2\u4ef6[{k}]\u7684\u9ad8\u7ea7\u53c2\u6570\u8bf4\u660e\uff1a\" + crazy_fns[k].get(\"ArgsReminder\", [f\"\u6ca1\u6709\u63d0\u4f9b\u9ad8\u7ea7\u53c2\u6570\u529f\u80fd\u8bf4\u660e\"]))})\n            else:\n                ret.update({plugin_advanced_arg: gr.update(visible=False, label=f\"\u63d2\u4ef6[{k}]\u4e0d\u9700\u8981\u9ad8\u7ea7\u53c2\u6570\u3002\")})\n            return ret\n        dropdown.select(on_dropdown_changed, [dropdown], [switchy_bt, plugin_advanced_arg] )\n        def on_md_dropdown_changed(k):\n            return {chatbot: gr.update(label=\"\u5f53\u524d\u6a21\u578b\uff1a\"+k)}\n        md_dropdown.select(on_md_dropdown_changed, [md_dropdown], [chatbot] )\n        def route(k, *args, **kwargs):\n            if k in [r\"\u6253\u5f00\u63d2\u4ef6\u5217\u8868\", r\"\u8bf7\u5148\u4ece\u63d2\u4ef6\u5217\u8868\u4e2d\u9009\u62e9\"]: return\n            yield from ArgsGeneralWrapper(crazy_fns[k][\"Function\"])(*args, **kwargs)\n        click_handle = switchy_bt.click(route,[switchy_bt, *input_combo, gr.State(PORT)], output_combo)\n        click_handle.then(on_report_generated, [file_upload, chatbot], [file_upload, chatbot])\n        cancel_handles.append(click_handle)\n        stopBtn.click(fn=None, inputs=None, outputs=None, cancels=cancel_handles)\n        stopBtn2.click(fn=None, inputs=None, outputs=None, cancels=cancel_handles)\n    def auto_opentab_delay():\n        import threading, webbrowser, time\n        print(f\"\u5982\u679c\u6d4f\u89c8\u5668\u6ca1\u6709\u81ea\u52a8\u6253\u5f00\uff0c\u8bf7\u590d\u5236\u5e76\u8f6c\u5230\u4ee5\u4e0bURL\uff1a\")\n        print(f\"\\t\uff08\u4eae\u8272\u4e3b\u9898\uff09: http://localhost:{PORT}\")\n        print(f\"\\t\uff08\u6697\u8272\u4e3b\u9898\uff09: http://localhost:{PORT}/?__theme=dark\")\n        def open():\n            time.sleep(2)\n            DARK_MODE, = get_conf('DARK_MODE')\n            if DARK_MODE: webbrowser.open_new_tab(f\"http://localhost:{PORT}/?__theme=dark\")\n            else: webbrowser.open_new_tab(f\"http://localhost:{PORT}\")\n        threading.Thread(target=open, name=\"open-browser\", daemon=True).start()\n        threading.Thread(target=auto_update, name=\"self-upgrade\", daemon=True).start()\n        threading.Thread(target=warm_up_modules, name=\"warm-up\", daemon=True).start()\n    auto_opentab_delay()\n    demo.queue(concurrency_count=CONCURRENT_COUNT).launch(server_name=\"0.0.0.0\", server_port=PORT, auth=AUTHENTICATION, favicon_path=\"docs/logo.png\")",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-33979",
        "description": "[{'lang': 'en', 'value': 'gpt_academic provides a graphical interface for ChatGPT/GLM. A vulnerability was found in gpt_academic 3.37 and prior. This issue affects some unknown processing of the component Configuration File Handler. The manipulation of the argument file leads to information disclosure. Since no sensitive files are configured to be off-limits, sensitive information files in some working directories can be read through the `/file` route, leading to sensitive information leakage. This affects users that uses file configurations via `config.py`, `config_private.py`, `Dockerfile`. A patch is available at commit 1dcc2873d2168ad2d3d70afcb453ac1695fbdf02. As a workaround, one may use environment variables instead of `config*.py` files to configure this project, or use docker-compose installation to configure this project.'}]",
        "cwe_number": 200
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-19",
      "code": "def from_unix_seconds(seconds: Union[int, float]) -> datetime:\n    while abs(seconds) > MS_WATERSHED:\n        seconds /= 1000\n    dt = EPOCH + timedelta(seconds=seconds)\n    return dt.replace(tzinfo=timezone.utc)\ndef _parse_timezone(value: Optional[str], error: Type[Exception]) -> Union[None, int, timezone]:\n    if value == 'Z':\n        return timezone.utc\n    elif value is not None:\n        offset_mins = int(value[-2:]) if len(value) > 3 else 0\n        offset = 60 * int(value[1:3]) + offset_mins\n        if value[0] == '-':\n            offset = -offset\n        try:\n            return timezone(timedelta(minutes=offset))\n        except ValueError:\n            raise error()\n    else:\n        return None",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-29510",
        "description": "[{'lang': 'en', 'value': \"Pydantic is a data validation and settings management using Python type hinting. In affected versions passing either `'infinity'`, `'inf'` or `float('inf')` (or their negatives) to `datetime` or `date` fields causes validation to run forever with 100% CPU usage (on one CPU). Pydantic has been patched with fixes available in the following versions: v1.8.2, v1.7.4, v1.6.2. All these versions are available on pypi(https://pypi.org/project/pydantic/#history), and will be available on conda-forge(https://anaconda.org/conda-forge/pydantic) soon. See the changelog(https://pydantic-docs.helpmanual.io/) for details. If you absolutely can't upgrade, you can work around this risk using a validator(https://pydantic-docs.helpmanual.io/usage/validators/) to catch these values. This is not an ideal solution (in particular you'll need a slightly different function for datetimes), instead of a hack like this you should upgrade pydantic. If you are not using v1.8.x, v1.7.x or v1.6.x and are unable to upgrade to a fixed version of pydantic, please create an issue at https://github.com/samuelcolvin/pydantic/issues requesting a back-port, and we will endeavour to release a patch for earlier versions of pydantic.\"}]",
        "cwe_number": 835
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-20",
      "code": "def parse_soap_enveloped_saml(text, body_class, header_class=None):\n    \"\"\"Parses a SOAP enveloped SAML thing and returns header parts and body\n    :param text: The SOAP object as XML\n    :return: header parts and body as saml.samlbase instances\n    \"\"\"\n    envelope = ElementTree.fromstring(text)\n    assert envelope.tag == '{%s}Envelope' % NAMESPACE\n    body = None\n    header = {}\n    for part in envelope:\n        if part.tag == '{%s}Body' % NAMESPACE:\n            for sub in part:\n                try:\n                    body = saml2.create_class_from_element_tree(body_class, sub)\n                except Exception:\n                    raise Exception(\n                        \"Wrong body type (%s) in SOAP envelope\" % sub.tag)\n        elif part.tag == '{%s}Header' % NAMESPACE:\n            if not header_class:\n                raise Exception(\"Header where I didn't expect one\")\n            for sub in part:\n                for klass in header_class:\n                    if sub.tag == \"{%s}%s\" % (klass.c_namespace, klass.c_tag):\n                        header[sub.tag] = \\\n                            saml2.create_class_from_element_tree(klass, sub)\n                        break\n    return body, header\ndef parse_soap_enveloped_saml_thingy(text, expected_tags):\n    \"\"\"Parses a SOAP enveloped SAML thing and returns the thing as\n    a string.\n    :param text: The SOAP object as XML string\n    :param expected_tags: What the tag of the SAML thingy is expected to be.\n    :return: SAML thingy as a string\n    \"\"\"\n    envelope = ElementTree.fromstring(text)\n    assert envelope.tag == '{%s}Envelope' % soapenv.NAMESPACE\n    assert len(envelope) >= 1\n    body = None\n    for part in envelope:\n        if part.tag == '{%s}Body' % soapenv.NAMESPACE:\n            assert len(part) == 1\n            body = part\n            break\n    if body is None:\n        return \"\"\n    saml_part = body[0]\n    if saml_part.tag in expected_tags:\n        return ElementTree.tostring(saml_part, encoding=\"UTF-8\")\n    else:\n        raise WrongMessageType(\"Was '%s' expected one of %s\" % (saml_part.tag,\n                                                                expected_tags))\ndef class_instances_from_soap_enveloped_saml_thingies(text, modules):\n    \"\"\"Parses a SOAP enveloped header and body SAML thing and returns the\n    thing as a dictionary class instance.\n    :param text: The SOAP object as XML\n    :param modules: modules representing xsd schemas\n    :return: The body and headers as class instances\n    \"\"\"\n    try:\n        envelope = ElementTree.fromstring(text)\n    except Exception as exc:\n        raise XmlParseError(\"%s\" % exc)\n    assert envelope.tag == '{%s}Envelope' % soapenv.NAMESPACE\n    assert len(envelope) >= 1\n    env = {\"header\": [], \"body\": None}\n    for part in envelope:\n        if part.tag == '{%s}Body' % soapenv.NAMESPACE:\n            assert len(part) == 1\n            env[\"body\"] = instanciate_class(part[0], modules)\n        elif part.tag == \"{%s}Header\" % soapenv.NAMESPACE:\n            for item in part:\n                env[\"header\"].append(instanciate_class(item, modules))\n    return env\ndef open_soap_envelope(text):\n    \"\"\"\n    :param text: SOAP message\n    :return: dictionary with two keys \"body\"/\"header\"\n    \"\"\"\n    try:\n        envelope = ElementTree.fromstring(text)\n    except Exception as exc:\n        raise XmlParseError(\"%s\" % exc)\n    assert envelope.tag == '{%s}Envelope' % soapenv.NAMESPACE\n    assert len(envelope) >= 1\n    content = {\"header\": [], \"body\": None}\n    for part in envelope:\n        if part.tag == '{%s}Body' % soapenv.NAMESPACE:\n            assert len(part) == 1\n            content[\"body\"] = ElementTree.tostring(part[0], encoding=\"UTF-8\")\n        elif part.tag == \"{%s}Header\" % soapenv.NAMESPACE:\n            for item in part:\n                _str = ElementTree.tostring(item, encoding=\"UTF-8\")\n                content[\"header\"].append(_str)\n    return content\ndef create_class_from_xml_string(target_class, xml_string):\n    \"\"\"Creates an instance of the target class from a string.\n    :param target_class: The class which will be instantiated and populated\n        with the contents of the XML. This class must have a c_tag and a\n        c_namespace class variable.\n    :param xml_string: A string which contains valid XML. The root element\n        of the XML string should match the tag and namespace of the desired\n        class.\n    :return: An instance of the target class with members assigned according to\n        the contents of the XML - or None if the root XML tag and namespace did\n        not match those of the target class.\n    \"\"\"\n    if not isinstance(xml_string, six.binary_type):\n        xml_string = xml_string.encode('utf-8')\n    tree = ElementTree.fromstring(xml_string)\n    return create_class_from_element_tree(target_class, tree)\ndef extension_element_from_string(xml_string):\n    element_tree = ElementTree.fromstring(xml_string)\n    return _extension_element_from_element_tree(element_tree)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2016-10127",
        "description": "[{'lang': 'en', 'value': 'PySAML2 allows remote attackers to conduct XML external entity (XXE) attacks via a crafted SAML XML request or response.'}]",
        "cwe_number": 611
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-21",
      "code": "def gravatar(context, user, size=None):\n    \"\"\"\n    Outputs the HTML for displaying a user's gravatar.\n    This can take an optional size of the image (defaults to 80 if not\n    specified).\n    This is also influenced by the following settings:\n        GRAVATAR_SIZE    - Default size for gravatars\n        GRAVATAR_RATING  - Maximum allowed rating (g, pg, r, x)\n        GRAVATAR_DEFAULT - Default image set to show if the user hasn't\n                           specified a gravatar (identicon, monsterid, wavatar)\n    See http://www.gravatar.com/ for more information.\n    \"\"\"\n    url = get_gravatar_url(context['request'], user, size)\n    if url:\n        return ('<img src=\"%s\" width=\"%s\" height=\"%s\" alt=\"%s\" '\n                '     class=\"gravatar\"/>' %\n                (url, size, size, user.get_full_name() or user.username))\n    else:\n        return ''",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2014-3995",
        "description": "[{'lang': 'en', 'value': 'Cross-site scripting (XSS) vulnerability in gravatars/templatetags/gravatars.py in Djblets before 0.7.30 and 0.8.x before 0.8.3 for Django allows remote attackers to inject arbitrary web script or HTML via a user display name.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-22",
      "code": "def proxy(\n    request,\n    url=None,\n    response_callback=None,\n    sec_chk_hosts=True,\n    sec_chk_rules=True,\n    timeout=None,\n    allowed_hosts=[],\n    headers=None,\n    access_token=None,\n    **kwargs,\n):\n    from geonode.geoserver.helpers import ogc_server_settings\n    if not timeout:\n        timeout = getattr(ogc_server_settings, \"TIMEOUT\", TIMEOUT)\n    PROXY_ALLOWED_HOSTS = getattr(settings, \"PROXY_ALLOWED_HOSTS\", ())\n    if \"url\" not in request.GET and not url:\n        return HttpResponse(\n            \"The proxy service requires a URL-encoded URL as a parameter.\", status=400, content_type=\"text/plain\"\n        )\n    raw_url = url or request.GET[\"url\"]\n    raw_url = urljoin(settings.SITEURL, raw_url) if raw_url.startswith(\"/\") else raw_url\n    url = urlsplit(raw_url)\n    scheme = str(url.scheme)\n    locator = str(url.path)\n    if url.query != \"\":\n        locator += f\"?{url.query}\"\n    if url.fragment != \"\":\n        locator += f\"\n    site_url = urlsplit(settings.SITEURL)\n    if sec_chk_hosts and not settings.DEBUG:\n        if site_url.hostname not in PROXY_ALLOWED_HOSTS:\n            PROXY_ALLOWED_HOSTS += (site_url.hostname,)\n        hostname = (ogc_server_settings.hostname,) if ogc_server_settings else ()\n        if hostname not in PROXY_ALLOWED_HOSTS:\n            PROXY_ALLOWED_HOSTS += hostname\n        if url.query and ows_regexp.match(url.query):\n            ows_tokens = ows_regexp.match(url.query).groups()\n            if (\n                len(ows_tokens) == 4\n                and \"version\" == ows_tokens[0]\n                and StrictVersion(ows_tokens[1]) >= StrictVersion(\"1.0.0\")\n                and StrictVersion(ows_tokens[1]) <= StrictVersion(\"3.0.0\")\n                and ows_tokens[2].lower() in (\"getcapabilities\")\n                and ows_tokens[3].upper() in (\"OWS\", \"WCS\", \"WFS\", \"WMS\", \"WPS\", \"CSW\")\n            ):\n                if url.hostname not in PROXY_ALLOWED_HOSTS:\n                    PROXY_ALLOWED_HOSTS += (url.hostname,)\n        from geonode.services.models import Service\n        for _s in Service.objects.all():\n            _remote_host = urlsplit(_s.base_url).hostname\n            PROXY_ALLOWED_HOSTS += (_remote_host,)\n        if not validate_host(url.hostname, PROXY_ALLOWED_HOSTS):\n            return HttpResponse(\n                \"DEBUG is set to False but the host of the path provided to the proxy service\"\n                \" is not in the PROXY_ALLOWED_HOSTS setting.\",\n                status=403,\n                content_type=\"text/plain\",\n            )\n    if sec_chk_rules:\n        pass\n    if not headers:\n        headers, access_token = get_headers(request, url, raw_url, allowed_hosts=allowed_hosts)\n    if not access_token:\n        auth_header = None\n        if \"Authorization\" in headers:\n            auth_header = headers[\"Authorization\"]\n        elif \"HTTP_AUTHORIZATION\" in request.META:\n            auth_header = request.META.get(\"HTTP_AUTHORIZATION\", request.META.get(\"HTTP_AUTHORIZATION2\"))\n        if auth_header:\n            access_token = get_token_from_auth_header(auth_header, create_if_not_exists=True)\n    user = get_auth_user(access_token)\n    parsed = urlparse(raw_url)\n    parsed._replace(path=locator.encode(\"utf8\"))\n    if parsed.netloc == site_url.netloc and scheme != site_url.scheme:\n        parsed = parsed._replace(scheme=site_url.scheme)\n    _url = parsed.geturl()\n    _url = URL.from_text(_url).normalize().to_text()\n    if request.method == \"GET\" and access_token and \"access_token\" not in _url:\n        query_separator = \"&\" if \"?\" in _url else \"?\"\n        _url = f\"{_url}{query_separator}access_token={access_token}\"\n    _data = request.body.decode(\"utf-8\")\n    if check_ogc_backend(geoserver.BACKEND_PACKAGE):\n        from geonode.geoserver.helpers import ogc_server_settings\n        _url = _url.replace(f\"{settings.SITEURL}geoserver\", ogc_server_settings.LOCATION.rstrip(\"/\"))\n        _data = _data.replace(f\"{settings.SITEURL}geoserver\", ogc_server_settings.LOCATION.rstrip(\"/\"))\n    response, content = http_client.request(\n        _url, method=request.method, data=_data.encode(\"utf-8\"), headers=headers, timeout=timeout, user=user\n    )\n    if response is None:\n        return HttpResponse(content=content, reason=content, status=500)\n    content = response.content or response.reason\n    status = response.status_code\n    response_headers = response.headers\n    content_type = response.headers.get(\"Content-Type\")\n    if status >= 400:\n        _response = HttpResponse(content=content, reason=content, status=status, content_type=content_type)\n        return fetch_response_headers(_response, response_headers)\n    if content and content_type and content_type == \"gzip\":\n        buf = io.BytesIO(content)\n        with gzip.GzipFile(fileobj=buf) as f:\n            content = f.read()\n        buf.close()\n    PLAIN_CONTENT_TYPES = [\"text\", \"plain\", \"html\", \"json\", \"xml\", \"gml\"]\n    for _ct in PLAIN_CONTENT_TYPES:\n        if content_type and _ct in content_type and not isinstance(content, str):\n            try:\n                content = content.decode()\n                break\n            except Exception:\n                pass\n    if response and response_callback:\n        kwargs = {} if not kwargs else kwargs\n        kwargs.update(\n            {\n                \"response\": response,\n                \"content\": content,\n                \"status\": status,\n                \"response_headers\": response_headers,\n                \"content_type\": content_type,\n            }\n        )\n        return response_callback(**kwargs)\n    else:\n        if status and status in (301, 302, 303, 307):\n            _response = HttpResponse(\n                (\n                    f\"This proxy does not support redirects. The server in '{url}' \"\n                    f\"asked for a redirect to '{response.getheader('Location')}'\"\n                ),\n                status=status,\n                content_type=content_type,\n            )\n            _response[\"Location\"] = response.getheader(\"Location\")\n            return fetch_response_headers(_response, response_headers)\n        else:\n            def _get_message(text):\n                _s = text\n                if isinstance(text, bytes):\n                    _s = text.decode(\"utf-8\", \"replace\")\n                try:\n                    found = re.search(\"<b>Message</b>(.+?)</p>\", _s).group(1).strip()\n                except Exception:\n                    found = _s\n                return found\n            _response = HttpResponse(\n                content=content,\n                reason=_get_message(content) if status not in (200, 201) else None,\n                status=status,\n                content_type=content_type,\n            )\n            return fetch_response_headers(_response, response_headers)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-40017",
        "description": "[{'lang': 'en', 'value': 'GeoNode is an open source platform that facilitates the creation, sharing, and collaborative use of geospatial data. In versions 3.2.0 through 4.1.2, the endpoint `/proxy/?url=` does not properly protect against server-side request forgery. This allows an attacker to port scan internal hosts and request information from internal hosts. A patch is available at commit a9eebae80cb362009660a1fd49e105e7cdb499b9.'}]",
        "cwe_number": 918
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-23",
      "code": "    def _read_lockfile(self, lockfile: Mapping[str, Any]) -> None:\n        root = self.environment.project.root\n        with cd(root):\n            for package in lockfile.get(\"package\", []):\n                version = package.get(\"version\")\n                if version:\n                    package[\"version\"] = f\"=={version}\"\n                package_name = package.pop(\"name\")\n                req_dict = {\n                    k: v for k, v in package.items() if k not in (\"dependencies\", \"requires_python\", \"summary\", \"files\")\n                }\n                req = Requirement.from_req_dict(package_name, req_dict)\n                if req.is_file_or_url and req.path and not req.url:\n                    req.url = path_to_url(posixpath.join(root, req.path))\n                can = make_candidate(req, name=package_name, version=version)\n                can.hashes = package.get(\"files\", [])\n                can_id = self._identify_candidate(can)\n                self.packages[can_id] = can\n                candidate_info: CandidateInfo = (\n                    package.get(\"dependencies\", []),\n                    package.get(\"requires_python\", \"\"),\n                    package.get(\"summary\", \"\"),\n                )\n                self.candidate_info[can_id] = candidate_info\n    def _identify_candidate(self, candidate: Candidate) -> CandidateKey:\n        url: str | None = None\n        if candidate.link is not None:\n            url = candidate.link.url_without_fragment\n            url = self.environment.project.backend.expand_line(cast(str, url))\n            if url.startswith(\"file://\"):\n                path = posixpath.normpath(url_to_path(url))\n                url = path_to_url(path)\n        return (\n            candidate.identify(),\n            candidate.version if not url else None,\n            url,\n            candidate.req.editable,\n        )",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-45805",
        "description": "[{'lang': 'en', 'value': \"pdm is a Python package and dependency manager supporting the latest PEP standards. It's possible to craft a malicious `pdm.lock` file that could allow e.g. an insider or a malicious open source project to appear to depend on a trusted PyPI project, but actually install another project. A project `foo` can be targeted by creating the project `foo-2` and uploading the file `foo-2-2.tar.gz` to pypi.org. PyPI will see this as project `foo-2` version `2`, while PDM will see this as project `foo` version `2-2`. The version must only be `parseable as a version` and the filename must be a prefix of the project name, but it's not verified to match the version being installed. Version `2-2` is also not a valid normalized version per PEP 440. Matching the project name exactly (not just prefix) would fix the issue. When installing dependencies with PDM, what's actually installed could differ from what's listed in `pyproject.toml` (including arbitrary code execution on install). It could also be used for downgrade attacks by only changing the version. This issue has been addressed in commit `6853e2642df` which is included in release version `2.9.4`. Users are advised to upgrade. There are no known workarounds for this vulnerability.\"}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-24",
      "code": "    def urlopen(\n        self, method: str, url: str, redirect: bool = True, **kw: typing.Any\n    ) -> BaseHTTPResponse:\n        \"\"\"\n        Same as :meth:`urllib3.HTTPConnectionPool.urlopen`\n        with custom cross-host redirect logic and only sends the request-uri\n        portion of the ``url``.\n        The given ``url`` parameter must be absolute, such that an appropriate\n        :class:`urllib3.connectionpool.ConnectionPool` can be chosen for it.\n        \"\"\"\n        u = parse_url(url)\n        if u.scheme is None:\n            warnings.warn(\n                \"URLs without a scheme (ie 'https://') are deprecated and will raise an error \"\n                \"in a future version of urllib3. To avoid this DeprecationWarning ensure all URLs \"\n                \"start with 'https://' or 'http://'. Read more in this issue: \"\n                \"https://github.com/urllib3/urllib3/issues/2920\",\n                category=DeprecationWarning,\n                stacklevel=2,\n            )\n        conn = self.connection_from_host(u.host, port=u.port, scheme=u.scheme)\n        kw[\"assert_same_host\"] = False\n        kw[\"redirect\"] = False\n        if \"headers\" not in kw:\n            kw[\"headers\"] = self.headers\n        if self._proxy_requires_url_absolute_form(u):\n            response = conn.urlopen(method, url, **kw)\n        else:\n            response = conn.urlopen(method, u.request_uri, **kw)\n        redirect_location = redirect and response.get_redirect_location()\n        if not redirect_location:\n            return response\n        redirect_location = urljoin(url, redirect_location)\n        if response.status == 303:\n            method = \"GET\"\n        retries = kw.get(\"retries\")\n        if not isinstance(retries, Retry):\n            retries = Retry.from_int(retries, redirect=redirect)\n        if retries.remove_headers_on_redirect and not conn.is_same_host(\n            redirect_location\n        ):\n            new_headers = kw[\"headers\"].copy()\n            for header in kw[\"headers\"]:\n                if header.lower() in retries.remove_headers_on_redirect:\n                    new_headers.pop(header, None)\n            kw[\"headers\"] = new_headers\n        try:\n            retries = retries.increment(method, url, response=response, _pool=conn)\n        except MaxRetryError:\n            if retries.raise_on_redirect:\n                response.drain_conn()\n                raise\n            return response\n        kw[\"retries\"] = retries\n        kw[\"redirect\"] = redirect\n        log.info(\"Redirecting %s -> %s\", url, redirect_location)\n        response.drain_conn()\n        return self.urlopen(method, redirect_location, **kw)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-45803",
        "description": "[{'lang': 'en', 'value': \"urllib3 is a user-friendly HTTP client library for Python. urllib3 previously wouldn't remove the HTTP request body when an HTTP redirect response using status 301, 302, or 303 after the request had its method changed from one that could accept a request body (like `POST`) to `GET` as is required by HTTP RFCs. Although this behavior is not specified in the section for redirects, it can be inferred by piecing together information from different sections and we have observed the behavior in other major HTTP client implementations like curl and web browsers. Because the vulnerability requires a previously trusted service to become compromised in order to have an impact on confidentiality we believe the exploitability of this vulnerability is low. Additionally, many users aren't putting sensitive data in HTTP request bodies, if this is the case then this vulnerability isn't exploitable. Both of the following conditions must be true to be affected by this vulnerability: 1. Using urllib3 and submitting sensitive information in the HTTP request body (such as form data or JSON) and 2. The origin service is compromised and starts redirecting using 301, 302, or 303 to a malicious peer or the redirected-to service becomes compromised. This issue has been addressed in versions 1.26.18 and 2.0.7 and users are advised to update to resolve this issue. Users unable to update should disable redirects for services that aren't expecting to respond with redirects with `redirects=False` and disable automatic redirects with `redirects=False` and handle 301, 302, and 303 redirects manually by stripping the HTTP request body.\\n\"}]",
        "cwe_number": 200
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-25",
      "code": "def get_parser():\n    parser = configargparse.ArgumentParser(\n        prog='rdiffweb',\n        description='Web interface to browse and restore rdiff-backup repositories.',\n        default_config_files=['/etc/rdiffweb/rdw.conf', '/etc/rdiffweb/rdw.conf.d/*.conf'],\n        add_env_var_help=True,\n        auto_env_var_prefix='RDIFFWEB_',\n        config_file_parser_class=ConfigFileParser,\n        conflict_handler='resolve',\n    )\n    parser.add_argument(\n        '-f', '--config', is_config_file=True, metavar='FILE', help='location of Rdiffweb configuration file'\n    )\n    parser.add(\n        '--database-uri',\n        '--sqlitedb-file',\n        '--sqlitedbfile',\n        metavar='URI',\n        help=\"\"\"Location of the database used for persistence. SQLite and PostgreSQL\n            database are supported officially. To use a SQLite database you may\n            define the location using a file path or a URI.\n            e.g.: /srv/rdiffweb/file.db or sqlite:///srv/rdiffweb/file.db`.\n            To use PostgreSQL server you must provide\n            a URI similar to postgresql://user:pass@10.255.1.34/dbname and you\n            must install required dependencies.\n            By default, Rdiffweb uses a SQLite embedded database located at\n            /etc/rdiffweb/rdw.db.\"\"\",\n        default='/etc/rdiffweb/rdw.db',\n    )\n    parser.add_argument(\n        '-d',\n        '--debug',\n        action='store_true',\n        help='enable rdiffweb debug mode - change the log level to DEBUG, print exception stack trace to the web interface and show SQL query in logs',\n    )\n    parser.add_argument(\n        '--admin-user',\n        '--adminuser',\n        metavar='USERNAME',\n        help='administrator username. The administrator user get created on startup if the database is empty.',\n        default='admin',\n    )\n    parser.add_argument(\n        '--admin-password',\n        metavar='USERNAME',\n        help=\"\"\"administrator encrypted password as SSHA. Read online\n            documentation to know more about how to encrypt your password\n            into SSHA or use http://projects.marsching.org/weave4j/util/genpassword.php\n            When defined, administrator password cannot be updated using the web interface.\n            When undefined, default administrator password is `admin123` and\n            it can be updated using the web interface.\"\"\",\n    )\n    parser.add_argument(\n        '--default-theme',\n        '--defaulttheme',\n        help='define the default theme. Either: default, blue or orange. Define the CSS file to be loaded in the web interface. You may manually edit a CSS file to customize it. The location is similar to `/usr/local/lib/python3.9/dist-packages/rdiffweb/static/`',\n        choices=['default', 'blue', 'orange'],\n        default='default',\n    )\n    parser.add_argument(\n        '--environment',\n        choices=['development', 'production'],\n        help='define the type of environment: development, production. This is used to limit the information shown to the user when an error occur.',\n        default='production',\n    )\n    parser.add_argument(\n        '--email-encryption',\n        '--emailencryption',\n        choices=['none', 'ssl', 'starttls'],\n        help='type of encryption to be used when establishing communication with SMTP server. Default: none',\n        default='none',\n    )\n    parser.add_argument(\n        '--email-host',\n        '--emailhost',\n        metavar='HOST',\n        help='SMTP server used to send email in the form <host>:<port>. If the port is not provided, default to standard port 25 or 465 is used. e.g.: smtp.gmail.com:587',\n    )\n    parser.add_argument(\n        '--email-sender',\n        '--emailsender',\n        metavar='EMAIL',\n        help='email addres used for the `from:` field when sending email.',\n    )\n    parser.add_argument(\n        '--email-notification-time',\n        '--emailnotificationtime',\n        metavar='TIME',\n        help='time when the email notifcation should be sent for inactive backups. e.g.: 22:00 Default value: 23:00',\n        default='23:00',\n    )\n    parser.add_argument(\n        '--email-username',\n        '--emailusername',\n        metavar='USERNAME',\n        help='username used for authentication with the SMTP server.',\n    )\n    parser.add_argument(\n        '--email-password',\n        '--emailpassword',\n        metavar='PASSWORD',\n        help='password used for authentication with the SMTP server.',\n    )\n    parser.add_argument(\n        '--email-send-changed-notification',\n        '--emailsendchangednotification',\n        help='True to send notification when sensitive information get change in user profile.',\n        action='store_true',\n        default=False,\n    )\n    parser.add_argument(\n        '--favicon',\n        help='location of an icon to be used as a favicon displayed in web browser.',\n        default=pkg_resources.resource_filename('rdiffweb', 'static/favicon.ico'),\n    )\n    parser.add_argument(\n        '--footer-name', '--footername', help=argparse.SUPPRESS, default='rdiffweb'\n    )\n    parser.add_argument(\n        '--footer-url', '--footerurl', help=argparse.SUPPRESS, default='https://rdiffweb.org/'\n    )\n    parser.add_argument(\n        '--header-logo',\n        '--headerlogo',\n        help='location of an image (preferably a .png) to be used as a replacement for the rdiffweb logo.',\n    )\n    parser.add_argument(\n        '--header-name',\n        '--headername',\n        help='application name displayed in the title bar and header menu.',\n        default='Rdiffweb',\n    )\n    parser.add_argument(\n        '--ldap-add-missing-user',\n        '--addmissinguser',\n        action='store_true',\n        help='enable creation of users from LDAP when the credential are valid.',\n        default=False,\n    )\n    parser.add_argument(\n        '--ldap-add-user-default-role',\n        help='default role used when creating users from LDAP. This parameter is only useful when `--ldap-add-missing-user` is enabled.',\n        default='user',\n        choices=['admin', 'maintainer', 'user'],\n    )\n    parser.add_argument(\n        '--ldap-add-user-default-userroot',\n        help='default user root directory used when creating users from LDAP. LDAP attributes may be used to define the default location. e.g.: `/backups/{uid[0]}/`. This parameter is only useful when `--ldap-add-missing-user` is enabled.',\n        default='',\n    )\n    parser.add_argument(\n        '--ldap-uri',\n        '--ldapuri',\n        help='URL to the LDAP server used to validate user credentials. e.g.: ldap://localhost:389',\n    )\n    parser.add_argument(\n        '--ldap-base-dn',\n        '--ldapbasedn',\n        metavar='DN',\n        help='DN of the branch of the directory where all searches should start from. e.g.: dc=my,dc=domain',\n        default=\"\",\n    )\n    parser.add_argument(\n        '--ldap-scope',\n        '--ldapscope',\n        help='scope of the search. Can be either base, onelevel or subtree',\n        choices=['base', 'onelevel', 'subtree'],\n        default=\"subtree\",\n    )\n    parser.add_argument('--ldap-tls', '--ldaptls', action='store_true', help='enable TLS')\n    parser.add_argument(\n        '--ldap-username-attribute',\n        '--ldapattribute',\n        metavar='ATTRIBUTE',\n        help=\"The attribute to search username. If no attributes are provided, the default is to use `uid`. It's a good idea to choose an attribute that will be unique across all entries in the subtree you will be using.\",\n        default='uid',\n    )\n    parser.add_argument(\n        '--ldap-filter',\n        '--ldapfilter',\n        help=\"search filter to limit LDAP lookup. If not provided, defaults to (objectClass=*), which searches for all objects in the tree.\",\n        default='(objectClass=*)',\n    )\n    parser.add_argument(\n        '--ldap-required-group',\n        '--ldaprequiredgroup',\n        metavar='GROUPNAME',\n        help=\"name of the group of which the user must be a member to access rdiffweb. Should be used with ldap-group-attribute and ldap-group-attribute-is-dn.\",\n    )\n    parser.add_argument(\n        '--ldap-group-attribute',\n        '--ldapgroupattribute',\n        metavar='ATTRIBUTE',\n        help=\"name of the attribute defining the groups of which the user is a member. Should be used with ldap-required-group and ldap-group-attribute-is-dn.\",\n        default='member',\n    )\n    parser.add_argument(\n        '--ldap-group-attribute-is-dn',\n        '--ldapgroupattributeisdn',\n        help=\"True if the content of the attribute `ldap-group-attribute` is a DN.\",\n        action='store_true',\n    )\n    parser.add_argument(\n        '--ldap-bind-dn',\n        '--ldapbinddn',\n        metavar='DN',\n        help=\"optional DN used to bind to the server when searching for entries. If not provided, will use an anonymous bind.\",\n        default=\"\",\n    )\n    parser.add_argument(\n        '--ldap-bind-password',\n        '--ldapbindpassword',\n        metavar='PASSWORD',\n        help=\"password to use in conjunction with LdapBindDn. Note that the bind password is probably sensitive data, and should be properly protected. You should only use the LdapBindDn and LdapBindPassword if you absolutely need them to search the directory.\",\n        default=\"\",\n    )\n    parser.add_argument(\n        '--ldap-version',\n        '--ldapversion',\n        '--ldapprotocolversion',\n        help=\"version of LDAP in use either 2 or 3. Default to 3.\",\n        default=3,\n        type=int,\n        choices=[2, 3],\n    )\n    parser.add_argument(\n        '--ldap-network-timeout',\n        '--ldapnetworktimeout',\n        metavar='SECONDS',\n        help=\"timeout in seconds value used for LDAP connection\",\n        default=100,\n        type=int,\n    )\n    parser.add_argument(\n        '--ldap-timeout',\n        '--ldaptimeout',\n        metavar='SECONDS',\n        help=\"timeout in seconds value used for LDAP request\",\n        default=300,\n        type=int,\n    )\n    parser.add_argument(\n        '--ldap-encoding',\n        '--ldapencoding',\n        metavar='ENCODING',\n        help=\"encoding used by your LDAP server.\",\n        default=\"utf-8\",\n    )\n    parser.add_argument(\n        '--log-access-file', '--logaccessfile', metavar='FILE', help='location of Rdiffweb log access file.'\n    )\n    parser.add_argument(\n        '--log-file',\n        '--logfile',\n        metavar='FILE',\n        help='location of Rdiffweb log file. Print log to the console if not define in config file.',\n    )\n    parser.add_argument(\n        '--log-level',\n        '--loglevel',\n        help='Define the log level.',\n        choices=['ERROR', 'WARN', 'INFO', 'DEBUG'],\n        default='INFO',\n    )\n    parser.add_argument(\n        '--max-depth',\n        '--maxdepth',\n        metavar='DEPTH',\n        help=\"define the maximum folder depthness to search into the user's root directory to find repositories. This is commonly used if you repositories are organised with multiple sub-folder.\",\n        type=int,\n        default=3,\n    )\n    parser.add('--quota-set-cmd', '--quotasetcmd', metavar='COMMAND', help=\"command line to set the user's quota.\")\n    parser.add('--quota-get-cmd', '--quotagetcmd', metavar='COMMAND', help=\"command line to get the user's quota.\")\n    parser.add(\n        '--quota-used-cmd', '--quotausedcmd', metavar='COMMAND', help=\"Command line to get user's quota disk usage.\"\n    )\n    parser.add(\n        '--remove-older-time',\n        '--removeoldertime',\n        metavar='TIME',\n        help=\"Time when to execute the remove older scheduled job. e.g.: 22:30\",\n        default='23:00',\n    )\n    parser.add('--server-host', '--serverhost', metavar='IP', default='127.0.0.1', help='IP address to listen to')\n    parser.add(\n        '--server-port',\n        '--serverport',\n        metavar='PORT',\n        help='port to listen to for HTTP request',\n        default='8080',\n        type=int,\n    )\n    parser.add(\n        '--rate-limit-dir',\n        '--session-dir',\n        '--sessiondir',\n        metavar='FOLDER',\n        help='location where to store rate-limit information. When undefined, the data is kept in memory. `--session-dir` are deprecated and kept for backward compatibility.',\n    )\n    parser.add(\n        '--session-timeout',\n        metavar='MINUTES',\n        help='Sessions will be revoke after this period of inactivity, unless the user selected \"remember me\". Default 15 minutes.',\n        default=15,\n    )\n    parser.add(\n        '--session-persistent-timeout',\n        metavar='MINUTES',\n        help='Persistent sessions (remember me) will be revoke after this period of inactivity. Default 30 days.',\n        default=43200,\n    )\n    parser.add(\n        '--rate-limit',\n        metavar='LIMIT',\n        type=int,\n        default=30,\n        help='maximum number of requests per minute that can be made by an IP address for an unauthenticated connection. When this limit is reached, an HTTP 429 message is returned to the user. This security measure is used to limit brute force attacks on the login page and the RESTful API.',\n    )\n    parser.add(\n        '--ssl-certificate',\n        '--sslcertificate',\n        metavar='CERT',\n        help='location of the SSL Certification to enable HTTPS (not recommended)',\n    )\n    parser.add(\n        '--ssl-private-key',\n        '--sslprivatekey',\n        metavar='KEY',\n        help='location of the SSL Private Key to enable HTTPS (not recommended)',\n    )\n    parser.add(\n        '--tempdir',\n        metavar='FOLDER',\n        help='alternate temporary folder to be used when restoring files. Might be useful if the default location has limited disk space. Default to TEMPDIR environment or `/tmp`.',\n    )\n    parser.add(\n        '--disable-ssh-keys',\n        action='store_true',\n        help='used to hide SSH Key management to avoid users to add or remove SSH Key using the web application',\n        default=False,\n    )\n    parser.add(\n        '--password-min-length',\n        type=int,\n        help=\"Minimum length of the user's password\",\n        default=8,\n    )\n    parser.add(\n        '--password-max-length',\n        type=int,\n        help=\"Maximum length of the user's password\",\n        default=128,\n    )\n    parser.add(\n        '--password-score',\n        type=lambda x: max(1, min(int(x), 4)),\n        help=\"Minimum zxcvbn's score for password. Value from 1 to 4. Default value 2. Read more about it here: https://github.com/dropbox/zxcvbn\",\n        default=2,\n    )\n    parser.add_argument('--version', action='version', version='%(prog)s ' + VERSION)\n    flags = ['--welcome-msg'] + ['--welcome-msg-' + i for i in ['ca', 'en', 'es', 'fr', 'ru']] + ['--welcomemsg']\n    parser.add_argument(\n        *flags,\n        metavar='HTML',\n        help='replace the welcome message displayed in the login page for default locale or for a specific locale',\n        action=LocaleAction\n    )\n    return parser\n    def __init__(self, cfg):\n        self.cfg = cfg\n        db_uri = self.cfg.database_uri if '://' in self.cfg.database_uri else \"sqlite:///\" + self.cfg.database_uri\n        cherrypy.config.update(\n            {\n                'environment': 'development' if cfg.debug else cfg.environment,\n                'tools.db.uri': db_uri,\n                'tools.db.debug': cfg.debug,\n                'ldap.uri': cfg.ldap_uri,\n                'ldap.base_dn': cfg.ldap_base_dn,\n                'ldap.bind_dn': cfg.ldap_bind_dn,\n                'ldap.bind_password': cfg.ldap_bind_password,\n                'ldap.scope': cfg.ldap_scope,\n                'ldap.tls': cfg.ldap_tls,\n                'ldap.username_attribute': cfg.ldap_username_attribute,\n                'ldap.required_group': cfg.ldap_required_group,\n                'ldap.group_attribute': cfg.ldap_group_attribute,\n                'ldap.group_attribute_is_dn': cfg.ldap_group_attribute_is_dn,\n                'ldap.version': cfg.ldap_version,\n                'ldap.network_timeout': cfg.ldap_network_timeout,\n                'ldap.timeout': cfg.ldap_timeout,\n                'ldap.encoding': cfg.ldap_encoding,\n                'login.add_missing_user': cfg.ldap_add_missing_user,\n                'login.add_user_default_role': cfg.ldap_add_user_default_role,\n                'login.add_user_default_userroot': cfg.ldap_add_user_default_userroot,\n                'smtp.server': cfg.email_host,\n                'smtp.username': cfg.email_username,\n                'smtp.password': cfg.email_password,\n                'smtp.email_from': cfg.email_sender\n                and '%s <%s>'\n                % (\n                    cfg.header_name,\n                    cfg.email_sender,\n                ),\n                'smtp.encryption': cfg.email_encryption,\n                'remove_older.execution_time': self.cfg.remove_older_time,\n                'notification.execution_time': self.cfg.email_notification_time,\n                'notification.send_changed': self.cfg.email_send_changed_notification,\n                'quota.set_quota_cmd': self.cfg.quota_set_cmd,\n                'quota.get_quota_cmd': self.cfg.quota_get_cmd,\n                'quota.get_usage_cmd': self.cfg.quota_used_cmd,\n            }\n        )\n        cherrypy.tools.db.create_all()\n        self.templates = rdw_templating.TemplateManager()\n        rate_limit_storage_class = rdiffweb.tools.ratelimit.RamRateLimit\n        if cfg.rate_limit_dir:\n            rate_limit_storage_class = rdiffweb.tools.ratelimit.FileRateLimit\n        config = {\n            '/': {\n                'request.uri_encoding': 'ISO-8859-1',\n                'tools.i18n.on': True,\n                'tools.i18n.default': 'en_US',\n                'tools.i18n.mo_dir': pkg_resources.resource_filename('rdiffweb', 'locales'),\n                'tools.i18n.domain': 'messages',\n                'tools.encode.on': True,\n                'tools.encode.encoding': 'utf-8',\n                'tools.gzip.on': True,\n                'error_page.default': self.error_page,\n                'tools.sessions.on': True,\n                'tools.sessions.debug': cfg.debug,\n                'tools.sessions.storage_class': DbSession,\n                'tools.sessions.httponly': True,\n                'tools.sessions.timeout': cfg.session_timeout,\n                'tools.sessions.persistent': False,\n                'tools.auth_form.timeout': cfg.session_persistent_timeout,\n                'tools.ratelimit.debug': cfg.debug,\n                'tools.ratelimit.delay': 60,\n                'tools.ratelimit.anonymous_limit': cfg.rate_limit,\n                'tools.ratelimit.storage_class': rate_limit_storage_class,\n                'tools.ratelimit.storage_path': cfg.rate_limit_dir,\n            },\n        }\n        Application.__init__(self, root=Root(), config=config)\n        self.root.favicon_ico = staticfile(self._favicon)\n        if self._header_logo:\n            self.root.header_logo = staticfile(self._header_logo)\n        if self._tempdir:\n            os.environ[\"TMPDIR\"] = self._tempdir\n        UserObject.create_admin_user(cfg.admin_user, cfg.admin_password)\n    def get_and_increment(self, token, delay):\n        lock = self._locks.setdefault(token, threading.RLock())\n        with lock:\n            tracker = self._load(token)\n            if tracker is None or tracker.timeout < time.time():\n                tracker = Tracker(token=token, hits=0, timeout=int(time.time() + delay))\n            tracker = tracker._replace(hits=tracker.hits + 1)\n            self._save(tracker)\n        return tracker.hits\n    def _load(self, token):\n        path = self._path(token)\n        try:\n            f = open(path, 'rb')\n            try:\n                return pickle.load(f)\n            finally:\n                f.close()\n        except (IOError, EOFError):\n            pass\n        return None\ndef check_ratelimit(delay=60, anonymous_limit=0, registered_limit=0, rate_exceed_status=429, debug=False, **conf):\n    \"\"\"\n    Verify the ratelimit. By default return a 429 HTTP error code (Too Many Request).\n    Usage:\n    @cherrypy.tools.ratelimit(on=True, anonymous_limit=5, registered_limit=50, storage_class=FileRateLimit, storage_path='/tmp')\n    def index(self):\n        pass\n    \"\"\"\n    datastore = getattr(cherrypy, '_ratelimit_datastore', None)\n    if datastore is None:\n        storage_class = conf.get('storage_class', RamRateLimit)\n        datastore = storage_class(**conf)\n        cherrypy._ratelimit_datastore = datastore\n    token = cherrypy.request.login or cherrypy.request.remote.ip\n    limit = registered_limit if cherrypy.request.login else anonymous_limit\n    if limit is None or limit <= 0:\n        return\n    hits = datastore.get_and_increment(token, delay)\n    if debug:\n        cherrypy.log(\n            'check and increase rate limit for token %s, limit %s, hits %s' % (token, limit, hits), 'TOOLS.RATELIMIT'\n        )\n    if limit <= hits:\n        raise cherrypy.HTTPError(rate_exceed_status)\ncherrypy.tools.ratelimit = cherrypy.Tool('before_handler', check_ratelimit, priority=60)\ndef _checkpassword(realm, username, password):\n    \"\"\"\n    Check basic authentication.\n    \"\"\"\n    userobj = UserObject.get_user(username)\n    if userobj is not None:\n        if userobj.validate_access_token(password):\n            return True\n        if userobj.mfa == UserObject.ENABLED_MFA:\n            return False\n    return any(cherrypy.engine.publish('login', username, password))\nclass ApiCurrentUser(Controller):\n    def default(self):\n        u = self.app.currentuser\n        u.refresh_repos()\n        return {\n            \"email\": u.email,\n            \"username\": u.username,\n            \"repos\": [\n                {\n                    \"name\": repo_obj.name,\n                    \"maxage\": repo_obj.maxage,\n                    \"keepdays\": repo_obj.keepdays,\n                    \"display_name\": repo_obj.display_name,\n                    \"last_backup_date\": repo_obj.last_backup_date,\n                    \"status\": repo_obj.status[0],\n                    \"encoding\": repo_obj.encoding,\n                }\n                for repo_obj in u.repo_objs\n            ],\n        }\n    def __init__(self, **kwargs):\n        if 'formdata' in kwargs:\n            formdata = kwargs.pop('formdata')\n        else:\n            formdata = _AUTO if CherryForm.is_submitted(self) else None\n        super().__init__(formdata=formdata, **kwargs)\n    def is_submitted(self):\n        \"\"\"\n        Consider the form submitted if there is an active request and\n        the method is ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.\n        \"\"\"\n        return cherrypy.request.method in SUBMIT_METHODS",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-3456",
        "description": "[{'lang': 'en', 'value': 'Allocation of Resources Without Limits or Throttling in GitHub repository ikus060/rdiffweb prior to 2.5.0.'}]",
        "cwe_number": 770
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-26",
      "code": "if __name__ == \"__main__\":\n  test.main()",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-35963",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. The implementation of `FractionalAvgPoolGrad` does not fully validate the input `orig_input_tensor_shape`. This results in an overflow that results in a `CHECK` failure which can be used to trigger a denial of service attack. We have patched the issue in GitHub commit 03a659d7be9a1154fdf5eeac221e5950fec07dad. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue.'}]",
        "cwe_number": 617
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-27",
      "code": "def find_package_data():\n    \"\"\"\n    Find package_data.\n    \"\"\"\n    excludes = [\n        pjoin('static', 'components'),\n        pjoin('static', '*', 'less'),\n        pjoin('static', '*', 'node_modules')\n    ]\n    cwd = os.getcwd()\n    os.chdir('notebook')\n    static_data = []\n    for parent, dirs, files in os.walk('static'):\n        if any(fnmatch(parent, pat) for pat in excludes):\n            dirs[:] = []\n            continue\n        for f in files:\n            static_data.append(pjoin(parent, f))\n    for app in ['auth', 'edit', 'notebook', 'terminal', 'tree']:\n        static_data.append(pjoin('static', app, 'js', 'main.min.js'))\n    components = pjoin(\"static\", \"components\")\n    static_data.extend([\n        pjoin(components, \"backbone\", \"backbone-min.js\"),\n        pjoin(components, \"bootstrap\", \"dist\", \"js\", \"bootstrap.min.js\"),\n        pjoin(components, \"bootstrap-tour\", \"build\", \"css\", \"bootstrap-tour.min.css\"),\n        pjoin(components, \"bootstrap-tour\", \"build\", \"js\", \"bootstrap-tour.min.js\"),\n        pjoin(components, \"create-react-class\", \"index.js\"),\n        pjoin(components, \"font-awesome\", \"css\", \"*.css\"),\n        pjoin(components, \"es6-promise\", \"*.js\"),\n        pjoin(components, \"font-awesome\", \"fonts\", \"*.*\"),\n        pjoin(components, \"google-caja\", \"html-css-sanitizer-minified.js\"),\n        pjoin(components, \"jed\", \"jed.js\"),\n        pjoin(components, \"jquery\", \"jquery.min.js\"),\n        pjoin(components, \"jquery-typeahead\", \"dist\", \"jquery.typeahead.min.js\"),\n        pjoin(components, \"jquery-typeahead\", \"dist\", \"jquery.typeahead.min.css\"),\n        pjoin(components, \"jquery-ui\", \"jquery-ui.min.js\"),\n        pjoin(components, \"jquery-ui\", \"themes\", \"smoothness\", \"jquery-ui.min.css\"),\n        pjoin(components, \"jquery-ui\", \"themes\", \"smoothness\", \"images\", \"*\"),\n        pjoin(components, \"marked\", \"lib\", \"marked.js\"),\n        pjoin(components, \"react\", \"react.production.min.js\"),\n        pjoin(components, \"react\", \"react-dom.production.min.js\"),\n        pjoin(components, \"requirejs\", \"require.js\"),\n        pjoin(components, \"requirejs-plugins\", \"src\", \"json.js\"),\n        pjoin(components, \"requirejs-text\", \"text.js\"),\n        pjoin(components, \"underscore\", \"underscore-min.js\"),\n        pjoin(components, \"moment\", \"moment.js\"),\n        pjoin(components, \"moment\", \"min\", \"*.js\"),\n        pjoin(components, \"xterm.js\", \"index.js\"),\n        pjoin(components, \"xterm.js-css\", \"index.css\"),\n        pjoin(components, \"xterm.js-fit\", \"index.js\"),\n        pjoin(components, \"text-encoding\", \"lib\", \"encoding.js\"),\n    ])\n    for parent, dirs, files in os.walk(pjoin(components, 'codemirror')):\n        for f in files:\n            if f.endswith(('.js', '.css')):\n                static_data.append(pjoin(parent, f))\n    mj = lambda *path: pjoin(components, 'MathJax', *path)\n    static_data.extend([\n        mj('MathJax.js'),\n        mj('config', 'TeX-AMS-MML_HTMLorMML-full.js'),\n        mj('config', 'Safe.js'),\n    ])\n    trees = []\n    mj_out = mj('jax', 'output')\n    if os.path.exists(mj_out):\n        for output in os.listdir(mj_out):\n            path = pjoin(mj_out, output)\n            static_data.append(pjoin(path, '*.js'))\n            autoload = pjoin(path, 'autoload')\n            if os.path.isdir(autoload):\n                trees.append(autoload)\n    for tree in trees + [\n        mj('localization'),\n        mj('fonts', 'HTML-CSS', 'STIX-Web', 'woff'),\n        mj('extensions'),\n        mj('jax', 'input', 'TeX'),\n        mj('jax', 'output', 'HTML-CSS', 'fonts', 'STIX-Web'),\n        mj('jax', 'output', 'SVG', 'fonts', 'STIX-Web'),\n        mj('jax', 'element', 'mml'),\n    ]:\n        for parent, dirs, files in os.walk(tree):\n            for f in files:\n                static_data.append(pjoin(parent, f))\n    os.chdir(os.path.join('tests',))\n    js_tests = glob('*.js') + glob('*/*.js')\n    os.chdir(cwd)\n    package_data = {\n        'notebook' : ['templates/*'] + static_data,\n        'notebook.tests' : js_tests,\n        'notebook.bundler.tests': ['resources/*', 'resources/*/*', 'resources/*/*/.*'],\n        'notebook.services.api': ['api.yaml'],\n        'notebook.i18n': ['*/LC_MESSAGES/*.*'],\n    }\n    return package_data\n    def should_run(self):\n        if self.force:\n            return True\n        if not os.path.exists(self.bower_dir):\n            return True\n        return mtime(self.bower_dir) < mtime(pjoin(repo_root, 'bower.json'))",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-32798",
        "description": "[{'lang': 'en', 'value': 'The Jupyter notebook is a web-based notebook environment for interactive computing. In affected versions untrusted notebook can execute code on load. Jupyter Notebook uses a deprecated version of Google Caja to sanitize user inputs. A public Caja bypass can be used to trigger an XSS when a victim opens a malicious ipynb document in Jupyter Notebook. The XSS allows an attacker to execute arbitrary code on the victim computer using Jupyter APIs.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-28",
      "code": "    def build_IR(self, expr, args, kwargs, context):\n        to, data = args\n        gas, value, outsize, delegate_call, static_call, revert_on_failure = (\n            kwargs[\"gas\"],\n            kwargs[\"value\"],\n            kwargs[\"max_outsize\"],\n            kwargs[\"is_delegate_call\"],\n            kwargs[\"is_static_call\"],\n            kwargs[\"revert_on_failure\"],\n        )\n        if delegate_call and static_call:\n            raise ArgumentException(\n                \"Call may use one of `is_delegate_call` or `is_static_call`, not both\", expr\n            )\n        if not static_call and context.is_constant():\n            raise StateAccessViolation(\n                f\"Cannot make modifying calls from {context.pp_constancy()},\"\n                \" use `is_static_call=True` to perform this action\",\n                expr,\n            )\n        if data.value == \"~calldata\":\n            call_ir = [\"with\", \"mem_ofst\", \"msize\"]\n            args_ofst = [\"seq\", [\"calldatacopy\", \"mem_ofst\", 0, \"calldatasize\"], \"mem_ofst\"]\n            args_len = \"calldatasize\"\n        else:\n            eval_input_buf = ensure_in_memory(data, context)\n            input_buf = eval_seq(eval_input_buf)\n            if input_buf is None:\n                call_ir = [\"with\", \"arg_buf\", eval_input_buf]\n                input_buf = IRnode.from_list(\"arg_buf\")\n            else:\n                call_ir = [\"seq\", eval_input_buf]\n            args_ofst = add_ofst(input_buf, 32)\n            args_len = [\"mload\", input_buf]\n        output_node = IRnode.from_list(\n            context.new_internal_variable(BytesT(outsize)), typ=BytesT(outsize), location=MEMORY\n        )\n        bool_ty = BoolT()\n        common_call_args = [\n            args_ofst,\n            args_len,\n            add_ofst(output_node, 32) if outsize else 0,\n            outsize,\n        ]\n        if delegate_call:\n            call_op = [\"delegatecall\", gas, to, *common_call_args]\n        elif static_call:\n            call_op = [\"staticcall\", gas, to, *common_call_args]\n        else:\n            call_op = [\"call\", gas, to, value, *common_call_args]\n        call_ir += [call_op]\n        if outsize:\n            size = [\"select\", [\"lt\", outsize, \"returndatasize\"], outsize, \"returndatasize\"]\n            store_output_size = [\"seq\", [\"mstore\", output_node, size], output_node]\n            bytes_ty = BytesT(outsize)\n            if revert_on_failure:\n                typ = bytes_ty\n                ret_ir = [\"seq\", check_external_call(call_ir), store_output_size]\n            else:\n                typ = TupleT([bool_ty, bytes_ty])\n                ret_ir = [\n                    \"multi\",\n                    IRnode.from_list(call_ir, typ=bool_ty),\n                    IRnode.from_list(store_output_size, typ=bytes_ty, location=MEMORY),\n                ]\n        else:\n            if revert_on_failure:\n                typ = None\n                ret_ir = check_external_call(call_ir)\n            else:\n                typ = bool_ty\n                ret_ir = call_ir\n        return IRnode.from_list(ret_ir, typ=typ, location=MEMORY)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-30629",
        "description": "[{'lang': 'en', 'value': 'Vyper is a Pythonic Smart Contract Language for the ethereum virtual machine. In versions 0.3.1 through 0.3.7, the Vyper compiler generates the wrong bytecode. Any contract that uses the `raw_call` with `revert_on_failure=False` and `max_outsize=0` receives the wrong response from `raw_call`. Depending on the memory garbage, the result can be either `True` or `False`. A patch is available and, as of time of publication, anticipated to be part of Vyper 0.3.8. As a workaround, one may always put  `max_outsize>0`.'}]",
        "cwe_number": 670
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-29",
      "code": "def _get_object(data, position, as_class, tz_aware, uuid_subtype):\n    obj_size = struct.unpack(\"<i\", data[position:position + 4])[0]\n    encoded = data[position + 4:position + obj_size - 1]\n    object = _elements_to_dict(encoded, as_class, tz_aware, uuid_subtype)\n    position += obj_size\n    if \"$ref\" in object:\n        return (DBRef(object.pop(\"$ref\"), object.pop(\"$id\"),\n                      object.pop(\"$db\", None), object), position)\n    return object, position",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2013-2132",
        "description": "[{'lang': 'en', 'value': 'bson/_cbsonmodule.c in the mongo-python-driver (aka. pymongo) before 2.5.2, as used in MongoDB, allows context-dependent attackers to cause a denial of service (NULL pointer dereference and crash) via vectors related to decoding of an \"invalid DBRef.\"'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-30",
      "code": "    def get(self):\n        server_id = request.args[\"server\"]\n        path = request.args[\"path\"]\n        servers = current_app.config[\"BYOND_SERVERS\"]\n        assert path\n        assert server_id\n        server = None\n        for srv in servers:\n            if srv.id == server_id:\n                server = srv\n                break\n        if server is None:\n            abort(404)\n        file_path = os.path.join(server.logs_path, path)\n        return send_file(file_path, as_attachment=True)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-31501",
        "description": "[{'lang': 'en', 'value': 'The ChaoticOnyx/OnyxForum repository before 2022-05-04 on GitHub allows absolute path traversal because the Flask send_file function is used unsafely.'}]",
        "cwe_number": 22
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-31",
      "code": "    def _floatToString(self, x):\n        if -1e-10 < x < 1e-10:\n            return '0'\n        elif -1e-10 < int(x) - x < 1e-10:\n            return str(int(x))\n        else:\n            return str(x)\n    def _complexToString(self, x):\n        realS = self._floatToString(x.real)\n        imagS = self._floatToString(x.imag)\n        if imagS == '0':\n            return realS\n        elif imagS == '1':\n            imagS = '+i'\n        elif imagS == '-1':\n            imagS = '-i'\n        elif x.imag < 0:\n            imagS = '%si' % imagS\n        else:\n            imagS = '+%si' % imagS\n        if realS == '0' and imagS == '0':\n            return '0'\n        elif realS == '0':\n            return imagS.lstrip('+')\n        elif imagS == '0':\n            return realS\n        else:\n            return '%s%s' % (realS, imagS)\n    _calc_match_forbidden_chars = re.compile('[_\\[\\]]')\n    _calc_remover = utils.str.MultipleRemover('_[] \\t')\n    def calc(self, irc, msg, args, text):\n        \"\"\"<math expression>\n        Returns the value of the evaluated <math expression>.  The syntax is\n        Python syntax; the type of arithmetic is floating point.  Floating\n        point arithmetic is used in order to prevent a user from being able to\n        crash to the bot with something like '10**10**10**10'.  One consequence\n        is that large values such as '10**24' might not be exact.\n        \"\"\"\n        try:\n            text = str(text)\n        except UnicodeEncodeError:\n            irc.error(_(\"There's no reason you should have fancy non-ASCII \"\n                            \"characters in your mathematical expression. \"\n                            \"Please remove them.\"))\n            return\n        if self._calc_match_forbidden_chars.match(text):\n            irc.error(_('There\\'s really no reason why you should have '\n                           'underscores or brackets in your mathematical '\n                           'expression.  Please remove them.'))\n            return\n        text = self._calc_remover(text)\n        if 'lambda' in text:\n            irc.error(_('You can\\'t use lambda in this command.'))\n            return\n        text = text.lower()\n        def handleMatch(m):\n            s = m.group(1)\n            if s.startswith('0x'):\n                i = int(s, 16)\n            elif s.startswith('0') and '.' not in s:\n                try:\n                    i = int(s, 8)\n                except ValueError:\n                    i = int(s)\n            else:\n                i = float(s)\n            x = complex(i)\n            if x.imag == 0:\n                x = x.real\n                return '%.16f' % x\n            return str(x)\n        text = self._mathRe.sub(handleMatch, text)\n        try:\n            self.log.info('evaluating %q from %s', text, msg.prefix)\n            x = complex(eval(text, self._mathSafeEnv, self._mathSafeEnv))\n            irc.reply(self._complexToString(x))\n        except OverflowError:\n            maxFloat = math.ldexp(0.9999999999999999, 1024)\n            irc.error(_('The answer exceeded %s or so.') % maxFloat)\n        except TypeError:\n            irc.error(_('Something in there wasn\\'t a valid number.'))\n        except NameError as e:\n            irc.error(_('%s is not a defined function.') % str(e).split()[1])\n        except Exception as e:\n            irc.error(str(e))",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2019-19010",
        "description": "[{'lang': 'en', 'value': 'Eval injection in the Math plugin of Limnoria (before 2019.11.09) and Supybot (through 2018-05-09) allows remote unprivileged attackers to disclose information or possibly have unspecified other impact via the calc and icalc IRC commands.'}]",
        "cwe_number": 94
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-32",
      "code": "    def create_app(\n        blocks: gradio.Blocks, app_kwargs: Dict[str, Any] | None = None\n    ) -> App:\n        app_kwargs = app_kwargs or {}\n        app_kwargs.setdefault(\"default_response_class\", ORJSONResponse)\n        app = App(**app_kwargs)\n        app.configure_app(blocks)\n        if not wasm_utils.IS_WASM:\n            app.add_middleware(\n                CORSMiddleware,\n                allow_origins=[\"*\"],\n                allow_methods=[\"*\"],\n                allow_headers=[\"*\"],\n            )\n        @app.get(\"/user\")\n        @app.get(\"/user/\")\n        def get_current_user(request: fastapi.Request) -> Optional[str]:\n            token = request.cookies.get(\n                f\"access-token-{app.cookie_id}\"\n            ) or request.cookies.get(f\"access-token-unsecure-{app.cookie_id}\")\n            return app.tokens.get(token)\n        @app.get(\"/login_check\")\n        @app.get(\"/login_check/\")\n        def login_check(user: str = Depends(get_current_user)):\n            if app.auth is None or user is not None:\n                return\n            raise HTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Not authenticated\"\n            )\n        @app.get(\"/token\")\n        @app.get(\"/token/\")\n        def get_token(request: fastapi.Request) -> dict:\n            token = request.cookies.get(f\"access-token-{app.cookie_id}\")\n            return {\"token\": token, \"user\": app.tokens.get(token)}\n        @app.get(\"/app_id\")\n        @app.get(\"/app_id/\")\n        def app_id(request: fastapi.Request) -> dict:\n            return {\"app_id\": app.get_blocks().app_id}\n        @app.get(\"/dev/reload\", dependencies=[Depends(login_check)])\n        async def notify_changes(\n            request: fastapi.Request,\n        ):\n            async def reload_checker(request: fastapi.Request):\n                heartbeat_rate = 15\n                check_rate = 0.05\n                last_heartbeat = time.perf_counter()\n                while True:\n                    if await request.is_disconnected():\n                        return\n                    if app.change_event and app.change_event.is_set():\n                        app.change_event.clear()\n                        yield \"\"\"data: CHANGE\\n\\n\"\"\"\n                    await asyncio.sleep(check_rate)\n                    if time.perf_counter() - last_heartbeat > heartbeat_rate:\n                        yield \"\"\"data: HEARTBEAT\\n\\n\"\"\"\n                        last_heartbeat = time.time()\n            return StreamingResponse(\n                reload_checker(request),\n                media_type=\"text/event-stream\",\n            )\n        @app.post(\"/login\")\n        @app.post(\"/login/\")\n        def login(form_data: OAuth2PasswordRequestForm = Depends()):\n            username, password = form_data.username.strip(), form_data.password\n            if app.auth is None:\n                return RedirectResponse(url=\"/\", status_code=status.HTTP_302_FOUND)\n            if (\n                not callable(app.auth)\n                and username in app.auth\n                and app.auth[username] == password\n            ) or (callable(app.auth) and app.auth.__call__(username, password)):\n                token = secrets.token_urlsafe(16)\n                app.tokens[token] = username\n                response = JSONResponse(content={\"success\": True})\n                response.set_cookie(\n                    key=f\"access-token-{app.cookie_id}\",\n                    value=token,\n                    httponly=True,\n                    samesite=\"none\",\n                    secure=True,\n                )\n                response.set_cookie(\n                    key=f\"access-token-unsecure-{app.cookie_id}\",\n                    value=token,\n                    httponly=True,\n                )\n                return response\n            else:\n                raise HTTPException(status_code=400, detail=\"Incorrect credentials.\")\n        if app.blocks is not None and app.blocks.expects_oauth:\n            attach_oauth(app)\n        @app.head(\"/\", response_class=HTMLResponse)\n        @app.get(\"/\", response_class=HTMLResponse)\n        def main(request: fastapi.Request, user: str = Depends(get_current_user)):\n            mimetypes.add_type(\"application/javascript\", \".js\")\n            blocks = app.get_blocks()\n            root_path = (\n                request.scope.get(\"root_path\")\n                or request.headers.get(\"X-Direct-Url\")\n                or \"\"\n            )\n            if app.auth is None or user is not None:\n                config = app.get_blocks().config\n                config[\"root\"] = route_utils.strip_url(root_path)\n            else:\n                config = {\n                    \"auth_required\": True,\n                    \"auth_message\": blocks.auth_message,\n                    \"space_id\": app.get_blocks().space_id,\n                    \"root\": route_utils.strip_url(root_path),\n                }\n            try:\n                template = (\n                    \"frontend/share.html\" if blocks.share else \"frontend/index.html\"\n                )\n                return templates.TemplateResponse(\n                    template,\n                    {\"request\": request, \"config\": config},\n                )\n            except TemplateNotFound as err:\n                if blocks.share:\n                    raise ValueError(\n                        \"Did you install Gradio from source files? Share mode only \"\n                        \"works when Gradio is installed through the pip package.\"\n                    ) from err\n                else:\n                    raise ValueError(\n                        \"Did you install Gradio from source files? You need to build \"\n                        \"the frontend by running /scripts/build_frontend.sh\"\n                    ) from err\n        @app.get(\"/info/\", dependencies=[Depends(login_check)])\n        @app.get(\"/info\", dependencies=[Depends(login_check)])\n        def api_info(serialize: bool = True):\n            return app.get_blocks().get_api_info()\n        @app.get(\"/config/\", dependencies=[Depends(login_check)])\n        @app.get(\"/config\", dependencies=[Depends(login_check)])\n        def get_config(request: fastapi.Request):\n            root_path = (\n                request.scope.get(\"root_path\")\n                or request.headers.get(\"X-Direct-Url\")\n                or \"\"\n            )\n            config = app.get_blocks().config\n            config[\"root\"] = route_utils.strip_url(root_path)\n            return config\n        @app.get(\"/static/{path:path}\")\n        def static_resource(path: str):\n            static_file = safe_join(STATIC_PATH_LIB, path)\n            return FileResponse(static_file)\n        @app.get(\"/custom_component/{id}/{type}/{file_name}\")\n        def custom_component_path(id: str, type: str, file_name: str):\n            config = app.get_blocks().config\n            components = config[\"components\"]\n            location = next(\n                (item for item in components if item[\"component_class_id\"] == id), None\n            )\n            if location is None:\n                raise HTTPException(status_code=404, detail=\"Component not found.\")\n            component_instance = app.get_blocks().get_component(location[\"id\"])\n            module_name = component_instance.__class__.__module__\n            module_path = sys.modules[module_name].__file__\n            if module_path is None or component_instance is None:\n                raise HTTPException(status_code=404, detail=\"Component not found.\")\n            return FileResponse(\n                safe_join(\n                    str(Path(module_path).parent),\n                    f\"{component_instance.__class__.TEMPLATE_DIR}/{type}/{file_name}\",\n                )\n            )\n        @app.get(\"/assets/{path:path}\")\n        def build_resource(path: str):\n            build_file = safe_join(BUILD_PATH_LIB, path)\n            return FileResponse(build_file)\n        @app.get(\"/favicon.ico\")\n        async def favicon():\n            blocks = app.get_blocks()\n            if blocks.favicon_path is None:\n                return static_resource(\"img/logo.svg\")\n            else:\n                return FileResponse(blocks.favicon_path)\n        @app.head(\"/proxy={url_path:path}\", dependencies=[Depends(login_check)])\n        @app.get(\"/proxy={url_path:path}\", dependencies=[Depends(login_check)])\n        async def reverse_proxy(url_path: str):\n            try:\n                rp_req = app.build_proxy_request(url_path)\n            except PermissionError as err:\n                raise HTTPException(status_code=400, detail=str(err)) from err\n            rp_resp = await client.send(rp_req, stream=True)\n            return StreamingResponse(\n                rp_resp.aiter_raw(),\n                status_code=rp_resp.status_code,\n                headers=rp_resp.headers,\n                background=BackgroundTask(rp_resp.aclose),\n            )\n        @app.head(\"/file={path_or_url:path}\", dependencies=[Depends(login_check)])\n        @app.get(\"/file={path_or_url:path}\", dependencies=[Depends(login_check)])\n        async def file(path_or_url: str, request: fastapi.Request):\n            blocks = app.get_blocks()\n            if utils.validate_url(path_or_url):\n                return RedirectResponse(\n                    url=path_or_url, status_code=status.HTTP_302_FOUND\n                )\n            abs_path = utils.abspath(path_or_url)\n            in_blocklist = any(\n                utils.is_in_or_equal(abs_path, blocked_path)\n                for blocked_path in blocks.blocked_paths\n            )\n            is_dir = abs_path.is_dir()\n            if in_blocklist or is_dir:\n                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")\n            created_by_app = str(abs_path) in set().union(*blocks.temp_file_sets)\n            in_allowlist = any(\n                utils.is_in_or_equal(abs_path, allowed_path)\n                for allowed_path in blocks.allowed_paths\n            )\n            was_uploaded = utils.is_in_or_equal(abs_path, app.uploaded_file_dir)\n            is_cached_example = utils.is_in_or_equal(\n                abs_path, utils.abspath(CACHED_FOLDER)\n            )\n            if not (\n                created_by_app or in_allowlist or was_uploaded or is_cached_example\n            ):\n                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")\n            if not abs_path.exists():\n                raise HTTPException(404, f\"File not found: {path_or_url}.\")\n            range_val = request.headers.get(\"Range\", \"\").strip()\n            if range_val.startswith(\"bytes=\") and \"-\" in range_val:\n                range_val = range_val[6:]\n                start, end = range_val.split(\"-\")\n                if start.isnumeric() and end.isnumeric():\n                    start = int(start)\n                    end = int(end)\n                    response = ranged_response.RangedFileResponse(\n                        abs_path,\n                        ranged_response.OpenRange(start, end),\n                        dict(request.headers),\n                        stat_result=os.stat(abs_path),\n                    )\n                    return response\n            return FileResponse(abs_path, headers={\"Accept-Ranges\": \"bytes\"})\n        @app.get(\n            \"/stream/{session_hash}/{run}/{component_id}\",\n            dependencies=[Depends(login_check)],\n        )\n        async def stream(\n            session_hash: str, run: int, component_id: int, request: fastapi.Request\n        ):\n            stream: list = (\n                app.get_blocks()\n                .pending_streams[session_hash]\n                .get(run, {})\n                .get(component_id, None)\n            )\n            if stream is None:\n                raise HTTPException(404, \"Stream not found.\")\n            def stream_wrapper():\n                check_stream_rate = 0.01\n                max_wait_time = 120\n                wait_time = 0\n                while True:\n                    if len(stream) == 0:\n                        if wait_time > max_wait_time:\n                            return\n                        wait_time += check_stream_rate\n                        time.sleep(check_stream_rate)\n                        continue\n                    wait_time = 0\n                    next_stream = stream.pop(0)\n                    if next_stream is None:\n                        return\n                    yield next_stream\n            return StreamingResponse(stream_wrapper())\n        @app.get(\"/file/{path:path}\", dependencies=[Depends(login_check)])\n        async def file_deprecated(path: str, request: fastapi.Request):\n            return await file(path, request)\n        @app.post(\"/reset/\")\n        @app.post(\"/reset\")\n        async def reset_iterator(body: ResetBody):\n            if body.event_id not in app.iterators:\n                return {\"success\": False}\n            async with app.lock:\n                del app.iterators[body.event_id]\n                app.iterators_to_reset.add(body.event_id)\n                await app.get_blocks()._queue.clean_events(event_id=body.event_id)\n            return {\"success\": True}\n        @app.post(\"/run/{api_name}\", dependencies=[Depends(login_check)])\n        @app.post(\"/run/{api_name}/\", dependencies=[Depends(login_check)])\n        @app.post(\"/api/{api_name}\", dependencies=[Depends(login_check)])\n        @app.post(\"/api/{api_name}/\", dependencies=[Depends(login_check)])\n        async def predict(\n            api_name: str,\n            body: PredictBody,\n            request: fastapi.Request,\n            username: str = Depends(get_current_user),\n        ):\n            fn_index_inferred = route_utils.infer_fn_index(\n                app=app, api_name=api_name, body=body\n            )\n            if not app.get_blocks().api_open and app.get_blocks().queue_enabled_for_fn(\n                fn_index_inferred\n            ):\n                raise HTTPException(\n                    detail=\"This API endpoint does not accept direct HTTP POST requests. Please join the queue to use this API.\",\n                    status_code=status.HTTP_404_NOT_FOUND,\n                )\n            gr_request = route_utils.compile_gr_request(\n                app,\n                body,\n                fn_index_inferred=fn_index_inferred,\n                username=username,\n                request=request,\n            )\n            try:\n                output = await route_utils.call_process_api(\n                    app=app,\n                    body=body,\n                    gr_request=gr_request,\n                    fn_index_inferred=fn_index_inferred,\n                )\n            except BaseException as error:\n                show_error = app.get_blocks().show_error or isinstance(error, Error)\n                traceback.print_exc()\n                return JSONResponse(\n                    content={\"error\": str(error) if show_error else None},\n                    status_code=500,\n                )\n            return output\n        @app.get(\"/queue/data\", dependencies=[Depends(login_check)])\n        async def queue_data(\n            request: fastapi.Request,\n            session_hash: str,\n        ):\n            blocks = app.get_blocks()\n            async def sse_stream(request: fastapi.Request):\n                try:\n                    last_heartbeat = time.perf_counter()\n                    while True:\n                        if await request.is_disconnected():\n                            await blocks._queue.clean_events(session_hash=session_hash)\n                            return\n                        if (\n                            session_hash\n                            not in blocks._queue.pending_messages_per_session\n                        ):\n                            raise HTTPException(\n                                status_code=status.HTTP_404_NOT_FOUND,\n                                detail=\"Session not found.\",\n                            )\n                        heartbeat_rate = 15\n                        check_rate = 0.05\n                        message = None\n                        try:\n                            messages = blocks._queue.pending_messages_per_session[\n                                session_hash\n                            ]\n                            message = messages.get_nowait()\n                        except EmptyQueue:\n                            await asyncio.sleep(check_rate)\n                            if time.perf_counter() - last_heartbeat > heartbeat_rate:\n                                message = {\"msg\": ServerMessage.heartbeat}\n                                last_heartbeat = time.perf_counter()\n                        if blocks._queue.stopped:\n                            message = {\n                                \"msg\": ServerMessage.server_stopped,\n                                \"success\": False,\n                            }\n                        if message:\n                            yield f\"data: {json.dumps(message)}\\n\\n\"\n                            if message[\"msg\"] == ServerMessage.process_completed:\n                                blocks._queue.pending_event_ids_session[\n                                    session_hash\n                                ].remove(message[\"event_id\"])\n                                if message[\"msg\"] == ServerMessage.server_stopped or (\n                                    message[\"msg\"] == ServerMessage.process_completed\n                                    and (\n                                        len(\n                                            blocks._queue.pending_event_ids_session[\n                                                session_hash\n                                            ]\n                                        )\n                                        == 0\n                                    )\n                                ):\n                                    return\n                except asyncio.CancelledError as e:\n                    del blocks._queue.pending_messages_per_session[session_hash]\n                    await blocks._queue.clean_events(session_hash=session_hash)\n                    raise e\n            return StreamingResponse(\n                sse_stream(request),\n                media_type=\"text/event-stream\",\n            )\n        @app.post(\"/queue/join\", dependencies=[Depends(login_check)])\n        async def queue_join(\n            body: PredictBody,\n            request: fastapi.Request,\n            username: str = Depends(get_current_user),\n        ):\n            if blocks._queue.server_app is None:\n                blocks._queue.set_server_app(app)\n            success, event_id = await blocks._queue.push(body, request, username)\n            if not success:\n                status_code = (\n                    status.HTTP_503_SERVICE_UNAVAILABLE\n                    if \"Queue is full.\" in event_id\n                    else status.HTTP_400_BAD_REQUEST\n                )\n                raise HTTPException(status_code=status_code, detail=event_id)\n            return {\"event_id\": event_id}\n        @app.post(\"/component_server\", dependencies=[Depends(login_check)])\n        @app.post(\"/component_server/\", dependencies=[Depends(login_check)])\n        def component_server(body: ComponentServerBody):\n            state = app.state_holder[body.session_hash]\n            component_id = body.component_id\n            block: Block\n            if component_id in state:\n                block = state[component_id]\n            else:\n                block = app.get_blocks().blocks[component_id]\n            fn = getattr(block, body.fn_name)\n            return fn(body.data)\n        @app.get(\n            \"/queue/status\",\n            dependencies=[Depends(login_check)],\n            response_model=Estimation,\n        )\n        async def get_queue_status():\n            return app.get_blocks()._queue.get_estimation()\n        @app.get(\"/upload_progress\")\n        def get_upload_progress(upload_id: str, request: fastapi.Request):\n            async def sse_stream(request: fastapi.Request):\n                last_heartbeat = time.perf_counter()\n                is_done = False\n                while True:\n                    if await request.is_disconnected():\n                        file_upload_statuses.stop_tracking(upload_id)\n                        return\n                    if is_done:\n                        file_upload_statuses.stop_tracking(upload_id)\n                        return\n                    heartbeat_rate = 15\n                    check_rate = 0.05\n                    message = None\n                    try:\n                        if update := file_upload_statuses.status(upload_id).popleft():\n                            if update.is_done:\n                                message = {\"msg\": \"done\"}\n                                is_done = True\n                            else:\n                                message = {\n                                    \"msg\": \"update\",\n                                    \"orig_name\": update.filename,\n                                    \"chunk_size\": update.chunk_size,\n                                }\n                        else:\n                            await asyncio.sleep(check_rate)\n                            if time.perf_counter() - last_heartbeat > heartbeat_rate:\n                                message = {\"msg\": \"heartbeat\"}\n                                last_heartbeat = time.perf_counter()\n                        if message:\n                            yield f\"data: {json.dumps(message)}\\n\\n\"\n                    except IndexError:\n                        if not file_upload_statuses.is_tracked(upload_id):\n                            return\n                        continue\n            return StreamingResponse(\n                sse_stream(request),\n                media_type=\"text/event-stream\",\n            )\n        @app.post(\"/upload\", dependencies=[Depends(login_check)])\n        async def upload_file(\n            request: fastapi.Request,\n            bg_tasks: BackgroundTasks,\n            upload_id: Optional[str] = None,\n        ):\n            content_type_header = request.headers.get(\"Content-Type\")\n            content_type: bytes\n            content_type, _ = parse_options_header(content_type_header)\n            if content_type != b\"multipart/form-data\":\n                raise HTTPException(status_code=400, detail=\"Invalid content type.\")\n            try:\n                if upload_id:\n                    file_upload_statuses.track(upload_id)\n                multipart_parser = GradioMultiPartParser(\n                    request.headers,\n                    request.stream(),\n                    max_files=1000,\n                    max_fields=1000,\n                    upload_id=upload_id if upload_id else None,\n                    upload_progress=file_upload_statuses if upload_id else None,\n                )\n                form = await multipart_parser.parse()\n            except MultiPartException as exc:\n                raise HTTPException(status_code=400, detail=exc.message) from exc\n            output_files = []\n            files_to_copy = []\n            locations: list[str] = []\n            for temp_file in form.getlist(\"files\"):\n                assert isinstance(temp_file, GradioUploadFile)\n                if temp_file.filename:\n                    file_name = Path(temp_file.filename).name\n                    name = client_utils.strip_invalid_filename_characters(file_name)\n                else:\n                    name = f\"tmp{secrets.token_hex(5)}\"\n                directory = Path(app.uploaded_file_dir) / temp_file.sha.hexdigest()\n                directory.mkdir(exist_ok=True, parents=True)\n                dest = (directory / name).resolve()\n                temp_file.file.close()\n                try:\n                    os.rename(temp_file.file.name, dest)\n                except OSError:\n                    files_to_copy.append(temp_file.file.name)\n                    locations.append(str(dest))\n                output_files.append(dest)\n            if files_to_copy:\n                bg_tasks.add_task(\n                    move_uploaded_files_to_cache, files_to_copy, locations\n                )\n            return output_files\n        @app.on_event(\"startup\")\n        @app.get(\"/startup-events\")\n        async def startup_events():\n            if not app.startup_events_triggered:\n                app.get_blocks().startup_events()\n                app.startup_events_triggered = True\n                return True\n            return False\n        @app.get(\"/theme.css\", response_class=PlainTextResponse)\n        def theme_css():\n            return PlainTextResponse(app.get_blocks().theme_css, media_type=\"text/css\")\n        @app.get(\"/robots.txt\", response_class=PlainTextResponse)\n        def robots_txt():\n            if app.get_blocks().share:\n                return \"User-agent: *\\nDisallow: /\"\n            else:\n                return \"User-agent: *\\nDisallow: \"\n        return app",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-51449",
        "description": "[{'lang': 'en', 'value': 'Gradio is an open-source Python package that allows you to quickly build a demo or web application for your machine learning model, API, or any arbitary Python function. Versions of `gradio` prior to 4.11.0 contained a vulnerability in the `/file` route which made them susceptible to file traversal attacks in which an attacker could access arbitrary files on a machine running a Gradio app with a public URL (e.g. if the demo was created with `share=True`, or on Hugging Face Spaces) if they knew the path of files to look for. This issue has been patched in version 4.11.0.'}]",
        "cwe_number": 22
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-33",
      "code": "def parse_soap_enveloped_saml(text, body_class, header_class=None):\n    \"\"\"Parses a SOAP enveloped SAML thing and returns header parts and body\n    :param text: The SOAP object as XML\n    :return: header parts and body as saml.samlbase instances\n    \"\"\"\n    envelope = ElementTree.fromstring(text)\n    assert envelope.tag == '{%s}Envelope' % NAMESPACE\n    body = None\n    header = {}\n    for part in envelope:\n        if part.tag == '{%s}Body' % NAMESPACE:\n            for sub in part:\n                try:\n                    body = saml2.create_class_from_element_tree(body_class, sub)\n                except Exception:\n                    raise Exception(\n                        \"Wrong body type (%s) in SOAP envelope\" % sub.tag)\n        elif part.tag == '{%s}Header' % NAMESPACE:\n            if not header_class:\n                raise Exception(\"Header where I didn't expect one\")\n            for sub in part:\n                for klass in header_class:\n                    if sub.tag == \"{%s}%s\" % (klass.c_namespace, klass.c_tag):\n                        header[sub.tag] = \\\n                            saml2.create_class_from_element_tree(klass, sub)\n                        break\n    return body, header\ndef parse_soap_enveloped_saml_thingy(text, expected_tags):\n    \"\"\"Parses a SOAP enveloped SAML thing and returns the thing as\n    a string.\n    :param text: The SOAP object as XML string\n    :param expected_tags: What the tag of the SAML thingy is expected to be.\n    :return: SAML thingy as a string\n    \"\"\"\n    envelope = ElementTree.fromstring(text)\n    assert envelope.tag == '{%s}Envelope' % soapenv.NAMESPACE\n    assert len(envelope) >= 1\n    body = None\n    for part in envelope:\n        if part.tag == '{%s}Body' % soapenv.NAMESPACE:\n            assert len(part) == 1\n            body = part\n            break\n    if body is None:\n        return \"\"\n    saml_part = body[0]\n    if saml_part.tag in expected_tags:\n        return ElementTree.tostring(saml_part, encoding=\"UTF-8\")\n    else:\n        raise WrongMessageType(\"Was '%s' expected one of %s\" % (saml_part.tag,\n                                                                expected_tags))\ndef class_instances_from_soap_enveloped_saml_thingies(text, modules):\n    \"\"\"Parses a SOAP enveloped header and body SAML thing and returns the\n    thing as a dictionary class instance.\n    :param text: The SOAP object as XML\n    :param modules: modules representing xsd schemas\n    :return: The body and headers as class instances\n    \"\"\"\n    try:\n        envelope = ElementTree.fromstring(text)\n    except Exception as exc:\n        raise XmlParseError(\"%s\" % exc)\n    assert envelope.tag == '{%s}Envelope' % soapenv.NAMESPACE\n    assert len(envelope) >= 1\n    env = {\"header\": [], \"body\": None}\n    for part in envelope:\n        if part.tag == '{%s}Body' % soapenv.NAMESPACE:\n            assert len(part) == 1\n            env[\"body\"] = instanciate_class(part[0], modules)\n        elif part.tag == \"{%s}Header\" % soapenv.NAMESPACE:\n            for item in part:\n                env[\"header\"].append(instanciate_class(item, modules))\n    return env\ndef open_soap_envelope(text):\n    \"\"\"\n    :param text: SOAP message\n    :return: dictionary with two keys \"body\"/\"header\"\n    \"\"\"\n    try:\n        envelope = ElementTree.fromstring(text)\n    except Exception as exc:\n        raise XmlParseError(\"%s\" % exc)\n    assert envelope.tag == '{%s}Envelope' % soapenv.NAMESPACE\n    assert len(envelope) >= 1\n    content = {\"header\": [], \"body\": None}\n    for part in envelope:\n        if part.tag == '{%s}Body' % soapenv.NAMESPACE:\n            assert len(part) == 1\n            content[\"body\"] = ElementTree.tostring(part[0], encoding=\"UTF-8\")\n        elif part.tag == \"{%s}Header\" % soapenv.NAMESPACE:\n            for item in part:\n                _str = ElementTree.tostring(item, encoding=\"UTF-8\")\n                content[\"header\"].append(_str)\n    return content\ndef create_class_from_xml_string(target_class, xml_string):\n    \"\"\"Creates an instance of the target class from a string.\n    :param target_class: The class which will be instantiated and populated\n        with the contents of the XML. This class must have a c_tag and a\n        c_namespace class variable.\n    :param xml_string: A string which contains valid XML. The root element\n        of the XML string should match the tag and namespace of the desired\n        class.\n    :return: An instance of the target class with members assigned according to\n        the contents of the XML - or None if the root XML tag and namespace did\n        not match those of the target class.\n    \"\"\"\n    if not isinstance(xml_string, six.binary_type):\n        xml_string = xml_string.encode('utf-8')\n    tree = ElementTree.fromstring(xml_string)\n    return create_class_from_element_tree(target_class, tree)\ndef extension_element_from_string(xml_string):\n    element_tree = ElementTree.fromstring(xml_string)\n    return _extension_element_from_element_tree(element_tree)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2016-10149",
        "description": "[{'lang': 'en', 'value': 'XML External Entity (XXE) vulnerability in PySAML2 4.4.0 and earlier allows remote attackers to read arbitrary files via a crafted SAML XML request or response.'}]",
        "cwe_number": 611
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-34",
      "code": "    def read_config(self, config, **kwargs):\n        self.enable_registration = strtobool(\n            str(config.get(\"enable_registration\", False))\n        )\n        if \"disable_registration\" in config:\n            self.enable_registration = not strtobool(\n                str(config[\"disable_registration\"])\n            )\n        self.account_validity = AccountValidityConfig(\n            config.get(\"account_validity\") or {}, config\n        )\n        self.registrations_require_3pid = config.get(\"registrations_require_3pid\", [])\n        self.allowed_local_3pids = config.get(\"allowed_local_3pids\", [])\n        self.enable_3pid_lookup = config.get(\"enable_3pid_lookup\", True)\n        self.registration_shared_secret = config.get(\"registration_shared_secret\")\n        self.bcrypt_rounds = config.get(\"bcrypt_rounds\", 12)\n        self.trusted_third_party_id_servers = config.get(\n            \"trusted_third_party_id_servers\", [\"matrix.org\", \"vector.im\"]\n        )\n        account_threepid_delegates = config.get(\"account_threepid_delegates\") or {}\n        self.account_threepid_delegate_email = account_threepid_delegates.get(\"email\")\n        self.account_threepid_delegate_msisdn = account_threepid_delegates.get(\"msisdn\")\n        self.default_identity_server = config.get(\"default_identity_server\")\n        self.allow_guest_access = config.get(\"allow_guest_access\", False)\n        if config.get(\"invite_3pid_guest\", False):\n            raise ConfigError(\"invite_3pid_guest is no longer supported\")\n        self.auto_join_rooms = config.get(\"auto_join_rooms\", [])\n        for room_alias in self.auto_join_rooms:\n            if not RoomAlias.is_valid(room_alias):\n                raise ConfigError(\"Invalid auto_join_rooms entry %s\" % (room_alias,))\n        self.autocreate_auto_join_rooms = config.get(\"autocreate_auto_join_rooms\", True)\n        self.autocreate_auto_join_rooms_federated = config.get(\n            \"autocreate_auto_join_rooms_federated\", True\n        )\n        self.autocreate_auto_join_room_preset = (\n            config.get(\"autocreate_auto_join_room_preset\")\n            or RoomCreationPreset.PUBLIC_CHAT\n        )\n        self.auto_join_room_requires_invite = self.autocreate_auto_join_room_preset in {\n            RoomCreationPreset.PRIVATE_CHAT,\n            RoomCreationPreset.TRUSTED_PRIVATE_CHAT,\n        }\n        mxid_localpart = config.get(\"auto_join_mxid_localpart\")\n        self.auto_join_user_id = None\n        if mxid_localpart:\n            self.auto_join_user_id = UserID(\n                mxid_localpart, self.server_name\n            ).to_string()\n        if self.autocreate_auto_join_rooms:\n            if self.autocreate_auto_join_room_preset not in {\n                RoomCreationPreset.PUBLIC_CHAT,\n                RoomCreationPreset.PRIVATE_CHAT,\n                RoomCreationPreset.TRUSTED_PRIVATE_CHAT,\n            }:\n                raise ConfigError(\"Invalid value for autocreate_auto_join_room_preset\")\n            if self.auto_join_room_requires_invite:\n                if not mxid_localpart:\n                    raise ConfigError(\n                        \"The configuration option `auto_join_mxid_localpart` is required if \"\n                        \"`autocreate_auto_join_room_preset` is set to private_chat or trusted_private_chat, such that \"\n                        \"Synapse knows who to send invitations from. Please \"\n                        \"configure `auto_join_mxid_localpart`.\"\n                    )\n        self.auto_join_rooms_for_guests = config.get(\"auto_join_rooms_for_guests\", True)\n        self.enable_set_displayname = config.get(\"enable_set_displayname\", True)\n        self.enable_set_avatar_url = config.get(\"enable_set_avatar_url\", True)\n        self.enable_3pid_changes = config.get(\"enable_3pid_changes\", True)\n        self.disable_msisdn_registration = config.get(\n            \"disable_msisdn_registration\", False\n        )\n        session_lifetime = config.get(\"session_lifetime\")\n        if session_lifetime is not None:\n            session_lifetime = self.parse_duration(session_lifetime)\n        self.session_lifetime = session_lifetime\n        self.fallback_success_template = self.read_templates(\n            [\"auth_success.html\"], autoescape=True\n        )[0]\n    def read_templates(\n        self,\n        filenames: List[str],\n        custom_template_directory: Optional[str] = None,\n        autoescape: bool = False,\n    ) -> List[jinja2.Template]:\n        \"\"\"Load a list of template files from disk using the given variables.\n        This function will attempt to load the given templates from the default Synapse\n        template directory. If `custom_template_directory` is supplied, that directory\n        is tried first.\n        Files read are treated as Jinja templates. These templates are not rendered yet.\n        Args:\n            filenames: A list of template filenames to read.\n            custom_template_directory: A directory to try to look for the templates\n                before using the default Synapse template directory instead.\n            autoescape: Whether to autoescape variables before inserting them into the\n                template.\n        Raises:\n            ConfigError: if the file's path is incorrect or otherwise cannot be read.\n        Returns:\n            A list of jinja2 templates.\n        \"\"\"\n        templates = []\n        search_directories = [self.default_template_dir]\n        if custom_template_directory:\n            if not self.path_exists(custom_template_directory):\n                raise ConfigError(\n                    \"Configured template directory does not exist: %s\"\n                    % (custom_template_directory,)\n                )\n            search_directories.insert(0, custom_template_directory)\n        loader = jinja2.FileSystemLoader(search_directories)\n        env = jinja2.Environment(loader=loader, autoescape=autoescape)\n        env.filters.update(\n            {\n                \"format_ts\": _format_ts_filter,\n                \"mxc_to_http\": _create_mxc_to_http_filter(self.public_baseurl),\n            }\n        )\n        for filename in filenames:\n            template = env.get_template(filename)\n            templates.append(template)\n        return templates\ndef _format_ts_filter(value: int, format: str):\n    return time.strftime(format, time.localtime(value / 1000))\ndef _create_mxc_to_http_filter(public_baseurl: str) -> Callable:\n    \"\"\"Create and return a jinja2 filter that converts MXC urls to HTTP\n    Args:\n        public_baseurl: The public, accessible base URL of the homeserver\n    \"\"\"\n    def mxc_to_http_filter(value, width, height, resize_method=\"crop\"):\n        if value[0:6] != \"mxc://\":\n            return \"\"\n        server_and_media_id = value[6:]\n        fragment = None\n        if \"\n            server_and_media_id, fragment = server_and_media_id.split(\"\n            fragment = \"\n        params = {\"width\": width, \"height\": height, \"method\": resize_method}\n        return \"%s_matrix/media/v1/thumbnail/%s?%s%s\" % (\n            public_baseurl,\n            server_and_media_id,\n            urllib.parse.urlencode(params),\n            fragment or \"\",\n        )\n    return mxc_to_http_filter\n    def read_config(self, config, **kwargs):\n        consent_config = config.get(\"user_consent\")\n        self.terms_template = self.read_templates([\"terms.html\"], autoescape=True)[0]\n        if consent_config is None:\n            return\n        self.user_consent_version = str(consent_config[\"version\"])\n        self.user_consent_template_dir = self.abspath(consent_config[\"template_dir\"])\n        if not path.isdir(self.user_consent_template_dir):\n            raise ConfigError(\n                \"Could not find template directory '%s'\"\n                % (self.user_consent_template_dir,)\n            )\n        self.user_consent_server_notice_content = consent_config.get(\n            \"server_notice_content\"\n        )\n        self.block_events_without_consent_error = consent_config.get(\n            \"block_events_error\"\n        )\n        self.user_consent_server_notice_to_guests = bool(\n            consent_config.get(\"send_server_notice_to_guests\", False)\n        )\n        self.user_consent_at_registration = bool(\n            consent_config.get(\"require_at_registration\", False)\n        )\n        self.user_consent_policy_name = consent_config.get(\n            \"policy_name\", \"Privacy Policy\"\n        )\ndef safe_markup(raw_html: str) -> jinja2.Markup:\n    return jinja2.Markup(\n        bleach.linkify(\n            bleach.clean(\n                raw_html,\n                tags=ALLOWED_TAGS,\n                attributes=ALLOWED_ATTRS,\n                strip=True,\n            )\n        )\n    )\ndef safe_text(raw_text: str) -> jinja2.Markup:\n    \"\"\"\n    Process text: treat it as HTML but escape any tags (ie. just escape the\n    HTML) then linkify it.\n    \"\"\"\n    return jinja2.Markup(\n        bleach.linkify(bleach.clean(raw_text, tags=[], attributes={}, strip=False))\n    )\ndef deduped_ordered_list(it: Iterable[T]) -> List[T]:\n    seen = set()\n    ret = []\n    for item in it:\n        if item not in seen:\n            seen.add(item)\n            ret.append(item)\n    return ret\ndef string_ordinal_total(s: str) -> int:\n    tot = 0\n    for c in s:\n        tot += ord(c)\n    return tot",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-21332",
        "description": "[{'lang': 'en', 'value': 'Synapse is a Matrix reference homeserver written in python (pypi package matrix-synapse). Matrix is an ecosystem for open federated Instant Messaging and VoIP. In Synapse before version 1.27.0, the password reset endpoint served via Synapse was vulnerable to cross-site scripting (XSS) attacks. The impact depends on the configuration of the domain that Synapse is deployed on, but may allow access to cookies and other browser data, CSRF vulnerabilities, and access to other resources served on the same domain or parent domains. This is fixed in version 1.27.0.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-35",
      "code": "def lines_with_leading_tabs_expanded(s: str) -> List[str]:\n    \"\"\"\n    Splits string into lines and expands only leading tabs (following the normal\n    Python rules)\n    \"\"\"\n    lines = []\n    for line in s.splitlines():\n        match = FIRST_NON_WHITESPACE_RE.match(line)\n        if match:\n            first_non_whitespace_idx = match.start(1)\n            lines.append(\n                line[:first_non_whitespace_idx].expandtabs()\n                + line[first_non_whitespace_idx:]\n            )\n        else:\n            lines.append(line)\n    if s.endswith(\"\\n\"):\n        lines.append(\"\")\n    return lines",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-21503",
        "description": "[{'lang': 'en', 'value': 'Versions of the package black before 24.3.0 are vulnerable to Regular Expression Denial of Service (ReDoS) via the lines_with_leading_tabs_expanded function in the strings.py file. An attacker could exploit this vulnerability by crafting a malicious input that causes a denial of service.\\r\\rExploiting this vulnerability is possible when running Black on untrusted input, or if you habitually put thousands of leading tab characters in your docstrings.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-36",
      "code": "    def redirect_request(self, req, fp, code, msg, headers, newurl):\n        if code not in (301, 302, 303, 307, 308):\n            raise urllib.error.HTTPError(req.full_url, code, msg, headers, fp)\n        new_method = req.get_method()\n        new_data = req.data\n        remove_headers = []\n        if code == 303 and req.get_method() != 'HEAD':\n            new_method = 'GET'\n        elif code in (301, 302) and req.get_method() == 'POST':\n            new_method = 'GET'\n        if new_method != req.get_method():\n            new_data = None\n            remove_headers.extend(['Content-Length', 'Content-Type'])\n        new_headers = {k: v for k, v in req.headers.items() if k.lower() not in remove_headers}\n        return urllib.request.Request(\n            newurl, headers=new_headers, origin_req_host=req.origin_req_host,\n            unverifiable=True, method=new_method, data=new_data)\ndef extract_timezone(date_str):\n    m = re.search(\n        r'''(?x)\n            ^.{8,}?\n            (?P<tz>Z|\n                (?:(?<=.\\b\\d{4}|\\b\\d{2}:\\d\\d)|\n                   (?<!.\\b[a-zA-Z]{3}|[a-zA-Z]{4}|..\\b\\d\\d))\n                   [ ]?\n                (?P<sign>\\+|-)\n                (?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})\n            $)\n        ''', date_str)\n    if not m:\n        m = re.search(r'\\d{1,2}:\\d{1,2}(?:\\.\\d+)?(?P<tz>\\s*[A-Z]+)$', date_str)\n        timezone = TIMEZONE_NAMES.get(m and m.group('tz').strip())\n        if timezone is not None:\n            date_str = date_str[:-len(m.group('tz'))]\n        timezone = datetime.timedelta(hours=timezone or 0)\n    else:\n        date_str = date_str[:-len(m.group('tz'))]\n        if not m.group('sign'):\n            timezone = datetime.timedelta()\n        else:\n            sign = 1 if m.group('sign') == '+' else -1\n            timezone = datetime.timedelta(\n                hours=sign * int(m.group('hours')),\n                minutes=sign * int(m.group('minutes')))\n    return timezone, date_str",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-35934",
        "description": "[{'lang': 'en', 'value': \"yt-dlp is a command-line program to download videos from video sites. During file downloads, yt-dlp or the external downloaders that yt-dlp employs may leak cookies on HTTP redirects to a different host, or leak them when the host for download fragments differs from their parent manifest's host. This vulnerable behavior is present in yt-dlp prior to 2023.07.06 and nightly 2023.07.06.185519. All native and external downloaders are affected, except for `curl` and `httpie` (version 3.1.0 or later).\\n\\nAt the file download stage, all cookies are passed by yt-dlp to the file downloader as a `Cookie` header, thereby losing their scope. This also occurs in yt-dlp's info JSON output, which may be used by external tools. As a result, the downloader or external tool may indiscriminately send cookies with requests to domains or paths for which the cookies are not scoped.\\n\\nyt-dlp version 2023.07.06 and nightly 2023.07.06.185519 fix this issue by removing the `Cookie` header upon HTTP redirects; having native downloaders calculate the `Cookie` header from the cookiejar, utilizing external downloaders' built-in support for cookies instead of passing them as header arguments, disabling HTTP redirectiong if the external downloader does not have proper cookie support, processing cookies passed as HTTP headers to limit their scope, and having a separate field for cookies in the info dict storing more information about scoping\\n\\nSome workarounds are available for those who are unable to upgrade. Avoid using cookies and user authentication methods. While extractors may set custom cookies, these usually do not contain sensitive information. Alternatively, avoid using `--load-info-json`. Or, if authentication is a must: verify the integrity of download links from unknown sources in browser (including redirects) before passing them to yt-dlp; use `curl` as external downloader, since it is not impacted; and/or avoid fragmented formats such as HLS/m3u8, DASH/mpd and ISM.\"}]",
        "cwe_number": 200
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-37",
      "code": "    def to_html(self) -> str:\n        \"\"\"\n        Returns a rendered HTML representing the content of the tab.\n        :return: a HTML string\n        \"\"\"\n        import jinja2\n        j2_env = jinja2.Environment(loader=jinja2.BaseLoader()).from_string(self.template)\n        return j2_env.render({**self._context})",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-6709",
        "description": "[{'lang': 'en', 'value': 'Improper Neutralization of Special Elements Used in a Template Engine in GitHub repository mlflow/mlflow prior to 2.9.2.'}]",
        "cwe_number": 1336
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-38",
      "code": "    def add_authorizedkey(self, key, comment=None):\n        \"\"\"\n        Add the given key to the user. Adding the key to his `authorized_keys`\n        file if it exists and adding it to database.\n        \"\"\"\n        assert key\n        key = authorizedkeys.check_publickey(key)\n        key = authorizedkeys.AuthorizedKey(\n            options=None, keytype=key.keytype, key=key.key, comment=comment or key.comment\n        )\n        filename = os.path.join(self.user_root, '.ssh', 'authorized_keys')\n        if os.path.isfile(filename):\n            with open(filename, mode=\"r+\", encoding='utf-8') as fh:\n                if authorizedkeys.exists(fh, key):\n                    raise DuplicateSSHKeyError(_(\"SSH key already exists\"))\n                logger.info(\"add key [%s] to [%s] authorized_keys\", key, self.username)\n                authorizedkeys.add(fh, key)\n        else:\n            logger.info(\"add key [%s] to [%s] database\", key, self.username)\n            try:\n                SshKey(userid=self.userid, fingerprint=key.fingerprint, key=key.getvalue()).add().flush()\n            except IntegrityError:\n                raise DuplicateSSHKeyError(\n                    _(\"Duplicate key. This key already exists or is associated to another user.\")\n                )\n        cherrypy.engine.publish('user_attr_changed', self, {'authorizedkeys': True})\n    def start(self):\n        self.bus.log('Start Notification plugin')\n        self.bus.publish('schedule_job', self.execution_time, self.notification_job)\n        self.bus.subscribe('access_token_added', self.access_token_added)\n        self.bus.subscribe('user_attr_changed', self.user_attr_changed)\n        self.bus.subscribe('user_password_changed', self.user_password_changed)\n    def stop(self):\n        self.bus.log('Stop Notification plugin')\n        self.bus.publish('unschedule_job', self.notification_job)\n        self.bus.unsubscribe('access_token_added', self.access_token_added)\n        self.bus.unsubscribe('user_attr_changed', self.user_attr_changed)\n        self.bus.unsubscribe('user_password_changed', self.user_password_changed)\n    stop.priority = 45\n    def user_attr_changed(self, userobj, attrs={}):\n        if not self.send_changed:\n            return\n        if 'email' in attrs:\n            old_email = attrs['email'][0]\n            if not old_email:\n                logger.info(\"can't sent mail to user [%s] without an email\", userobj.username)\n                return\n            subject = _(\"Email address changed\")\n            body = self.app.templates.compile_template(\n                \"email_changed.html\", **{\"header_name\": self.app.cfg.header_name, 'user': userobj}\n            )\n            self.bus.publish('queue_mail', to=old_email, subject=str(subject), message=body)\n        if 'mfa' in attrs:\n            if not userobj.email:\n                logger.info(\"can't sent mail to user [%s] without an email\", userobj.username)\n                return\n            subject = (\n                _(\"Two-Factor Authentication turned off\")\n                if userobj.mfa == UserObject.DISABLED_MFA\n                else _(\"Two-Factor Authentication turned on\")\n            )\n            body = self.app.templates.compile_template(\n                \"email_mfa.html\", **{\"header_name\": self.app.cfg.header_name, 'user': userobj}\n            )\n            self.bus.publish('queue_mail', to=userobj.email, subject=str(subject), message=body)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-4719",
        "description": "[{'lang': 'en', 'value': 'Business Logic Errors in GitHub repository ikus060/rdiffweb prior to 2.5.5.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-39",
      "code": "    def do_transform(self, request, response, config):\n        response += check_update(config)\n        link_label = 'Search result'\n        if 'properties.mispevent' in request.entity.fields:\n            misp = get_misp_connection(config, request.parameters)\n            try:\n                if request.entity.value == '0':\n                    return response\n                eventid = int(request.entity.value)\n                events_json = misp.search(controller='events', eventid=eventid, with_attachments=False)\n                for e in events_json:\n                    response += event_to_entity(e, link_label=link_label, link_direction=LinkDirection.OutputToInput)\n                return response\n            except ValueError:\n                pass\n            events_json = misp.search(controller='events', eventinfo=request.entity.value, with_attachments=False)\n            for e in events_json:\n                response += event_to_entity(e, link_label=link_label, link_direction=LinkDirection.OutputToInput)\n            return response\n        if 'properties.mispgalaxy' in request.entity.fields or 'properties.temp' in request.entity.fields:\n            if request.entity.value == '-':\n                return response\n            keyword = get_entity_property(request.entity, 'Temp')\n            if not keyword:\n                keyword = request.entity.value\n            potential_clusters = search_galaxy_cluster(keyword)\n            if potential_clusters:\n                for potential_cluster in potential_clusters:\n                    new_entity = galaxycluster_to_entity(potential_cluster, link_label=link_label)\n                    if isinstance(new_entity, MISPGalaxy):\n                        response += new_entity\n            if 'properties.temp' in request.entity.fields:\n                keyword = get_entity_property(request.entity, 'Temp')\n                if not keyword:\n                    keyword = request.entity.value\n                misp = get_misp_connection(config, request.parameters)\n                result = misp.direct_call('tags/search', {'name': keyword})\n                for t in result:\n                    if t['Tag']['name'].startswith('misp-galaxy'):\n                        continue\n                    response += Hashtag(t['Tag']['name'], link_label=link_label, bookmark=Bookmark.Green)\n            return response\n        misp = get_misp_connection(config, request.parameters)\n        events_json = misp.search(controller='events', value=request.entity.value, with_attachments=False)\n        for e in events_json:\n            attr = get_attribute_in_event(e, request.entity.value, substring=True)\n            if attr:\n                for item in attribute_to_entity(attr, only_self=True):\n                    response += item\n            if 'Object' in e['Event']:\n                for o in e['Event']['Object']:\n                    if get_attribute_in_object(o, attribute_value=request.entity.value, substring=True).get('value'):\n                        response += object_to_entity(o, link_label=link_label)\n        return response\ndef get_misp_connection(config=None, parameters=None):\n    global misp_connection\n    if misp_connection:\n        return misp_connection\n    if not config:\n        raise MaltegoException(\"ERROR: MISP connection not yet established, and config not provided as parameter.\")\n    misp_verify = True\n    misp_debug = False\n    misp_url = None\n    misp_key = None\n    try:\n        if is_local_exec_mode():\n            misp_url = config['MISP_maltego.local.misp_url']\n            misp_key = config['MISP_maltego.local.misp_key']\n            if config['MISP_maltego.local.misp_verify'] in ['False', 'false', 0, 'no', 'No']:\n                misp_verify = False\n            if config['MISP_maltego.local.misp_debug'] in ['True', 'true', 1, 'yes', 'Yes']:\n                misp_debug = True\n        if is_remote_exec_mode():\n            try:\n                misp_url = parameters['mispurl'].value\n                misp_key = parameters['mispkey'].value\n            except AttributeError:\n                raise MaltegoException(\"ERROR: mispurl and mispkey need to be set to something valid\")\n        misp_connection = PyMISP(misp_url, misp_key, misp_verify, 'json', misp_debug, tool='misp_maltego')\n    except Exception:\n        if is_local_exec_mode():\n            raise MaltegoException(\"ERROR: Cannot connect to MISP server. Please verify your MISP_Maltego.conf settings.\")\n        if is_remote_exec_mode():\n            raise MaltegoException(\"ERROR: Cannot connect to MISP server. Please verify your settings (MISP URL and API key), and ensure the MISP server is reachable from the internet.\")\n    return misp_connection\ndef entity_obj_to_entity(entity_obj, v, t, **kwargs):\n    if entity_obj == Hash:\n        return entity_obj(v, _type=t, **kwargs)\n    return entity_obj(v, **kwargs)\ndef get_entity_property(entity, name):\n    for k, v in entity.fields.items():\n        if k == name:\n            return v.value\n    return None\ndef attribute_to_entity(a, link_label=None, event_tags=[], only_self=False):\n    a['data'] = None\n    if a['type'] == 'malware-sample':\n        a['type'] = 'filename|md5'\n    if a['type'] == 'regkey|value':\n        a['type'] = 'regkey'\n    combined_tags = event_tags\n    if 'Galaxy' in a and not only_self:\n        for g in a['Galaxy']:\n            for c in g['GalaxyCluster']:\n                yield galaxycluster_to_entity(c)\n    if 'Tag' in a and not only_self:\n            for t in a['Tag']:\n                combined_tags.append(t['name'])\n                if t['name'].startswith('misp-galaxy'):\n                    continue\n                if tag_matches_note_prefix(t['name']):\n                    continue\n                yield Hashtag(t['name'], bookmark=Bookmark.Green)\n    notes = convert_tags_to_note(combined_tags)\n    if a['type'] in ('url', 'uri'):\n        yield(URL(url=a['value'], short_title=a['value'], link_label=link_label, notes=notes, bookmark=Bookmark.Green))\n        return\n    if a.get('object_relation') and mapping_misp_to_maltego.get(a['object_relation']):\n        entity_obj = mapping_misp_to_maltego[a['object_relation']][0]\n        yield entity_obj(a['value'], labels=[Label('comment', a.get('comment'))], link_label=link_label, notes=notes, bookmark=Bookmark.Green)\n    elif '|' in a['type']:\n        t_1, t_2 = a['type'].split('|')\n        v_1, v_2 = a['value'].split('|')\n        if t_1 in mapping_misp_to_maltego:\n            entity_obj = mapping_misp_to_maltego[t_1][0]\n            labels = [Label('comment', a.get('comment'))]\n            if entity_obj == File:\n                labels.append(Label('hash', v_2))\n            yield entity_obj_to_entity(entity_obj, v_1, t_1, labels=labels, link_label=link_label, notes=notes, bookmark=Bookmark.Green)\n        if t_2 in mapping_misp_to_maltego:\n            entity_obj = mapping_misp_to_maltego[t_2][0]\n            labels = [Label('comment', a.get('comment'))]\n            if entity_obj == Hash:\n                labels.append(Label('filename', v_1))\n            yield entity_obj_to_entity(entity_obj, v_2, t_2, labels=labels, link_label=link_label, notes=notes, bookmark=Bookmark.Green)\n    elif a['type'] in mapping_misp_to_maltego:\n        entity_obj = mapping_misp_to_maltego[a['type']][0]\n        yield entity_obj_to_entity(entity_obj, a['value'], a['type'], labels=[Label('comment', a.get('comment'))], link_label=link_label, notes=notes, bookmark=Bookmark.Green)\n    def __init__(self):\n        self.request = None\n        self.response = None\n        self.config = None\n        self.misp = None\n        self.event_json = None\n        self.event_tags = None\n    def do_transform(self, request, response, config):\n        self.request = request\n        self.response = response\n        self.config = config\n        self.response += check_update(config)\n        maltego_misp_event = request.entity\n        self.misp = get_misp_connection(config, request.parameters)\n        event_id = maltego_misp_event.id\n        search_result = self.misp.search(controller='events', eventid=event_id, with_attachments=False)\n        if search_result:\n            self.event_json = search_result.pop()\n        else:\n            return False\n        self.response += event_to_entity(self.event_json)\n        return True\n    def gen_response_objects(self):\n        for o in self.event_json['Event']['Object']:\n            self.response += object_to_entity(o)\n    def do_transform(self, request, response, config):\n        response += check_update(config)\n        misp = get_misp_connection(config, request.parameters)\n        if request.entity.tag_name:\n            tag_name = request.entity.tag_name\n        else:\n            tag_name = request.entity.value\n        events_json = misp.search(controller='events', tags=tag_name, with_attachments=False)\n        for e in events_json:\n            response += MISPEvent(e['Event']['id'], uuid=e['Event']['uuid'], info=e['Event']['info'], link_direction=LinkDirection.OutputToInput)\n        return response",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2020-12889",
        "description": "[{'lang': 'en', 'value': 'MISP MISP-maltego 1.4.4 incorrectly shares a MISP connection across users in a remote-transform use case.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-40",
      "code": "def upload_user_data(user_stats_dict):\n    \"\"\"\n    Takes the User Stats dict that is populated by the other functions and\n    then populates the user_info and user_system_summary_stats tables\n    in the metrics MySQL DB.\n    \"\"\"\n    total_users = len(user_stats_dict.keys())\n    rows_info_inserted = 0\n    rows_info_updated = 0\n    rows_stats_inserted = 0\n    db_connection = mysql.connect(\n        host=sql_host, user=\"metrics\", passwd=metrics_mysql_password, database=\"metrics\"\n    )\n    cursor = db_connection.cursor()\n    query = \"use \" + query_on\n    cursor.execute(query)\n    counter_user_id = -1\n    get_max_user_id_q = (\n\t\"select max(user_id) from metrics.user_info \"\n    )\n    cursor.execute(get_max_user_id_q)\n    for row in cursor:\n        counter_user_id = row[0]\n    existing_user_info = dict()\n    query = (\n        \"select username, display_name, email, orcid, globus_login, google_login, \"\n        \"kb_internal_user, institution, country, \"\n        \"signup_date, last_signin_date, department, job_title, job_title_other, \"\n        \"city, state, postal_code, funding_source, research_statement, \"\n        \"research_interests, avatar_option, gravatar_default , \"\n        \"how_u_hear_selected, how_u_hear_other from metrics.user_info\"\n    )\n    cursor.execute(query)\n    for (\n            username,\n            display_name,\n            email,\n            orcid,\n            globus_login,\n            google_login,\n            kb_internal_user,\n            institution,\n            country,\n            signup_date,\n            last_signin_date,\n            department,\n            job_title,\n            job_title_other,\n            city,\n            state,\n            postal_code,\n            funding_source,\n            research_statement,\n            research_interests,\n            avatar_option,\n            gravatar_default,\n            how_u_hear_selected,\n            how_u_hear_other\n    ) in cursor:\n        existing_user_info[username] = {\n            \"name\": display_name,\n            \"email\": email,\n            \"orcid\": orcid,\n            \"globus_login\": globus_login,\n            \"google_login\": google_login,\n            \"kb_internal_user\": kb_internal_user,\n            \"institution\": institution,\n            \"country\": country,\n            \"signup_date\": signup_date,\n            \"last_signin_date\": last_signin_date,\n            \"department\": department,\n            \"job_title\": job_title,\n            \"job_title_other\": job_title_other,\n            \"city\" : city,\n            \"state\" : state,\n            \"postal_code\" : postal_code,\n            \"funding_source\" : funding_source,\n            \"research_statement\" : research_statement,\n            \"research_interests\" : research_interests,\n            \"avatar_option\" : avatar_option,\n            \"gravatar_default\" : gravatar_default,\n            \"how_u_hear_selected\" : how_u_hear_selected,\n            \"how_u_hear_other\" : how_u_hear_other\n        }\n    print(\"Number of existing users:\" + str(len(existing_user_info)))\n    prep_cursor = db_connection.cursor(prepared=True)\n    user_info_insert_statement = (\n        \"insert into user_info \"\n        \"(username, display_name, email, orcid, \"\n        \"globus_login, google_login, \"\n        \"user_id, kb_internal_user, institution, \"\n        \"country, signup_date, last_signin_date, \"\n        \"department, job_title, job_title_other, \"\n        \"city, state, postal_code, funding_source, \"\n        \"research_statement, research_interests, \"\n        \"avatar_option, gravatar_default, \"\n        \"how_u_hear_selected, how_u_hear_other)\"\n        \"values(%s, %s, %s, %s, \"\n        \"%s, %s, \"\n        \"%s, %s, %s, \"\n        \"%s, %s, %s, \"\n        \"%s, %s, %s, \"\n        \"%s, %s, %s, %s, \"\n        \"%s, %s, \"\n        \"%s, %s, \"\n        \"%s, %s);\")\n    update_prep_cursor = db_connection.cursor(prepared=True)\n    user_info_update_statement = (\n        \"update user_info \"\n        \"set display_name = %s, email = %s, \"\n        \"orcid = %s, globus_login = %s, \"\n        \"google_login = %s, kb_internal_user = %s, \"\n        \"institution = %s, country = %s, \"\n        \"signup_date = %s, last_signin_date = %s, \"\n        \"department = %s, job_title = %s, \"\n        \"job_title_other = %s, \"\n        \"city = %s, state = %s, \"\n        \"postal_code = %s, funding_source = %s, \"\n        \"research_statement = %s, \"\n        \"research_interests = %s, \"\n        \"avatar_option = %s, \"\n        \"gravatar_default = %s, \"\n        \"how_u_hear_selected = %s, \"\n        \"how_u_hear_other = %s \"\n        \"where username = %s;\"\n    )\n    new_user_info_count = 0\n    users_info_updated_count = 0\n    for username in user_stats_dict:\n        if username not in existing_user_info:\n            counter_user_id += 1\n            input = (\n                username,\n                user_stats_dict[username][\"name\"],\n                user_stats_dict[username][\"email\"],\n                user_stats_dict[username][\"orcid\"],\n                user_stats_dict[username][\"globus_login\"],\n                user_stats_dict[username][\"google_login\"],\n                counter_user_id,\n                user_stats_dict[username][\"kbase_internal_user\"],\n                user_stats_dict[username][\"institution\"],\n                user_stats_dict[username][\"country\"],\n                user_stats_dict[username][\"signup_date\"],\n                user_stats_dict[username][\"last_signin_date\"],\n                user_stats_dict[username][\"department\"],\n                user_stats_dict[username][\"job_title\"],\n                user_stats_dict[username][\"job_title_other\"],\n                user_stats_dict[username][\"city\"],\n                user_stats_dict[username][\"state\"],\n                user_stats_dict[username][\"postal_code\"],\n                user_stats_dict[username][\"funding_source\"],\n                user_stats_dict[username][\"research_statement\"],\n                user_stats_dict[username][\"research_interests\"],\n                user_stats_dict[username][\"avatar_option\"],\n                user_stats_dict[username][\"gravatar_default\"],\n                user_stats_dict[username][\"how_u_hear_selected\"],\n                user_stats_dict[username][\"how_u_hear_other\"],\n            )\n            prep_cursor.execute(user_info_insert_statement, input)\n            new_user_info_count += 1\n        else:\n            if not (\n                (\n                    user_stats_dict[username][\"last_signin_date\"] is None\n                    or user_stats_dict[username][\"last_signin_date\"].strftime(\n                        \"%Y-%m-%d %H:%M:%S\"\n                    )\n                    == str(existing_user_info[username][\"last_signin_date\"])\n                )\n                and (\n                    user_stats_dict[username][\"signup_date\"].strftime(\n                        \"%Y-%m-%d %H:%M:%S\"\n                    )\n                    == str(existing_user_info[username][\"signup_date\"])\n                )\n                and user_stats_dict[username][\"country\"]\n                    == existing_user_info[username][\"country\"]\n                and user_stats_dict[username][\"institution\"]\n                    == existing_user_info[username][\"institution\"]\n                and user_stats_dict[username][\"kbase_internal_user\"]\n                    == existing_user_info[username][\"kb_internal_user\"]\n                and user_stats_dict[username][\"orcid\"]\n                    == existing_user_info[username][\"orcid\"]\n                and user_stats_dict[username][\"globus_login\"]\n                    == existing_user_info[username][\"globus_login\"]\n                and user_stats_dict[username][\"google_login\"]\n                    == existing_user_info[username][\"google_login\"]\n                and user_stats_dict[username][\"email\"]\n                    == existing_user_info[username][\"email\"]\n                and user_stats_dict[username][\"name\"]\n                    == existing_user_info[username][\"name\"]\n                and user_stats_dict[username][\"department\"]\n                    == existing_user_info[username][\"department\"]\n                and user_stats_dict[username][\"job_title\"]\n                    == existing_user_info[username][\"job_title\"]\n                and user_stats_dict[username][\"job_title_other\"]\n                    == existing_user_info[username][\"job_title_other\"]\n                and user_stats_dict[username][\"city\"]\n                    == existing_user_info[username][\"city\"]\n                and user_stats_dict[username][\"state\"]\n                    == existing_user_info[username][\"state\"]\n                and user_stats_dict[username][\"postal_code\"]\n                    == existing_user_info[username][\"postal_code\"]\n                and user_stats_dict[username][\"funding_source\"]\n                    == existing_user_info[username][\"funding_source\"]\n                and user_stats_dict[username][\"research_statement\"]\n                    == existing_user_info[username][\"research_statement\"]\n                and user_stats_dict[username][\"research_interests\"]\n                    == existing_user_info[username][\"research_interests\"]\n                and user_stats_dict[username][\"avatar_option\"]\n                    == existing_user_info[username][\"avatar_option\"]\n                and user_stats_dict[username][\"gravatar_default\"]\n                    == existing_user_info[username][\"gravatar_default\"]\n                and user_stats_dict[username][\"how_u_hear_selected\"]\n                    == existing_user_info[username][\"how_u_hear_selected\"]\n                and user_stats_dict[username][\"how_u_hear_other\"]\n                    == existing_user_info[username][\"how_u_hear_other\"]\n            ):\n                input = (\n                    user_stats_dict[username][\"name\"],\n                    user_stats_dict[username][\"email\"],\n                    user_stats_dict[username][\"orcid\"],\n                    user_stats_dict[username][\"globus_login\"],\n                    user_stats_dict[username][\"google_login\"],\n                    user_stats_dict[username][\"kbase_internal_user\"],\n                    user_stats_dict[username][\"institution\"],\n                    user_stats_dict[username][\"country\"],\n                    user_stats_dict[username][\"signup_date\"],\n                    user_stats_dict[username][\"last_signin_date\"],\n                    user_stats_dict[username][\"department\"],\n                    user_stats_dict[username][\"job_title\"],\n                    user_stats_dict[username][\"job_title_other\"],\n                    user_stats_dict[username][\"city\"],\n                    user_stats_dict[username][\"state\"],\n                    user_stats_dict[username][\"postal_code\"],\n                    user_stats_dict[username][\"funding_source\"],\n                    user_stats_dict[username][\"research_statement\"],\n                    user_stats_dict[username][\"research_interests\"],\n                    user_stats_dict[username][\"avatar_option\"],\n                    user_stats_dict[username][\"gravatar_default\"],\n                    user_stats_dict[username][\"how_u_hear_selected\"],\n                    user_stats_dict[username][\"how_u_hear_other\"],\n                    username,\n                )\n                update_prep_cursor.execute(user_info_update_statement, input)\n                users_info_updated_count += 1\n    db_connection.commit()\n    print(\"Number of new users info inserted:\" + str(new_user_info_count))\n    print(\"Number of users updated:\" + str(users_info_updated_count))\n    dev_tokens_users = get_dev_token_users_from_mongo()\n    dev_tokens_string = \"', '\".join(dev_tokens_users)\n    update_new_dev_tokens_statement = (\n        \"update user_info set dev_token_first_seen = now() \"\n        \"where dev_token_first_seen is null and \"\n        \"username in ('\" + dev_tokens_string + \"')\"\n        )\n    cursor.execute(update_new_dev_tokens_statement)\n    db_connection.commit()\n    user_summary_stats_insert_statement = (\n        \"insert into user_system_summary_stats \"\n        \"(username,num_orgs, narrative_count, \"\n        \"shared_count, narratives_shared) \"\n        \"values(%s,%s,%s,%s,%s);\"\n    )\n    existing_user_summary_stats = dict()\n    query = (\n        \"select username, num_orgs, narrative_count, shared_count, narratives_shared \"\n        \"from user_system_summary_stats_current\"\n    )\n    cursor.execute(query)\n    for (\n        username,\n        num_orgs,\n        narrative_count,\n        shared_count,\n        narratives_shared,\n    ) in cursor:\n        existing_user_summary_stats[username] = {\n            \"num_orgs\": num_orgs,\n            \"narrative_count\": narrative_count,\n            \"shared_count\": shared_count,\n            \"narratives_shared\": narratives_shared,\n        }\n    print(\"Number of existing user summaries:\" + str(len(existing_user_summary_stats)))\n    new_user_summary_count = 0\n    existing_user_summary_count = 0\n    for username in user_stats_dict:\n        if username not in existing_user_summary_stats:\n            input = (\n                username,\n                user_stats_dict[username][\"num_orgs\"],\n                user_stats_dict[username][\"narrative_count\"],\n                user_stats_dict[username][\"shared_count\"],\n                user_stats_dict[username][\"narratives_shared\"],\n            )\n            prep_cursor.execute(user_summary_stats_insert_statement, input)\n            new_user_summary_count += 1\n        else:\n            if not (\n                user_stats_dict[username][\"num_orgs\"]\n                == existing_user_summary_stats[username][\"num_orgs\"]\n                and user_stats_dict[username][\"narrative_count\"]\n                == existing_user_summary_stats[username][\"narrative_count\"]\n                and user_stats_dict[username][\"shared_count\"]\n                == existing_user_summary_stats[username][\"shared_count\"]\n                and user_stats_dict[username][\"narratives_shared\"]\n                == existing_user_summary_stats[username][\"narratives_shared\"]\n            ):\n                input = (\n                    username,\n                    user_stats_dict[username][\"num_orgs\"],\n                    user_stats_dict[username][\"narrative_count\"],\n                    user_stats_dict[username][\"shared_count\"],\n                    user_stats_dict[username][\"narratives_shared\"],\n                )\n                prep_cursor.execute(user_summary_stats_insert_statement, input)\n                existing_user_summary_count += 1\n    db_connection.commit()\n    query = \"UPDATE metrics.user_info set exclude = False where last_signin_date is not NULL\"\n    cursor.execute(query)\n    db_connection.commit()\n    print(\"Number of new users summary inserted:\" + str(new_user_summary_count))\n    print(\n        \"Number of existing users summary inserted:\" + str(existing_user_summary_count)\n    )\n    return 1",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-4860",
        "description": "[{'lang': 'en', 'value': 'A vulnerability was found in KBase Metrics. It has been classified as critical. This affects the function upload_user_data of the file source/daily_cron_jobs/methods_upload_user_stats.py. The manipulation leads to sql injection. The patch is named 959dfb6b05991e30b0fa972a1ecdcaae8e1dae6d. It is recommended to apply a patch to fix this issue. The associated identifier of this vulnerability is VDB-217059.'}]",
        "cwe_number": 89
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-41",
      "code": "    def handle_get(self) -> bool:\n        if self.do_log:\n            logmsg = \"%-4s %s @%s\" % (self.mode, self.req, self.uname)\n            if \"range\" in self.headers:\n                try:\n                    rval = self.headers[\"range\"].split(\"=\", 1)[1]\n                except:\n                    rval = self.headers[\"range\"]\n                logmsg += \" [\\033[36m\" + rval + \"\\033[0m]\"\n            self.log(logmsg)\n        if self.vpath.startswith(\".cpr\"):\n            if self.vpath.startswith(\".cpr/ico/\"):\n                return self.tx_ico(self.vpath.split(\"/\")[-1], exact=True)\n            if self.vpath.startswith(\".cpr/ssdp\"):\n                return self.conn.hsrv.ssdp.reply(self)\n            if self.vpath.startswith(\".cpr/dd/\") and self.args.mpmc:\n                if self.args.mpmc == \".\":\n                    raise Pebkac(404)\n                loc = self.args.mpmc.rstrip(\"/\") + self.vpath[self.vpath.rfind(\"/\") :]\n                h = {\"Location\": loc, \"Cache-Control\": \"max-age=39\"}\n                self.reply(b\"\", 301, headers=h)\n                return True\n            static_path = os.path.join(self.E.mod, \"web/\", self.vpath[5:])\n            return self.tx_file(static_path)\n        if \"cf_challenge\" in self.uparam:\n            self.reply(self.j2s(\"cf\").encode(\"utf-8\", \"replace\"))\n            return True\n        if not self.can_read and not self.can_write and not self.can_get:\n            t = \"@{} has no access to [{}]\"\n            self.log(t.format(self.uname, self.vpath))\n            if \"on403\" in self.vn.flags:\n                ret = self.on40x(self.vn.flags[\"on403\"], self.vn, self.rem)\n                if ret == \"true\":\n                    return True\n                elif ret == \"false\":\n                    return False\n                elif ret == \"allow\":\n                    self.log(\"plugin override; access permitted\")\n                    self.can_read = self.can_write = self.can_move = True\n                    self.can_delete = self.can_get = self.can_upget = True\n                    self.can_admin = True\n                else:\n                    return self.tx_404(True)\n            else:\n                if self.vpath:\n                    return self.tx_404(True)\n                self.uparam[\"h\"] = \"\"\n        if \"tree\" in self.uparam:\n            return self.tx_tree()\n        if \"scan\" in self.uparam:\n            return self.scanvol()\n        if self.args.getmod:\n            if \"delete\" in self.uparam:\n                return self.handle_rm([])\n            if \"move\" in self.uparam:\n                return self.handle_mv()\n        if not self.vpath:\n            if \"reload\" in self.uparam:\n                return self.handle_reload()\n            if \"stack\" in self.uparam:\n                return self.tx_stack()\n            if \"ups\" in self.uparam:\n                return self.tx_ups()\n            if \"k304\" in self.uparam:\n                return self.set_k304()\n            if \"setck\" in self.uparam:\n                return self.setck()\n            if \"reset\" in self.uparam:\n                return self.set_cfg_reset()\n            if \"hc\" in self.uparam:\n                return self.tx_svcs()\n        if \"h\" in self.uparam:\n            return self.tx_mounts()\n        if self.vpath == \"\" and not self.ouparam:\n            nread = len(self.rvol)\n            nwrite = len(self.wvol)\n            if nread + nwrite == 1 or (self.rvol == self.wvol and nread == 1):\n                if nread == 1:\n                    vpath = self.rvol[0]\n                else:\n                    vpath = self.wvol[0]\n                if self.vpath != vpath:\n                    self.redirect(vpath, flavor=\"redirecting to\", use302=True)\n                    return True\n        return self.tx_browser()",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-37474",
        "description": "[{'lang': 'en', 'value': 'Copyparty is a portable file server. Versions prior to 1.8.2 are subject to a path traversal vulnerability detected in the `.cpr` subfolder. The Path Traversal attack technique allows an attacker access to files, directories, and commands that reside outside the web document root directory. This issue has been addressed in commit `043e3c7d` which has been included in release 1.8.2. Users are advised to upgrade. There are no known workarounds for this vulnerability.'}]",
        "cwe_number": 22
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-42",
      "code": "    def parse_cmd(self, cmd, info):\n        tmpl, tmpl_dict = self._downloader.prepare_outtmpl(cmd, info)\n        if tmpl_dict:\n            return self._downloader.escape_outtmpl(tmpl) % tmpl_dict\n        filepath = info.get('filepath', info.get('_filename'))\n        if filepath:\n            if '{}' not in cmd:\n                cmd += ' {}'\n            cmd = cmd.replace('{}', compat_shlex_quote(filepath))\n        return cmd\n    def run(self, info):\n        for tmpl in self.exec_cmd:\n            cmd = self.parse_cmd(tmpl, info)\n            self.to_screen('Executing command: %s' % cmd)\n            retCode = subprocess.call(encodeArgument(cmd), shell=True)\n            if retCode != 0:\n                raise PostProcessingError('Command returned error code %d' % retCode)\n        return [], info\n    def __init__(self, *args, env=None, text=False, **kwargs):\n        if env is None:\n            env = os.environ.copy()\n        self._fix_pyinstaller_ld_path(env)\n        self.__text_mode = kwargs.get('encoding') or kwargs.get('errors') or text or kwargs.get('universal_newlines')\n        if text is True:\n            kwargs['universal_newlines'] = True\n            kwargs.setdefault('encoding', 'utf-8')\n            kwargs.setdefault('errors', 'replace')\n        super().__init__(*args, env=env, **kwargs, startupinfo=self._startupinfo)\n    def communicate_or_kill(self, *args, **kwargs):\n        try:\n            return self.communicate(*args, **kwargs)\n        except BaseException:\n            self.kill(timeout=None)\n            raise\n    def kill(self, *, timeout=0):\n        super().kill()\n        if timeout != 0:\n            self.wait(timeout=timeout)\ncompat_os_name = os._name if os.name == 'java' else os.name\nif compat_os_name == 'nt':\n    def compat_shlex_quote(s):\n        import re\n        return s if re.match(r'^[-_\\w./]+$', s) else '\"%s\"' % s.replace('\"', '\\\\\"')\nelse:\n    from shlex import quote as compat_shlex_quote",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-22423",
        "description": "[{'lang': 'en', 'value': 'yt-dlp is a youtube-dl fork with additional features and fixes. The patch that addressed CVE-2023-40581 attempted to prevent RCE when using `--exec` with `%q` by replacing double quotes with two double quotes. However, this escaping is not sufficient, and still allows expansion of environment variables. Support for output template expansion in `--exec`, along with this vulnerable behavior, was added to `yt-dlp` in version 2021.04.11. yt-dlp version 2024.04.09 fixes this issue by properly escaping `%`. It replaces them with `%%cd:~,%`, a variable that expands to nothing, leaving only the leading percent. It is recommended to upgrade yt-dlp to version 2024.04.09 as soon as possible. Also, always be careful when using `--exec`, because while this specific vulnerability has been patched, using unvalidated input in shell commands is inherently dangerous. For Windows users who are not able to upgrade, avoid using any output template expansion in `--exec` other than `{}` (filepath); if expansion in `--exec` is needed, verify the fields you are using do not contain `\"`, `|` or `&`; and/or instead of using `--exec`, write the info json and load the fields from it instead.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-43",
      "code": "def parse_html_description(tree: \"etree.Element\") -> Optional[str]:\n    \"\"\"\n    Calculate a text description based on an HTML document.\n    Grabs any text nodes which are inside the <body/> tag, unless they are within\n    an HTML5 semantic markup tag (<header/>, <nav/>, <aside/>, <footer/>), or\n    if they are within a <script/>, <svg/> or <style/> tag, or if they are within\n    a tag whose content is usually only shown to old browsers\n    (<iframe/>, <video/>, <canvas/>, <picture/>).\n    This is a very very very coarse approximation to a plain text render of the page.\n    Args:\n        tree: The parsed HTML document.\n    Returns:\n        The plain text description, or None if one cannot be generated.\n    \"\"\"\n    from lxml import etree\n    TAGS_TO_REMOVE = (\n        \"header\",\n        \"nav\",\n        \"aside\",\n        \"footer\",\n        \"script\",\n        \"noscript\",\n        \"style\",\n        \"svg\",\n        \"iframe\",\n        \"video\",\n        \"canvas\",\n        \"img\",\n        \"picture\",\n        etree.Comment,\n    )\n    text_nodes = (\n        re.sub(r\"\\s+\", \"\\n\", el).strip()\n        for el in _iterate_over_text(tree.find(\"body\"), *TAGS_TO_REMOVE)\n    )\n    return summarize_paragraphs(text_nodes)\ndef _iterate_over_text(\n    tree: \"etree.Element\", *tags_to_ignore: Union[str, \"etree.Comment\"]\n) -> Generator[str, None, None]:\n    \"\"\"Iterate over the tree returning text nodes in a depth first fashion,\n    skipping text nodes inside certain tags.\n    \"\"\"\n    elements = iter([tree])\n    while True:\n        el = next(elements, None)\n        if el is None:\n            return\n        if isinstance(el, str):\n            yield el\n        elif el.tag not in tags_to_ignore:\n            if el.get(\"role\") in ARIA_ROLES_TO_IGNORE:\n                continue\n            if el.text:\n                yield el.text\n            elements = itertools.chain(\n                itertools.chain.from_iterable(\n                    [child, child.tail] if child.tail else [child]\n                    for child in el.iterchildren()\n                ),\n                elements,\n            )\ndef summarize_paragraphs(\n    text_nodes: Iterable[str], min_size: int = 200, max_size: int = 500\n) -> Optional[str]:\n    \"\"\"\n    Try to get a summary respecting first paragraph and then word boundaries.\n    Args:\n        text_nodes: The paragraphs to summarize.\n        min_size: The minimum number of words to include.\n        max_size: The maximum number of words to include.\n    Returns:\n        A summary of the text nodes, or None if that was not possible.\n    \"\"\"\n    description = \"\"\n    for text_node in text_nodes:\n        if len(description) < min_size:\n            text_node = re.sub(r\"[\\t \\r\\n]+\", \" \", text_node)\n            description += text_node + \"\\n\\n\"\n        else:\n            break\n    description = description.strip()\n    description = re.sub(r\"[\\t ]+\", \" \", description)\n    description = re.sub(r\"[\\t \\r\\n]*[\\r\\n]+\", \"\\n\\n\", description)\n    if len(description) > max_size:\n        new_desc = \"\"\n        for match in re.finditer(r\"\\s*\\S+\", description):\n            word = match.group()\n            if len(word) + len(new_desc) < max_size:\n                new_desc += word\n            else:\n                if len(new_desc) < min_size:\n                    new_desc += word\n                break\n        if len(new_desc) > max_size:\n            new_desc = new_desc[:max_size]\n        description = new_desc.strip() + \"\u2026\"\n    return description if description else None",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-31052",
        "description": "[{'lang': 'en', 'value': \"Synapse is an open source home server implementation for the Matrix chat network. In versions prior to 1.61.1 URL previews of some web pages can exhaust the available stack space for the Synapse process due to unbounded recursion. This is sometimes recoverable and leads to an error for the request causing the problem, but in other cases the Synapse process may crash altogether. It is possible to exploit this maliciously, either by malicious users on the homeserver, or by remote users sending URLs that a local user's client may automatically request a URL preview for. Remote users are not able to exploit this directly, because the URL preview endpoint is authenticated. Deployments with `url_preview_enabled: false` set in configuration are not affected. Deployments with `url_preview_enabled: true` set in configuration **are** affected. Deployments with no configuration value set for `url_preview_enabled` are not affected, because the default is `false`. Administrators of homeservers with URL previews enabled are advised to upgrade to v1.61.1 or higher. Users unable to upgrade should set `url_preview_enabled` to false.\"}]",
        "cwe_number": 674
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-44",
      "code": "  def testSymmetricMirrorPadGrad(self):\n    t = np.broadcast_to(np.arange(0, 7), (3, 2, 1, 7))\n    paddings = constant_op.constant([\n        [1, 1],\n        [0, 0],\n        [0, 0],\n        [2, 2],\n    ])\n    expected = np.broadcast_to(np.array([9, 27, 27]), (1, 2, 1, 3))\n    result = gen_array_ops.mirror_pad_grad(t, paddings, \"SYMMETRIC\")\n    self.assertAllEqual(result, expected)\n  def testReflectMirrorPadGrad(self):\n    t = np.broadcast_to(np.reshape(np.arange(0, 7), (7, 1)), (1, 4, 7, 1))\n    paddings = constant_op.constant([\n        [0, 0],\n        [1, 1],\n        [2, 2],\n        [0, 0],\n    ])\n    expected = np.broadcast_to(\n        np.reshape(np.array([16, 18, 8]), (3, 1)), (1, 2, 3, 1))\n    result = gen_array_ops.mirror_pad_grad(t, paddings, \"REFLECT\")\n    self.assertAllEqual(result, expected)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-41895",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. If `MirrorPadGrad` is given outsize input `paddings`, TensorFlow will give a heap OOB error. We have patched the issue in GitHub commit 717ca98d8c3bba348ff62281fdf38dcb5ea1ec92. The fix will be included in TensorFlow 2.11. We will also cherrypick this commit on TensorFlow 2.10.1, 2.9.3, and TensorFlow 2.8.4, as these are also affected and still in supported range.'}]",
        "cwe_number": 125
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-45",
      "code": "    def train(self, train_sents, max_rules=200, min_score=2, min_acc=None):\n        \"\"\"\n        Trains the Brill tagger on the corpus *train_sents*,\n        producing at most *max_rules* transformations, each of which\n        reduces the net number of errors in the corpus by at least\n        *min_score*, and each of which has accuracy not lower than\n        *min_acc*.\n        >>>\n        >>> from nltk.tbl.template import Template\n        >>> from nltk.tag.brill import Pos, Word\n        >>> from nltk.tag import untag, RegexpTagger, BrillTaggerTrainer\n        >>>\n        >>> from nltk.corpus import treebank\n        >>> training_data = treebank.tagged_sents()[:100]\n        >>> baseline_data = treebank.tagged_sents()[100:200]\n        >>> gold_data = treebank.tagged_sents()[200:300]\n        >>> testing_data = [untag(s) for s in gold_data]\n        >>> backoff = RegexpTagger([\n        ... (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),\n        ... (r'(The|the|A|a|An|an)$', 'AT'),\n        ... (r'.*able$', 'JJ'),\n        ... (r'.*ness$', 'NN'),\n        ... (r'.*ly$', 'RB'),\n        ... (r'.*s$', 'NNS'),\n        ... (r'.*ing$', 'VBG'),\n        ... (r'.*ed$', 'VBD'),\n        ... (r'.*', 'NN')\n        ... ])\n        >>> baseline = backoff\n        >>> baseline.evaluate(gold_data)\n        0.2450142...\n        >>>\n        >>> Template._cleartemplates()\n        >>> templates = [Template(Pos([-1])), Template(Pos([-1]), Word([0]))]\n        >>>\n        >>> tt = BrillTaggerTrainer(baseline, templates, trace=3)\n        >>> tagger1 = tt.train(training_data, max_rules=10)\n        TBL train (fast) (seqs: 100; tokens: 2417; tpls: 2; min score: 2; min acc: None)\n        Finding initial useful rules...\n            Found 845 useful rules.\n        <BLANKLINE>\n                   B      |\n           S   F   r   O  |        Score = Fixed - Broken\n           c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct\n           o   x   k   h  |  u     Broken = num tags changed correct -> incorrect\n           r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect\n           e   d   n   r  |  e\n        ------------------+-------------------------------------------------------\n         132 132   0   0  | AT->DT if Pos:NN@[-1]\n          85  85   0   0  | NN->, if Pos:NN@[-1] & Word:,@[0]\n          69  69   0   0  | NN->. if Pos:NN@[-1] & Word:.@[0]\n          51  51   0   0  | NN->IN if Pos:NN@[-1] & Word:of@[0]\n          47  63  16 161  | NN->IN if Pos:NNS@[-1]\n          33  33   0   0  | NN->TO if Pos:NN@[-1] & Word:to@[0]\n          26  26   0   0  | IN->. if Pos:NNS@[-1] & Word:.@[0]\n          24  24   0   0  | IN->, if Pos:NNS@[-1] & Word:,@[0]\n          22  27   5  24  | NN->-NONE- if Pos:VBD@[-1]\n          17  17   0   0  | NN->CC if Pos:NN@[-1] & Word:and@[0]\n        >>> tagger1.rules()[1:3]\n        (Rule('001', 'NN', ',', [(Pos([-1]),'NN'), (Word([0]),',')]), Rule('001', 'NN', '.', [(Pos([-1]),'NN'), (Word([0]),'.')]))\n        >>> train_stats = tagger1.train_stats()\n        >>> [train_stats[stat] for stat in ['initialerrors', 'finalerrors', 'rulescores']]\n        [1775, 1269, [132, 85, 69, 51, 47, 33, 26, 24, 22, 17]]\n        >>> tagger1.print_template_statistics(printunused=False)\n        TEMPLATE STATISTICS (TRAIN)  2 templates, 10 rules)\n        TRAIN (   2417 tokens) initial  1775 0.2656 final:  1269 0.4750\n        --------------------------------------------\n        001 |   305   0.603 |   7   0.700 | Template(Pos([-1]),Word([0]))\n        000 |   201   0.397 |   3   0.300 | Template(Pos([-1]))\n        <BLANKLINE>\n        <BLANKLINE>\n        >>> tagger1.evaluate(gold_data)\n        0.43996...\n        >>> tagged, test_stats = tagger1.batch_tag_incremental(testing_data, gold_data)\n        >>> tagged[33][12:] == [('foreign', 'IN'), ('debt', 'NN'), ('of', 'IN'), ('$', 'NN'), ('64', 'CD'),\n        ... ('billion', 'NN'), ('*U*', 'NN'), ('--', 'NN'), ('the', 'DT'), ('third-highest', 'NN'), ('in', 'NN'),\n        ... ('the', 'DT'), ('developing', 'VBG'), ('world', 'NN'), ('.', '.')]\n        True\n        >>> [test_stats[stat] for stat in ['initialerrors', 'finalerrors', 'rulescores']]\n        [1855, 1376, [100, 85, 67, 58, 27, 36, 27, 16, 31, 32]]\n        >>>\n        >>> tagger2 = tt.train(training_data, max_rules=10, min_acc=0.99)\n        TBL train (fast) (seqs: 100; tokens: 2417; tpls: 2; min score: 2; min acc: 0.99)\n        Finding initial useful rules...\n            Found 845 useful rules.\n        <BLANKLINE>\n                   B      |\n           S   F   r   O  |        Score = Fixed - Broken\n           c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct\n           o   x   k   h  |  u     Broken = num tags changed correct -> incorrect\n           r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect\n           e   d   n   r  |  e\n        ------------------+-------------------------------------------------------\n         132 132   0   0  | AT->DT if Pos:NN@[-1]\n          85  85   0   0  | NN->, if Pos:NN@[-1] & Word:,@[0]\n          69  69   0   0  | NN->. if Pos:NN@[-1] & Word:.@[0]\n          51  51   0   0  | NN->IN if Pos:NN@[-1] & Word:of@[0]\n          36  36   0   0  | NN->TO if Pos:NN@[-1] & Word:to@[0]\n          26  26   0   0  | NN->. if Pos:NNS@[-1] & Word:.@[0]\n          24  24   0   0  | NN->, if Pos:NNS@[-1] & Word:,@[0]\n          19  19   0   6  | NN->VB if Pos:TO@[-1]\n          18  18   0   0  | CD->-NONE- if Pos:NN@[-1] & Word:0@[0]\n          18  18   0   0  | NN->CC if Pos:NN@[-1] & Word:and@[0]\n        >>> tagger2.evaluate(gold_data)\n        0.44159544...\n        >>> tagger2.rules()[2:4]\n        (Rule('001', 'NN', '.', [(Pos([-1]),'NN'), (Word([0]),'.')]), Rule('001', 'NN', 'IN', [(Pos([-1]),'NN'), (Word([0]),'of')]))\n        :param train_sents: training data\n        :type train_sents: list(list(tuple))\n        :param max_rules: output at most max_rules rules\n        :type max_rules: int\n        :param min_score: stop training when no rules better than min_score can be found\n        :type min_score: int\n        :param min_acc: discard any rule with lower accuracy than min_acc\n        :type min_acc: float or None\n        :return: the learned tagger\n        :rtype: BrillTagger\n        \"\"\"\n        test_sents = [\n            list(self._initial_tagger.tag(untag(sent))) for sent in train_sents\n        ]\n        trainstats = {}\n        trainstats[\"min_acc\"] = min_acc\n        trainstats[\"min_score\"] = min_score\n        trainstats[\"tokencount\"] = sum(len(t) for t in test_sents)\n        trainstats[\"sequencecount\"] = len(test_sents)\n        trainstats[\"templatecount\"] = len(self._templates)\n        trainstats[\"rulescores\"] = []\n        trainstats[\"initialerrors\"] = sum(\n            tag[1] != truth[1]\n            for paired in zip(test_sents, train_sents)\n            for (tag, truth) in zip(*paired)\n        )\n        trainstats[\"initialacc\"] = (\n            1 - trainstats[\"initialerrors\"] / trainstats[\"tokencount\"]\n        )\n        if self._trace > 0:\n            print(\n                \"TBL train (fast) (seqs: {sequencecount}; tokens: {tokencount}; \"\n                \"tpls: {templatecount}; min score: {min_score}; min acc: {min_acc})\".format(\n                    **trainstats\n                )\n            )\n        if self._trace:\n            print(\"Finding initial useful rules...\")\n        self._init_mappings(test_sents, train_sents)\n        if self._trace:\n            print(f\"    Found {len(self._rule_scores)} useful rules.\")\n        if self._trace > 2:\n            self._trace_header()\n        elif self._trace == 1:\n            print(\"Selecting rules...\")\n        rules = []\n        try:\n            while len(rules) < max_rules:\n                rule = self._best_rule(train_sents, test_sents, min_score, min_acc)\n                if rule:\n                    rules.append(rule)\n                    score = self._rule_scores[rule]\n                    trainstats[\"rulescores\"].append(score)\n                else:\n                    break\n                if self._trace > 1:\n                    self._trace_rule(rule)\n                self._apply_rule(rule, test_sents)\n                self._update_tag_positions(rule)\n                self._update_rules(rule, train_sents, test_sents)\n        except KeyboardInterrupt:\n            print(f\"Training stopped manually -- {len(rules)} rules found\")\n        self._clean()\n        trainstats[\"finalerrors\"] = trainstats[\"initialerrors\"] - sum(\n            trainstats[\"rulescores\"]\n        )\n        trainstats[\"finalacc\"] = (\n            1 - trainstats[\"finalerrors\"] / trainstats[\"tokencount\"]\n        )\n        return BrillTagger(self._initial_tagger, rules, trainstats)\n    def print_template_statistics(self, test_stats=None, printunused=True):\n        \"\"\"\n        Print a list of all templates, ranked according to efficiency.\n        If test_stats is available, the templates are ranked according to their\n        relative contribution (summed for all rules created from a given template,\n        weighted by score) to the performance on the test set. If no test_stats, then\n        statistics collected during training are used instead. There is also\n        an unweighted measure (just counting the rules). This is less informative,\n        though, as many low-score rules will appear towards end of training.\n        :param test_stats: dictionary of statistics collected during testing\n        :type test_stats: dict of str -> any (but usually numbers)\n        :param printunused: if True, print a list of all unused templates\n        :type printunused: bool\n        :return: None\n        :rtype: None\n        \"\"\"\n        tids = [r.templateid for r in self._rules]\n        train_stats = self.train_stats()\n        trainscores = train_stats[\"rulescores\"]\n        assert len(trainscores) == len(\n            tids\n        ), \"corrupt statistics: \" \"{} train scores for {} rules\".format(\n            trainscores, tids\n        )\n        template_counts = Counter(tids)\n        weighted_traincounts = Counter()\n        for (tid, score) in zip(tids, trainscores):\n            weighted_traincounts[tid] += score\n        tottrainscores = sum(trainscores)\n        def det_tplsort(tpl_value):\n            return (tpl_value[1], repr(tpl_value[0]))\n        def print_train_stats():\n            print(\n                \"TEMPLATE STATISTICS (TRAIN)  {} templates, {} rules)\".format(\n                    len(template_counts), len(tids)\n                )\n            )\n            print(\n                \"TRAIN ({tokencount:7d} tokens) initial {initialerrors:5d} {initialacc:.4f} \"\n                \"final: {finalerrors:5d} {finalacc:.4f} \".format(**train_stats)\n            )\n            head = \"\n            print(head, \"\\n\", \"-\" * len(head), sep=\"\")\n            train_tplscores = sorted(\n                weighted_traincounts.items(), key=det_tplsort, reverse=True\n            )\n            for (tid, trainscore) in train_tplscores:\n                s = \"{} | {:5d}   {:5.3f} |{:4d}   {:.3f} | {}\".format(\n                    tid,\n                    trainscore,\n                    trainscore / tottrainscores,\n                    template_counts[tid],\n                    template_counts[tid] / len(tids),\n                    Template.ALLTEMPLATES[int(tid)],\n                )\n                print(s)\n        def print_testtrain_stats():\n            testscores = test_stats[\"rulescores\"]\n            print(\n                \"TEMPLATE STATISTICS (TEST AND TRAIN) ({} templates, {} rules)\".format(\n                    len(template_counts), len(tids)\n                )\n            )\n            print(\n                \"TEST  ({tokencount:7d} tokens) initial {initialerrors:5d} {initialacc:.4f} \"\n                \"final: {finalerrors:5d} {finalacc:.4f} \".format(**test_stats)\n            )\n            print(\n                \"TRAIN ({tokencount:7d} tokens) initial {initialerrors:5d} {initialacc:.4f} \"\n                \"final: {finalerrors:5d} {finalacc:.4f} \".format(**train_stats)\n            )\n            weighted_testcounts = Counter()\n            for (tid, score) in zip(tids, testscores):\n                weighted_testcounts[tid] += score\n            tottestscores = sum(testscores)\n            head = \"\n            print(head, \"\\n\", \"-\" * len(head), sep=\"\")\n            test_tplscores = sorted(\n                weighted_testcounts.items(), key=det_tplsort, reverse=True\n            )\n            for (tid, testscore) in test_tplscores:\n                s = \"{:s} |{:5d}  {:6.3f} |  {:4d}   {:.3f} |{:4d}   {:.3f} | {:s}\".format(\n                    tid,\n                    testscore,\n                    testscore / tottestscores,\n                    weighted_traincounts[tid],\n                    weighted_traincounts[tid] / tottrainscores,\n                    template_counts[tid],\n                    template_counts[tid] / len(tids),\n                    Template.ALLTEMPLATES[int(tid)],\n                )\n                print(s)\n        def print_unused_templates():\n            usedtpls = {int(tid) for tid in tids}\n            unused = [\n                (tid, tpl)\n                for (tid, tpl) in enumerate(Template.ALLTEMPLATES)\n                if tid not in usedtpls\n            ]\n            print(f\"UNUSED TEMPLATES ({len(unused)})\")\n            for (tid, tpl) in unused:\n                print(f\"{tid:03d} {str(tpl):s}\")\n        if test_stats is None:\n            print_train_stats()\n        else:\n            print_testtrain_stats()\n        print()\n        if printunused:\n            print_unused_templates()\n        print()\ndef malt_regex_tagger():\n    from nltk.tag import RegexpTagger\n    _tagger = RegexpTagger(\n        [\n            (r\"\\.$\", \".\"),\n            (r\"\\,$\", \",\"),\n            (r\"\\?$\", \"?\"),\n            (r\"\\($\", \"(\"),\n            (r\"\\)$\", \")\"),\n            (r\"\\[$\", \"[\"),\n            (r\"\\]$\", \"]\"),\n            (r\"^-?[0-9]+(.[0-9]+)?$\", \"CD\"),\n            (r\"(The|the|A|a|An|an)$\", \"DT\"),\n            (r\"(He|he|She|she|It|it|I|me|Me|You|you)$\", \"PRP\"),\n            (r\"(His|his|Her|her|Its|its)$\", \"PRP$\"),\n            (r\"(my|Your|your|Yours|yours)$\", \"PRP$\"),\n            (r\"(on|On|in|In|at|At|since|Since)$\", \"IN\"),\n            (r\"(for|For|ago|Ago|before|Before)$\", \"IN\"),\n            (r\"(till|Till|until|Until)$\", \"IN\"),\n            (r\"(by|By|beside|Beside)$\", \"IN\"),\n            (r\"(under|Under|below|Below)$\", \"IN\"),\n            (r\"(over|Over|above|Above)$\", \"IN\"),\n            (r\"(across|Across|through|Through)$\", \"IN\"),\n            (r\"(into|Into|towards|Towards)$\", \"IN\"),\n            (r\"(onto|Onto|from|From)$\", \"IN\"),\n            (r\".*able$\", \"JJ\"),\n            (r\".*ness$\", \"NN\"),\n            (r\".*ly$\", \"RB\"),\n            (r\".*s$\", \"NNS\"),\n            (r\".*ing$\", \"VBG\"),\n            (r\".*ed$\", \"VBD\"),\n            (r\".*\", \"NN\"),\n        ]\n    )\n    return _tagger.tag\n    def get_pos_tagger(self):\n        from nltk.corpus import brown\n        regexp_tagger = RegexpTagger(\n            [\n                (r\"^-?[0-9]+(.[0-9]+)?$\", \"CD\"),\n                (r\"(The|the|A|a|An|an)$\", \"AT\"),\n                (r\".*able$\", \"JJ\"),\n                (r\".*ness$\", \"NN\"),\n                (r\".*ly$\", \"RB\"),\n                (r\".*s$\", \"NNS\"),\n                (r\".*ing$\", \"VBG\"),\n                (r\".*ed$\", \"VBD\"),\n                (r\".*\", \"NN\"),\n            ]\n        )\n        brown_train = brown.tagged_sents(categories=\"news\")\n        unigram_tagger = UnigramTagger(brown_train, backoff=regexp_tagger)\n        bigram_tagger = BigramTagger(brown_train, backoff=unigram_tagger)\n        trigram_tagger = TrigramTagger(brown_train, backoff=bigram_tagger)\n        main_tagger = RegexpTagger(\n            [(r\"(A|a|An|an)$\", \"ex_quant\"), (r\"(Every|every|All|all)$\", \"univ_quant\")],\n            backoff=trigram_tagger,\n        )\n        return main_tagger",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-3842",
        "description": "[{'lang': 'en', 'value': 'nltk is vulnerable to Inefficient Regular Expression Complexity'}]",
        "cwe_number": 1333
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-46",
      "code": "def publish(request, user_id=None):\n    initial = None\n    if user_id:\n        user_to = get_object_or_404(User, pk=user_id)\n        initial = {'users': [user_to.st.nickname]}\n    user = request.user\n    tform = TopicForPrivateForm(\n        user=user, data=post_data(request))\n    cform = CommentForm(\n        user=user, data=post_data(request))\n    tpform = TopicPrivateManyForm(\n        user=user, data=post_data(request), initial=initial)\n    if (is_post(request) and\n            all([tform.is_valid(), cform.is_valid(), tpform.is_valid()]) and\n            not request.is_limited()):\n        if not user.st.update_post_hash(tform.get_topic_hash()):\n            return redirect(\n                request.POST.get('next', None) or\n                tform.category.get_absolute_url())\n        topic = tform.save()\n        cform.topic = topic\n        comment = cform.save()\n        comment_posted(comment=comment, mentions=None)\n        tpform.topic = topic\n        tpform.save_m2m()\n        TopicNotification.bulk_create(\n            users=tpform.get_users(), comment=comment)\n        return redirect(topic.get_absolute_url())\n    return render(\n        request=request,\n        template_name='spirit/topic/private/publish.html',\n        context={\n            'tform': tform,\n            'cform': cform,\n            'tpform': tpform})\ndef create_access(request, topic_id):\n    topic_private = TopicPrivate.objects.for_create_or_404(topic_id, request.user)\n    form = TopicPrivateInviteForm(\n        topic=topic_private.topic,\n        data=post_data(request))\n    if form.is_valid():\n        form.save()\n        notify_access(user=form.get_user(), topic_private=topic_private)\n    else:\n        messages.error(request, utils.render_form_errors(form))\n    return redirect(request.POST.get('next', topic_private.get_absolute_url()))\ndef delete_access(request, pk):\n    topic_private = TopicPrivate.objects.for_delete_or_404(pk, request.user)\n    if request.method == 'POST':\n        topic_private.delete()\n        if request.user.pk == topic_private.user_id:\n            return redirect(reverse(\"spirit:topic:private:index\"))\n        return redirect(request.POST.get('next', topic_private.get_absolute_url()))\n    return render(\n        request=request,\n        template_name='spirit/topic/private/delete.html',\n        context={'topic_private': topic_private})\ndef join_in(request, topic_id):\n    topic = get_object_or_404(\n        Topic,\n        pk=topic_id,\n        user=request.user,\n        category_id=settings.ST_TOPIC_PRIVATE_CATEGORY_PK)\n    form = TopicPrivateJoinForm(\n        topic=topic,\n        user=request.user,\n        data=post_data(request))\n    if is_post(request) and form.is_valid():\n        topic_private = form.save()\n        notify_access(user=form.get_user(), topic_private=topic_private)\n        return redirect(request.POST.get('next', topic.get_absolute_url()))\n    return render(\n        request=request,\n        template_name='spirit/topic/private/join.html',\n        context={\n            'topic': topic,\n            'form': form})\ndef publish(request, topic_id, pk=None):\n    initial = None\n    if pk:\n        comment = get_object_or_404(\n            Comment.objects.for_access(user=request.user), pk=pk)\n        quote = markdown.quotify(comment.comment, comment.user.st.nickname)\n        initial = {'comment': quote}\n    user = request.user\n    topic = get_object_or_404(\n        Topic.objects.opened().for_access(user),\n        pk=topic_id)\n    form = CommentForm(\n        user=user,\n        topic=topic,\n        data=post_data(request),\n        initial=initial)\n    if is_post(request) and not request.is_limited() and form.is_valid():\n        if not user.st.update_post_hash(form.get_comment_hash()):\n            return redirect(\n                request.POST.get('next', None) or\n                Comment\n                .get_last_for_topic(topic_id)\n                .get_absolute_url())\n        comment = form.save()\n        comment_posted(comment=comment, mentions=form.mentions)\n        return redirect(request.POST.get('next', comment.get_absolute_url()))\n    return render(\n        request=request,\n        template_name='spirit/comment/publish.html',\n        context={\n            'form': form,\n            'topic': topic})\ndef update(request, pk):\n    comment = Comment.objects.for_update_or_404(pk, request.user)\n    form = CommentForm(data=post_data(request), instance=comment)\n    if is_post(request) and form.is_valid():\n        pre_comment_update(comment=Comment.objects.get(pk=comment.pk))\n        comment = form.save()\n        post_comment_update(comment=comment)\n        return redirect(request.POST.get('next', comment.get_absolute_url()))\n    return render(\n        request=request,\n        template_name='spirit/comment/update.html',\n        context={'form': form})\ndef delete(request, pk, remove=True):\n    comment = get_object_or_404(Comment, pk=pk)\n    if is_post(request):\n        (Comment.objects\n         .filter(pk=pk)\n         .update(is_removed=remove))\n        return redirect(request.GET.get('next', comment.get_absolute_url()))\n    return render(\n        request=request,\n        template_name='spirit/comment/moderate.html',\n        context={'comment': comment})\ndef move(request, topic_id):\n    topic = get_object_or_404(Topic, pk=topic_id)\n    form = CommentMoveForm(topic=topic, data=request.POST)\n    if form.is_valid():\n        comments = form.save()\n        for comment in comments:\n            comment_posted(comment=comment, mentions=None)\n            topic.decrease_comment_count()\n            post_comment_move(comment=comment, topic=topic)\n    else:\n        messages.error(request, render_form_errors(form))\n    return redirect(request.POST.get('next', topic.get_absolute_url()))\ndef publish(request, category_id=None):\n    if category_id:\n        get_object_or_404(\n            Category.objects.visible(),\n            pk=category_id)\n    user = request.user\n    form = TopicForm(\n        user=user,\n        data=post_data(request),\n        initial={'category': category_id})\n    cform = CommentForm(\n        user=user,\n        data=post_data(request))\n    if (is_post(request) and\n            all([form.is_valid(), cform.is_valid()]) and\n            not request.is_limited()):\n        if not user.st.update_post_hash(form.get_topic_hash()):\n            return redirect(\n                request.POST.get('next', None) or\n                form.get_category().get_absolute_url())\n        topic = form.save()\n        cform.topic = topic\n        comment = cform.save()\n        comment_posted(comment=comment, mentions=cform.mentions)\n        return redirect(topic.get_absolute_url())\n    return render(\n        request=request,\n        template_name='spirit/topic/publish.html',\n        context={'form': form, 'cform': cform})\ndef update(request, pk):\n    topic = Topic.objects.for_update_or_404(pk, request.user)\n    category_id = topic.category_id\n    form = TopicForm(\n        user=request.user,\n        data=post_data(request),\n        instance=topic)\n    if is_post(request) and form.is_valid():\n        topic = form.save()\n        if topic.category_id != category_id:\n            Comment.create_moderation_action(\n                user=request.user, topic=topic, action=Comment.MOVED)\n        return redirect(request.POST.get('next', topic.get_absolute_url()))\n    return render(\n        request=request,\n        template_name='spirit/topic/update.html',\n        context={'form': form})\ndef _moderate(request, pk, field_name, to_value, action=None, message=None):\n    topic = get_object_or_404(Topic, pk=pk)\n    if is_post(request):\n        count = (\n            Topic.objects\n            .filter(pk=pk)\n            .exclude(**{field_name: to_value})\n            .update(**{\n                field_name: to_value,\n                'reindex_at': timezone.now()}))\n        if count and action is not None:\n            Comment.create_moderation_action(\n                user=request.user,\n                topic=topic,\n                action=action)\n        if message is not None:\n            messages.info(request, message)\n        return redirect(request.POST.get(\n            'next', topic.get_absolute_url()))\n    return render(\n        request=request,\n        template_name='spirit/topic/moderate.html',\n        context={'topic': topic})\ndef custom_login(request, **kwargs):\n    if request.user.is_authenticated:\n        return redirect(request.GET.get('next', request.user.st.get_absolute_url()))\n    if request.method == \"POST\" and request.is_limited():\n        return redirect(request.get_full_path())\n    return _login_view(request, authentication_form=LoginForm, **kwargs)\ndef custom_logout(request, **kwargs):\n    if not request.user.is_authenticated:\n        return redirect(request.GET.get('next', reverse(settings.LOGIN_URL)))\n    if request.method == 'POST':\n        return _logout_view(request, **kwargs)\n    return render(request, 'spirit/user/auth/logout.html')\ndef register(request, registration_form=RegistrationForm):\n    if request.user.is_authenticated:\n        return redirect(request.GET.get('next', reverse('spirit:user:update')))\n    form = registration_form(data=post_data(request))\n    if (is_post(request) and\n            not request.is_limited() and\n            form.is_valid()):\n        user = form.save()\n        send_activation_email(request, user)\n        messages.info(\n            request, _(\n                \"We have sent you an email to %(email)s \"\n                \"so you can activate your account!\") % {'email': form.get_email()})\n        return redirect(reverse(settings.LOGIN_URL))\n    return render(\n        request=request,\n        template_name='spirit/user/auth/register.html',\n        context={'form': form})\ndef resend_activation_email(request):\n    if request.user.is_authenticated:\n        return redirect(request.GET.get('next', reverse('spirit:user:update')))\n    form = ResendActivationForm(data=post_data(request))\n    if is_post(request):\n        if not request.is_limited() and form.is_valid():\n            user = form.get_user()\n            send_activation_email(request, user)\n        messages.info(\n            request, _(\n                \"If you don't receive an email, please make sure you've entered \"\n                \"the address you registered with, and check your spam folder.\"))\n        return redirect(reverse(settings.LOGIN_URL))\n    return render(\n        request=request,\n        template_name='spirit/user/auth/activation_resend.html',\n        context={'form': form})\ndef create(request, comment_id):\n    comment = get_object_or_404(\n        Comment.objects.exclude(user=request.user),\n        pk=comment_id)\n    form = LikeForm(\n        user=request.user,\n        comment=comment,\n        data=post_data(request))\n    if is_post(request) and form.is_valid():\n        like = form.save()\n        like.comment.increase_likes_count()\n        if is_ajax(request):\n            return json_response({'url_delete': like.get_delete_url()})\n        return redirect(request.POST.get('next', comment.get_absolute_url()))\n    return render(\n        request=request,\n        template_name='spirit/comment/like/create.html',\n        context={\n            'form': form,\n            'comment': comment})\ndef delete(request, pk):\n    like = get_object_or_404(CommentLike, pk=pk, user=request.user)\n    if is_post(request):\n        like.delete()\n        like.comment.decrease_likes_count()\n        if is_ajax(request):\n            url = reverse(\n                'spirit:comment:like:create',\n                kwargs={'comment_id': like.comment.pk})\n            return json_response({'url_create': url, })\n        return redirect(request.POST.get('next', like.comment.get_absolute_url()))\n    return render(\n        request=request,\n        template_name='spirit/comment/like/delete.html',\n        context={'like': like})\ndef close_or_open(request, pk, close=True):\n    poll = get_object_or_404(\n        CommentPoll,\n        pk=pk,\n        comment__user=request.user\n    )\n    if close:\n        close_at = timezone.now()\n    else:\n        close_at = None\n    (CommentPoll.objects\n     .filter(pk=poll.pk)\n     .update(close_at=close_at))\n    return redirect(request.GET.get('next', poll.get_absolute_url()))\ndef vote(request, pk):\n    poll = get_object_or_404(\n        CommentPoll.objects.unremoved(),\n        pk=pk\n    )\n    if not request.user.is_authenticated:\n        return redirect_to_login(next=poll.get_absolute_url())\n    form = PollVoteManyForm(user=request.user, poll=poll, data=request.POST)\n    if form.is_valid():\n        CommentPollChoice.decrease_vote_count(poll=poll, voter=request.user)\n        form.save_m2m()\n        CommentPollChoice.increase_vote_count(poll=poll, voter=request.user)\n        return redirect(request.POST.get('next', poll.get_absolute_url()))\n    messages.error(request, utils.render_form_errors(form))\n    return redirect(request.POST.get('next', poll.get_absolute_url()))\ndef create(request, comment_id):\n    comment = get_object_or_404(Comment, pk=comment_id)\n    form = FlagForm(\n        user=request.user,\n        comment=comment,\n        data=post_data(request))\n    if is_post(request) and form.is_valid():\n        form.save()\n        return redirect(request.POST.get('next', comment.get_absolute_url()))\n    return render(\n        request=request,\n        template_name='spirit/comment/flag/create.html',\n        context={\n            'form': form,\n            'comment': comment})\ndef create(request, topic_id):\n    topic = get_object_or_404(\n        Topic.objects.for_access(request.user),\n        pk=topic_id)\n    form = NotificationCreationForm(\n        user=request.user,\n        topic=topic,\n        data=request.POST)\n    if form.is_valid():\n        form.save()\n    else:\n        messages.error(request, utils.render_form_errors(form))\n    return redirect(request.POST.get('next', topic.get_absolute_url()))\ndef update(request, pk):\n    notification = get_object_or_404(TopicNotification, pk=pk, user=request.user)\n    form = NotificationForm(data=request.POST, instance=notification)\n    if form.is_valid():\n        form.save()\n    else:\n        messages.error(request, utils.render_form_errors(form))\n    return redirect(request.POST.get(\n        'next', notification.topic.get_absolute_url()))\ndef mark_all_as_read(request):\n    (TopicNotification.objects\n        .for_access(request.user)\n        .filter(is_read=False)\n        .update(is_read=True))\n    return redirect(request.POST.get(\n        'next', reverse('spirit:topic:notification:index')))\ndef edit(request, user_id):\n    user = get_object_or_404(User, pk=user_id)\n    uform = UserForm(data=post_data(request), instance=user)\n    form = UserProfileForm(data=post_data(request), instance=user.st)\n    if is_post(request) and all([uform.is_valid(), form.is_valid()]):\n        uform.save()\n        form.save()\n        messages.info(request, _(\"This profile has been updated!\"))\n        return redirect(request.GET.get(\"next\", request.get_full_path()))\n    return render(\n        request=request,\n        template_name='spirit/user/admin/edit.html',\n        context={'form': form, 'uform': uform})\ndef config_basic(request):\n    form = BasicConfigForm(data=post_data(request))\n    if is_post(request) and form.is_valid():\n        form.save()\n        messages.info(request, _(\"Settings updated!\"))\n        return redirect(request.GET.get(\"next\", request.get_full_path()))\n    return render(\n        request=request,\n        template_name='spirit/admin/config_basic.html',\n        context={'form': form})\ndef guest_only(view_func):\n    @wraps(view_func)\n    def wrapper(request, *args, **kwargs):\n        if request.user.is_authenticated:\n            return redirect(request.GET.get('next', request.user.st.get_absolute_url()))\n        return view_func(request, *args, **kwargs)\n    return wrapper\ndef create(request, topic_id):\n    topic = get_object_or_404(Topic, pk=topic_id)\n    form = FavoriteForm(user=request.user, topic=topic, data=request.POST)\n    if form.is_valid():\n        form.save()\n    else:\n        messages.error(request, utils.render_form_errors(form))\n    return redirect(request.POST.get('next', topic.get_absolute_url()))\ndef delete(request, pk):\n    favorite = get_object_or_404(TopicFavorite, pk=pk, user=request.user)\n    favorite.delete()\n    return redirect(request.POST.get('next', favorite.topic.get_absolute_url()))",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-0869",
        "description": "[{'lang': 'en', 'value': 'Multiple Open Redirect in GitHub repository nitely/spirit prior to 0.12.3.'}]",
        "cwe_number": 601
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-47",
      "code": "def _default_logfile(exe_name):\n    '''\n    Retrieve the logfile name\n    '''\n    if salt.utils.is_windows():\n        logfile_tmp = tempfile.NamedTemporaryFile(dir=os.environ['TMP'],\n                                                  prefix=exe_name,\n                                                  suffix='.log',\n                                                  delete=False)\n        logfile = logfile_tmp.name\n        logfile_tmp.close()\n    else:\n        logfile = salt.utils.path_join(\n            '/var/log',\n            '{0}.log'.format(exe_name)\n        )\n    return logfile",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2015-1839",
        "description": "[{'lang': 'en', 'value': 'modules/chef.py in SaltStack before 2014.7.4 does not properly handle files in /tmp.'}]",
        "cwe_number": 19
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-48",
      "code": "def get_parser():\n    parser = configargparse.ArgumentParser(\n        prog='rdiffweb',\n        description='Web interface to browse and restore rdiff-backup repositories.',\n        default_config_files=['/etc/rdiffweb/rdw.conf', '/etc/rdiffweb/rdw.conf.d/*.conf'],\n        add_env_var_help=True,\n        auto_env_var_prefix='RDIFFWEB_',\n        config_file_parser_class=ConfigFileParser,\n        conflict_handler='resolve',\n    )\n    parser.add_argument(\n        '-f', '--config', is_config_file=True, metavar='FILE', help='location of Rdiffweb configuration file'\n    )\n    parser.add(\n        '--database-uri',\n        '--sqlitedb-file',\n        '--sqlitedbfile',\n        metavar='URI',\n        help=\"\"\"Location of the database used for persistence. SQLite and PostgreSQL\n            database are supported officially. To use a SQLite database you may\n            define the location using a file path or a URI.\n            e.g.: /srv/rdiffweb/file.db or sqlite:///srv/rdiffweb/file.db`.\n            To use PostgreSQL server you must provide\n            a URI similar to postgresql://user:pass@10.255.1.34/dbname and you\n            must install required dependencies.\n            By default, Rdiffweb uses a SQLite embedded database located at\n            /etc/rdiffweb/rdw.db.\"\"\",\n        default='/etc/rdiffweb/rdw.db',\n    )\n    parser.add_argument(\n        '-d',\n        '--debug',\n        action='store_true',\n        help='enable rdiffweb debug mode - change the log level to DEBUG, print exception stack trace to the web interface and show SQL query in logs',\n    )\n    parser.add_argument(\n        '--admin-user',\n        '--adminuser',\n        metavar='USERNAME',\n        help='administrator username. The administrator user get created on startup if the database is empty.',\n        default='admin',\n    )\n    parser.add_argument(\n        '--admin-password',\n        metavar='USERNAME',\n        help=\"\"\"administrator encrypted password as SSHA. Read online\n            documentation to know more about how to encrypt your password\n            into SSHA or use http://projects.marsching.org/weave4j/util/genpassword.php\n            When defined, administrator password cannot be updated using the web interface.\n            When undefined, default administrator password is `admin123` and\n            it can be updated using the web interface.\"\"\",\n    )\n    parser.add_argument(\n        '--default-theme',\n        '--defaulttheme',\n        help='define the default theme. Either: default, blue or orange. Define the CSS file to be loaded in the web interface. You may manually edit a CSS file to customize it. The location is similar to `/usr/local/lib/python3.9/dist-packages/rdiffweb/static/`',\n        choices=['default', 'blue', 'orange'],\n        default='default',\n    )\n    parser.add_argument(\n        '--environment',\n        choices=['development', 'production'],\n        help='define the type of environment: development, production. This is used to limit the information shown to the user when an error occur.',\n        default='production',\n    )\n    parser.add_argument(\n        '--email-encryption',\n        '--emailencryption',\n        choices=['none', 'ssl', 'starttls'],\n        help='type of encryption to be used when establishing communication with SMTP server. Default: none',\n        default='none',\n    )\n    parser.add_argument(\n        '--email-host',\n        '--emailhost',\n        metavar='HOST',\n        help='SMTP server used to send email in the form <host>:<port>. If the port is not provided, default to standard port 25 or 465 is used. e.g.: smtp.gmail.com:587',\n    )\n    parser.add_argument(\n        '--email-sender',\n        '--emailsender',\n        metavar='EMAIL',\n        help='email addres used for the `from:` field when sending email.',\n    )\n    parser.add_argument(\n        '--email-notification-time',\n        '--emailnotificationtime',\n        metavar='TIME',\n        help='time when the email notifcation should be sent for inactive backups. e.g.: 22:00 Default value: 23:00',\n        default='23:00',\n    )\n    parser.add_argument(\n        '--email-username',\n        '--emailusername',\n        metavar='USERNAME',\n        help='username used for authentication with the SMTP server.',\n    )\n    parser.add_argument(\n        '--email-password',\n        '--emailpassword',\n        metavar='PASSWORD',\n        help='password used for authentication with the SMTP server.',\n    )\n    parser.add_argument(\n        '--email-send-changed-notification',\n        '--emailsendchangednotification',\n        help='True to send notification when sensitive information get change in user profile.',\n        action='store_true',\n        default=False,\n    )\n    parser.add_argument(\n        '--favicon',\n        help='location of an icon to be used as a favicon displayed in web browser.',\n        default=pkg_resources.resource_filename('rdiffweb', 'static/favicon.ico'),\n    )\n    parser.add_argument(\n        '--footer-name', '--footername', help=argparse.SUPPRESS, default='rdiffweb'\n    )\n    parser.add_argument(\n        '--footer-url', '--footerurl', help=argparse.SUPPRESS, default='https://rdiffweb.org/'\n    )\n    parser.add_argument(\n        '--header-logo',\n        '--headerlogo',\n        help='location of an image (preferably a .png) to be used as a replacement for the rdiffweb logo.',\n    )\n    parser.add_argument(\n        '--header-name',\n        '--headername',\n        help='application name displayed in the title bar and header menu.',\n        default='Rdiffweb',\n    )\n    parser.add_argument(\n        '--ldap-add-missing-user',\n        '--addmissinguser',\n        action='store_true',\n        help='enable creation of users from LDAP when the credential are valid.',\n        default=False,\n    )\n    parser.add_argument(\n        '--ldap-add-user-default-role',\n        help='default role used when creating users from LDAP. This parameter is only useful when `--ldap-add-missing-user` is enabled.',\n        default='user',\n        choices=['admin', 'maintainer', 'user'],\n    )\n    parser.add_argument(\n        '--ldap-add-user-default-userroot',\n        help='default user root directory used when creating users from LDAP. LDAP attributes may be used to define the default location. e.g.: `/backups/{uid[0]}/`. This parameter is only useful when `--ldap-add-missing-user` is enabled.',\n        default='',\n    )\n    parser.add_argument(\n        '--ldap-uri',\n        '--ldapuri',\n        help='URL to the LDAP server used to validate user credentials. e.g.: ldap://localhost:389',\n    )\n    parser.add_argument(\n        '--ldap-base-dn',\n        '--ldapbasedn',\n        metavar='DN',\n        help='DN of the branch of the directory where all searches should start from. e.g.: dc=my,dc=domain',\n        default=\"\",\n    )\n    parser.add_argument(\n        '--ldap-scope',\n        '--ldapscope',\n        help='scope of the search. Can be either base, onelevel or subtree',\n        choices=['base', 'onelevel', 'subtree'],\n        default=\"subtree\",\n    )\n    parser.add_argument('--ldap-tls', '--ldaptls', action='store_true', help='enable TLS')\n    parser.add_argument(\n        '--ldap-username-attribute',\n        '--ldapattribute',\n        metavar='ATTRIBUTE',\n        help=\"The attribute to search username. If no attributes are provided, the default is to use `uid`. It's a good idea to choose an attribute that will be unique across all entries in the subtree you will be using.\",\n        default='uid',\n    )\n    parser.add_argument(\n        '--ldap-filter',\n        '--ldapfilter',\n        help=\"search filter to limit LDAP lookup. If not provided, defaults to (objectClass=*), which searches for all objects in the tree.\",\n        default='(objectClass=*)',\n    )\n    parser.add_argument(\n        '--ldap-required-group',\n        '--ldaprequiredgroup',\n        metavar='GROUPNAME',\n        help=\"name of the group of which the user must be a member to access rdiffweb. Should be used with ldap-group-attribute and ldap-group-attribute-is-dn.\",\n    )\n    parser.add_argument(\n        '--ldap-group-attribute',\n        '--ldapgroupattribute',\n        metavar='ATTRIBUTE',\n        help=\"name of the attribute defining the groups of which the user is a member. Should be used with ldap-required-group and ldap-group-attribute-is-dn.\",\n        default='member',\n    )\n    parser.add_argument(\n        '--ldap-group-attribute-is-dn',\n        '--ldapgroupattributeisdn',\n        help=\"True if the content of the attribute `ldap-group-attribute` is a DN.\",\n        action='store_true',\n    )\n    parser.add_argument(\n        '--ldap-bind-dn',\n        '--ldapbinddn',\n        metavar='DN',\n        help=\"optional DN used to bind to the server when searching for entries. If not provided, will use an anonymous bind.\",\n        default=\"\",\n    )\n    parser.add_argument(\n        '--ldap-bind-password',\n        '--ldapbindpassword',\n        metavar='PASSWORD',\n        help=\"password to use in conjunction with LdapBindDn. Note that the bind password is probably sensitive data, and should be properly protected. You should only use the LdapBindDn and LdapBindPassword if you absolutely need them to search the directory.\",\n        default=\"\",\n    )\n    parser.add_argument(\n        '--ldap-version',\n        '--ldapversion',\n        '--ldapprotocolversion',\n        help=\"version of LDAP in use either 2 or 3. Default to 3.\",\n        default=3,\n        type=int,\n        choices=[2, 3],\n    )\n    parser.add_argument(\n        '--ldap-network-timeout',\n        '--ldapnetworktimeout',\n        metavar='SECONDS',\n        help=\"timeout in seconds value used for LDAP connection\",\n        default=100,\n        type=int,\n    )\n    parser.add_argument(\n        '--ldap-timeout',\n        '--ldaptimeout',\n        metavar='SECONDS',\n        help=\"timeout in seconds value used for LDAP request\",\n        default=300,\n        type=int,\n    )\n    parser.add_argument(\n        '--ldap-encoding',\n        '--ldapencoding',\n        metavar='ENCODING',\n        help=\"encoding used by your LDAP server.\",\n        default=\"utf-8\",\n    )\n    parser.add_argument(\n        '--log-access-file', '--logaccessfile', metavar='FILE', help='location of Rdiffweb log access file.'\n    )\n    parser.add_argument(\n        '--log-file',\n        '--logfile',\n        metavar='FILE',\n        help='location of Rdiffweb log file. Print log to the console if not define in config file.',\n    )\n    parser.add_argument(\n        '--log-level',\n        '--loglevel',\n        help='Define the log level.',\n        choices=['ERROR', 'WARN', 'INFO', 'DEBUG'],\n        default='INFO',\n    )\n    parser.add_argument(\n        '--max-depth',\n        '--maxdepth',\n        metavar='DEPTH',\n        help=\"define the maximum folder depthness to search into the user's root directory to find repositories. This is commonly used if you repositories are organised with multiple sub-folder.\",\n        type=int,\n        default=3,\n    )\n    parser.add('--quota-set-cmd', '--quotasetcmd', metavar='COMMAND', help=\"command line to set the user's quota.\")\n    parser.add('--quota-get-cmd', '--quotagetcmd', metavar='COMMAND', help=\"command line to get the user's quota.\")\n    parser.add(\n        '--quota-used-cmd', '--quotausedcmd', metavar='COMMAND', help=\"Command line to get user's quota disk usage.\"\n    )\n    parser.add(\n        '--remove-older-time',\n        '--removeoldertime',\n        metavar='TIME',\n        help=\"Time when to execute the remove older scheduled job. e.g.: 22:30\",\n        default='23:00',\n    )\n    parser.add('--server-host', '--serverhost', metavar='IP', default='127.0.0.1', help='IP address to listen to')\n    parser.add(\n        '--server-port',\n        '--serverport',\n        metavar='PORT',\n        help='port to listen to for HTTP request',\n        default='8080',\n        type=int,\n    )\n    parser.add(\n        '--rate-limit-dir',\n        '--session-dir',\n        '--sessiondir',\n        metavar='FOLDER',\n        help='location where to store rate-limit information. When undefined, the data is kept in memory. `--session-dir` are deprecated and kept for backward compatibility.',\n    )\n    parser.add(\n        '--rate-limit',\n        metavar='LIMIT',\n        type=int,\n        default=20,\n        help='maximum number of requests per hour that can be made on sensitive endpoints. When this limit is reached, an HTTP 429 message is returned to the user or the user is logged out. This security measure is used to limit brute force attacks on the login page and the RESTful API.',\n    )\n    parser.add(\n        '--session-idle-timeout',\n        metavar='MINUTES',\n        help='This timeout defines the amount of time a session will remain active in case there is no activity in the session. User Session will be revoke after this period of inactivity, unless the user selected \"remember me\". Default 5 minutes.',\n        default=5,\n    )\n    parser.add(\n        '--session-absolute-timeout',\n        metavar='MINUTES',\n        help='This timeout defines the maximum amount of time a session can be active. After this period, user is forced to (re)authenticate, unless the user selected \"remember me\". Default 20 minutes.',\n        default=20,\n    )\n    parser.add(\n        '--session-persistent-timeout',\n        metavar='MINUTES',\n        help='This timeout defines the maximum amount of time to remember and trust a user device. This timeout is used when user select \"remember me\". Default 30 days.',\n        default=43200,\n    )\n    parser.add(\n        '--ssl-certificate',\n        '--sslcertificate',\n        metavar='CERT',\n        help='location of the SSL Certification to enable HTTPS (not recommended)',\n    )\n    parser.add(\n        '--ssl-private-key',\n        '--sslprivatekey',\n        metavar='KEY',\n        help='location of the SSL Private Key to enable HTTPS (not recommended)',\n    )\n    parser.add(\n        '--tempdir',\n        metavar='FOLDER',\n        help='alternate temporary folder to be used when restoring files. Might be useful if the default location has limited disk space. Default to TEMPDIR environment or `/tmp`.',\n    )\n    parser.add(\n        '--disable-ssh-keys',\n        action='store_true',\n        help='used to hide SSH Key management to avoid users to add or remove SSH Key using the web application',\n        default=False,\n    )\n    parser.add(\n        '--password-min-length',\n        type=int,\n        help=\"Minimum length of the user's password\",\n        default=8,\n    )\n    parser.add(\n        '--password-max-length',\n        type=int,\n        help=\"Maximum length of the user's password\",\n        default=128,\n    )\n    parser.add(\n        '--password-score',\n        type=lambda x: max(1, min(int(x), 4)),\n        help=\"Minimum zxcvbn's score for password. Value from 1 to 4. Default value 2. Read more about it here: https://github.com/dropbox/zxcvbn\",\n        default=2,\n    )\n    parser.add_argument('--version', action='version', version='%(prog)s ' + VERSION)\n    flags = ['--welcome-msg'] + ['--welcome-msg-' + i for i in ['ca', 'en', 'es', 'fr', 'ru']] + ['--welcomemsg']\n    parser.add_argument(\n        *flags,\n        metavar='HTML',\n        help='replace the welcome message displayed in the login page for default locale or for a specific locale',\n        action=LocaleAction\n    )\n    return parser\n    def add_user(cls, username, password=None, **attrs):\n        \"\"\"\n        Used to add a new user with an optional password.\n        \"\"\"\n        assert password is None or isinstance(password, str)\n        if UserObject.get_user(username):\n            raise ValueError(_(\"User %s already exists.\" % (username,)))\n        logger.info(\"adding new user [%s]\", username)\n        userobj = UserObject(\n            username=username,\n            hash_password=hash_password(password) if password else '',\n            **attrs,\n        ).add()\n        cherrypy.engine.publish('user_added', userobj)\n        return userobj\n    def username(self):\n        return self._username\n    def username(self, value):\n        oldvalue = self._username\n        self._username = value\n        if oldvalue != value:\n            cherrypy.engine.publish('user_attr_changed', self, {'username': (oldvalue, value)})\n    def user_root(self, value):\n        oldvalue = self._user_root\n        self._user_root = value\n        if oldvalue != value:\n            cherrypy.engine.publish('user_attr_changed', self, {'user_root': (oldvalue, value)})\n    def validate_access_token(self, token):\n        \"\"\"\n        Check if the given token matches.\n        \"\"\"\n        for access_token in Token.query.all():\n            if access_token.is_expired:\n                access_token.delete()\n                continue\n            if check_password(token, access_token.hash_token):\n                access_token.access_time = datetime.datetime.utcnow\n                return True\n        return False\n    def user_attr_changed(self, userobj, attrs={}):\n        if not self.send_changed:\n            return\n        if 'email' not in attrs:\n            return\n        old_email = attrs['email'][0]\n        if not old_email:\n            logger.info(\"can't sent mail to user [%s] without an email\", userobj.username)\n            return\n        body = self.app.templates.compile_template(\n            \"email_changed.html\", **{\"header_name\": self.app.cfg.header_name, 'user': userobj}\n        )\n        self.bus.publish('queue_mail', to=old_email, subject=_(\"Email address changed\"), message=body)\n    def user_password_changed(self, userobj):\n        if not self.send_changed:\n            return\n        if not userobj.email:\n            logger.info(\"can't sent mail to user [%s] without an email\", userobj.username)\n            return\n        body = self.app.templates.compile_template(\n            \"password_changed.html\", **{\"header_name\": self.app.cfg.header_name, 'user': userobj}\n        )\n        self.bus.publish('queue_mail', to=userobj.email, subject=_(\"Password changed\"), message=body)\n    def send_code(self):\n        userobj = cherrypy.serving.request.currentuser\n        if not userobj.email:\n            flash(\n                _(\n                    \"Multi-factor authentication is enabled for your account, but your account does not have a valid email address to send the verification code to. Check your account settings with your administrator.\"\n                )\n            )\n        else:\n            code = cherrypy.tools.auth_mfa.generate_code()\n            body = self.app.templates.compile_template(\n                \"email_mfa.html\", **{\"header_name\": self.app.cfg.header_name, 'user': userobj, 'code': code}\n            )\n            cherrypy.engine.publish('queue_mail', to=userobj.email, subject=_(\"Your verification code\"), message=body)\n            flash(_(\"A new verification code has been sent to your email.\"))\n    def send_code(self):\n        userobj = self.app.currentuser\n        if not userobj.email:\n            flash(_(\"To continue, you must set up an email address for your account.\"), level='warning')\n            return\n        code = cherrypy.tools.auth_mfa.generate_code()\n        body = self.app.templates.compile_template(\n            \"email_mfa.html\", **{\"header_name\": self.app.cfg.header_name, 'user': userobj, 'code': code}\n        )\n        cherrypy.engine.publish('queue_mail', to=userobj.email, subject=_(\"Your verification code\"), message=body)\n        flash(_(\"A new verification code has been sent to your email.\"))",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-3363",
        "description": "[{'lang': 'en', 'value': 'Business Logic Errors in GitHub repository ikus060/rdiffweb prior to 2.5.0a7.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-49",
      "code": "  def _RunAndVerifyBackpropInputDilation(self, input_sizes, filter_sizes,\n                                         output_sizes, strides, dilations,\n                                         padding, data_format, use_gpu, err):\n    x1 = self._CreateNumpyTensor(input_sizes)\n    x2 = self._CreateNumpyTensor(filter_sizes)\n    default_dilations = (dilations[0] == 1 and dilations[1] == 1)\n    if default_dilations or use_gpu:\n      with self.cached_session(use_gpu=use_gpu) as sess:\n        if data_format == \"NCHW\":\n          input_sizes = test_util.NHWCToNCHW(input_sizes)\n        t1 = constant_op.constant(x1, shape=input_sizes)\n        t2 = constant_op.constant(x2, shape=filter_sizes)\n        full_strides = [1] + strides + [1]\n        full_dilations = [1] + dilations + [1]\n        if data_format == \"NCHW\":\n          full_strides = test_util.NHWCToNCHW(full_strides)\n          full_dilations = test_util.NHWCToNCHW(full_dilations)\n        conv_forward = nn_ops.conv2d(\n            t1,\n            t2,\n            strides=full_strides,\n            dilations=full_dilations,\n            padding=padding,\n            data_format=data_format)\n        conv_forward_2 = nn_ops.convolution(\n            t1,\n            t2,\n            padding=padding,\n            strides=strides,\n            dilation_rate=dilations,\n            data_format=data_format)\n        if data_format == \"NCHW\":\n          conv_forward = test_util.NCHWToNHWC(conv_forward)\n          conv_forward_2 = test_util.NCHWToNHWC(conv_forward_2)\n        conv = gradients_impl.gradients(conv_forward, t1)[0]\n        conv_2 = gradients_impl.gradients(conv_forward_2, t1)[0]\n        value = self.evaluate(conv)\n        value_2 = self.evaluate(conv_2)\n        self.assertShapeEqual(value, conv)\n        self.assertShapeEqual(value_2, conv_2)\n      tf_logging.debug(\"expected = %s\", value_2)\n      tf_logging.debug(\"actual = %s\", value)\n      self.assertArrayNear(value_2.flatten(), value.flatten(), err)\n  def _RunAndVerifyBackpropFilterDilation(self, input_sizes, filter_sizes,\n                                          output_sizes, strides, dilations,\n                                          padding, data_format, use_gpu, err):\n    x1 = self._CreateNumpyTensor(input_sizes)\n    x2 = self._CreateNumpyTensor(filter_sizes)\n    default_dilations = (dilations[0] == 1 and dilations[1] == 1)\n    if default_dilations or use_gpu:\n      with self.cached_session(use_gpu=use_gpu) as sess:\n        if data_format == \"NCHW\":\n          input_sizes = test_util.NHWCToNCHW(input_sizes)\n        t1 = constant_op.constant(x1, shape=input_sizes)\n        t2 = constant_op.constant(x2, shape=filter_sizes)\n        full_strides = [1] + strides + [1]\n        full_dilations = [1] + dilations + [1]\n        if data_format == \"NCHW\":\n          full_strides = test_util.NHWCToNCHW(full_strides)\n          full_dilations = test_util.NHWCToNCHW(full_dilations)\n        conv_forward = nn_ops.conv2d(\n            t1,\n            t2,\n            strides=full_strides,\n            dilations=full_dilations,\n            padding=padding,\n            data_format=data_format)\n        conv_forward_2 = nn_ops.convolution(\n            t1,\n            t2,\n            padding=padding,\n            strides=strides,\n            dilation_rate=dilations,\n            data_format=data_format)\n        if data_format == \"NCHW\":\n          conv_forward = test_util.NCHWToNHWC(conv_forward)\n          conv_forward_2 = test_util.NCHWToNHWC(conv_forward_2)\n        conv = gradients_impl.gradients(conv_forward, t2)[0]\n        conv_2 = gradients_impl.gradients(conv_forward, t2)[0]\n        value = self.evaluate(conv)\n        value_2 = self.evaluate(conv_2)\n        self.assertShapeEqual(value, conv)\n        self.assertShapeEqual(value_2, conv_2)\n      tf_logging.debug(\"expected = %s\", value_2)\n      tf_logging.debug(\"actual = %s\", value)\n      self.assertArrayNear(value_2.flatten(), value.flatten(), err)\n  def _VerifyValues(self, tensor_in_sizes, filter_in_sizes, stride, padding,\n                    expected):\n    \"\"\"Verifies the output values of the convolution function.\n    Args:\n      tensor_in_sizes: Input tensor dimensions in [batch, input_rows,\n        input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in [filter_rows, filter_cols,\n        input_depth, depth_multiplier].\n      stride: Stride.\n      padding: Padding type.\n      expected: An array containing the expected operation outputs.\n    \"\"\"\n    total_size_1 = 1\n    total_size_2 = 1\n    for s in tensor_in_sizes:\n      total_size_1 *= s\n    for s in filter_in_sizes:\n      total_size_2 *= s\n    x1 = [f * 1.0 for f in range(1, total_size_1 + 1)]\n    x2 = [f * 1.0 for f in range(1, total_size_2 + 1)]\n    with self.cached_session() as sess:\n      t1 = constant_op.constant(x1, shape=tensor_in_sizes)\n      t1.set_shape(tensor_in_sizes)\n      t2 = constant_op.constant(x2, shape=filter_in_sizes)\n      conv = nn_impl.depthwise_conv2d(\n          t1, t2, strides=[1, stride, stride, 1], padding=padding)\n      value = self.evaluate(conv)\n    tf_logging.debug(\"value = %s\", value)\n    self.assertArrayNear(expected, np.ravel(value), 1e-5)\n    self.assertShapeEqual(value, conv)\n  def _CompareFwdConv2D(self, tensor_in_sizes, filter_in_sizes, conv_strides,\n                        padding):\n    \"\"\"Verifies that DeepConv2D and Conv2D produce the same values.\n    Args:\n      tensor_in_sizes: Input tensor dimensions in\n        [batch, input_rows, input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in\n        [kernel_rows, kernel_cols, input_depth, output_depth].\n      conv_strides: [row_stride, col_stride] for the convolution;\n      padding: Padding type.\n    \"\"\"\n    x1 = np.random.rand(*tensor_in_sizes).astype(np.float32)\n    x2 = np.random.rand(*filter_in_sizes).astype(np.float32)\n    with self.cached_session(use_gpu=False) as sess:\n      t1 = constant_op.constant(x1, shape=tensor_in_sizes)\n      t2 = constant_op.constant(x2, shape=filter_in_sizes)\n      strides = [1] + conv_strides + [1]\n      conv = nn_ops.conv2d(t1, t2, strides=strides, padding=padding)\n      os.environ[\"TF_USE_DEEP_CONV2D\"] = \"0\"\n      values_expect = self.evaluate([conv])\n      os.environ[\"TF_USE_DEEP_CONV2D\"] = \"1\"\n      values_test = self.evaluate([conv])\n      self.assertAllClose(values_expect, values_test, rtol=1e-5, atol=1e-5)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-35969",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. The implementation of `Conv2DBackpropInput` requires `input_sizes` to be 4-dimensional. Otherwise, it gives a `CHECK` failure which can be used to trigger a denial of service attack. We have patched the issue in GitHub commit 50156d547b9a1da0144d7babe665cf690305b33c. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue.'}]",
        "cwe_number": 617
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-50",
      "code": "    def __init__(self, bus, object_path):\n        super(Engine, self).__init__(connection=bus.get_connection(),\n                                     object_path=object_path)\n        self.__context = Anthy.GContext()\n        self.__context.set_encoding(Anthy.UTF8_ENCODING)\n        self.__idle_id = 0\n        self.__prop_dict = {}\n        try:\n            self.__is_utf8 = (getpreferredencoding().lower() == 'utf-8')\n        except:\n            self.__is_utf8 = False\n        self.__ibus_version = 0.0\n        size = self.__prefs.get_value('common', 'page_size')\n        self.__lookup_table = IBus.LookupTable.new(page_size=size,\n                                                   cursor_pos=0,\n                                                   cursor_visible=True,\n                                                   round=True)\n        self.__prop_list = self.__init_props()\n        self.__init_signal()\n        self.__reset()\n        ibus_config = bus.get_config()\n        if ibus_config != None:\n            ibus_config.connect('value-changed',\n                                self.__config_value_changed_cb)\n    def __get_ibus_version(self):\n        if self.__ibus_version == 0.0:\n            self.__ibus_version = \\\n                IBus.MAJOR_VERSION + IBus.MINOR_VERSION / 1000.0 + \\\n                IBus.MICRO_VERSION / 1000000.0\n        return self.__ibus_version\n    def do_focus_out(self):\n        mode = self.__prefs.get_value('common', 'behavior_on_focus_out')\n        if mode == 0 or mode == 1:\n            self.__reset()\n            self.__invalidate()\n    def do_disable(self):\n        self.__reset()\n        self.__invalidate()\n    def do_reset(self):\n        self.__reset()\n        self.__invalidate()\n    def do_destroy(self):\n        if self.__idle_id != 0:\n            GLib.source_remove(self.__idle_id)\n            self.__idle_id = 0\n        self.__remove_dict_files()\n        super(Engine,self).destroy()\n    def __process_key_event_internal2(self, keyval, keycode, state):\n        if Engine.__typing_mode == jastring.TYPING_MODE_THUMB_SHIFT and \\\n           Engine.__input_mode not in [INPUT_MODE_LATIN, INPUT_MODE_WIDE_LATIN]:\n            return self.process_key_event_thumb(keyval, keycode, state)\n        is_press = (state & IBus.ModifierType.RELEASE_MASK) == 0\n        state = state & (IBus.ModifierType.SHIFT_MASK |\n                         IBus.ModifierType.CONTROL_MASK |\n                         IBus.ModifierType.MOD1_MASK)\n        if not is_press:\n            return False\n        if keyval in KP_Table and self.__prefs.get_value('common',\n                                                         'ten_key_mode'):\n            keyval = KP_Table[keyval]\n        key = self._mk_key(keyval, state)\n        for cmd in self.__keybind.get(key, []):\n            if config.DEBUG:\n                print 'cmd =', cmd\n            try:\n                if getattr(self, cmd)(keyval, state):\n                    return True\n            except:\n                print >> sys.stderr, 'Unknown command = %s' % cmd\n        if state & (IBus.ModifierType.CONTROL_MASK | IBus.ModifierType.MOD1_MASK):\n            return False\n        if (IBus.KEY_exclam <= keyval <= IBus.KEY_asciitilde or\n            keyval == IBus.KEY_yen):\n            if Engine.__typing_mode == jastring.TYPING_MODE_KANA:\n                if keyval == IBus.KEY_0 and state == IBus.ModifierType.SHIFT_MASK:\n                    keyval = IBus.KEY_asciitilde\n                elif keyval == IBus.KEY_backslash and keycode in [132-8, 133-8]:\n                    keyval = IBus.KEY_yen\n            ret = self.__on_key_common(keyval, state)\n            if (Engine.__input_mode != INPUT_MODE_LATIN and\n                unichr(keyval) in u',.' and\n                self.__prefs.get_value('common', 'behavior_on_period')):\n                return self.__cmd_convert(keyval, state)\n            return ret\n        else:\n            if not self.__preedit_ja_string.is_empty():\n                return True\n            return False\n    def _chk_mode(self, mode):\n        if '0' in mode and self.__preedit_ja_string.is_empty():\n            return True\n        if self.__convert_mode == CONV_MODE_OFF:\n            if '1' in mode and not self.__preedit_ja_string.is_empty():\n                return True\n        elif self.__convert_mode == CONV_MODE_ANTHY:\n            if '2' in mode and not self.__lookup_table_visible:\n                return True\n        elif self.__convert_mode == CONV_MODE_PREDICTION:\n            if '3' in mode and not self.__lookup_table_visible:\n                return True\n        else:\n            if '4' in mode:\n                return True\n        if '5' in mode and self.__lookup_table_visible:\n            return True\n        return False",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2013-4509",
        "description": "[{'lang': 'en', 'value': 'The default configuration of IBUS 1.5.4, and possibly 1.5.2 and earlier, when IBus.InputPurpose.PASSWORD is not set and used with GNOME 3, does not obscure the entered password characters, which allows physically proximate attackers to obtain a user password by reading the lockscreen.'}]",
        "cwe_number": 255
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-51",
      "code": "    def format_details(self, replace=False):\n        env = self.environment\n        text = _(\"Additional Information:\\n\")\n        text += format_2_column_name_value(_(\"Source Context\"),        self.scontext.format())\n        text += format_2_column_name_value(_(\"Target Context\"),        self.tcontext.format())\n        text += format_2_column_name_value(_(\"Target Objects\"),        self.format_target_object())\n        text += format_2_column_name_value(_(\"Source\"),                default_text(self.source))\n        text += format_2_column_name_value(_(\"Source Path\"),           default_text(self.spath))\n        text += format_2_column_name_value(_(\"Port\"),                  default_text(self.port))\n        if (replace):\n            text += format_2_column_name_value(_(\"Host\"),              \"(removed)\")\n        else:\n            text += format_2_column_name_value(_(\"Host\"),                  default_text(self.sig.host))\n        text += format_2_column_name_value(_(\"Source RPM Packages\"),   default_text(self.format_rpm_list(self.src_rpm_list)))\n        text += format_2_column_name_value(_(\"Target RPM Packages\"),   default_text(self.format_rpm_list(self.tgt_rpm_list)))\n        text += format_2_column_name_value(_(\"Policy RPM\"),            default_text(env.policy_rpm))\n        text += format_2_column_name_value(_(\"Selinux Enabled\"),       default_text(env.selinux_enabled))\n        text += format_2_column_name_value(_(\"Policy Type\"),           default_text(env.policy_type))\n        text += format_2_column_name_value(_(\"Enforcing Mode\"),        default_text(env.enforce))\n        if replace:\n            text += format_2_column_name_value(_(\"Host Name\"),\"(removed)\")\n        else:\n            text += format_2_column_name_value(_(\"Host Name\"),         default_text(env.hostname))\n        if replace:\n            uname = env.uname.split()\n            uname[1] = \"(removed)\"\n            text += format_2_column_name_value(_(\"Platform\"),          default_text(\" \".join(uname)))\n        else:\n            text += format_2_column_name_value(_(\"Platform\"),              default_text(env.uname))\n        text += format_2_column_name_value(_(\"Alert Count\"),           default_text(self.report_count))\n        date_format = \"%Y-%m-%d %H:%M:%S %Z\"\n        text += format_2_column_name_value(_(\"First Seen\"),            self.first_seen_date.format(date_format))\n        text += format_2_column_name_value(_(\"Last Seen\"),             self.last_seen_date.format(date_format))\n        text += format_2_column_name_value(_(\"Local ID\"),              default_text(self.local_id))\n        text += '\\n' + _(\"Raw Audit Messages\")\n        avcbuf = \"\"\n        for audit_record in self.audit_event.records:\n            if audit_record.record_type == 'AVC':\n                avcbuf += \"\\n\" + audit_record.to_text() + \"\\n\"\n            else:\n                avcbuf += \"\\ntype=%s msg=%s: \" % (audit_record.record_type, audit_record.event_id)\n                avcbuf += ' '.join([\"%s=%s\" % (k, audit_record.fields[k]) for k in audit_record.fields_ord]) +\"\\n\"\n        avcbuf += \"\\nHash: \" + self.get_hash_str()\n        try:\n            audit2allow = \"/usr/bin/audit2allow\"\n            if os.path.exist(audit2allow):\n                newbuf = \"\\n\\naudit2allow\"\n                p =  Popen([audit2allow], shell=True,stdin=PIPE, stdout=PIPE)\n                newbuf += p.communicate(avcbuf)[0]\n                if os.path.exists(\"/var/lib/sepolgen/interface_info\"):\n                    newbuf += \"\\naudit2allow -R\"\n                    p =  Popen([\"%s -R\" % audit2allow ], shell=True,stdin=PIPE, stdout=PIPE)\n                    newbuf += p.communicate(avcbuf)[0]\n                avcbuf += newbuf\n        except:\n            pass\n        text += avcbuf + '\\n'\n        return text",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2016-4445",
        "description": "[{'lang': 'en', 'value': 'The fix_lookup_id function in sealert in setroubleshoot before 3.2.23 allows local users to execute arbitrary commands as root by triggering an SELinux denial with a crafted file name, related to executing external commands with the commands.getstatusoutput function.'}]",
        "cwe_number": 77
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-52",
      "code": "    async def clear_products_command(self, interaction: Interaction):\n        if interaction.user.id not in config.Bot.Owners:\n            await interaction.response.send_message(\n                embed=Embed(\n                    title=\"Error\",\n                    description=\"You are not allowed to use this command.\",\n                    colour=interaction.user.colour,\n                    timestamp=utils.utcnow(),\n                ).set_footer(text=f\"Redon Hub \u2022 Version {self.bot.version}\"),\n            )\n            return\n        try:\n            products = await get_products()\n        except Exception:\n            products = []\n        if products:\n            view = ConfirmView(interaction.user)\n            await interaction.response.send_message(\n                embed=Embed(\n                    title=\"Clear Products\",\n                    description=f\"Are you sure you want to delete all products?\\n\\n**Warning:** This action is irreversible.\",\n                    colour=interaction.user.colour,\n                    timestamp=utils.utcnow(),\n                ).set_footer(text=f\"Redon Hub \u2022 Version {self.bot.version}\"),\n                view=view,\n            )\n            await view.wait()\n            await interaction.edit_original_response(\n                embed=Embed(\n                    title=\"Clearing Products\",\n                    description=f\"Please wait...\",\n                    colour=interaction.user.colour,\n                    timestamp=utils.utcnow(),\n                ).set_footer(text=f\"Redon Hub \u2022 Version {self.bot.version}\"),\n                view=None,\n            )\n            if view.value == True:\n                for product in products:\n                    try:\n                        await delete_product(product.id)\n                    except Exception as e:\n                        _log.error(e)\n                        await interaction.channel.send(\n                            f\"I was unable to delete `{product.name}`\"\n                        )\n                await interaction.edit_original_response(\n                    embed=Embed(\n                        title=\"Cleared Products\",\n                        description=f\"I have deleted all products.\",\n                        colour=interaction.user.colour,\n                        timestamp=utils.utcnow(),\n                    ).set_footer(text=f\"Redon Hub \u2022 Version {self.bot.version}\"),\n                    view=None,\n                )\n            else:\n                await interaction.edit_original_response(\n                    embed=Embed(\n                        title=\"Cancelled\",\n                        description=\"I have cancelled the action.\",\n                        colour=interaction.user.colour,\n                        timestamp=utils.utcnow(),\n                    ).set_footer(text=f\"Redon Hub \u2022 Version {self.bot.version}\"),\n                    view=None,\n                )\n        else:\n            await interaction.response.send_message(\n                embed=Embed(\n                    title=\"Not Found\",\n                    description=f\"I was unable to find any products.\",\n                    colour=interaction.user.colour,\n                    timestamp=utils.utcnow(),\n                ).set_footer(text=f\"Redon Hub \u2022 Version {self.bot.version}\")\n            )",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-31442",
        "description": "[{'lang': 'en', 'value': 'Redon Hub is a Roblox Product Delivery Bot, also known as a Hub. In all hubs before version 1.0.2, all commands are capable of being ran by all users, including admin commands. This allows users to receive products for free and delete/create/update products/tags/etc. The only non-affected command is `/products admin clear` as this was already programmed for bot owners only. All users should upgrade to version 1.0.2 to receive a patch.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-53",
      "code": "def config():\n    config_disabled = (\n            app.config['CONFIG_DISABLE'] or\n            not valid_user_session(session))\n    if request.method == 'GET':\n        return json.dumps(g.user_config.__dict__)\n    elif request.method == 'PUT' and not config_disabled:\n        if 'name' in request.args:\n            config_pkl = os.path.join(\n                app.config['CONFIG_PATH'],\n                request.args.get('name'))\n            session['config'] = (pickle.load(open(config_pkl, 'rb'))\n                                 if os.path.exists(config_pkl)\n                                 else session['config'])\n            return json.dumps(session['config'])\n        else:\n            return json.dumps({})\n    elif not config_disabled:\n        config_data = request.form.to_dict()\n        if 'url' not in config_data or not config_data['url']:\n            config_data['url'] = g.user_config.url\n        if 'name' in request.args:\n            pickle.dump(\n                config_data,\n                open(os.path.join(\n                    app.config['CONFIG_PATH'],\n                    request.args.get('name')), 'wb'))\n        session['config'] = config_data\n        return redirect(config_data['url'])\n    else:\n        return redirect(url_for('.index'), code=403)\ndef imgres():\n    return redirect(request.args.get('imgurl'))\ndef element():\n    element_url = src_url = request.args.get('url')\n    if element_url.startswith('gAAAAA'):\n        try:\n            cipher_suite = Fernet(g.session_key)\n            src_url = cipher_suite.decrypt(element_url.encode()).decode()\n        except (InvalidSignature, InvalidToken) as e:\n            return render_template(\n                'error.html',\n                error_message=str(e)), 401\n    src_type = request.args.get('type')\n    try:\n        file_data = g.user_request.send(base_url=src_url).content\n        tmp_mem = io.BytesIO()\n        tmp_mem.write(file_data)\n        tmp_mem.seek(0)\n        return send_file(tmp_mem, mimetype=src_type)\n    except exceptions.RequestException:\n        pass\n    empty_gif = base64.b64decode(\n        'R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==')\n    return send_file(io.BytesIO(empty_gif), mimetype='image/gif')\ndef window():\n    target_url = request.args.get('location')\n    if target_url.startswith('gAAAAA'):\n        cipher_suite = Fernet(g.session_key)\n        target_url = cipher_suite.decrypt(target_url.encode()).decode()\n    content_filter = Filter(\n        g.session_key,\n        root_url=request.url_root,\n        config=g.user_config)\n    target = urlparse.urlparse(target_url)\n    host_url = f'{target.scheme}://{target.netloc}'\n    get_body = g.user_request.send(base_url=target_url).text\n    results = bsoup(get_body, 'html.parser')\n    src_attrs = ['src', 'href', 'srcset', 'data-srcset', 'data-src']\n    for element in results.find_all():\n        for attr in src_attrs:\n            if not element.has_attr(attr) or not element[attr].startswith('/'):\n                continue\n            element[attr] = host_url + element[attr]\n    for script in results.find_all('script', {'src': True}):\n        if 'nojs' in request.args:\n            script.decompose()\n        else:\n            content_filter.update_element_src(script, 'application/javascript')\n    img_sources = ['src', 'data-src', 'data-srcset', 'srcset']\n    for img in results.find_all('img'):\n        _ = [\n            content_filter.update_element_src(img, 'image/png', attr=_)\n            for _ in img_sources if img.has_attr(_)\n        ]\n    for link in results.find_all('link', {'href': True}):\n        content_filter.update_element_src(link, 'text/css', attr='href')\n    for a in results.find_all('a', {'href': True}):\n        a['href'] = f'{Endpoint.window}?location=' + a['href'] + (\n            '&nojs=1' if 'nojs' in request.args else '')\n    for iframe in results.find_all('iframe'):\n        iframe.decompose()\n    return render_template(\n        'display.html',\n        response=results,\n        translation=app.config['TRANSLATIONS'][\n            g.user_config.get_localization_lang()\n        ]\n    )\ndef robots():\n    response = make_response(\n'''User-Agent: *\nDisallow: /''', 200)\n    response.mimetype = 'text/plain'\n    return response\ndef page_not_found(e):\n    return render_template('error.html', error_message=str(e)), 404\ndef run_app() -> None:\n    parser = argparse.ArgumentParser(\n        description='Whoogle Search console runner')\n    parser.add_argument(\n        '--port',\n        default=5000,\n        metavar='<port number>',\n        help='Specifies a port to run on (default 5000)')\n    parser.add_argument(\n        '--host',\n        default='127.0.0.1',\n        metavar='<ip address>',\n        help='Specifies the host address to use (default 127.0.0.1)')\n    parser.add_argument(\n        '--unix-socket',\n        default='',\n        metavar='</path/to/unix.sock>',\n        help='Listen for app on unix socket instead of host:port')\n    parser.add_argument(\n        '--debug',\n        default=False,\n        action='store_true',\n        help='Activates debug mode for the server (default False)')\n    parser.add_argument(\n        '--https-only',\n        default=False,\n        action='store_true',\n        help='Enforces HTTPS redirects for all requests')\n    parser.add_argument(\n        '--userpass',\n        default='',\n        metavar='<username:password>',\n        help='Sets a username/password basic auth combo (default None)')\n    parser.add_argument(\n        '--proxyauth',\n        default='',\n        metavar='<username:password>',\n        help='Sets a username/password for a HTTP/SOCKS proxy (default None)')\n    parser.add_argument(\n        '--proxytype',\n        default='',\n        metavar='<socks4|socks5|http>',\n        help='Sets a proxy type for all connections (default None)')\n    parser.add_argument(\n        '--proxyloc',\n        default='',\n        metavar='<location:port>',\n        help='Sets a proxy location for all connections (default None)')\n    args = parser.parse_args()\n    if args.userpass:\n        user_pass = args.userpass.split(':')\n        os.environ['WHOOGLE_USER'] = user_pass[0]\n        os.environ['WHOOGLE_PASS'] = user_pass[1]\n    if args.proxytype and args.proxyloc:\n        if args.proxyauth:\n            proxy_user_pass = args.proxyauth.split(':')\n            os.environ['WHOOGLE_PROXY_USER'] = proxy_user_pass[0]\n            os.environ['WHOOGLE_PROXY_PASS'] = proxy_user_pass[1]\n        os.environ['WHOOGLE_PROXY_TYPE'] = args.proxytype\n        os.environ['WHOOGLE_PROXY_LOC'] = args.proxyloc\n    if args.https_only:\n        os.environ['HTTPS_ONLY'] = '1'\n    if args.debug:\n        app.run(host=args.host, port=args.port, debug=args.debug)\n    elif args.unix_socket:\n        waitress.serve(app, unix_socket=args.unix_socket)\n    else:\n        waitress.serve(\n            app,\n            listen=\"{}:{}\".format(args.host, args.port),\n            url_prefix=os.environ.get('WHOOGLE_URL_PREFIX', ''))",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-22204",
        "description": "[{'lang': 'en', 'value': 'Whoogle Search is a self-hosted metasearch engine. Versions 0.8.3 and prior have a limited file write vulnerability when the configuration options in Whoogle are enabled. The `config` function in `app/routes.py` does not validate the user-controlled `name` variable on line 447 and `config_data` variable on line 437. The `name` variable is insecurely concatenated in `os.path.join`, leading to path manipulation. The POST data from the `config_data` variable is saved with `pickle.dump` which leads to a limited file write. However, the data that is saved is earlier transformed into a dictionary and the `url` key value pair is added before the file is saved on the system. All in all, the issue allows us to save and overwrite files on the system that the application has permissions to, with a dictionary containing arbitrary data and the `url` key value, which is a limited file write. Version 0.8.4 contains a patch for this issue.'}]",
        "cwe_number": 22
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-54",
      "code": "def remove_logical_volumes(*paths):\n    \"\"\"Remove one or more logical volume.\"\"\"\n    if paths:\n        lvremove = ('lvremove', '-f') + paths\n        execute(*lvremove, attempts=3, run_as_root=True)\ndef pick_disk_driver_name(is_block_dev=False):\n    \"\"\"Pick the libvirt primary backend driver name\n    If the hypervisor supports multiple backend drivers, then the name\n    attribute selects the primary backend driver name, while the optional\n    type attribute provides the sub-type.  For example, xen supports a name\n    of \"tap\", \"tap2\", \"phy\", or \"file\", with a type of \"aio\" or \"qcow2\",\n    while qemu only supports a name of \"qemu\", but multiple types including\n    \"raw\", \"bochs\", \"qcow2\", and \"qed\".\n    :param is_block_dev:\n    :returns: driver_name or None\n    \"\"\"\n    if CONF.libvirt_type == \"xen\":\n        if is_block_dev:\n            return \"phy\"\n        else:\n            return \"tap\"\n    elif CONF.libvirt_type in ('kvm', 'qemu'):\n        return \"qemu\"\n    else:\n        return None\ndef get_disk_size(path):\n    \"\"\"Get the (virtual) size of a disk image\n    :param path: Path to the disk image\n    :returns: Size (in bytes) of the given disk image as it would be seen\n              by a virtual machine.\n    \"\"\"\n    size = images.qemu_img_info(path).virtual_size\n    return int(size)\ndef get_disk_backing_file(path):\n    \"\"\"Get the backing file of a disk image\n    :param path: Path to the disk image\n    :returns: a path to the image's backing store\n    \"\"\"\n    backing_file = images.qemu_img_info(path).backing_file\n    if backing_file:\n        backing_file = os.path.basename(backing_file)\n    return backing_file",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2012-5625",
        "description": "[{'lang': 'en', 'value': 'OpenStack Compute (Nova) Folsom before 2012.2.2 and Grizzly, when using libvirt and LVM backed instances, does not properly clear physical volume (PV) content when reallocating for instances, which allows attackers to obtain sensitive information by reading the memory of the previous logical volume (LV).'}]",
        "cwe_number": 200
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-55",
      "code": "    def skip(self, type):\n        if type == TType.STOP:\n            return\n        elif type == TType.BOOL:\n            self.readBool()\n        elif type == TType.BYTE:\n            self.readByte()\n        elif type == TType.I16:\n            self.readI16()\n        elif type == TType.I32:\n            self.readI32()\n        elif type == TType.I64:\n            self.readI64()\n        elif type == TType.DOUBLE:\n            self.readDouble()\n        elif type == TType.FLOAT:\n            self.readFloat()\n        elif type == TType.STRING:\n            self.readString()\n        elif type == TType.STRUCT:\n            name = self.readStructBegin()\n            while True:\n                (name, type, id) = self.readFieldBegin()\n                if type == TType.STOP:\n                    break\n                self.skip(type)\n                self.readFieldEnd()\n            self.readStructEnd()\n        elif type == TType.MAP:\n            (ktype, vtype, size) = self.readMapBegin()\n            for _ in range(size):\n                self.skip(ktype)\n                self.skip(vtype)\n            self.readMapEnd()\n        elif type == TType.SET:\n            (etype, size) = self.readSetBegin()\n            for _ in range(size):\n                self.skip(etype)\n            self.readSetEnd()\n        elif type == TType.LIST:\n            (etype, size) = self.readListBegin()\n            for _ in range(size):\n                self.skip(etype)\n            self.readListEnd()\n    def readIntegral(self, type):\n        if type == TType.BOOL:\n            return self.readBool()\n        elif type == TType.BYTE:\n            return self.readByte()\n        elif type == TType.I16:\n            return self.readI16()\n        elif type == TType.I32:\n            return self.readI32()\n        elif type == TType.I64:\n            return self.readI64()\n        else:\n            raise Exception(\"Unknown integral type: %s\" % str(type))",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2019-3552",
        "description": "[{'lang': 'en', 'value': 'C++ Facebook Thrift servers (using cpp2) would not error upon receiving messages with containers of fields of unknown type. As a result, malicious clients could send short messages which would take a long time for the server to parse, potentially leading to denial of service. This issue affects Facebook Thrift prior to v2019.02.18.00.'}]",
        "cwe_number": 755
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-56",
      "code": "    def get_type_string(self) -> str:\n        \"\"\" Get a string representation of type that should be used when declaring this property \"\"\"\n        if self.required:\n            return self._type_string\n        return f\"Optional[{self._type_string}]\"\n    def __post_init__(self) -> None:\n        super().__post_init__()\n        if self.default is not None:\n            self.default = f'\"{self.default}\"'\n    def get_imports(self, *, prefix: str) -> Set[str]:\n        \"\"\"\n        Get a set of import strings that should be included when this property is used somewhere\n        Args:\n            prefix: A prefix to put before any relative (local) module names.\n        \"\"\"\n        imports = super().get_imports(prefix=prefix)\n        imports.update({\"from datetime import datetime\", \"from typing import cast\"})\n        return imports\n    def __post_init__(self, title: str) -> None:\n        super().__post_init__()\n        reference = Reference.from_ref(title)\n        dedup_counter = 0\n        while reference.class_name in _existing_enums:\n            existing = _existing_enums[reference.class_name]\n            if self.values == existing.values:\n                break\n            dedup_counter += 1\n            reference = Reference.from_ref(f\"{reference.class_name}{dedup_counter}\")\n        self.reference = reference\n        inverse_values = {v: k for k, v in self.values.items()}\n        if self.default is not None:\n            self.default = f\"{self.reference.class_name}.{inverse_values[self.default]}\"\n        _existing_enums[self.reference.class_name] = self\n    def values_from_list(values: List[str]) -> Dict[str, str]:\n        \"\"\" Convert a list of values into dict of {name: value} \"\"\"\n        output: Dict[str, str] = {}\n        for i, value in enumerate(values):\n            if value[0].isalpha():\n                key = value.upper()\n            else:\n                key = f\"VALUE_{i}\"\n            if key in output:\n                raise ValueError(f\"Duplicate key {key} in Enum\")\n            output[key] = value\n        return output\ndef property_from_data(\n    name: str, required: bool, data: Union[oai.Reference, oai.Schema]\n) -> Union[Property, PropertyError]:\n    \"\"\" Generate a Property from the OpenAPI dictionary representation of it \"\"\"\n    if isinstance(data, oai.Reference):\n        return RefProperty(name=name, required=required, reference=Reference.from_ref(data.ref), default=None)\n    if data.enum:\n        return EnumProperty(\n            name=name,\n            required=required,\n            values=EnumProperty.values_from_list(data.enum),\n            title=data.title or name,\n            default=data.default,\n        )\n    if data.anyOf:\n        sub_properties: List[Property] = []\n        for sub_prop_data in data.anyOf:\n            sub_prop = property_from_data(name=name, required=required, data=sub_prop_data)\n            if isinstance(sub_prop, PropertyError):\n                return PropertyError(detail=f\"Invalid property in union {name}\", data=sub_prop_data)\n            sub_properties.append(sub_prop)\n        return UnionProperty(name=name, required=required, default=data.default, inner_properties=sub_properties)\n    if not data.type:\n        return PropertyError(data=data, detail=\"Schemas must either have one of enum, anyOf, or type defined.\")\n    if data.type == \"string\":\n        return _string_based_property(name=name, required=required, data=data)\n    elif data.type == \"number\":\n        return FloatProperty(name=name, default=data.default, required=required)\n    elif data.type == \"integer\":\n        return IntProperty(name=name, default=data.default, required=required)\n    elif data.type == \"boolean\":\n        return BooleanProperty(name=name, required=required, default=data.default)\n    elif data.type == \"array\":\n        if data.items is None:\n            return PropertyError(data=data, detail=\"type array must have items defined\")\n        inner_prop = property_from_data(name=f\"{name}_item\", required=True, data=data.items)\n        if isinstance(inner_prop, PropertyError):\n            return PropertyError(data=inner_prop.data, detail=f\"invalid data in items of array {name}\")\n        return ListProperty(name=name, required=required, default=data.default, inner_property=inner_prop,)\n    elif data.type == \"object\":\n        return DictProperty(name=name, required=required, default=data.default)\n    return PropertyError(data=data, detail=f\"unknown type {data.type}\")\n    def to_dict(self) -> Dict[str, Any]:\n        an_enum_value = self.an_enum_value.value\n        if isinstance(self.a_camel_date_time, datetime):\n            a_camel_date_time = self.a_camel_date_time.isoformat()\n        else:\n            a_camel_date_time = self.a_camel_date_time.isoformat()\n        a_date = self.a_date.isoformat()\n        if self.nested_list_of_enums is None:\n            nested_list_of_enums = None\n        else:\n            nested_list_of_enums = []\n            for nested_list_of_enums_item_data in self.nested_list_of_enums:\n                nested_list_of_enums_item = []\n                for nested_list_of_enums_item_item_data in nested_list_of_enums_item_data:\n                    nested_list_of_enums_item_item = nested_list_of_enums_item_item_data.value\n                    nested_list_of_enums_item.append(nested_list_of_enums_item_item)\n                nested_list_of_enums.append(nested_list_of_enums_item)\n        some_dict = self.some_dict\n        return {\n            \"an_enum_value\": an_enum_value,\n            \"aCamelDateTime\": a_camel_date_time,\n            \"a_date\": a_date,\n            \"nested_list_of_enums\": nested_list_of_enums,\n            \"some_dict\": some_dict,\n        }\n    def from_dict(d: Dict[str, Any]) -> AModel:\n        an_enum_value = AnEnum(d[\"an_enum_value\"])\n        def _parse_a_camel_date_time(data: Dict[str, Any]) -> Union[datetime, date]:\n            a_camel_date_time: Union[datetime, date]\n            try:\n                a_camel_date_time = datetime.fromisoformat(d[\"aCamelDateTime\"])\n                return a_camel_date_time\n            except:\n                pass\n            a_camel_date_time = date.fromisoformat(d[\"aCamelDateTime\"])\n            return a_camel_date_time\n        a_camel_date_time = _parse_a_camel_date_time(d[\"aCamelDateTime\"])\n        a_date = date.fromisoformat(d[\"a_date\"])\n        nested_list_of_enums = []\n        for nested_list_of_enums_item_data in d.get(\"nested_list_of_enums\") or []:\n            nested_list_of_enums_item = []\n            for nested_list_of_enums_item_item_data in nested_list_of_enums_item_data:\n                nested_list_of_enums_item_item = DifferentEnum(nested_list_of_enums_item_item_data)\n                nested_list_of_enums_item.append(nested_list_of_enums_item_item)\n            nested_list_of_enums.append(nested_list_of_enums_item)\n        some_dict = d.get(\"some_dict\")\n        return AModel(\n            an_enum_value=an_enum_value,\n            a_camel_date_time=a_camel_date_time,\n            a_date=a_date,\n            nested_list_of_enums=nested_list_of_enums,\n            some_dict=some_dict,\n        )\nasync def get_user_list(\n    *, client: Client, an_enum_value: List[AnEnum], some_date: Union[date, datetime],\n) -> Union[\n    List[AModel], HTTPValidationError,\n]:\n    \"\"\" Get a list of things  \"\"\"\n    url = \"{}/tests/\".format(client.base_url,)\n    headers: Dict[str, Any] = client.get_headers()\n    json_an_enum_value = []\n    for an_enum_value_item_data in an_enum_value:\n        an_enum_value_item = an_enum_value_item_data.value\n        json_an_enum_value.append(an_enum_value_item)\n    if isinstance(some_date, date):\n        json_some_date = some_date.isoformat()\n    else:\n        json_some_date = some_date.isoformat()\n    params: Dict[str, Any] = {\n        \"an_enum_value\": json_an_enum_value,\n        \"some_date\": json_some_date,\n    }\n    async with httpx.AsyncClient() as _client:\n        response = await _client.get(url=url, headers=headers, params=params,)\n    if response.status_code == 200:\n        return [AModel.from_dict(item) for item in cast(List[Dict[str, Any]], response.json())]\n    if response.status_code == 422:\n        return HTTPValidationError.from_dict(cast(Dict[str, Any], response.json()))\n    else:\n        raise ApiResponseError(response=response)\ndef get_user_list(\n    *, client: Client, an_enum_value: List[AnEnum], some_date: Union[date, datetime],\n) -> Union[\n    List[AModel], HTTPValidationError,\n]:\n    \"\"\" Get a list of things  \"\"\"\n    url = \"{}/tests/\".format(client.base_url)\n    headers: Dict[str, Any] = client.get_headers()\n    json_an_enum_value = []\n    for an_enum_value_item_data in an_enum_value:\n        an_enum_value_item = an_enum_value_item_data.value\n        json_an_enum_value.append(an_enum_value_item)\n    if isinstance(some_date, date):\n        json_some_date = some_date.isoformat()\n    else:\n        json_some_date = some_date.isoformat()\n    params: Dict[str, Any] = {\n        \"an_enum_value\": json_an_enum_value,\n        \"some_date\": json_some_date,\n    }\n    response = httpx.get(url=url, headers=headers, params=params,)\n    if response.status_code == 200:\n        return [AModel.from_dict(item) for item in cast(List[Dict[str, Any]], response.json())]\n    if response.status_code == 422:\n        return HTTPValidationError.from_dict(cast(Dict[str, Any], response.json()))\n    else:\n        raise ApiResponseError(response=response)\n    def from_data(*, data: oai.Operation, path: str, method: str, tag: str) -> Union[Endpoint, ParseError]:\n        \"\"\" Construct an endpoint from the OpenAPI data \"\"\"\n        if data.operationId is None:\n            return ParseError(data=data, detail=\"Path operations with operationId are not yet supported\")\n        endpoint = Endpoint(\n            path=path,\n            method=method,\n            description=data.description,\n            name=data.operationId,\n            requires_security=bool(data.security),\n            tag=tag,\n        )\n        result = Endpoint._add_parameters(endpoint, data)\n        if isinstance(result, ParseError):\n            return result\n        result = Endpoint._add_responses(result, data.responses)\n        if isinstance(result, ParseError):\n            return result\n        result = Endpoint._add_body(result, data)\n        return result\n    def __init__(self, *, openapi: GeneratorData) -> None:\n        self.openapi: GeneratorData = openapi\n        self.env: Environment = Environment(loader=PackageLoader(__package__), trim_blocks=True, lstrip_blocks=True)\n        self.project_name: str = self.project_name_override or f\"{utils.kebab_case(openapi.title).lower()}-client\"\n        self.project_dir: Path = Path.cwd() / self.project_name\n        self.package_name: str = self.package_name_override or self.project_name.replace(\"-\", \"_\")\n        self.package_dir: Path = self.project_dir / self.package_name\n        self.package_description: str = f\"A client library for accessing {self.openapi.title}\"\n        self.version: str = openapi.version\n        self.env.filters.update(self.TEMPLATE_FILTERS)\ndef _sanitize(value: str) -> str:\n    return re.sub(r\"[^\\w _-]+\", \"\", value)\ndef group_title(value: str) -> str:\n    value = re.sub(r\"([A-Z]{2,})([A-Z][a-z]|[ -_]|$)\", lambda m: m.group(1).title() + m.group(2), value.strip())\n    value = re.sub(r\"(^|[ _-])([A-Z])\", lambda m: m.group(1) + m.group(2).lower(), value)\n    return value\ndef snake_case(value: str) -> str:\n    return stringcase.snakecase(group_title(_sanitize(value)))\ndef pascal_case(value: str) -> str:\n    return stringcase.pascalcase(_sanitize(value))\ndef kebab_case(value: str) -> str:\n    return stringcase.spinalcase(group_title(_sanitize(value)))\ndef get_list(an_enum_value: List[AnEnum] = Query(...), some_date: Union[date, datetime] = Query(...)):\n    \"\"\" Get a list of things \"\"\"\n    return",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2020-15142",
        "description": "[{'lang': 'en', 'value': 'In openapi-python-client before version 0.5.3, clients generated with a maliciously crafted OpenAPI Document can generate arbitrary Python code. Subsequent execution of this malicious client is arbitrary code execution.'}]",
        "cwe_number": 94
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-57",
      "code": "def config():\n    config_disabled = (\n            app.config['CONFIG_DISABLE'] or\n            not valid_user_session(session))\n    if request.method == 'GET':\n        return json.dumps(g.user_config.__dict__)\n    elif request.method == 'PUT' and not config_disabled:\n        if 'name' in request.args:\n            config_pkl = os.path.join(\n                app.config['CONFIG_PATH'],\n                request.args.get('name'))\n            session['config'] = (pickle.load(open(config_pkl, 'rb'))\n                                 if os.path.exists(config_pkl)\n                                 else session['config'])\n            return json.dumps(session['config'])\n        else:\n            return json.dumps({})\n    elif not config_disabled:\n        config_data = request.form.to_dict()\n        if 'url' not in config_data or not config_data['url']:\n            config_data['url'] = g.user_config.url\n        if 'name' in request.args:\n            pickle.dump(\n                config_data,\n                open(os.path.join(\n                    app.config['CONFIG_PATH'],\n                    request.args.get('name')), 'wb'))\n        session['config'] = config_data\n        return redirect(config_data['url'])\n    else:\n        return redirect(url_for('.index'), code=403)\ndef element():\n    element_url = src_url = request.args.get('url')\n    if element_url.startswith('gAAAAA'):\n        try:\n            cipher_suite = Fernet(g.session_key)\n            src_url = cipher_suite.decrypt(element_url.encode()).decode()\n        except (InvalidSignature, InvalidToken) as e:\n            return render_template(\n                'error.html',\n                error_message=str(e)), 401\n    src_type = request.args.get('type')\n    try:\n        file_data = g.user_request.send(base_url=src_url).content\n        tmp_mem = io.BytesIO()\n        tmp_mem.write(file_data)\n        tmp_mem.seek(0)\n        return send_file(tmp_mem, mimetype=src_type)\n    except exceptions.RequestException:\n        pass\n    empty_gif = base64.b64decode(\n        'R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==')\n    return send_file(io.BytesIO(empty_gif), mimetype='image/gif')",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-22417",
        "description": "[{'lang': 'en', 'value': 'Whoogle Search is a self-hosted metasearch engine. In versions 0.8.3 and prior, the `element` method in `app/routes.py` does not validate the user-controlled `src_type` and `element_url` variables and passes them to the `send` method which sends a `GET` request on lines 339-343 in `requests.py`. The returned contents of the URL are then passed to and reflected back to the user in the `send_file` function on line 484, together with the user-controlled `src_type`, which allows the attacker to control the HTTP response content type leading to a cross-site scripting vulnerability. An attacker could craft a special URL to point to a malicious website and send the link to a victim. The fact that the link would contain a trusted domain (e.g. from one of public Whoogle instances) could be used to trick the user into clicking the link. The malicious website could, for example, be a copy of a real website, meant to steal a person\u2019s credentials to the website, or trick that person in another way. Version 0.8.4 contains a patch for this issue.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-58",
      "code": "def set_headers(\n    xfo='DENY',\n    no_cache=True,\n    referrer='same-origin',\n    nosniff=True,\n    xxp='1; mode=block',\n    csp=\"default-src 'self'; style-src 'self' 'unsafe-inline'; script-src 'self' 'unsafe-inline'\",\n):\n    \"\"\"\n    This tool provide CSRF mitigation.\n    * Define X-Frame-Options = DENY\n    * Define Cookies SameSite=Lax\n    * Define Cookies Secure when https is detected\n    * Validate `Origin` and `Referer` on POST, PUT, PATCH, DELETE\n    * Define Cache-Control by default\n    * Define Referrer-Policy to 'same-origin'\n    Ref.:\n    https://cheatsheetseries.owasp.org/cheatsheets/Cross-Site_Request_Forgery_Prevention_Cheat_Sheet.html\n    https://cheatsheetseries.owasp.org/cheatsheets/Clickjacking_Defense_Cheat_Sheet.html\n    \"\"\"\n    request = cherrypy.request\n    response = cherrypy.serving.response\n    if request.method in ['POST', 'PUT', 'PATCH', 'DELETE']:\n        origin = request.headers.get('Origin', None)\n        if origin and not origin.startswith(request.base):\n            raise cherrypy.HTTPError(403, 'Unexpected Origin header')\n    https = request.base.startswith('https')\n    if xfo:\n        response.headers['X-Frame-Options'] = xfo\n    cookie = response.cookie.get('session_id', None)\n    if cookie:\n        cookie['samesite'] = 'Lax'\n        if https:\n            cookie['secure'] = 1\n    if no_cache:\n        response.headers['Cache-control'] = 'no-cache, no-store, must-revalidate, max-age=0'\n        response.headers['Pragma'] = 'no-cache'\n        response.headers['Expires'] = '0'\n    if referrer:\n        response.headers['Referrer-Policy'] = referrer\n    if nosniff:\n        response.headers['X-Content-Type-Options'] = 'nosniff'\n    if xxp:\n        response.headers['X-XSS-Protection'] = xxp\n    if csp:\n        response.headers['Content-Security-Policy'] = csp\n    if https:\n        response.headers['Strict-Transport-Security'] = \"max-age=31536000; includeSubDomains\"",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-3457",
        "description": "[{'lang': 'en', 'value': 'Origin Validation Error in GitHub repository ikus060/rdiffweb prior to 2.5.0a5.'}]",
        "cwe_number": 346
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-59",
      "code": "    def __init__(self, bot):\n        super().__init__()\n        self.bot = bot\n        self.config = Config.get_conf(self, identifier=2_113_674_295, force_registration=True)\n        self.config.register_global(custom={}, tenorkey=None)\n        self.config.register_guild(custom={})\n        self.try_after = None\n    async def initialize(self, bot):\n        key = await self.config.tenorkey()\n        if not key:\n            return\n        await bot.set_shared_api_tokens(\"tenor\", api_key=key)\n        await self.config.tenorkey.clear()\n    async def act(self, ctx, *, target: Union[discord.Member, str] = None):\n        \"\"\"\n        Acts on the specified user.\n        \"\"\"\n        if not target or isinstance(target, str):\n            return\n        try:\n            if not ctx.guild:\n                raise KeyError()\n            message = await self.config.guild(ctx.guild).get_raw(\"custom\", ctx.invoked_with)\n        except KeyError:\n            try:\n                message = await self.config.get_raw(\"custom\", ctx.invoked_with)\n            except KeyError:\n                message = NotImplemented\n        if message is None:\n            return\n        elif message is NotImplemented:\n            action = inflection.humanize(ctx.invoked_with).split()\n            iverb = -1\n            for cycle in range(2):\n                if iverb > -1:\n                    break\n                for i, act in enumerate(action):\n                    act = act.lower()\n                    if (\n                        act in NOLY_ADV\n                        or act in CONJ\n                        or (act.endswith(\"ly\") and act not in LY_VERBS)\n                        or (not cycle and act in SOFT_VERBS)\n                    ):\n                        continue\n                    action[i] = inflection.pluralize(action[i])\n                    iverb = max(iverb, i)\n            if iverb < 0:\n                return\n            action.insert(iverb + 1, target.mention)\n            message = italics(\" \".join(action))\n        else:\n            message = message.format(target, user=target)\n        if self.try_after and ctx.message.created_at < self.try_after:\n            return await ctx.send(message)\n        if not await ctx.embed_requested():\n            return await ctx.send(message)\n        key = (await ctx.bot.get_shared_api_tokens(\"tenor\")).get(\"api_key\")\n        if not key:\n            return await ctx.send(message)\n        async with aiohttp.request(\n            \"GET\",\n            \"https://api.tenor.com/v1/search\",\n            params={\n                \"q\": ctx.invoked_with,\n                \"key\": key,\n                \"anon_id\": str(ctx.author.id ^ ctx.me.id),\n                \"media_filter\": \"minimal\",\n                \"contentfilter\": \"off\" if getattr(ctx.channel, \"nsfw\", False) else \"low\",\n                \"ar_range\": \"wide\",\n                \"limit\": \"8\",\n                \"locale\": get_locale(),\n            },\n        ) as response:\n            json: dict\n            if response.status == 429:\n                self.try_after = ctx.message.created_at + 30\n                json = {}\n            elif response.status >= 400:\n                json = {}\n            else:\n                json = await response.json()\n        if not json.get(\"results\"):\n            return await ctx.send(message)\n        message = f\"{message}\\n\\n{random.choice(json['results'])['itemurl']}\"\n        await ctx.send(\n            message,\n            allowed_mentions=discord.AllowedMentions(\n                users=False if target in ctx.message.mentions else [target]\n            ),\n        )\n    async def actset(self, ctx):\n        \"\"\"\n        Configure various settings for the act cog.\n        \"\"\"\n    async def customize(self, ctx, command: str.lower, *, response: str = None):\n        \"\"\"\n        Customize the response to an action.\n        You can use {0} or {user} to dynamically replace with the specified target of the action.\n        Formats like {0.name} or {0.mention} can also be used.\n        \"\"\"\n        if not response:\n            await self.config.guild(ctx.guild).clear_raw(\"custom\", command)\n        else:\n            await self.config.guild(ctx.guild).set_raw(\"custom\", command, value=response)\n        await ctx.tick()\n    async def customize_global(self, ctx, command: str.lower, *, response: str = None):\n        \"\"\"\n        Globally customize the response to an action.\n        You can use {0} or {user} to dynamically replace with the specified target of the action.\n        Formats like {0.name} or {0.mention} can also be used.\n        \"\"\"\n        if not response:\n            await self.config.clear_raw(\"custom\", command)\n        else:\n            await self.config.set_raw(\"custom\", command, value=response)\n        await ctx.tick()\n    async def ignore(self, ctx, command: str.lower):\n        \"\"\"\n        Ignore or unignore the specified action.\n        The bot will no longer respond to these actions.\n        \"\"\"\n        try:\n            custom = await self.config.guild(ctx.guild).get_raw(\"custom\", command)\n        except KeyError:\n            custom = NotImplemented\n        if custom is None:\n            await self.config.guild(ctx.guild).clear_raw(\"custom\", command)\n            await ctx.send(\"I will no longer ignore the {command} action\".format(command=command))\n        else:\n            await self.config.guild(ctx.guild).set_raw(\"custom\", command, value=None)\n            await ctx.send(\"I will now ignore the {command} action\".format(command=command))\n    async def ignore_global(self, ctx, command: str.lower):\n        \"\"\"\n        Globally ignore or unignore the specified action.\n        The bot will no longer respond to these actions.\n        \"\"\"\n        try:\n            await self.config.get_raw(\"custom\", command)\n        except KeyError:\n            await self.config.set_raw(\"custom\", command, value=None)\n        else:\n            await self.config.clear_raw(\"custom\", command)\n        await ctx.tick()\n    async def tenorkey(self, ctx):\n        \"\"\"\n        Sets a Tenor GIF API key to enable reaction gifs with act commands.\n        You can obtain a key from here: https://tenor.com/developer/dashboard\n        \"\"\"\n        instructions = [\n            \"Go to the Tenor developer dashboard: https://tenor.com/developer/dashboard\",\n            \"Log in or sign up if you haven't already.\",\n            \"Click `+ Create new app` and fill out the form.\",\n            \"Copy the key from the app you just created.\",\n            \"Give the key to Red with this command:\\n\"\n            f\"`{ctx.prefix}set api tenor api_key your_api_key`\\n\"\n            \"Replace `your_api_key` with the key you just got.\\n\"\n            \"Everything else should be the same.\",\n        ]\n        instructions = [f\"**{i}.** {v}\" for i, v in enumerate(instructions, 1)]\n        await ctx.maybe_send_embed(\"\\n\".join(instructions))\n    async def on_message(self, message):\n        if message.author.bot:\n            return\n        ctx = await self.bot.get_context(message)\n        if ctx.prefix is None or not ctx.invoked_with.replace(\"_\", \"\").isalpha():\n            return\n        if ctx.valid and ctx.command.enabled:\n            try:\n                if await ctx.command.can_run(ctx):\n                    return\n            except commands.errors.CheckFailure:\n                return\n        ctx.command = self.act\n        await self.bot.invoke(ctx)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2020-15172",
        "description": "[{'lang': 'en', 'value': 'The Act module for Red Discord Bot before commit 6b9f3b86 is vulnerable to Remote Code Execution. With this exploit, Discord users can use specially crafted messages to perform destructive actions and/or access sensitive information. Unloading the Act module with `unload act` can render this exploit inaccessible.'}]",
        "cwe_number": 502
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-60",
      "code": "def is_local_uri(uri, is_tracking_or_registry_uri=True):\n    \"\"\"\n    Returns true if the specified URI is a local file path (/foo or file:/foo).\n    :param uri: The URI.\n    :param is_tracking_uri: Whether or not the specified URI is an MLflow Tracking or MLflow\n                            Model Registry URI. Examples of other URIs are MLflow artifact URIs,\n                            filesystem paths, etc.\n    \"\"\"\n    if uri == \"databricks\" and is_tracking_or_registry_uri:\n        return False\n    if is_windows() and uri.startswith(\"\\\\\\\\\"):\n        return False\n    parsed_uri = urllib.parse.urlparse(uri)\n    scheme = parsed_uri.scheme\n    if scheme == \"\":\n        return True\n    if parsed_uri.hostname and not (\n        parsed_uri.hostname == \".\"\n        or parsed_uri.hostname.startswith(\"localhost\")\n        or parsed_uri.hostname.startswith(\"127.0.0.1\")\n    ):\n        return False\n    if scheme == \"file\":\n        return True\n    if is_windows() and len(scheme) == 1 and scheme.lower() == pathlib.Path(uri).drive.lower()[0]:\n        return True\n    return False\ndef is_file_uri(uri):\n    return urllib.parse.urlparse(uri).scheme == \"file\"\ndef is_http_uri(uri):\n    scheme = urllib.parse.urlparse(uri).scheme\n    return scheme == \"http\" or scheme == \"https\"\ndef is_databricks_uri(uri):\n    \"\"\"\n    Databricks URIs look like 'databricks' (default profile) or 'databricks://profile'\n    or 'databricks://secret_scope:secret_key_prefix'.\n    \"\"\"\n    scheme = urllib.parse.urlparse(uri).scheme\n    return scheme == \"databricks\" or uri == \"databricks\"",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-3573",
        "description": "[{'lang': 'en', 'value': \"mlflow/mlflow is vulnerable to Local File Inclusion (LFI) due to improper parsing of URIs, allowing attackers to bypass checks and read arbitrary files on the system. The issue arises from the 'is_local_uri' function's failure to properly handle URIs with empty or 'file' schemes, leading to the misclassification of URIs as non-local. Attackers can exploit this by crafting malicious model versions with specially crafted 'source' parameters, enabling the reading of sensitive files within at least two directory levels from the server's root.\"}]",
        "cwe_number": 29
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-61",
      "code": "    def create_option(self, name, value, label, selected, index, subindex=None, attrs=None):\n        result = super(TagFormWidget, self).create_option(\n            name=name, value=value, label=label, selected=selected,\n            index=index, subindex=subindex, attrs=attrs\n        )\n        result['attrs']['data-color'] = self.queryset.get(pk=value).color\n        return result",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2018-16407",
        "description": "[{'lang': 'en', 'value': 'An issue was discovered in Mayan EDMS before 3.0.3. The Tags app has XSS because tag label values are mishandled.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-62",
      "code": "    def get_csv_incident_list(self) -> str:\n        csvOutput = io.StringIO()\n        writer = csv.writer(csvOutput)\n        if len(self.triggered_details.hits) > 0:\n            hit_class_dict = dict(self.triggered_details.hits[0])\n            headers = [\n                (i)\n                for i in hit_class_dict.keys()\n                if i not in [\"token_type\", \"time_of_hit\"]\n            ]\n            writer.writerow([\"Timestamp\"] + headers)\n            for hit in self.triggered_details.hits:\n                timestamp = hit.time_of_hit\n                hit_id = datetime.fromtimestamp(timestamp).strftime(\n                    \"%Y-%m-%d %H:%M:%S.%f\"\n                )\n                hit_dict = dict(hit)\n                data = [hit_id]\n                for key in headers:\n                    data.append(hit_dict.get(key, \"N/A\"))\n                writer.writerow(data)\n        else:\n            writer.writerow(\"the token has not been triggered\")\n        return csvOutput.getvalue()\n    def _do_ns_response(self, name=None):\n        \"\"\"\n        Calculate the response to a query.\n        \"\"\"\n        answer = dns.RRHeader(\n            name=name,\n            payload=dns.Record_NS(\n                ttl=300,\n                name=\".\".join([\"ns1\", name.decode()]),\n            ),\n            type=dns.NS,\n            auth=True,\n            ttl=300\n        )\n        additional = dns.RRHeader(\n            name=\".\".join([\"ns1\", name.decode()]),\n            payload=dns.Record_A(ttl=10, address=self.frontend_settings.PUBLIC_IP),\n            type=dns.A,\n            auth=True,\n            ttl=300,\n        )\n        answers = [answer]\n        authority: list[str] = []\n        additional = [additional]\n        return answers, authority, additional",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-28111",
        "description": "[{'lang': 'en', 'value': \"Canarytokens helps track activity and actions on a network. Canarytokens.org supports exporting the history of a Canarytoken's incidents in CSV format. The generation of these CSV files is vulnerable to a CSV Injection vulnerability. This flaw can be used by an attacker who discovers an HTTP-based Canarytoken to target the Canarytoken's owner, if the owner exports the incident history to CSV and opens in a reader application such as Microsoft Excel. The impact is that this issue could lead to code execution on the machine on which the CSV file is opened. Version sha-c595a1f8 contains a fix for this issue.\"}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-63",
      "code": "    def _expand(self, key_material):\n        output = [b\"\"]\n        counter = 1\n        while (self._algorithm.digest_size // 8) * len(output) < self._length:\n            h = hmac.HMAC(key_material, self._algorithm, backend=self._backend)\n            h.update(output[-1])\n            h.update(self._info)\n            h.update(six.int2byte(counter))\n            output.append(h.finalize())\n            counter += 1\n        return b\"\".join(output)[:self._length]",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2016-9243",
        "description": "[{'lang': 'en', 'value': 'HKDF in cryptography before 1.5.2 returns an empty byte-string if used with a length less than algorithm.digest_size.'}]",
        "cwe_number": 20
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-64",
      "code": "def send_nscript(title, msg, gtype, force=False, test=None):\n    \"\"\"Run user's notification script\"\"\"\n    if test:\n        script = test.get(\"nscript_script\")\n        nscript_parameters = test.get(\"nscript_parameters\")\n    else:\n        script = sabnzbd.cfg.nscript_script()\n        nscript_parameters = sabnzbd.cfg.nscript_parameters()\n    nscript_parameters = nscript_parameters.split()\n    if not script:\n        return T(\"Cannot send, missing required data\")\n    title = \"SABnzbd: \" + T(NOTIFICATION.get(gtype, \"other\"))\n    if force or check_classes(gtype, \"nscript\"):\n        script_path = make_script_path(script)\n        if script_path:\n            ret = -1\n            output = None\n            try:\n                p = build_and_run_command([script_path, gtype, title, msg] + nscript_parameters, env=create_env())\n                output = p.stdout.read()\n                ret = p.wait()\n            except:\n                logging.info(\"Failed script %s, Traceback: \", script, exc_info=True)\n            if ret:\n                logging.error(T('Script returned exit code %s and output \"%s\"'), ret, output)\n                return T('Script returned exit code %s and output \"%s\"') % (ret, output)\n            else:\n                logging.info(\"Successfully executed notification script %s\", script_path)\n        else:\n            return T('Notification script \"%s\" does not exist') % script_path\n    return \"\"",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-34237",
        "description": "[{'lang': 'en', 'value': 'SABnzbd is an open source automated Usenet download tool. A design flaw was discovered in SABnzbd that could allow remote code execution. Manipulating the Parameters setting in the Notification Script functionality allows code execution with the privileges of the SABnzbd process. Exploiting the vulnerabilities requires access to the web interface. Remote exploitation is possible if users[exposed their setup to the internet or other untrusted networks without setting a username/password. By default SABnzbd is only accessible from `localhost`, with no authentication required for the web interface. This issue has been patched in commits `e3a722` and `422b4f` which have been included in the 4.0.2 release. Users are advised to upgrade. Users unable to upgrade should ensure that a username and password have been set if their instance is web accessible.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-65",
      "code": "def change_password_for_user(username):\n    if not userManager.enabled:\n        return jsonify(SUCCESS)\n    if (\n        current_user is not None\n        and not current_user.is_anonymous\n        and (current_user.get_name() == username or current_user.is_admin)\n    ):\n        if \"application/json\" not in request.headers[\"Content-Type\"]:\n            abort(400, description=\"Expected content-type JSON\")\n        data = request.get_json()\n        if data is None:\n            abort(400, description=\"Malformed JSON body in request\")\n        if \"password\" not in data or not data[\"password\"]:\n            abort(400, description=\"password is missing\")\n        try:\n            userManager.change_user_password(username, data[\"password\"])\n        except users.UnknownUser:\n            abort(404)\n        return jsonify(SUCCESS)\n    else:\n        abort(403)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-2930",
        "description": "[{'lang': 'en', 'value': 'Unverified Password Change in GitHub repository octoprint/octoprint prior to 1.8.3.'}]",
        "cwe_number": 620
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-66",
      "code": "async def largest_content_len(urls: list[str]) -> tuple[str, int]:\n    largest_url = \"\"\n    largest_len = 0\n    async def do(client: AsyncClient, url: str) -> Response:\n        return await client.head(url, headers={\"User-Agent\": _FIREFOX_UA})\n    async with AsyncClient() as client:\n        tasks = [do(client, url) for url in urls]\n        responses: list[Response] = await gather_with_concurrency(10, *tasks, ignore_exceptions=True)\n        for response in responses:\n            len_int = int(response.headers.get(\"Content-Length\", 0))\n            if len_int > largest_len:\n                largest_url = str(response.url)\n                largest_len = len_int\n    return largest_url, largest_len\nclass NotAnImageError(Exception):\n    def _validate_image_url(url: str) -> bool:\n        \"\"\"\n        Validates that the URL is of an allowed source and restricts certain sources to prevent\n        malicious images from being downloaded.\n        \"\"\"\n        invalid_domains = {\"127.0.0.1\", \"localhost\"}\n        for domain in invalid_domains:\n            if domain in url:\n                return False\n        return True\n    async def scrape_image(self, image_url) -> None:\n        self.logger.info(f\"Image URL: {image_url}\")\n        if not self._validate_image_url(image_url):\n            self.logger.error(f\"Invalid image URL: {image_url}\")\n            raise InvalidDomainError(f\"Invalid domain: {image_url}\")\n        if isinstance(image_url, str):\n            pass\n        elif isinstance(image_url, list):\n            image_url, _ = await largest_content_len(image_url)\n        elif isinstance(image_url, dict):\n            for key in image_url:\n                if key == \"url\":\n                    image_url = image_url.get(\"url\")\n        ext = image_url.split(\".\")[-1]\n        if ext not in img.IMAGE_EXTENSIONS:\n            ext = \"jpg\"\n        file_name = f\"{str(self.recipe_id)}.{ext}\"\n        file_path = Recipe.directory_from_id(self.recipe_id).joinpath(\"images\", file_name)\n        async with AsyncClient() as client:\n            try:\n                r = await client.get(image_url, headers={\"User-Agent\": _FIREFOX_UA})\n            except Exception:\n                self.logger.exception(\"Fatal Image Request Exception\")\n                return None\n            if r.status_code != 200:\n                return None\n            content_type = r.headers.get(\"content-type\", \"\")\n            if \"image\" not in content_type:\n                self.logger.error(f\"Content-Type: {content_type} is not an image\")\n                raise NotAnImageError(f\"Content-Type {content_type} is not an image\")\n            self.logger.debug(f\"File Name Suffix {file_path.suffix}\")\n            self.write_image(r.read(), file_path.suffix)\n            file_path.unlink(missing_ok=True)\nasync def safe_scrape_html(url: str) -> str:\n    \"\"\"\n    Scrapes the html from a url but will cancel the request\n    if the request takes longer than 15 seconds. This is used to mitigate\n    DDOS attacks from users providing a url with arbitrary large content.\n    \"\"\"\n    async with AsyncClient() as client:\n        html_bytes = b\"\"\n        async with client.stream(\"GET\", url, timeout=SCRAPER_TIMEOUT, headers={\"User-Agent\": _FIREFOX_UA}) as resp:\n            start_time = time.time()\n            async for chunk in resp.aiter_bytes(chunk_size=1024):\n                html_bytes += chunk\n                if time.time() - start_time > SCRAPER_TIMEOUT:\n                    raise ForceTimeoutException()\n        content = None\n        encoding = resp.encoding\n        if not html_bytes:\n            return \"\"\n        if encoding is None:\n            encoding = resp.apparent_encoding\n        try:\n            content = str(html_bytes, encoding, errors=\"replace\")\n        except (LookupError, TypeError):\n            content = str(html_bytes, errors=\"replace\")\n        return content\nasync def create_from_url(url: str, translator: Translator) -> tuple[Recipe, ScrapedExtras | None]:\n    \"\"\"Main entry point for generating a recipe from a URL. Pass in a URL and\n    a Recipe object will be returned if successful.\n    Args:\n        url (str): a valid string representing a URL\n    Returns:\n        Recipe: Recipe Object\n    \"\"\"\n    scraper = RecipeScraper(translator)\n    new_recipe, extras = await scraper.scrape(url)\n    if not new_recipe:\n        raise HTTPException(status.HTTP_400_BAD_REQUEST, {\"details\": ParserErrors.BAD_RECIPE_DATA.value})\n    new_recipe.id = uuid4()\n    logger = get_logger()\n    logger.debug(f\"Image {new_recipe.image}\")\n    recipe_data_service = RecipeDataService(new_recipe.id)\n    try:\n        await recipe_data_service.scrape_image(new_recipe.image)\n        if new_recipe.name is None:\n            new_recipe.name = \"Untitled\"\n        new_recipe.slug = slugify(new_recipe.name)\n        new_recipe.image = cache.new_key(4)\n    except Exception as e:\n        recipe_data_service.logger.exception(f\"Error Scraping Image: {e}\")\n        new_recipe.image = \"no image\"\n    if new_recipe.name is None or new_recipe.name == \"\":\n        new_recipe.name = f\"No Recipe Name Found - {str(uuid4())}\"\n        new_recipe.slug = slugify(new_recipe.name)\n    return new_recipe, extras",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-31991",
        "description": "[{'lang': 'en', 'value': 'Mealie is a self hosted recipe manager and meal planner. Prior to 1.4.0, the safe_scrape_html function utilizes a user-controlled URL to issue a request to a remote server. Based on the content of the response, it will either parse the content or disregard it. This function, nor those that call it, add any restrictions on the URL that can be provided, nor is it restricted to being an FQDN (i.e., an IP address can be provided). As this function\u2019s return will be handled differently by its caller depending on the response, it is possible for an attacker to use this functionality to positively identify HTTP(s) servers on the local network with any IP/port combination. This issue can result in any authenticated user being able to map HTTP servers on a local network that the Mealie service has access to. Note that by default any user can create an account on a Mealie server, and that the default changeme@example.com user is available with its hard-coded password. This vulnerability is fixed in 1.4.0.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-67",
      "code": "def add_events(sio:socketio):\n    @sio.on('generate_msg')\n    def handle_generate_msg(sid, data):\n        client_id = sid\n        lollmsElfServer.cancel_gen = False\n        client = lollmsElfServer.session.get_client(client_id)\n        client.generated_text=\"\"\n        client.cancel_generation=False\n        client.continuing=False\n        client.first_chunk=True\n        if not lollmsElfServer.model:\n            ASCIIColors.error(\"Model not selected. Please select a model\")\n            lollmsElfServer.error(\"Model not selected. Please select a model\", client_id=client_id)\n            return\n        if not lollmsElfServer.busy:\n            if lollmsElfServer.session.get_client(client_id).discussion is None:\n                if lollmsElfServer.db.does_last_discussion_have_messages():\n                    lollmsElfServer.session.get_client(client_id).discussion = lollmsElfServer.db.create_discussion()\n                else:\n                    lollmsElfServer.session.get_client(client_id).discussion = lollmsElfServer.db.load_last_discussion()\n            prompt = data[\"prompt\"]\n            ump = lollmsElfServer.config.discussion_prompt_separator +lollmsElfServer.config.user_name.strip() if lollmsElfServer.config.use_user_name_in_discussions else lollmsElfServer.personality.user_message_prefix\n            message = lollmsElfServer.session.get_client(client_id).discussion.add_message(\n                message_type    = MSG_TYPE.MSG_TYPE_FULL.value,\n                sender_type     = SENDER_TYPES.SENDER_TYPES_USER.value,\n                sender          = ump.replace(lollmsElfServer.config.discussion_prompt_separator,\"\").replace(\":\",\"\"),\n                content=prompt,\n                metadata=None,\n                parent_message_id=lollmsElfServer.message_id\n            )\n            ASCIIColors.green(\"Starting message generation by \"+lollmsElfServer.personality.name)\n            client.generation_thread = threading.Thread(target=lollmsElfServer.start_message_generation, args=(message, message.id, client_id))\n            client.generation_thread.start()\n            ASCIIColors.info(\"Started generation task\")\n            lollmsElfServer.busy=True\n        else:\n            lollmsElfServer.error(\"I am busy. Come back later.\", client_id=client_id)\n    @sio.on('generate_msg_with_internet')\n    def generate_msg_with_internet(sid, data):\n        client_id = sid\n        lollmsElfServer.cancel_gen = False\n        client = lollmsElfServer.session.get_client(client_id)\n        client.generated_text=\"\"\n        client.cancel_generation=False\n        client.continuing=False\n        client.first_chunk=True\n        if not lollmsElfServer.model:\n            ASCIIColors.error(\"Model not selected. Please select a model\")\n            lollmsElfServer.error(\"Model not selected. Please select a model\", client_id=client_id)\n            return\n        if not lollmsElfServer.busy:\n            if lollmsElfServer.session.get_client(client_id).discussion is None:\n                if lollmsElfServer.db.does_last_discussion_have_messages():\n                    lollmsElfServer.session.get_client(client_id).discussion = lollmsElfServer.db.create_discussion()\n                else:\n                    lollmsElfServer.session.get_client(client_id).discussion = lollmsElfServer.db.load_last_discussion()\n            prompt = data[\"prompt\"]\n            ump = lollmsElfServer.config.discussion_prompt_separator +lollmsElfServer.config.user_name.strip() if lollmsElfServer.config.use_user_name_in_discussions else lollmsElfServer.personality.user_message_prefix\n            message = lollmsElfServer.session.get_client(client_id).discussion.add_message(\n                message_type    = MSG_TYPE.MSG_TYPE_FULL.value,\n                sender_type     = SENDER_TYPES.SENDER_TYPES_USER.value,\n                sender          = ump.replace(lollmsElfServer.config.discussion_prompt_separator,\"\").replace(\":\",\"\"),\n                content=prompt,\n                metadata=None,\n                parent_message_id=lollmsElfServer.message_id\n            )\n            ASCIIColors.green(\"Starting message generation by \"+lollmsElfServer.personality.name)\n            client.generation_thread = threading.Thread(target=lollmsElfServer.start_message_generation, args=(message, message.id, client_id, False, None, True))\n            client.generation_thread.start()\n            ASCIIColors.info(\"Started generation task\")\n            lollmsElfServer.busy=True\n        else:\n            lollmsElfServer.error(\"I am busy. Come back later.\", client_id=client_id)\n    @sio.on('generate_msg_from')\n    def handle_generate_msg_from(sid, data):\n        client_id = sid\n        client = lollmsElfServer.session.get_client(client_id)\n        lollmsElfServer.cancel_gen = False\n        client.continuing=False\n        client.first_chunk=True\n        if lollmsElfServer.session.get_client(client_id).discussion is None:\n            ASCIIColors.warning(\"Please select a discussion\")\n            lollmsElfServer.error(\"Please select a discussion first\", client_id=client_id)\n            return\n        id_ = data['id']\n        generation_type = data.get('msg_type',None)\n        if id_==-1:\n            message = lollmsElfServer.session.get_client(client_id).discussion.current_message\n        else:\n            message = lollmsElfServer.session.get_client(client_id).discussion.load_message(id_)\n        if message is None:\n            return\n        client.generation_thread = threading.Thread(target=lollmsElfServer.start_message_generation, args=(message, message.id, client_id, False, generation_type))\n        client.generation_thread.start()\n    @sio.on('continue_generate_msg_from')\n    def handle_continue_generate_msg_from(sid, data):\n        client_id = sid\n        client = lollmsElfServer.session.get_client(client_id)\n        lollmsElfServer.cancel_gen = False\n        client.continuing=True\n        client.first_chunk=True\n        if lollmsElfServer.session.get_client(client_id).discussion is None:\n            ASCIIColors.yellow(\"Please select a discussion\")\n            lollmsElfServer.error(\"Please select a discussion\", client_id=client_id)\n            return\n        id_ = data['id']\n        if id_==-1:\n            message = lollmsElfServer.session.get_client(client_id).discussion.current_message\n        else:\n            message = lollmsElfServer.session.get_client(client_id).discussion.load_message(id_)\n        client.generated_text=message.content\n        client.generation_thread = threading.Thread(target=lollmsElfServer.start_message_generation, args=(message, message.id, client_id, True))\n        client.generation_thread.start()\ndef add_events(sio:socketio):\n    @sio.on('create_empty_message')\n    def create_empty_message(sid, data):\n        client_id = sid\n        type = int(data.get(\"type\",0))\n        message = data.get(\"message\",\"\")\n        if type==0:\n            ASCIIColors.info(f\"Building empty User message requested by : {client_id}\")\n            print(f\"Creating an empty message for AI answer orientation\")\n            if lollmsElfServer.session.get_client(client_id).discussion:\n                if not lollmsElfServer.model:\n                    lollmsElfServer.error(\"No model selected. Please make sure you select a model before starting generation\", client_id = client_id)\n                    return\n                lollmsElfServer.new_message(client_id, lollmsElfServer.config.user_name, message, sender_type=SENDER_TYPES.SENDER_TYPES_USER, open=True)\n        else:\n            if lollmsElfServer.personality is None:\n                lollmsElfServer.warning(\"Select a personality\")\n                return\n            ASCIIColors.info(f\"Building empty AI message requested by : {client_id}\")\n            print(f\"Creating an empty message for AI answer orientation\")\n            if lollmsElfServer.session.get_client(client_id).discussion:\n                if not lollmsElfServer.model:\n                    lollmsElfServer.error(\"No model selected. Please make sure you select a model before starting generation\", client_id=client_id)\n                    return\n                lollmsElfServer.new_message(client_id, lollmsElfServer.personality.name, \"[edit this to put your ai answer start]\", open=True)\n    @sio.on('add_webpage')\n    def add_webpage(sid, data):\n        lollmsElfServer.ShowBlockingMessage(\"Scraping web page\\nPlease wait...\")\n        ASCIIColors.yellow(\"Scaping web page\")\n        client = lollmsElfServer.session.get_client(sid)\n        url = data['url']\n        index =  find_first_available_file_index(lollmsElfServer.lollms_paths.personal_uploads_path,\"web_\",\".txt\")\n        file_path=lollmsElfServer.lollms_paths.personal_uploads_path/f\"web_{index}.txt\"\n        scrape_and_save(url=url, file_path=file_path)\n        try:\n            if not lollmsElfServer.personality.processor is None:\n                lollmsElfServer.personality.processor.add_file(file_path, client, partial(lollmsElfServer.process_chunk, client_id = sid))\n                run_async(partial(sio.emit,'web_page_added', {'status':True,}))\n            else:\n                lollmsElfServer.personality.add_file(file_path, client, partial(lollmsElfServer.process_chunk, client_id = sid))\n                run_async(partial(sio.emit,'web_page_added', {'status':True}))\n            lollmsElfServer.HideBlockingMessage()\n        except Exception as e:\n            run_async(partial(sio.emit,'web_page_added', {'status':False}))\n            lollmsElfServer.HideBlockingMessage()\n    @sio.on('take_picture')\n    def take_picture(sid):\n        try:\n            client = lollmsElfServer.session.get_client(sid)\n            lollmsElfServer.info(\"Loading camera\")\n            if not PackageManager.check_package_installed(\"cv2\"):\n                PackageManager.install_package(\"opencv-python\")\n            import cv2\n            cap = cv2.VideoCapture(0)\n            n = time.time()\n            lollmsElfServer.info(\"Stand by for taking a shot in 2s\")\n            while(time.time()-n<2):\n                _, frame = cap.read()\n            _, frame = cap.read()\n            cap.release()\n            lollmsElfServer.info(\"Shot taken\")\n            cam_shot_path = client.discussion.discussion_images_folder\n            cam_shot_path.mkdir(parents=True, exist_ok=True)\n            filename = find_first_available_file_index(cam_shot_path, \"cam_shot_\", extension=\".png\")\n            save_path = cam_shot_path/f\"cam_shot_{filename}.png\"\n            try:\n                cv2.imwrite(str(save_path), frame)\n                if not lollmsElfServer.personality.processor is None:\n                    lollmsElfServer.info(\"Sending file to scripted persona\")\n                    lollmsElfServer.personality.processor.add_file(save_path, client, partial(lollmsElfServer.process_chunk, client_id = sid))\n                    run_async(partial(sio.emit,'picture_taken', {'status':True, 'progress': 100}))\n                    lollmsElfServer.info(\"File sent to scripted persona\")\n                else:\n                    lollmsElfServer.info(\"Sending file to persona\")\n                    lollmsElfServer.personality.add_file(save_path, client, partial(lollmsElfServer.process_chunk, client_id = sid))\n                    run_async(partial(sio.emit,'picture_taken', {'status':True, 'progress': 100}))\n                    lollmsElfServer.info(\"File sent to persona\")\n            except Exception as e:\n                trace_exception(e)\n                run_async(partial(sio.emit,'picture_taken', {'status':False, 'error': str(e)}))\n        except Exception as ex:\n            trace_exception(ex)\n            lollmsElfServer.error(\"Couldn't use the webcam\")\ndef add_events(sio:socketio):\n        @sio.on('new_discussion')\n        async def new_discussion(sid, data):\n            if lollmsElfServer.personality is None:\n                lollmsElfServer.error(\"Please select a personality first\")\n                return\n            ASCIIColors.yellow(\"New descussion requested\")\n            client_id = sid\n            title = data[\"title\"]\n            lollmsElfServer.session.get_client(client_id).discussion = lollmsElfServer.db.create_discussion(title)\n            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n            if lollmsElfServer.session.get_client(client_id).discussion is None:\n                lollmsElfServer.session.get_client(client_id).discussion = lollmsElfServer.db.load_last_discussion()\n            if lollmsElfServer.personality.welcome_message!=\"\":\n                if lollmsElfServer.personality.welcome_audio_path.exists():\n                    for voice in lollmsElfServer.personality.welcome_audio_path.iterdir():\n                        if voice.suffix.lower() in [\".wav\",\".mp3\"]:\n                                try:\n                                    if not PackageManager.check_package_installed(\"pygame\"):\n                                        PackageManager.install_package(\"pygame\")\n                                    import pygame\n                                    pygame.mixer.init()\n                                    pygame.mixer.music.load(voice)\n                                    pygame.mixer.music.play()\n                                except Exception as ex:\n                                    pass\n                if lollmsElfServer.config.force_output_language_to_be and lollmsElfServer.config.force_output_language_to_be.lower().strip() !=\"english\":\n                    welcome_message = lollmsElfServer.personality.fast_gen(f\"!@>instruction: Translate the following text to {lollmsElfServer.config.force_output_language_to_be.lower()}:\\n{lollmsElfServer.personality.welcome_message}\\n!@>translation:\")\n                else:\n                    welcome_message = lollmsElfServer.personality.welcome_message\n                message = lollmsElfServer.session.get_client(client_id).discussion.add_message(\n                    message_type        = MSG_TYPE.MSG_TYPE_FULL.value if lollmsElfServer.personality.include_welcome_message_in_disucssion else MSG_TYPE.MSG_TYPE_FULL_INVISIBLE_TO_AI.value,\n                    sender_type         = SENDER_TYPES.SENDER_TYPES_AI.value,\n                    sender              = lollmsElfServer.personality.name,\n                    content             = welcome_message,\n                    metadata            = None,\n                    rank                = 0,\n                    parent_message_id   = -1,\n                    binding             = lollmsElfServer.config.binding_name,\n                    model               = lollmsElfServer.config.model_name,\n                    personality         = lollmsElfServer.config.personalities[lollmsElfServer.config.active_personality_id],\n                    created_at=None,\n                    finished_generating_at=None\n                )\n                await lollmsElfServer.sio.emit('discussion_created',\n                            {'id':lollmsElfServer.session.get_client(client_id).discussion.discussion_id},\n                            to=client_id\n                )\n            else:\n                await lollmsElfServer.sio.emit('discussion_created',\n                            {'id':0},\n                            to=client_id\n                )\n        @sio.on('load_discussion')\n        async def load_discussion(sid, data):\n            client_id = sid\n            ASCIIColors.yellow(f\"Loading discussion for client {client_id} ... \", end=\"\")\n            if \"id\" in data:\n                discussion_id = data[\"id\"]\n                lollmsElfServer.session.get_client(client_id).discussion = Discussion(discussion_id, lollmsElfServer.db)\n            else:\n                if lollmsElfServer.session.get_client(client_id).discussion is not None:\n                    discussion_id = lollmsElfServer.session.get_client(client_id).discussion.discussion_id\n                    lollmsElfServer.session.get_client(client_id).discussion = Discussion(discussion_id, lollmsElfServer.db)\n                else:\n                    lollmsElfServer.session.get_client(client_id).discussion = lollmsElfServer.db.create_discussion()\n            messages = lollmsElfServer.session.get_client(client_id).discussion.get_messages()\n            jsons = [m.to_json() for m in messages]\n            await lollmsElfServer.sio.emit('discussion',\n                        jsons,\n                        to=client_id\n            )\n            ASCIIColors.green(f\"ok\")\nFILE_PATH_REGEX = r'^[a-zA-Z0-9_\\-\\\\\\/]+$'\ndef validate_file_path(path):\n    return re.match(FILE_PATH_REGEX, path)\nasync def execute_code(request: CodeRequest):\n    \"\"\"\n    Executes Python code and returns the output.\n    :param request: The HTTP request object.\n    :return: A JSON response with the status of the operation.\n    \"\"\"\n    client = lollmsElfServer.session.get_client(request.client_id)\n    if lollmsElfServer.config.headless_server_mode:\n        return {\"status\":False,\"error\":\"Code execution is blocked when in headless mode for obvious security reasons!\"}\n    if lollmsElfServer.config.host!=\"localhost\" and lollmsElfServer.config.host!=\"127.0.0.1\":\n        return {\"status\":False,\"error\":\"Code execution is blocked when the server is exposed outside for very obvious reasons!\"}\n    if not lollmsElfServer.config.turn_on_code_execution:\n        return {\"status\":False,\"error\":\"Code execution is blocked by the configuration!\"}\n    if lollmsElfServer.config.turn_on_code_validation:\n        if not show_yes_no_dialog(\"Validation\",\"Do you validate the execution of the code?\"):\n            return {\"status\":False,\"error\":\"User refused the execution!\"}\n    try:\n        code = request.code\n        discussion_id = request.discussion_id\n        message_id = request.message_id\n        language = request.language\n        if language==\"python\":\n            ASCIIColors.info(\"Executing python code:\")\n            ASCIIColors.yellow(code)\n            return execute_python(code, client, message_id)\n        if language==\"javascript\":\n            ASCIIColors.info(\"Executing javascript code:\")\n            ASCIIColors.yellow(code)\n            return execute_javascript(code)\n        if language in [\"html\",\"html5\",\"svg\"]:\n            ASCIIColors.info(\"Executing javascript code:\")\n            ASCIIColors.yellow(code)\n            return execute_html(code)\n        elif language==\"latex\":\n            ASCIIColors.info(\"Executing latex code:\")\n            ASCIIColors.yellow(code)\n            return execute_latex(code, client, message_id)\n        elif language in [\"bash\",\"shell\",\"cmd\",\"powershell\"]:\n            ASCIIColors.info(\"Executing shell code:\")\n            ASCIIColors.yellow(code)\n            return execute_bash(code, client)\n        elif language in [\"mermaid\"]:\n            ASCIIColors.info(\"Executing mermaid code:\")\n            ASCIIColors.yellow(code)\n            return execute_mermaid(code)\n        elif language in [\"graphviz\",\"dot\"]:\n            ASCIIColors.info(\"Executing graphviz code:\")\n            ASCIIColors.yellow(code)\n            return execute_graphviz(code)\n        return {\"status\": False, \"error\": \"Unsupported language\", \"execution_time\": 0}\n    except Exception as ex:\n        trace_exception(ex)\n        lollmsElfServer.error(ex)\n        return {\"status\":False,\"error\":str(ex)}\ndef add_events(sio:socketio):\n    @sio.on('start_webcam_video_stream')\n    def start_webcam_video_stream(sid):\n        lollmsElfServer.info(\"Starting video capture\")\n        try:\n            from lollms.media import WebcamImageSender\n            lollmsElfServer.webcam = WebcamImageSender(sio,lollmsCom=lollmsElfServer)\n            lollmsElfServer.webcam.start_capture()\n        except:\n            lollmsElfServer.InfoMessage(\"Couldn't load media library.\\nYou will not be able to perform any of the media linked operations. please verify the logs and install any required installations\")\n    @sio.on('stop_webcam_video_stream')\n    def stop_webcam_video_stream(sid):\n        lollmsElfServer.info(\"Stopping video capture\")\n        lollmsElfServer.webcam.stop_capture()\n    @sio.on('start_audio_stream')\n    def start_audio_stream(sid):\n        lollmsElfServer.info(\"Starting audio capture\")\n        try:\n            from lollms.media import AudioRecorder\n            lollmsElfServer.rec_output_folder = lollmsElfServer.lollms_paths.personal_outputs_path/\"audio_rec\"\n            lollmsElfServer.rec_output_folder.mkdir(exist_ok=True, parents=True)\n            lollmsElfServer.summoned = False\n            lollmsElfServer.audio_cap = AudioRecorder(sio,lollmsElfServer.rec_output_folder/\"rt.wav\", callback=lollmsElfServer.audio_callback,lollmsCom=lollmsElfServer)\n            lollmsElfServer.audio_cap.start_recording()\n        except:\n            lollmsElfServer.InfoMessage(\"Couldn't load media library.\\nYou will not be able to perform any of the media linked operations. please verify the logs and install any required installations\")\n    @sio.on('stop_audio_stream')\n    def stop_audio_stream(sid):\n        lollmsElfServer.info(\"Stopping audio capture\")\n        lollmsElfServer.audio_cap.stop_recording()\nasync def add_webpage(request: AddWebPageRequest):\n    client = lollmsElfServer.session.get_client(request.client_id)\n    if client is None:\n        raise HTTPException(status_code=400, detail=\"Unknown client. This service only accepts lollms webui requests\")\n    def do_scraping():\n        lollmsElfServer.ShowBlockingMessage(\"Scraping web page\\nPlease wait...\")\n        ASCIIColors.yellow(\"Scaping web page\")\n        client = lollmsElfServer.session.get_client(request.client_id)\n        url = request.url\n        index =  find_first_available_file_index(lollmsElfServer.lollms_paths.personal_uploads_path,\"web_\",\".txt\")\n        file_path=lollmsElfServer.lollms_paths.personal_uploads_path/f\"web_{index}.txt\"\n        scrape_and_save(url=url, file_path=file_path)\n        try:\n            if not lollmsElfServer.personality.processor is None:\n                lollmsElfServer.personality.processor.add_file(file_path, client, partial(lollmsElfServer.process_chunk, client_id = request.client_id))\n            else:\n                lollmsElfServer.personality.add_file(file_path, client, partial(lollmsElfServer.process_chunk, client_id = request.client_id))\n            lollmsElfServer.HideBlockingMessage()\n            lollmsElfServer.refresh_files()\n        except Exception as e:\n            lollmsElfServer.HideBlockingMessage()\n            lollmsElfServer.refresh_files()\n            return {'status':False,\"error\":str(e)}\n    client.generation_thread = threading.Thread(target=do_scraping)\n    client.generation_thread.start()\n    return {'status':True}\nasync def restart_program():\n    \"\"\"Restart the program.\"\"\"\n    if lollmsElfServer.config.headless_server_mode:\n        return {\"status\":False,\"error\":\"Restarting app is blocked when in headless mode for obvious security reasons!\"}\n    if lollmsElfServer.config.host!=\"localhost\" and lollmsElfServer.config.host!=\"127.0.0.1\":\n        return {\"status\":False,\"error\":\"Restarting app is blocked when the server is exposed outside for very obvious reasons!\"}\n    lollmsElfServer.ShowBlockingMessage(\"Restarting program.\\nPlease stand by...\")\n    run_async(lollmsElfServer.sio.shutdown)\n    time.sleep(1)\n    lollmsElfServer.HideBlockingMessage()\n    ASCIIColors.info(\"\")\n    ASCIIColors.info(\"\")\n    ASCIIColors.info(\"\")\n    ASCIIColors.info(\" \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\")\n    ASCIIColors.info(\" \u2551              Restarting backend                  \u2551\")\n    ASCIIColors.info(\" \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\")\n    ASCIIColors.info(\"\")\n    ASCIIColors.info(\"\")\n    ASCIIColors.info(\"\")\n    lollmsElfServer.run_restart_script(lollmsElfServer.args)\nasync def update_software():\n    \"\"\"Update the software.\"\"\"\n    if lollmsElfServer.config.headless_server_mode:\n        return {\"status\":False,\"error\":\"Updating app is blocked when in headless mode for obvious security reasons!\"}\n    if lollmsElfServer.config.host!=\"localhost\" and lollmsElfServer.config.host!=\"127.0.0.1\":\n        return {\"status\":False,\"error\":\"Updating app is blocked when the server is exposed outside for very obvious reasons!\"}\n    ASCIIColors.info(\"\")\n    ASCIIColors.info(\"\")\n    ASCIIColors.info(\"\")\n    ASCIIColors.info(\"\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\")\n    ASCIIColors.info(\"\u2551                Updating backend                  \u2551\")\n    ASCIIColors.info(\"\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\")\n    ASCIIColors.info(\"\")\n    ASCIIColors.info(\"\")\n    ASCIIColors.info(\"\")\n    await lollmsElfServer.sio.shutdown()\n    time.sleep(1)\n    lollmsElfServer.run_update_script(lollmsElfServer.args)\n    sys.exit()\ndef check_update():\n    \"\"\"Checks if an update is available\"\"\"\n    if lollmsElfServer.config.headless_server_mode:\n        return {\"status\":False,\"error\":\"Checking updates is blocked when in headless mode for obvious security reasons!\"}\n    if lollmsElfServer.config.host!=\"localhost\" and lollmsElfServer.config.host!=\"127.0.0.1\":\n        return {\"status\":False,\"error\":\"Checking updates is blocked when the server is exposed outside for very obvious reasons!\"}\n    if lollmsElfServer.config.auto_update:\n        res = lollmsElfServer.check_update_()\n        return {'update_availability':res}\n    else:\n        return {'update_availability':False}\nasync def add_preset(preset_data: PresetData):\n    \"\"\"\n    Changes current voice\n    :param request: The HTTP request object.\n    :return: A JSON response with the status of the operation.\n    \"\"\"\n    try:\n        presets_folder = lollmsElfServer.lollms_paths.personal_discussions_path/\"lollms_playground_presets\"\n        if not presets_folder.exists():\n            presets_folder.mkdir(exist_ok=True, parents=True)\n        sanitize_path_from_endpoint(preset_data.name,exception_text=\"Invalid preset name\")\n        fn = preset_data.name.lower().replace(\" \",\"_\")\n        filename = presets_folder/f\"{fn}.yaml\"\n        with open(filename, 'w', encoding='utf-8') as file:\n            yaml.dump(preset_data, file)\n        return {\"status\": True}\n    except Exception as ex:\n        trace_exception(ex)\n        return {\"status\": False, \"error\": \"There was an error adding the preset\"}\nasync def del_preset(preset_data: PresetData):\n    \"\"\"\n    Saves a preset to a file.\n    :param preset_data: The data of the preset.\n    :return: A JSON response with the status of the operation.\n    \"\"\"\n    if preset_data.name is None:\n        raise HTTPException(status_code=400, detail=\"Preset name is missing in the request\")\n    sanitize_path_from_endpoint(preset_data.name,exception_text=\"Invalid preset name\")\n    presets_file = lollmsElfServer.lollms_paths.personal_discussions_path/\"lollms_playground_presets\"/preset_data.name\n    try:\n        presets_file.unlink()\n        return {\"status\":True}\n    except:\n        return {\"status\":False}\nasync def save_presets(preset_data: PresetDataWithValue):\n    \"\"\"\n    Saves a preset to a file.\n    :param preset_data: The data of the preset.\n    :return: A JSON response with the status of the operation.\n    \"\"\"\n    if preset_data.preset is None:\n        raise HTTPException(status_code=400, detail=\"Preset data is missing in the request\")\n    sanitize_path_from_endpoint(preset_data.name,exception_text=\"Invalid preset name\")\n    presets_file = lollmsElfServer.lollms_paths.personal_discussions_path/\"presets.json\"\n    with open(presets_file, \"w\") as f:\n        json.dump(preset_data.preset, f, indent=4)\n    return {\"status\":True,\"message\":\"Preset saved successfully!\"}\nasync def edit_message(edit_params: EditMessageParameters):\n    client_id = edit_params.client_id\n    message_id = edit_params.id\n    new_message = edit_params.message\n    metadata = json.dumps(edit_params.metadata,indent=4)\n    try:\n        lollmsElfServer.session.get_client(client_id).discussion.edit_message(message_id, new_message, new_metadata=metadata)\n        return {\"status\": True}\n    except Exception as ex:\n        trace_exception(ex)\n        return {\"status\": False, \"error\": \"There was an error editing the message\"}\nasync def message_rank_up(rank_params: MessageRankParameters):\n    client_id = rank_params.client_id\n    message_id = rank_params.id\n    try:\n        new_rank = lollmsElfServer.session.get_client(client_id).discussion.message_rank_up(message_id)\n        return {\"status\": True, \"new_rank\": new_rank}\n    except Exception as ex:\n        trace_exception(ex)\n        return {\"status\": False, \"error\": \"There was an error ranking up the message\"}\ndef message_rank_down(rank_params: MessageRankParameters):\n    client_id = rank_params.client_id\n    message_id = rank_params.id\n    try:\n        new_rank = lollmsElfServer.session.get_client(client_id).discussion.message_rank_down(message_id)\n        return {\"status\": True, \"new_rank\": new_rank}\n    except Exception as ex:\n        return {\"status\": False, \"error\":str(ex)}\nclass MessageDeleteParameters(BaseModel):\nasync def delete_message(delete_params: MessageDeleteParameters):\n    client_id = delete_params.client_id\n    message_id = delete_params.id\n    if lollmsElfServer.session.get_client(client_id).discussion is None:\n        return {\"status\": False,\"message\":\"No discussion is selected\"}\n    else:\n        try:\n            new_rank = lollmsElfServer.session.get_client(client_id).discussion.delete_message(message_id)\n            ASCIIColors.yellow(\"Message deleted\")\n            return {\"status\":True,\"new_rank\": new_rank}\n        except Exception as ex:\n            trace_exception(ex)\n            return {\"status\": False, \"error\": \"There was an error deleting the message\"}",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-3126",
        "description": "[{'lang': 'en', 'value': \"A command injection vulnerability exists in the 'run_xtts_api_server' function of the parisneo/lollms-webui application, specifically within the 'lollms_xtts.py' script. The vulnerability arises due to the improper neutralization of special elements used in an OS command. The affected function utilizes 'subprocess.Popen' to execute a command constructed with a Python f-string, without adequately sanitizing the 'xtts_base_url' input. This flaw allows attackers to execute arbitrary commands remotely by manipulating the 'xtts_base_url' parameter. The vulnerability affects versions up to and including the latest version before 9.5. Successful exploitation could lead to arbitrary remote code execution (RCE) on the system where the application is deployed.\"}]",
        "cwe_number": 78
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-68",
      "code": "    def __read_chunk_length(self, rfile):\n        buf = BytesIO()\n        while 1:\n            char = rfile.read(1)\n            if not char:\n                self._chunked_input_error = True\n                raise _InvalidClientInput(\"EOF before chunk end reached\")\n            if char == b'\\r':\n                break\n            if char == b';':\n                break\n            if char not in _HEX:\n                self._chunked_input_error = True\n                raise _InvalidClientInput(\"Non-hex data\", char)\n            buf.write(char)\n            if buf.tell() > 16:\n                self._chunked_input_error = True\n                raise _InvalidClientInput(\"Chunk-size too large.\")\n        if char == b';':\n            i = 0\n            while i < MAX_REQUEST_LINE:\n                char = rfile.read(1)\n                if char == b'\\r':\n                    break\n                i += 1\n            else:\n                self._chunked_input_error = True\n                raise _InvalidClientInput(\"Too large chunk extension\")\n        if char == b'\\r':\n            char = rfile.read(1)\n            if char != b'\\n':\n                self._chunked_input_error = True\n                raise _InvalidClientInput(\"Line didn't end in CRLF\")\n            return int(buf.getvalue(), 16)\n    def _chunked_read(self, length=None, use_readline=False):\n        rfile = self.rfile\n        self._send_100_continue()\n        if length == 0:\n            return b\"\"\n        if use_readline:\n            reader = self.rfile.readline\n        else:\n            reader = self.rfile.read\n        response = []\n        while self.chunk_length != 0:\n            maxreadlen = self.chunk_length - self.position\n            if length is not None and length < maxreadlen:\n                maxreadlen = length\n            if maxreadlen > 0:\n                data = reader(maxreadlen)\n                if not data:\n                    self.chunk_length = 0\n                    self._chunked_input_error = True\n                    raise IOError(\"unexpected end of file while parsing chunked data\")\n                datalen = len(data)\n                response.append(data)\n                self.position += datalen\n                if self.chunk_length == self.position:\n                    rfile.readline()\n                if length is not None:\n                    length -= datalen\n                    if length == 0:\n                        break\n                if use_readline and data[-1] == b\"\\n\"[0]:\n                    break\n            else:\n                self.chunk_length = self.__read_chunk_length(rfile)\n                self.position = 0\n                if self.chunk_length == 0:\n                    rfile.readline()\n        return b''.join(response)\n    def readlines(self, hint=None):\n        return list(self)\n    def __iter__(self):\n        return self\n    def next(self):\n        line = self.readline()\n        if not line:\n            raise StopIteration\n        return line\n    __next__ = next\ntry:\n    import mimetools\n    headers_factory = mimetools.Message\nexcept ImportError:\n    from http import client\n    class OldMessage(client.HTTPMessage):\n        def __init__(self, **kwargs):\n            super(client.HTTPMessage, self).__init__(**kwargs)\n            self.status = ''\n        def getheader(self, name, default=None):\n            return self.get(name, default)\n        @property\n        def headers(self):\n            for key, value in self._headers:\n                yield '%s: %s\\r\\n' % (key, value)\n        @property\n        def typeheader(self):\n            return self.get('content-type')\n    def headers_factory(fp, *args):\n        try:\n            ret = client.parse_headers(fp, _class=OldMessage)\n        except client.LineTooLong:\n            ret = OldMessage()\n            ret.status = 'Line too long'\n        return ret\nclass WSGIHandler(object):\n    protocol_version = 'HTTP/1.1'\n    def MessageClass(self, *args):\n        return headers_factory(*args)\n    status = None\n    _orig_status = None\n    response_headers = None\n    code = None\n    provided_date = None\n    provided_content_length = None\n    close_connection = False\n    time_start = 0\n    time_finish = 0\n    headers_sent = False\n    response_use_chunked = False\n    connection_upgraded = False\n    environ = None\n    application = None\n    requestline = None\n    def read_requestline(self):\n        \"\"\"\n        Read and return the HTTP request line.\n        Under both Python 2 and 3, this should return the native\n        ``str`` type; under Python 3, this probably means the bytes read\n        from the network need to be decoded (using the ISO-8859-1 charset, aka\n        latin-1).\n        \"\"\"\n        line = self.rfile.readline(MAX_REQUEST_LINE)\n        line = line.decode('latin-1')\n        return line\n    def handle_one_request(self):\n        \"\"\"\n        Handles one HTTP request using ``self.socket`` and ``self.rfile``.\n        Each invocation of this method will do several things, including (but not limited to):\n        - Read the request line using :meth:`read_requestline`;\n        - Read the rest of the request, including headers, with :meth:`read_request`;\n        - Construct a new WSGI environment in ``self.environ`` using :meth:`get_environ`;\n        - Store the application in ``self.application``, retrieving it from the server;\n        - Handle the remainder of the request, including invoking the application,\n          with :meth:`handle_one_response`\n        There are several possible return values to indicate the state\n        of the client connection:\n        - ``None``\n            The client connection is already closed or should\n            be closed because the WSGI application or client set the\n            ``Connection: close`` header. The request handling\n            loop should terminate and perform cleanup steps.\n        - (status, body)\n            An HTTP status and body tuple. The request was in error,\n            as detailed by the status and body. The request handling\n            loop should terminate, close the connection, and perform\n            cleanup steps. Note that the ``body`` is the complete contents\n            to send to the client, including all headers and the initial\n            status line.\n        - ``True``\n            The literal ``True`` value. The request was successfully handled\n            and the response sent to the client by :meth:`handle_one_response`.\n            The connection remains open to process more requests and the connection\n            handling loop should call this method again. This is the typical return\n            value.\n        .. seealso:: :meth:`handle`\n        .. versionchanged:: 1.1b6\n           Funnel exceptions having to do with invalid HTTP requests through\n           :meth:`_handle_client_error` to allow subclasses to customize. Note that\n           this is experimental and may change in the future.\n        \"\"\"\n        if self.rfile.closed:\n            return\n        try:\n            self.requestline = self.read_requestline()\n            if isinstance(self.requestline, bytes):\n                self.requestline = self.requestline.decode('latin-1')\n        except socket.error:\n            return\n        if not self.requestline:\n            return\n        self.response_length = 0\n        if len(self.requestline) >= MAX_REQUEST_LINE:\n            return ('414', _REQUEST_TOO_LONG_RESPONSE)\n        try:\n            if not self.read_request(self.requestline):\n                return ('400', _BAD_REQUEST_RESPONSE)\n        except Exception as ex:\n            return self._handle_client_error(ex)\n        self.environ = self.get_environ()\n        self.application = self.server.application\n        self.handle_one_response()\n        if self.close_connection:\n            return\n        if self.rfile.closed:\n            return\n        return True\n    def get_environ(self):\n        \"\"\"\n        Construct and return a new WSGI environment dictionary for a specific request.\n        This should begin with asking the server for the base environment\n        using :meth:`WSGIServer.get_environ`, and then proceed to add the\n        request specific values.\n        By the time this method is invoked the request line and request shall have\n        been parsed and ``self.headers`` shall be populated.\n        \"\"\"\n        env = self.server.get_environ()\n        env['REQUEST_METHOD'] = self.command\n        env['SCRIPT_NAME'] = ''\n        path, query = self.path.split('?', 1) if '?' in self.path else (self.path, '')\n        env['PATH_INFO'] = unquote_latin1(path)\n        env['QUERY_STRING'] = query\n        if self.headers.typeheader is not None:\n            env['CONTENT_TYPE'] = self.headers.typeheader\n        length = self.headers.getheader('content-length')\n        if length:\n            env['CONTENT_LENGTH'] = length\n        env['SERVER_PROTOCOL'] = self.request_version\n        client_address = self.client_address\n        if isinstance(client_address, tuple):\n            env['REMOTE_ADDR'] = str(client_address[0])\n            env['REMOTE_PORT'] = str(client_address[1])\n        for key, value in self._headers():\n            if key in env:\n                if 'COOKIE' in key:\n                    env[key] += '; ' + value\n                else:\n                    env[key] += ',' + value\n            else:\n                env[key] = value\n        sock = self.socket if env.get('HTTP_EXPECT') == '100-continue' else None\n        chunked = env.get('HTTP_TRANSFER_ENCODING', '').lower() == 'chunked'\n        handling_reads = not self._connection_upgrade_requested()\n        self.wsgi_input = Input(self.rfile, self.content_length, socket=sock, chunked_input=chunked)\n        env['wsgi.input'] = self.wsgi_input if handling_reads else self.rfile\n        env['wsgi.input_terminated'] = handling_reads\n        return env\n    def flush(self):\n        pass\n    def writelines(self, *args, **kwargs):\n        pass\nclass LoggingLogAdapter(object):\ndef check_output(*popenargs, **kwargs):\n    r\"\"\"\n    check_output(args, *, input=None, stdin=None, stderr=None, shell=False, universal_newlines=False, timeout=None) -> output\n    Run command with arguments and return its output.\n    If the exit code was non-zero it raises a :exc:`CalledProcessError`.  The\n    ``CalledProcessError`` object will have the return code in the returncode\n    attribute and output in the output attribute.\n    The arguments are the same as for the Popen constructor.  Example::\n        >>> check_output([\"ls\", \"-1\", \"/dev/null\"])\n        '/dev/null\\n'\n    The ``stdout`` argument is not allowed as it is used internally.\n    To capture standard error in the result, use ``stderr=STDOUT``::\n        >>> print(check_output([\"/bin/sh\", \"-c\",\n        ...               \"ls -l non_existent_file ; exit 0\"],\n        ...              stderr=STDOUT).decode('ascii').strip())\n        ls: non_existent_file: No such file or directory\n    There is an additional optional argument, \"input\", allowing you to\n    pass a string to the subprocess's stdin.  If you use this argument\n    you may not also use the Popen constructor's \"stdin\" argument, as\n    it too will be used internally.  Example::\n        >>> check_output([\"sed\", \"-e\", \"s/foo/bar/\"],\n        ...              input=b\"when in the course of fooman events\\n\")\n        'when in the course of barman events\\n'\n    If ``universal_newlines=True`` is passed, the return value will be a\n    string rather than bytes.\n    .. versionchanged:: 1.2a1\n       The ``timeout`` keyword argument is now accepted on all supported\n       versions of Python (not just Python 3) and if it expires will raise a\n       :exc:`TimeoutExpired` exception (under Python 2 this is a subclass of :exc:`~.Timeout`).\n    .. versionchanged:: 1.2a1\n       The ``input`` keyword argument is now accepted on all supported\n       versions of Python, not just Python 3\n    .. versionchanged:: 22.08.0\n       Passing the ``check`` keyword argument is forbidden, just as in Python 3.11.\n    \"\"\"\n    timeout = kwargs.pop('timeout', None)\n    if 'stdout' in kwargs:\n        raise ValueError('stdout argument not allowed, it will be overridden.')\n    if 'check' in kwargs:\n        raise ValueError('check argument not allowed, it will be overridden.')\n    if 'input' in kwargs:\n        if 'stdin' in kwargs:\n            raise ValueError('stdin and input arguments may not both be used.')\n        inputdata = kwargs['input']\n        del kwargs['input']\n        kwargs['stdin'] = PIPE\n    else:\n        inputdata = None\n    with Popen(*popenargs, stdout=PIPE, **kwargs) as process:\n        try:\n            output, unused_err = process.communicate(inputdata, timeout=timeout)\n        except TimeoutExpired:\n            process.kill()\n            output, unused_err = process.communicate()\n            raise TimeoutExpired(process.args, timeout, output=output)\n        except:\n            process.kill()\n            process.wait()\n            raise\n        retcode = process.poll()\n        if retcode:\n            raise CalledProcessError(retcode, process.args, output=output)\n    return output\n    def __new__(cls, classname, bases, classDict):\n        timeout = classDict.get('__timeout__', 'NONE')\n        if timeout == 'NONE':\n            timeout = getattr(bases[0], '__timeout__', None)\n            if sysinfo.RUN_LEAKCHECKS and timeout is not None:\n                timeout *= 6\n        check_totalrefcount = _get_class_attr(classDict, bases, 'check_totalrefcount', True)\n        error_fatal = _get_class_attr(classDict, bases, 'error_fatal', True)\n        uses_handle_error = _get_class_attr(classDict, bases, 'uses_handle_error', True)\n        for key, value in list(classDict.items()):\n            if key.startswith('test') and callable(value):\n                classDict.pop(key)\n                value = _wrap_timeout(timeout, value)\n                error_fatal = getattr(value, 'error_fatal', error_fatal)\n                if error_fatal:\n                    value = errorhandler.wrap_error_fatal(value)\n                if uses_handle_error:\n                    value = errorhandler.wrap_restore_handle_error(value)\n                if check_totalrefcount and sysinfo.RUN_LEAKCHECKS:\n                    value = leakcheck.wrap_refcount(value)\n                classDict[key] = value\n        return type.__new__(cls, classname, bases, classDict)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-41419",
        "description": "[{'lang': 'en', 'value': 'An issue in Gevent before version 23.9.0 allows a remote attacker to escalate privileges via a crafted script to the WSGIServer component.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-69",
      "code": "def create_parser():\n    def _list_from_options_callback(option, opt_str, value, parser, append=True, delim=',', process=str.strip):\n        current = list(getattr(parser.values, option.dest)) if append else []\n        value = list(filter(None, [process(value)] if delim is None else map(process, value.split(delim))))\n        setattr(\n            parser.values, option.dest,\n            current + value if append is True else value + current)\n    def _set_from_options_callback(\n            option, opt_str, value, parser, allowed_values, delim=',', aliases={},\n            process=lambda x: x.lower().strip()):\n        values = [process(value)] if delim is None else map(process, value.split(delim))\n        try:\n            requested = orderedSet_from_options(values, collections.ChainMap(aliases, {'all': allowed_values}),\n                                                start=getattr(parser.values, option.dest))\n        except ValueError as e:\n            raise optparse.OptionValueError(f'wrong {option.metavar} for {opt_str}: {e.args[0]}')\n        setattr(parser.values, option.dest, set(requested))\n    def _dict_from_options_callback(\n            option, opt_str, value, parser,\n            allowed_keys=r'[\\w-]+', delimiter=':', default_key=None, process=None, multiple_keys=True,\n            process_key=str.lower, append=False):\n        out_dict = dict(getattr(parser.values, option.dest))\n        multiple_args = not isinstance(value, str)\n        if multiple_keys:\n            allowed_keys = fr'({allowed_keys})(,({allowed_keys}))*'\n        mobj = re.match(\n            fr'(?is)(?P<keys>{allowed_keys}){delimiter}(?P<val>.*)$',\n            value[0] if multiple_args else value)\n        if mobj is not None:\n            keys, val = mobj.group('keys').split(','), mobj.group('val')\n            if multiple_args:\n                val = [val, *value[1:]]\n        elif default_key is not None:\n            keys, val = variadic(default_key), value\n        else:\n            raise optparse.OptionValueError(\n                f'wrong {opt_str} formatting; it should be {option.metavar}, not \"{value}\"')\n        try:\n            keys = map(process_key, keys) if process_key else keys\n            val = process(val) if process else val\n        except Exception as err:\n            raise optparse.OptionValueError(f'wrong {opt_str} formatting; {err}')\n        for key in keys:\n            out_dict[key] = [*out_dict.get(key, []), val] if append else val\n        setattr(parser.values, option.dest, out_dict)\n    def when_prefix(default):\n        return {\n            'default': {},\n            'type': 'str',\n            'action': 'callback',\n            'callback': _dict_from_options_callback,\n            'callback_kwargs': {\n                'allowed_keys': '|'.join(map(re.escape, POSTPROCESS_WHEN)),\n                'default_key': default,\n                'multiple_keys': False,\n                'append': True,\n            },\n        }\n    parser = _YoutubeDLOptionParser()\n    alias_group = optparse.OptionGroup(parser, 'Aliases')\n    Formatter = string.Formatter()\n    def _create_alias(option, opt_str, value, parser):\n        aliases, opts = value\n        try:\n            nargs = len({i if f == '' else f\n                         for i, (_, f, _, _) in enumerate(Formatter.parse(opts)) if f is not None})\n            opts.format(*map(str, range(nargs)))\n        except Exception as err:\n            raise optparse.OptionValueError(f'wrong {opt_str} OPTIONS formatting; {err}')\n        if alias_group not in parser.option_groups:\n            parser.add_option_group(alias_group)\n        aliases = (x if x.startswith('-') else f'--{x}' for x in map(str.strip, aliases.split(',')))\n        try:\n            args = [f'ARG{i}' for i in range(nargs)]\n            alias_group.add_option(\n                *aliases, nargs=nargs, dest=parser.ALIAS_DEST, type='str' if nargs else None,\n                metavar=' '.join(args), help=opts.format(*args), action='callback',\n                callback=_alias_callback, callback_kwargs={'opts': opts, 'nargs': nargs})\n        except Exception as err:\n            raise optparse.OptionValueError(f'wrong {opt_str} formatting; {err}')\n    def _alias_callback(option, opt_str, value, parser, opts, nargs):\n        counter = getattr(parser.values, option.dest)\n        counter[opt_str] += 1\n        if counter[opt_str] > parser.ALIAS_TRIGGER_LIMIT:\n            raise optparse.OptionValueError(f'Alias {opt_str} exceeded invocation limit')\n        if nargs == 1:\n            value = [value]\n        assert (nargs == 0 and value is None) or len(value) == nargs\n        parser.rargs[:0] = shlex.split(\n            opts if value is None else opts.format(*map(shlex.quote, value)))\n    general = optparse.OptionGroup(parser, 'General Options')\n    general.add_option(\n        '-h', '--help', dest='print_help', action='store_true',\n        help='Print this help text and exit')\n    general.add_option(\n        '--version',\n        action='version',\n        help='Print program version and exit')\n    general.add_option(\n        '-U', '--update',\n        action='store_const', dest='update_self', const=CHANNEL,\n        help=format_field(\n            is_non_updateable(), None, 'Check if updates are available. %s',\n            default=f'Update this program to the latest {CHANNEL} version'))\n    general.add_option(\n        '--no-update',\n        action='store_false', dest='update_self',\n        help='Do not check for updates (default)')\n    general.add_option(\n        '--update-to',\n        action='store', dest='update_self', metavar='[CHANNEL]@[TAG]',\n        help=(\n            'Upgrade/downgrade to a specific version. CHANNEL can be a repository as well. '\n            f'CHANNEL and TAG default to \"{CHANNEL.partition(\"@\")[0]}\" and \"latest\" respectively if omitted; '\n            f'See \"UPDATE\" for details. Supported channels: {\", \".join(UPDATE_SOURCES)}'))\n    general.add_option(\n        '-i', '--ignore-errors',\n        action='store_true', dest='ignoreerrors',\n        help='Ignore download and postprocessing errors. The download will be considered successful even if the postprocessing fails')\n    general.add_option(\n        '--no-abort-on-error',\n        action='store_const', dest='ignoreerrors', const='only_download',\n        help='Continue with next video on download errors; e.g. to skip unavailable videos in a playlist (default)')\n    general.add_option(\n        '--abort-on-error', '--no-ignore-errors',\n        action='store_false', dest='ignoreerrors',\n        help='Abort downloading of further videos if an error occurs (Alias: --no-ignore-errors)')\n    general.add_option(\n        '--dump-user-agent',\n        action='store_true', dest='dump_user_agent', default=False,\n        help='Display the current user-agent and exit')\n    general.add_option(\n        '--list-extractors',\n        action='store_true', dest='list_extractors', default=False,\n        help='List all supported extractors and exit')\n    general.add_option(\n        '--extractor-descriptions',\n        action='store_true', dest='list_extractor_descriptions', default=False,\n        help='Output descriptions of all supported extractors and exit')\n    general.add_option(\n        '--use-extractors', '--ies',\n        action='callback', dest='allowed_extractors', metavar='NAMES', type='str',\n        default=[], callback=_list_from_options_callback,\n        help=(\n            'Extractor names to use separated by commas. '\n            'You can also use regexes, \"all\", \"default\" and \"end\" (end URL matching); '\n            'e.g. --ies \"holodex.*,end,youtube\". '\n            'Prefix the name with a \"-\" to exclude it, e.g. --ies default,-generic. '\n            'Use --list-extractors for a list of extractor names. (Alias: --ies)'))\n    general.add_option(\n        '--force-generic-extractor',\n        action='store_true', dest='force_generic_extractor', default=False,\n        help=optparse.SUPPRESS_HELP)\n    general.add_option(\n        '--default-search',\n        dest='default_search', metavar='PREFIX',\n        help=(\n            'Use this prefix for unqualified URLs. '\n            'E.g. \"gvsearch2:python\" downloads two videos from google videos for the search term \"python\". '\n            'Use the value \"auto\" to let yt-dlp guess (\"auto_warning\" to emit a warning when guessing). '\n            '\"error\" just throws an error. The default value \"fixup_error\" repairs broken URLs, '\n            'but emits an error if this is not possible instead of searching'))\n    general.add_option(\n        '--ignore-config', '--no-config',\n        action='store_true', dest='ignoreconfig',\n        help=(\n            'Don\\'t load any more configuration files except those given to --config-locations. '\n            'For backward compatibility, if this option is found inside the system configuration file, the user configuration is not loaded. '\n            '(Alias: --no-config)'))\n    general.add_option(\n        '--no-config-locations',\n        action='store_const', dest='config_locations', const=[],\n        help=(\n            'Do not load any custom configuration files (default). When given inside a '\n            'configuration file, ignore all previous --config-locations defined in the current file'))\n    general.add_option(\n        '--config-locations',\n        dest='config_locations', metavar='PATH', action='append',\n        help=(\n            'Location of the main configuration file; either the path to the config or its containing directory '\n            '(\"-\" for stdin). Can be used multiple times and inside other configuration files'))\n    general.add_option(\n        '--flat-playlist',\n        action='store_const', dest='extract_flat', const='in_playlist', default=False,\n        help='Do not extract the videos of a playlist, only list them')\n    general.add_option(\n        '--no-flat-playlist',\n        action='store_false', dest='extract_flat',\n        help='Fully extract the videos of a playlist (default)')\n    general.add_option(\n        '--live-from-start',\n        action='store_true', dest='live_from_start',\n        help='Download livestreams from the start. Currently only supported for YouTube (Experimental)')\n    general.add_option(\n        '--no-live-from-start',\n        action='store_false', dest='live_from_start',\n        help='Download livestreams from the current time (default)')\n    general.add_option(\n        '--wait-for-video',\n        dest='wait_for_video', metavar='MIN[-MAX]', default=None,\n        help=(\n            'Wait for scheduled streams to become available. '\n            'Pass the minimum number of seconds (or range) to wait between retries'))\n    general.add_option(\n        '--no-wait-for-video',\n        dest='wait_for_video', action='store_const', const=None,\n        help='Do not wait for scheduled streams (default)')\n    general.add_option(\n        '--mark-watched',\n        action='store_true', dest='mark_watched', default=False,\n        help='Mark videos watched (even with --simulate)')\n    general.add_option(\n        '--no-mark-watched',\n        action='store_false', dest='mark_watched',\n        help='Do not mark videos watched (default)')\n    general.add_option(\n        '--no-colors', '--no-colours',\n        action='store_const', dest='color', const={\n            'stdout': 'no_color',\n            'stderr': 'no_color',\n        },\n        help=optparse.SUPPRESS_HELP)\n    general.add_option(\n        '--color',\n        dest='color', metavar='[STREAM:]POLICY', default={}, type='str',\n        action='callback', callback=_dict_from_options_callback,\n        callback_kwargs={\n            'allowed_keys': 'stdout|stderr',\n            'default_key': ['stdout', 'stderr'],\n            'process': str.strip,\n        }, help=(\n            'Whether to emit color codes in output, optionally prefixed by '\n            'the STREAM (stdout or stderr) to apply the setting to. '\n            'Can be one of \"always\", \"auto\" (default), \"never\", or '\n            '\"no_color\" (use non color terminal sequences). '\n            'Can be used multiple times'))\n    general.add_option(\n        '--compat-options',\n        metavar='OPTS', dest='compat_opts', default=set(), type='str',\n        action='callback', callback=_set_from_options_callback,\n        callback_kwargs={\n            'allowed_values': {\n                'filename', 'filename-sanitization', 'format-sort', 'abort-on-error', 'format-spec', 'no-playlist-metafiles',\n                'multistreams', 'no-live-chat', 'playlist-index', 'list-formats', 'no-direct-merge', 'playlist-match-filter',\n                'no-attach-info-json', 'embed-thumbnail-atomicparsley', 'no-external-downloader-progress',\n                'embed-metadata', 'seperate-video-versions', 'no-clean-infojson', 'no-keep-subs', 'no-certifi',\n                'no-youtube-channel-redirect', 'no-youtube-unavailable-videos', 'no-youtube-prefer-utc-upload-date',\n                'prefer-legacy-http-handler', 'manifest-filesize-approx',\n            }, 'aliases': {\n                'youtube-dl': ['all', '-multistreams', '-playlist-match-filter', '-manifest-filesize-approx'],\n                'youtube-dlc': ['all', '-no-youtube-channel-redirect', '-no-live-chat', '-playlist-match-filter', '-manifest-filesize-approx'],\n                '2021': ['2022', 'no-certifi', 'filename-sanitization'],\n                '2022': ['2023', 'no-external-downloader-progress', 'playlist-match-filter', 'prefer-legacy-http-handler', 'manifest-filesize-approx'],\n                '2023': [],\n            },\n        }, help=(\n            'Options that can help keep compatibility with youtube-dl or youtube-dlc '\n            'configurations by reverting some of the changes made in yt-dlp. '\n            'See \"Differences in default behavior\" for details'))\n    general.add_option(\n        '--alias', metavar='ALIASES OPTIONS', dest='_', type='str', nargs=2,\n        action='callback', callback=_create_alias,\n        help=(\n            'Create aliases for an option string. Unless an alias starts with a dash \"-\", it is prefixed with \"--\". '\n            'Arguments are parsed according to the Python string formatting mini-language. '\n            'E.g. --alias get-audio,-X \"-S=aext:{0},abr -x --audio-format {0}\" creates options '\n            '\"--get-audio\" and \"-X\" that takes an argument (ARG0) and expands to '\n            '\"-S=aext:ARG0,abr -x --audio-format ARG0\". All defined aliases are listed in the --help output. '\n            'Alias options can trigger more aliases; so be careful to avoid defining recursive options. '\n            f'As a safety measure, each alias may be triggered a maximum of {_YoutubeDLOptionParser.ALIAS_TRIGGER_LIMIT} times. '\n            'This option can be used multiple times'))\n    network = optparse.OptionGroup(parser, 'Network Options')\n    network.add_option(\n        '--proxy', dest='proxy',\n        default=None, metavar='URL',\n        help=(\n            'Use the specified HTTP/HTTPS/SOCKS proxy. To enable SOCKS proxy, specify a proper scheme, '\n            'e.g. socks5://user:pass@127.0.0.1:1080/. Pass in an empty string (--proxy \"\") for direct connection'))\n    network.add_option(\n        '--socket-timeout',\n        dest='socket_timeout', type=float, default=None, metavar='SECONDS',\n        help='Time to wait before giving up, in seconds')\n    network.add_option(\n        '--source-address',\n        metavar='IP', dest='source_address', default=None,\n        help='Client-side IP address to bind to',\n    )\n    network.add_option(\n        '--impersonate',\n        metavar='CLIENT[:OS]', dest='impersonate', default=None,\n        help=(\n            'Client to impersonate for requests. E.g. chrome, chrome-110, chrome:windows-10. '\n            'Pass --impersonate=\"\" to impersonate any client. Note that forcing impersonation '\n            'for all requests may have a detrimental impact on download speed and stability'),\n    )\n    network.add_option(\n        '--list-impersonate-targets',\n        dest='list_impersonate_targets', default=False, action='store_true',\n        help='List available clients to impersonate.',\n    )\n    network.add_option(\n        '-4', '--force-ipv4',\n        action='store_const', const='0.0.0.0', dest='source_address',\n        help='Make all connections via IPv4',\n    )\n    network.add_option(\n        '-6', '--force-ipv6',\n        action='store_const', const='::', dest='source_address',\n        help='Make all connections via IPv6',\n    )\n    network.add_option(\n        '--enable-file-urls', action='store_true',\n        dest='enable_file_urls', default=False,\n        help='Enable file:// URLs. This is disabled by default for security reasons.',\n    )\n    geo = optparse.OptionGroup(parser, 'Geo-restriction')\n    geo.add_option(\n        '--geo-verification-proxy',\n        dest='geo_verification_proxy', default=None, metavar='URL',\n        help=(\n            'Use this proxy to verify the IP address for some geo-restricted sites. '\n            'The default proxy specified by --proxy (or none, if the option is not present) is used for the actual downloading'))\n    geo.add_option(\n        '--cn-verification-proxy',\n        dest='cn_verification_proxy', default=None, metavar='URL',\n        help=optparse.SUPPRESS_HELP)\n    geo.add_option(\n        '--xff', metavar='VALUE',\n        dest='geo_bypass', default='default',\n        help=(\n            'How to fake X-Forwarded-For HTTP header to try bypassing geographic restriction. '\n            'One of \"default\" (only when known to be useful), \"never\", '\n            'an IP block in CIDR notation, or a two-letter ISO 3166-2 country code'))\n    geo.add_option(\n        '--geo-bypass',\n        action='store_const', dest='geo_bypass', const='default',\n        help=optparse.SUPPRESS_HELP)\n    geo.add_option(\n        '--no-geo-bypass',\n        action='store_const', dest='geo_bypass', const='never',\n        help=optparse.SUPPRESS_HELP)\n    geo.add_option(\n        '--geo-bypass-country', metavar='CODE', dest='geo_bypass',\n        help=optparse.SUPPRESS_HELP)\n    geo.add_option(\n        '--geo-bypass-ip-block', metavar='IP_BLOCK', dest='geo_bypass',\n        help=optparse.SUPPRESS_HELP)\n    selection = optparse.OptionGroup(parser, 'Video Selection')\n    selection.add_option(\n        '--playlist-start',\n        dest='playliststart', metavar='NUMBER', default=1, type=int,\n        help=optparse.SUPPRESS_HELP)\n    selection.add_option(\n        '--playlist-end',\n        dest='playlistend', metavar='NUMBER', default=None, type=int,\n        help=optparse.SUPPRESS_HELP)\n    selection.add_option(\n        '-I', '--playlist-items',\n        dest='playlist_items', metavar='ITEM_SPEC', default=None,\n        help=(\n            'Comma separated playlist_index of the items to download. '\n            'You can specify a range using \"[START]:[STOP][:STEP]\". For backward compatibility, START-STOP is also supported. '\n            'Use negative indices to count from the right and negative STEP to download in reverse order. '\n            'E.g. \"-I 1:3,7,-5::2\" used on a playlist of size 15 will download the items at index 1,2,3,7,11,13,15'))\n    selection.add_option(\n        '--match-title',\n        dest='matchtitle', metavar='REGEX',\n        help=optparse.SUPPRESS_HELP)\n    selection.add_option(\n        '--reject-title',\n        dest='rejecttitle', metavar='REGEX',\n        help=optparse.SUPPRESS_HELP)\n    selection.add_option(\n        '--min-filesize',\n        metavar='SIZE', dest='min_filesize', default=None,\n        help='Abort download if filesize is smaller than SIZE, e.g. 50k or 44.6M')\n    selection.add_option(\n        '--max-filesize',\n        metavar='SIZE', dest='max_filesize', default=None,\n        help='Abort download if filesize is larger than SIZE, e.g. 50k or 44.6M')\n    selection.add_option(\n        '--date',\n        metavar='DATE', dest='date', default=None,\n        help=(\n            'Download only videos uploaded on this date. '\n            'The date can be \"YYYYMMDD\" or in the format [now|today|yesterday][-N[day|week|month|year]]. '\n            'E.g. \"--date today-2weeks\" downloads only videos uploaded on the same day two weeks ago'))\n    selection.add_option(\n        '--datebefore',\n        metavar='DATE', dest='datebefore', default=None,\n        help=(\n            'Download only videos uploaded on or before this date. '\n            'The date formats accepted is the same as --date'))\n    selection.add_option(\n        '--dateafter',\n        metavar='DATE', dest='dateafter', default=None,\n        help=(\n            'Download only videos uploaded on or after this date. '\n            'The date formats accepted is the same as --date'))\n    selection.add_option(\n        '--min-views',\n        metavar='COUNT', dest='min_views', default=None, type=int,\n        help=optparse.SUPPRESS_HELP)\n    selection.add_option(\n        '--max-views',\n        metavar='COUNT', dest='max_views', default=None, type=int,\n        help=optparse.SUPPRESS_HELP)\n    selection.add_option(\n        '--match-filters',\n        metavar='FILTER', dest='match_filter', action='append',\n        help=(\n            'Generic video filter. Any \"OUTPUT TEMPLATE\" field can be compared with a '\n            'number or a string using the operators defined in \"Filtering Formats\". '\n            'You can also simply specify a field to match if the field is present, '\n            'use \"!field\" to check if the field is not present, and \"&\" to check multiple conditions. '\n            'Use a \"\\\\\" to escape \"&\" or quotes if needed. If used multiple times, '\n            'the filter matches if at least one of the conditions is met. E.g. --match-filter '\n            '!is_live --match-filter \"like_count>?100 & description~=\\'(?i)\\\\bcats \\\\& dogs\\\\b\\'\" '\n            'matches only videos that are not live OR those that have a like count more than 100 '\n            '(or the like field is not available) and also has a description '\n            'that contains the phrase \"cats & dogs\" (caseless). '\n            'Use \"--match-filter -\" to interactively ask whether to download each video'))\n    selection.add_option(\n        '--no-match-filters',\n        dest='match_filter', action='store_const', const=None,\n        help='Do not use any --match-filter (default)')\n    selection.add_option(\n        '--break-match-filters',\n        metavar='FILTER', dest='breaking_match_filter', action='append',\n        help='Same as \"--match-filters\" but stops the download process when a video is rejected')\n    selection.add_option(\n        '--no-break-match-filters',\n        dest='breaking_match_filter', action='store_const', const=None,\n        help='Do not use any --break-match-filters (default)')\n    selection.add_option(\n        '--no-playlist',\n        action='store_true', dest='noplaylist', default=False,\n        help='Download only the video, if the URL refers to a video and a playlist')\n    selection.add_option(\n        '--yes-playlist',\n        action='store_false', dest='noplaylist',\n        help='Download the playlist, if the URL refers to a video and a playlist')\n    selection.add_option(\n        '--age-limit',\n        metavar='YEARS', dest='age_limit', default=None, type=int,\n        help='Download only videos suitable for the given age')\n    selection.add_option(\n        '--download-archive', metavar='FILE',\n        dest='download_archive',\n        help='Download only videos not listed in the archive file. Record the IDs of all downloaded videos in it')\n    selection.add_option(\n        '--no-download-archive',\n        dest='download_archive', action='store_const', const=None,\n        help='Do not use archive file (default)')\n    selection.add_option(\n        '--max-downloads',\n        dest='max_downloads', metavar='NUMBER', type=int, default=None,\n        help='Abort after downloading NUMBER files')\n    selection.add_option(\n        '--break-on-existing',\n        action='store_true', dest='break_on_existing', default=False,\n        help='Stop the download process when encountering a file that is in the archive')\n    selection.add_option(\n        '--no-break-on-existing',\n        action='store_false', dest='break_on_existing',\n        help='Do not stop the download process when encountering a file that is in the archive (default)')\n    selection.add_option(\n        '--break-on-reject',\n        action='store_true', dest='break_on_reject', default=False,\n        help=optparse.SUPPRESS_HELP)\n    selection.add_option(\n        '--break-per-input',\n        action='store_true', dest='break_per_url', default=False,\n        help='Alters --max-downloads, --break-on-existing, --break-match-filter, and autonumber to reset per input URL')\n    selection.add_option(\n        '--no-break-per-input',\n        action='store_false', dest='break_per_url',\n        help='--break-on-existing and similar options terminates the entire download queue')\n    selection.add_option(\n        '--skip-playlist-after-errors', metavar='N',\n        dest='skip_playlist_after_errors', default=None, type=int,\n        help='Number of allowed failures until the rest of the playlist is skipped')\n    selection.add_option(\n        '--include-ads',\n        dest='include_ads', action='store_true',\n        help=optparse.SUPPRESS_HELP)\n    selection.add_option(\n        '--no-include-ads',\n        dest='include_ads', action='store_false',\n        help=optparse.SUPPRESS_HELP)\n    authentication = optparse.OptionGroup(parser, 'Authentication Options')\n    authentication.add_option(\n        '-u', '--username',\n        dest='username', metavar='USERNAME',\n        help='Login with this account ID')\n    authentication.add_option(\n        '-p', '--password',\n        dest='password', metavar='PASSWORD',\n        help='Account password. If this option is left out, yt-dlp will ask interactively')\n    authentication.add_option(\n        '-2', '--twofactor',\n        dest='twofactor', metavar='TWOFACTOR',\n        help='Two-factor authentication code')\n    authentication.add_option(\n        '-n', '--netrc',\n        action='store_true', dest='usenetrc', default=False,\n        help='Use .netrc authentication data')\n    authentication.add_option(\n        '--netrc-location',\n        dest='netrc_location', metavar='PATH',\n        help='Location of .netrc authentication data; either the path or its containing directory. Defaults to ~/.netrc')\n    authentication.add_option(\n        '--netrc-cmd',\n        dest='netrc_cmd', metavar='NETRC_CMD',\n        help='Command to execute to get the credentials for an extractor.')\n    authentication.add_option(\n        '--video-password',\n        dest='videopassword', metavar='PASSWORD',\n        help='Video-specific password')\n    authentication.add_option(\n        '--ap-mso',\n        dest='ap_mso', metavar='MSO',\n        help='Adobe Pass multiple-system operator (TV provider) identifier, use --ap-list-mso for a list of available MSOs')\n    authentication.add_option(\n        '--ap-username',\n        dest='ap_username', metavar='USERNAME',\n        help='Multiple-system operator account login')\n    authentication.add_option(\n        '--ap-password',\n        dest='ap_password', metavar='PASSWORD',\n        help='Multiple-system operator account password. If this option is left out, yt-dlp will ask interactively')\n    authentication.add_option(\n        '--ap-list-mso',\n        action='store_true', dest='ap_list_mso', default=False,\n        help='List all supported multiple-system operators')\n    authentication.add_option(\n        '--client-certificate',\n        dest='client_certificate', metavar='CERTFILE',\n        help='Path to client certificate file in PEM format. May include the private key')\n    authentication.add_option(\n        '--client-certificate-key',\n        dest='client_certificate_key', metavar='KEYFILE',\n        help='Path to private key file for client certificate')\n    authentication.add_option(\n        '--client-certificate-password',\n        dest='client_certificate_password', metavar='PASSWORD',\n        help='Password for client certificate private key, if encrypted. '\n             'If not provided, and the key is encrypted, yt-dlp will ask interactively')\n    video_format = optparse.OptionGroup(parser, 'Video Format Options')\n    video_format.add_option(\n        '-f', '--format',\n        action='store', dest='format', metavar='FORMAT', default=None,\n        help='Video format code, see \"FORMAT SELECTION\" for more details')\n    video_format.add_option(\n        '-S', '--format-sort', metavar='SORTORDER',\n        dest='format_sort', default=[], type='str', action='callback',\n        callback=_list_from_options_callback, callback_kwargs={'append': -1},\n        help='Sort the formats by the fields given, see \"Sorting Formats\" for more details')\n    video_format.add_option(\n        '--format-sort-force', '--S-force',\n        action='store_true', dest='format_sort_force', metavar='FORMAT', default=False,\n        help=(\n            'Force user specified sort order to have precedence over all fields, '\n            'see \"Sorting Formats\" for more details (Alias: --S-force)'))\n    video_format.add_option(\n        '--no-format-sort-force',\n        action='store_false', dest='format_sort_force', metavar='FORMAT', default=False,\n        help='Some fields have precedence over the user specified sort order (default)')\n    video_format.add_option(\n        '--video-multistreams',\n        action='store_true', dest='allow_multiple_video_streams', default=None,\n        help='Allow multiple video streams to be merged into a single file')\n    video_format.add_option(\n        '--no-video-multistreams',\n        action='store_false', dest='allow_multiple_video_streams',\n        help='Only one video stream is downloaded for each output file (default)')\n    video_format.add_option(\n        '--audio-multistreams',\n        action='store_true', dest='allow_multiple_audio_streams', default=None,\n        help='Allow multiple audio streams to be merged into a single file')\n    video_format.add_option(\n        '--no-audio-multistreams',\n        action='store_false', dest='allow_multiple_audio_streams',\n        help='Only one audio stream is downloaded for each output file (default)')\n    video_format.add_option(\n        '--all-formats',\n        action='store_const', dest='format', const='all',\n        help=optparse.SUPPRESS_HELP)\n    video_format.add_option(\n        '--prefer-free-formats',\n        action='store_true', dest='prefer_free_formats', default=False,\n        help=(\n            'Prefer video formats with free containers over non-free ones of same quality. '\n            'Use with \"-S ext\" to strictly prefer free containers irrespective of quality'))\n    video_format.add_option(\n        '--no-prefer-free-formats',\n        action='store_false', dest='prefer_free_formats', default=False,\n        help=\"Don't give any special preference to free containers (default)\")\n    video_format.add_option(\n        '--check-formats',\n        action='store_const', const='selected', dest='check_formats', default=None,\n        help='Make sure formats are selected only from those that are actually downloadable')\n    video_format.add_option(\n        '--check-all-formats',\n        action='store_true', dest='check_formats',\n        help='Check all formats for whether they are actually downloadable')\n    video_format.add_option(\n        '--no-check-formats',\n        action='store_false', dest='check_formats',\n        help='Do not check that the formats are actually downloadable')\n    video_format.add_option(\n        '-F', '--list-formats',\n        action='store_true', dest='listformats',\n        help='List available formats of each video. Simulate unless --no-simulate is used')\n    video_format.add_option(\n        '--list-formats-as-table',\n        action='store_true', dest='listformats_table', default=True,\n        help=optparse.SUPPRESS_HELP)\n    video_format.add_option(\n        '--list-formats-old', '--no-list-formats-as-table',\n        action='store_false', dest='listformats_table',\n        help=optparse.SUPPRESS_HELP)\n    video_format.add_option(\n        '--merge-output-format',\n        action='store', dest='merge_output_format', metavar='FORMAT', default=None,\n        help=(\n            'Containers that may be used when merging formats, separated by \"/\", e.g. \"mp4/mkv\". '\n            'Ignored if no merge is required. '\n            f'(currently supported: {\", \".join(sorted(FFmpegMergerPP.SUPPORTED_EXTS))})'))\n    video_format.add_option(\n        '--allow-unplayable-formats',\n        action='store_true', dest='allow_unplayable_formats', default=False,\n        help=optparse.SUPPRESS_HELP)\n    video_format.add_option(\n        '--no-allow-unplayable-formats',\n        action='store_false', dest='allow_unplayable_formats',\n        help=optparse.SUPPRESS_HELP)\n    subtitles = optparse.OptionGroup(parser, 'Subtitle Options')\n    subtitles.add_option(\n        '--write-subs', '--write-srt',\n        action='store_true', dest='writesubtitles', default=False,\n        help='Write subtitle file')\n    subtitles.add_option(\n        '--no-write-subs', '--no-write-srt',\n        action='store_false', dest='writesubtitles',\n        help='Do not write subtitle file (default)')\n    subtitles.add_option(\n        '--write-auto-subs', '--write-automatic-subs',\n        action='store_true', dest='writeautomaticsub', default=False,\n        help='Write automatically generated subtitle file (Alias: --write-automatic-subs)')\n    subtitles.add_option(\n        '--no-write-auto-subs', '--no-write-automatic-subs',\n        action='store_false', dest='writeautomaticsub', default=False,\n        help='Do not write auto-generated subtitles (default) (Alias: --no-write-automatic-subs)')\n    subtitles.add_option(\n        '--all-subs',\n        action='store_true', dest='allsubtitles', default=False,\n        help=optparse.SUPPRESS_HELP)\n    subtitles.add_option(\n        '--list-subs',\n        action='store_true', dest='listsubtitles', default=False,\n        help='List available subtitles of each video. Simulate unless --no-simulate is used')\n    subtitles.add_option(\n        '--sub-format',\n        action='store', dest='subtitlesformat', metavar='FORMAT', default='best',\n        help='Subtitle format; accepts formats preference, e.g. \"srt\" or \"ass/srt/best\"')\n    subtitles.add_option(\n        '--sub-langs', '--srt-langs',\n        action='callback', dest='subtitleslangs', metavar='LANGS', type='str',\n        default=[], callback=_list_from_options_callback,\n        help=(\n            'Languages of the subtitles to download (can be regex) or \"all\" separated by commas, e.g. --sub-langs \"en.*,ja\". '\n            'You can prefix the language code with a \"-\" to exclude it from the requested languages, e.g. --sub-langs all,-live_chat. '\n            'Use --list-subs for a list of available language tags'))\n    downloader = optparse.OptionGroup(parser, 'Download Options')\n    downloader.add_option(\n        '-N', '--concurrent-fragments',\n        dest='concurrent_fragment_downloads', metavar='N', default=1, type=int,\n        help='Number of fragments of a dash/hlsnative video that should be downloaded concurrently (default is %default)')\n    downloader.add_option(\n        '-r', '--limit-rate', '--rate-limit',\n        dest='ratelimit', metavar='RATE',\n        help='Maximum download rate in bytes per second, e.g. 50K or 4.2M')\n    downloader.add_option(\n        '--throttled-rate',\n        dest='throttledratelimit', metavar='RATE',\n        help='Minimum download rate in bytes per second below which throttling is assumed and the video data is re-extracted, e.g. 100K')\n    downloader.add_option(\n        '-R', '--retries',\n        dest='retries', metavar='RETRIES', default=10,\n        help='Number of retries (default is %default), or \"infinite\"')\n    downloader.add_option(\n        '--file-access-retries',\n        dest='file_access_retries', metavar='RETRIES', default=3,\n        help='Number of times to retry on file access error (default is %default), or \"infinite\"')\n    downloader.add_option(\n        '--fragment-retries',\n        dest='fragment_retries', metavar='RETRIES', default=10,\n        help='Number of retries for a fragment (default is %default), or \"infinite\" (DASH, hlsnative and ISM)')\n    downloader.add_option(\n        '--retry-sleep',\n        dest='retry_sleep', metavar='[TYPE:]EXPR', default={}, type='str',\n        action='callback', callback=_dict_from_options_callback,\n        callback_kwargs={\n            'allowed_keys': 'http|fragment|file_access|extractor',\n            'default_key': 'http',\n        }, help=(\n            'Time to sleep between retries in seconds (optionally) prefixed by the type of retry '\n            '(http (default), fragment, file_access, extractor) to apply the sleep to. '\n            'EXPR can be a number, linear=START[:END[:STEP=1]] or exp=START[:END[:BASE=2]]. '\n            'This option can be used multiple times to set the sleep for the different retry types, '\n            'e.g. --retry-sleep linear=1::2 --retry-sleep fragment:exp=1:20'))\n    downloader.add_option(\n        '--skip-unavailable-fragments', '--no-abort-on-unavailable-fragments',\n        action='store_true', dest='skip_unavailable_fragments', default=True,\n        help='Skip unavailable fragments for DASH, hlsnative and ISM downloads (default) (Alias: --no-abort-on-unavailable-fragments)')\n    downloader.add_option(\n        '--abort-on-unavailable-fragments', '--no-skip-unavailable-fragments',\n        action='store_false', dest='skip_unavailable_fragments',\n        help='Abort download if a fragment is unavailable (Alias: --no-skip-unavailable-fragments)')\n    downloader.add_option(\n        '--keep-fragments',\n        action='store_true', dest='keep_fragments', default=False,\n        help='Keep downloaded fragments on disk after downloading is finished')\n    downloader.add_option(\n        '--no-keep-fragments',\n        action='store_false', dest='keep_fragments',\n        help='Delete downloaded fragments after downloading is finished (default)')\n    downloader.add_option(\n        '--buffer-size',\n        dest='buffersize', metavar='SIZE', default='1024',\n        help='Size of download buffer, e.g. 1024 or 16K (default is %default)')\n    downloader.add_option(\n        '--resize-buffer',\n        action='store_false', dest='noresizebuffer',\n        help='The buffer size is automatically resized from an initial value of --buffer-size (default)')\n    downloader.add_option(\n        '--no-resize-buffer',\n        action='store_true', dest='noresizebuffer', default=False,\n        help='Do not automatically adjust the buffer size')\n    downloader.add_option(\n        '--http-chunk-size',\n        dest='http_chunk_size', metavar='SIZE', default=None,\n        help=(\n            'Size of a chunk for chunk-based HTTP downloading, e.g. 10485760 or 10M (default is disabled). '\n            'May be useful for bypassing bandwidth throttling imposed by a webserver (experimental)'))\n    downloader.add_option(\n        '--test',\n        action='store_true', dest='test', default=False,\n        help=optparse.SUPPRESS_HELP)\n    downloader.add_option(\n        '--playlist-reverse',\n        action='store_true', dest='playlist_reverse',\n        help=optparse.SUPPRESS_HELP)\n    downloader.add_option(\n        '--no-playlist-reverse',\n        action='store_false', dest='playlist_reverse',\n        help=optparse.SUPPRESS_HELP)\n    downloader.add_option(\n        '--playlist-random',\n        action='store_true', dest='playlist_random',\n        help='Download playlist videos in random order')\n    downloader.add_option(\n        '--lazy-playlist',\n        action='store_true', dest='lazy_playlist',\n        help='Process entries in the playlist as they are received. This disables n_entries, --playlist-random and --playlist-reverse')\n    downloader.add_option(\n        '--no-lazy-playlist',\n        action='store_false', dest='lazy_playlist',\n        help='Process videos in the playlist only after the entire playlist is parsed (default)')\n    downloader.add_option(\n        '--xattr-set-filesize',\n        dest='xattr_set_filesize', action='store_true',\n        help='Set file xattribute ytdl.filesize with expected file size')\n    downloader.add_option(\n        '--hls-prefer-native',\n        dest='hls_prefer_native', action='store_true', default=None,\n        help=optparse.SUPPRESS_HELP)\n    downloader.add_option(\n        '--hls-prefer-ffmpeg',\n        dest='hls_prefer_native', action='store_false', default=None,\n        help=optparse.SUPPRESS_HELP)\n    downloader.add_option(\n        '--hls-use-mpegts',\n        dest='hls_use_mpegts', action='store_true', default=None,\n        help=(\n            'Use the mpegts container for HLS videos; '\n            'allowing some players to play the video while downloading, '\n            'and reducing the chance of file corruption if download is interrupted. '\n            'This is enabled by default for live streams'))\n    downloader.add_option(\n        '--no-hls-use-mpegts',\n        dest='hls_use_mpegts', action='store_false',\n        help=(\n            'Do not use the mpegts container for HLS videos. '\n            'This is default when not downloading live streams'))\n    downloader.add_option(\n        '--download-sections',\n        metavar='REGEX', dest='download_ranges', action='append',\n        help=(\n            'Download only chapters that match the regular expression. '\n            'A \"*\" prefix denotes time-range instead of chapter. Negative timestamps are calculated from the end. '\n            '\"*from-url\" can be used to download between the \"start_time\" and \"end_time\" extracted from the URL. '\n            'Needs ffmpeg. This option can be used multiple times to download multiple sections, '\n            'e.g. --download-sections \"*10:15-inf\" --download-sections \"intro\"'))\n    downloader.add_option(\n        '--downloader', '--external-downloader',\n        dest='external_downloader', metavar='[PROTO:]NAME', default={}, type='str',\n        action='callback', callback=_dict_from_options_callback,\n        callback_kwargs={\n            'allowed_keys': 'http|ftp|m3u8|dash|rtsp|rtmp|mms',\n            'default_key': 'default',\n            'process': str.strip,\n        }, help=(\n            'Name or path of the external downloader to use (optionally) prefixed by '\n            'the protocols (http, ftp, m3u8, dash, rstp, rtmp, mms) to use it for. '\n            f'Currently supports native, {\", \".join(sorted(list_external_downloaders()))}. '\n            'You can use this option multiple times to set different downloaders for different protocols. '\n            'E.g. --downloader aria2c --downloader \"dash,m3u8:native\" will use '\n            'aria2c for http/ftp downloads, and the native downloader for dash/m3u8 downloads '\n            '(Alias: --external-downloader)'))\n    downloader.add_option(\n        '--downloader-args', '--external-downloader-args',\n        metavar='NAME:ARGS', dest='external_downloader_args', default={}, type='str',\n        action='callback', callback=_dict_from_options_callback,\n        callback_kwargs={\n            'allowed_keys': r'ffmpeg_[io]\\d*|{}'.format('|'.join(map(re.escape, list_external_downloaders()))),\n            'default_key': 'default',\n            'process': shlex.split,\n        }, help=(\n            'Give these arguments to the external downloader. '\n            'Specify the downloader name and the arguments separated by a colon \":\". '\n            'For ffmpeg, arguments can be passed to different positions using the same syntax as --postprocessor-args. '\n            'You can use this option multiple times to give different arguments to different downloaders '\n            '(Alias: --external-downloader-args)'))\n    workarounds = optparse.OptionGroup(parser, 'Workarounds')\n    workarounds.add_option(\n        '--encoding',\n        dest='encoding', metavar='ENCODING',\n        help='Force the specified encoding (experimental)')\n    workarounds.add_option(\n        '--legacy-server-connect',\n        action='store_true', dest='legacy_server_connect', default=False,\n        help='Explicitly allow HTTPS connection to servers that do not support RFC 5746 secure renegotiation')\n    workarounds.add_option(\n        '--no-check-certificates',\n        action='store_true', dest='no_check_certificate', default=False,\n        help='Suppress HTTPS certificate validation')\n    workarounds.add_option(\n        '--prefer-insecure', '--prefer-unsecure',\n        action='store_true', dest='prefer_insecure',\n        help='Use an unencrypted connection to retrieve information about the video (Currently supported only for YouTube)')\n    workarounds.add_option(\n        '--user-agent',\n        metavar='UA', dest='user_agent',\n        help=optparse.SUPPRESS_HELP)\n    workarounds.add_option(\n        '--referer',\n        metavar='URL', dest='referer', default=None,\n        help=optparse.SUPPRESS_HELP)\n    workarounds.add_option(\n        '--add-headers',\n        metavar='FIELD:VALUE', dest='headers', default={}, type='str',\n        action='callback', callback=_dict_from_options_callback,\n        callback_kwargs={'multiple_keys': False},\n        help='Specify a custom HTTP header and its value, separated by a colon \":\". You can use this option multiple times',\n    )\n    workarounds.add_option(\n        '--bidi-workaround',\n        dest='bidi_workaround', action='store_true',\n        help='Work around terminals that lack bidirectional text support. Requires bidiv or fribidi executable in PATH')\n    workarounds.add_option(\n        '--sleep-requests', metavar='SECONDS',\n        dest='sleep_interval_requests', type=float,\n        help='Number of seconds to sleep between requests during data extraction')\n    workarounds.add_option(\n        '--sleep-interval', '--min-sleep-interval', metavar='SECONDS',\n        dest='sleep_interval', type=float,\n        help=(\n            'Number of seconds to sleep before each download. '\n            'This is the minimum time to sleep when used along with --max-sleep-interval '\n            '(Alias: --min-sleep-interval)'))\n    workarounds.add_option(\n        '--max-sleep-interval', metavar='SECONDS',\n        dest='max_sleep_interval', type=float,\n        help='Maximum number of seconds to sleep. Can only be used along with --min-sleep-interval')\n    workarounds.add_option(\n        '--sleep-subtitles', metavar='SECONDS',\n        dest='sleep_interval_subtitles', default=0, type=int,\n        help='Number of seconds to sleep before each subtitle download')\n    verbosity = optparse.OptionGroup(parser, 'Verbosity and Simulation Options')\n    verbosity.add_option(\n        '-q', '--quiet',\n        action='store_true', dest='quiet', default=None,\n        help='Activate quiet mode. If used with --verbose, print the log to stderr')\n    verbosity.add_option(\n        '--no-quiet',\n        action='store_false', dest='quiet',\n        help='Deactivate quiet mode. (Default)')\n    verbosity.add_option(\n        '--no-warnings',\n        dest='no_warnings', action='store_true', default=False,\n        help='Ignore warnings')\n    verbosity.add_option(\n        '-s', '--simulate',\n        action='store_true', dest='simulate', default=None,\n        help='Do not download the video and do not write anything to disk')\n    verbosity.add_option(\n        '--no-simulate',\n        action='store_false', dest='simulate',\n        help='Download the video even if printing/listing options are used')\n    verbosity.add_option(\n        '--ignore-no-formats-error',\n        action='store_true', dest='ignore_no_formats_error', default=False,\n        help=(\n            'Ignore \"No video formats\" error. Useful for extracting metadata '\n            'even if the videos are not actually available for download (experimental)'))\n    verbosity.add_option(\n        '--no-ignore-no-formats-error',\n        action='store_false', dest='ignore_no_formats_error',\n        help='Throw error when no downloadable video formats are found (default)')\n    verbosity.add_option(\n        '--skip-download', '--no-download',\n        action='store_true', dest='skip_download', default=False,\n        help='Do not download the video but write all related files (Alias: --no-download)')\n    verbosity.add_option(\n        '-O', '--print',\n        metavar='[WHEN:]TEMPLATE', dest='forceprint', **when_prefix('video'),\n        help=(\n            'Field name or output template to print to screen, optionally prefixed with when to print it, separated by a \":\". '\n            'Supported values of \"WHEN\" are the same as that of --use-postprocessor (default: video). '\n            'Implies --quiet. Implies --simulate unless --no-simulate or later stages of WHEN are used. '\n            'This option can be used multiple times'))\n    verbosity.add_option(\n        '--print-to-file',\n        metavar='[WHEN:]TEMPLATE FILE', dest='print_to_file', nargs=2, **when_prefix('video'),\n        help=(\n            'Append given template to the file. The values of WHEN and TEMPLATE are same as that of --print. '\n            'FILE uses the same syntax as the output template. This option can be used multiple times'))\n    verbosity.add_option(\n        '-g', '--get-url',\n        action='store_true', dest='geturl', default=False,\n        help=optparse.SUPPRESS_HELP)\n    verbosity.add_option(\n        '-e', '--get-title',\n        action='store_true', dest='gettitle', default=False,\n        help=optparse.SUPPRESS_HELP)\n    verbosity.add_option(\n        '--get-id',\n        action='store_true', dest='getid', default=False,\n        help=optparse.SUPPRESS_HELP)\n    verbosity.add_option(\n        '--get-thumbnail',\n        action='store_true', dest='getthumbnail', default=False,\n        help=optparse.SUPPRESS_HELP)\n    verbosity.add_option(\n        '--get-description',\n        action='store_true', dest='getdescription', default=False,\n        help=optparse.SUPPRESS_HELP)\n    verbosity.add_option(\n        '--get-duration',\n        action='store_true', dest='getduration', default=False,\n        help=optparse.SUPPRESS_HELP)\n    verbosity.add_option(\n        '--get-filename',\n        action='store_true', dest='getfilename', default=False,\n        help=optparse.SUPPRESS_HELP)\n    verbosity.add_option(\n        '--get-format',\n        action='store_true', dest='getformat', default=False,\n        help=optparse.SUPPRESS_HELP)\n    verbosity.add_option(\n        '-j', '--dump-json',\n        action='store_true', dest='dumpjson', default=False,\n        help=(\n            'Quiet, but print JSON information for each video. Simulate unless --no-simulate is used. '\n            'See \"OUTPUT TEMPLATE\" for a description of available keys'))\n    verbosity.add_option(\n        '-J', '--dump-single-json',\n        action='store_true', dest='dump_single_json', default=False,\n        help=(\n            'Quiet, but print JSON information for each url or infojson passed. Simulate unless --no-simulate is used. '\n            'If the URL refers to a playlist, the whole playlist information is dumped in a single line'))\n    verbosity.add_option(\n        '--print-json',\n        action='store_true', dest='print_json', default=False,\n        help=optparse.SUPPRESS_HELP)\n    verbosity.add_option(\n        '--force-write-archive', '--force-write-download-archive', '--force-download-archive',\n        action='store_true', dest='force_write_download_archive', default=False,\n        help=(\n            'Force download archive entries to be written as far as no errors occur, '\n            'even if -s or another simulation option is used (Alias: --force-download-archive)'))\n    verbosity.add_option(\n        '--newline',\n        action='store_true', dest='progress_with_newline', default=False,\n        help='Output progress bar as new lines')\n    verbosity.add_option(\n        '--no-progress',\n        action='store_true', dest='noprogress', default=None,\n        help='Do not print progress bar')\n    verbosity.add_option(\n        '--progress',\n        action='store_false', dest='noprogress',\n        help='Show progress bar, even if in quiet mode')\n    verbosity.add_option(\n        '--console-title',\n        action='store_true', dest='consoletitle', default=False,\n        help='Display progress in console titlebar')\n    verbosity.add_option(\n        '--progress-template',\n        metavar='[TYPES:]TEMPLATE', dest='progress_template', default={}, type='str',\n        action='callback', callback=_dict_from_options_callback,\n        callback_kwargs={\n            'allowed_keys': '(download|postprocess)(-title)?',\n            'default_key': 'download',\n        }, help=(\n            'Template for progress outputs, optionally prefixed with one of \"download:\" (default), '\n            '\"download-title:\" (the console title), \"postprocess:\",  or \"postprocess-title:\". '\n            'The video\\'s fields are accessible under the \"info\" key and '\n            'the progress attributes are accessible under \"progress\" key. E.g. '\n            '--console-title --progress-template \"download-title:%(info.id)s-%(progress.eta)s\"'))\n    verbosity.add_option(\n        '--progress-delta',\n        metavar='SECONDS', action='store', dest='progress_delta', type=float, default=0,\n        help='Time between progress output (default: 0)')\n    verbosity.add_option(\n        '-v', '--verbose',\n        action='store_true', dest='verbose', default=False,\n        help='Print various debugging information')\n    verbosity.add_option(\n        '--dump-pages', '--dump-intermediate-pages',\n        action='store_true', dest='dump_intermediate_pages', default=False,\n        help='Print downloaded pages encoded using base64 to debug problems (very verbose)')\n    verbosity.add_option(\n        '--write-pages',\n        action='store_true', dest='write_pages', default=False,\n        help='Write downloaded intermediary pages to files in the current directory to debug problems')\n    verbosity.add_option(\n        '--load-pages',\n        action='store_true', dest='load_pages', default=False,\n        help=optparse.SUPPRESS_HELP)\n    verbosity.add_option(\n        '--youtube-print-sig-code',\n        action='store_true', dest='youtube_print_sig_code', default=False,\n        help=optparse.SUPPRESS_HELP)\n    verbosity.add_option(\n        '--print-traffic', '--dump-headers',\n        dest='debug_printtraffic', action='store_true', default=False,\n        help='Display sent and read HTTP traffic')\n    verbosity.add_option(\n        '-C', '--call-home',\n        dest='call_home', action='store_true', default=False,\n        help=optparse.SUPPRESS_HELP)\n    verbosity.add_option(\n        '--no-call-home',\n        dest='call_home', action='store_false',\n        help=optparse.SUPPRESS_HELP)\n    filesystem = optparse.OptionGroup(parser, 'Filesystem Options')\n    filesystem.add_option(\n        '-a', '--batch-file',\n        dest='batchfile', metavar='FILE',\n        help=(\n            'File containing URLs to download (\"-\" for stdin), one URL per line. '\n            'Lines starting with \"\n    filesystem.add_option(\n        '--no-batch-file',\n        dest='batchfile', action='store_const', const=None,\n        help='Do not read URLs from batch file (default)')\n    filesystem.add_option(\n        '--id', default=False,\n        action='store_true', dest='useid', help=optparse.SUPPRESS_HELP)\n    filesystem.add_option(\n        '-P', '--paths',\n        metavar='[TYPES:]PATH', dest='paths', default={}, type='str',\n        action='callback', callback=_dict_from_options_callback,\n        callback_kwargs={\n            'allowed_keys': 'home|temp|{}'.format('|'.join(map(re.escape, OUTTMPL_TYPES.keys()))),\n            'default_key': 'home',\n        }, help=(\n            'The paths where the files should be downloaded. '\n            'Specify the type of file and the path separated by a colon \":\". '\n            'All the same TYPES as --output are supported. '\n            'Additionally, you can also provide \"home\" (default) and \"temp\" paths. '\n            'All intermediary files are first downloaded to the temp path and '\n            'then the final files are moved over to the home path after download is finished. '\n            'This option is ignored if --output is an absolute path'))\n    filesystem.add_option(\n        '-o', '--output',\n        metavar='[TYPES:]TEMPLATE', dest='outtmpl', default={}, type='str',\n        action='callback', callback=_dict_from_options_callback,\n        callback_kwargs={\n            'allowed_keys': '|'.join(map(re.escape, OUTTMPL_TYPES.keys())),\n            'default_key': 'default',\n        }, help='Output filename template; see \"OUTPUT TEMPLATE\" for details')\n    filesystem.add_option(\n        '--output-na-placeholder',\n        dest='outtmpl_na_placeholder', metavar='TEXT', default='NA',\n        help=('Placeholder for unavailable fields in --output (default: \"%default\")'))\n    filesystem.add_option(\n        '--autonumber-size',\n        dest='autonumber_size', metavar='NUMBER', type=int,\n        help=optparse.SUPPRESS_HELP)\n    filesystem.add_option(\n        '--autonumber-start',\n        dest='autonumber_start', metavar='NUMBER', default=1, type=int,\n        help=optparse.SUPPRESS_HELP)\n    filesystem.add_option(\n        '--restrict-filenames',\n        action='store_true', dest='restrictfilenames', default=False,\n        help='Restrict filenames to only ASCII characters, and avoid \"&\" and spaces in filenames')\n    filesystem.add_option(\n        '--no-restrict-filenames',\n        action='store_false', dest='restrictfilenames',\n        help='Allow Unicode characters, \"&\" and spaces in filenames (default)')\n    filesystem.add_option(\n        '--windows-filenames',\n        action='store_true', dest='windowsfilenames', default=False,\n        help='Force filenames to be Windows-compatible')\n    filesystem.add_option(\n        '--no-windows-filenames',\n        action='store_false', dest='windowsfilenames',\n        help='Make filenames Windows-compatible only if using Windows (default)')\n    filesystem.add_option(\n        '--trim-filenames', '--trim-file-names', metavar='LENGTH',\n        dest='trim_file_name', default=0, type=int,\n        help='Limit the filename length (excluding extension) to the specified number of characters')\n    filesystem.add_option(\n        '-w', '--no-overwrites',\n        action='store_false', dest='overwrites', default=None,\n        help='Do not overwrite any files')\n    filesystem.add_option(\n        '--force-overwrites', '--yes-overwrites',\n        action='store_true', dest='overwrites',\n        help='Overwrite all video and metadata files. This option includes --no-continue')\n    filesystem.add_option(\n        '--no-force-overwrites',\n        action='store_const', dest='overwrites', const=None,\n        help='Do not overwrite the video, but overwrite related files (default)')\n    filesystem.add_option(\n        '-c', '--continue',\n        action='store_true', dest='continue_dl', default=True,\n        help='Resume partially downloaded files/fragments (default)')\n    filesystem.add_option(\n        '--no-continue',\n        action='store_false', dest='continue_dl',\n        help=(\n            'Do not resume partially downloaded fragments. '\n            'If the file is not fragmented, restart download of the entire file'))\n    filesystem.add_option(\n        '--part',\n        action='store_false', dest='nopart', default=False,\n        help='Use .part files instead of writing directly into output file (default)')\n    filesystem.add_option(\n        '--no-part',\n        action='store_true', dest='nopart',\n        help='Do not use .part files - write directly into output file')\n    filesystem.add_option(\n        '--mtime',\n        action='store_true', dest='updatetime', default=True,\n        help='Use the Last-modified header to set the file modification time (default)')\n    filesystem.add_option(\n        '--no-mtime',\n        action='store_false', dest='updatetime',\n        help='Do not use the Last-modified header to set the file modification time')\n    filesystem.add_option(\n        '--write-description',\n        action='store_true', dest='writedescription', default=False,\n        help='Write video description to a .description file')\n    filesystem.add_option(\n        '--no-write-description',\n        action='store_false', dest='writedescription',\n        help='Do not write video description (default)')\n    filesystem.add_option(\n        '--write-info-json',\n        action='store_true', dest='writeinfojson', default=None,\n        help='Write video metadata to a .info.json file (this may contain personal information)')\n    filesystem.add_option(\n        '--no-write-info-json',\n        action='store_false', dest='writeinfojson',\n        help='Do not write video metadata (default)')\n    filesystem.add_option(\n        '--write-annotations',\n        action='store_true', dest='writeannotations', default=False,\n        help=optparse.SUPPRESS_HELP)\n    filesystem.add_option(\n        '--no-write-annotations',\n        action='store_false', dest='writeannotations',\n        help=optparse.SUPPRESS_HELP)\n    filesystem.add_option(\n        '--write-playlist-metafiles',\n        action='store_true', dest='allow_playlist_files', default=None,\n        help=(\n            'Write playlist metadata in addition to the video metadata '\n            'when using --write-info-json, --write-description etc. (default)'))\n    filesystem.add_option(\n        '--no-write-playlist-metafiles',\n        action='store_false', dest='allow_playlist_files',\n        help='Do not write playlist metadata when using --write-info-json, --write-description etc.')\n    filesystem.add_option(\n        '--clean-info-json', '--clean-infojson',\n        action='store_true', dest='clean_infojson', default=None,\n        help=(\n            'Remove some internal metadata such as filenames from the infojson (default)'))\n    filesystem.add_option(\n        '--no-clean-info-json', '--no-clean-infojson',\n        action='store_false', dest='clean_infojson',\n        help='Write all fields to the infojson')\n    filesystem.add_option(\n        '--write-comments', '--get-comments',\n        action='store_true', dest='getcomments', default=False,\n        help=(\n            'Retrieve video comments to be placed in the infojson. '\n            'The comments are fetched even without this option if the extraction is known to be quick (Alias: --get-comments)'))\n    filesystem.add_option(\n        '--no-write-comments', '--no-get-comments',\n        action='store_false', dest='getcomments',\n        help='Do not retrieve video comments unless the extraction is known to be quick (Alias: --no-get-comments)')\n    filesystem.add_option(\n        '--load-info-json', '--load-info',\n        dest='load_info_filename', metavar='FILE',\n        help='JSON file containing the video information (created with the \"--write-info-json\" option)')\n    filesystem.add_option(\n        '--cookies',\n        dest='cookiefile', metavar='FILE',\n        help='Netscape formatted file to read cookies from and dump cookie jar in')\n    filesystem.add_option(\n        '--no-cookies',\n        action='store_const', const=None, dest='cookiefile', metavar='FILE',\n        help='Do not read/dump cookies from/to file (default)')\n    filesystem.add_option(\n        '--cookies-from-browser',\n        dest='cookiesfrombrowser', metavar='BROWSER[+KEYRING][:PROFILE][::CONTAINER]',\n        help=(\n            'The name of the browser to load cookies from. '\n            f'Currently supported browsers are: {\", \".join(sorted(SUPPORTED_BROWSERS))}. '\n            'Optionally, the KEYRING used for decrypting Chromium cookies on Linux, '\n            'the name/path of the PROFILE to load cookies from, '\n            'and the CONTAINER name (if Firefox) (\"none\" for no container) '\n            'can be given with their respective separators. '\n            'By default, all containers of the most recently accessed profile are used. '\n            f'Currently supported keyrings are: {\", \".join(map(str.lower, sorted(SUPPORTED_KEYRINGS)))}'))\n    filesystem.add_option(\n        '--no-cookies-from-browser',\n        action='store_const', const=None, dest='cookiesfrombrowser',\n        help='Do not load cookies from browser (default)')\n    filesystem.add_option(\n        '--cache-dir', dest='cachedir', default=None, metavar='DIR',\n        help=(\n            'Location in the filesystem where yt-dlp can store some downloaded information '\n            '(such as client ids and signatures) permanently. By default ${XDG_CACHE_HOME}/yt-dlp'))\n    filesystem.add_option(\n        '--no-cache-dir', action='store_false', dest='cachedir',\n        help='Disable filesystem caching')\n    filesystem.add_option(\n        '--rm-cache-dir',\n        action='store_true', dest='rm_cachedir',\n        help='Delete all filesystem cache files')\n    thumbnail = optparse.OptionGroup(parser, 'Thumbnail Options')\n    thumbnail.add_option(\n        '--write-thumbnail',\n        action='callback', dest='writethumbnail', default=False,\n        callback=lambda option, _, __, parser: setattr(\n            parser.values, option.dest, getattr(parser.values, option.dest) or True),\n        help='Write thumbnail image to disk')\n    thumbnail.add_option(\n        '--no-write-thumbnail',\n        action='store_false', dest='writethumbnail',\n        help='Do not write thumbnail image to disk (default)')\n    thumbnail.add_option(\n        '--write-all-thumbnails',\n        action='store_const', dest='writethumbnail', const='all',\n        help='Write all thumbnail image formats to disk')\n    thumbnail.add_option(\n        '--list-thumbnails',\n        action='store_true', dest='list_thumbnails', default=False,\n        help='List available thumbnails of each video. Simulate unless --no-simulate is used')\n    link = optparse.OptionGroup(parser, 'Internet Shortcut Options')\n    link.add_option(\n        '--write-link',\n        action='store_true', dest='writelink', default=False,\n        help='Write an internet shortcut file, depending on the current platform (.url, .webloc or .desktop). The URL may be cached by the OS')\n    link.add_option(\n        '--write-url-link',\n        action='store_true', dest='writeurllink', default=False,\n        help='Write a .url Windows internet shortcut. The OS caches the URL based on the file path')\n    link.add_option(\n        '--write-webloc-link',\n        action='store_true', dest='writewebloclink', default=False,\n        help='Write a .webloc macOS internet shortcut')\n    link.add_option(\n        '--write-desktop-link',\n        action='store_true', dest='writedesktoplink', default=False,\n        help='Write a .desktop Linux internet shortcut')\n    postproc = optparse.OptionGroup(parser, 'Post-Processing Options')\n    postproc.add_option(\n        '-x', '--extract-audio',\n        action='store_true', dest='extractaudio', default=False,\n        help='Convert video files to audio-only files (requires ffmpeg and ffprobe)')\n    postproc.add_option(\n        '--audio-format', metavar='FORMAT', dest='audioformat', default='best',\n        help=(\n            'Format to convert the audio to when -x is used. '\n            f'(currently supported: best (default), {\", \".join(sorted(FFmpegExtractAudioPP.SUPPORTED_EXTS))}). '\n            'You can specify multiple rules using similar syntax as --remux-video'))\n    postproc.add_option(\n        '--audio-quality', metavar='QUALITY',\n        dest='audioquality', default='5',\n        help=(\n            'Specify ffmpeg audio quality to use when converting the audio with -x. '\n            'Insert a value between 0 (best) and 10 (worst) for VBR or a specific bitrate like 128K (default %default)'))\n    postproc.add_option(\n        '--remux-video',\n        metavar='FORMAT', dest='remuxvideo', default=None,\n        help=(\n            'Remux the video into another container if necessary '\n            f'(currently supported: {\", \".join(FFmpegVideoRemuxerPP.SUPPORTED_EXTS)}). '\n            'If target container does not support the video/audio codec, remuxing will fail. You can specify multiple rules; '\n            'e.g. \"aac>m4a/mov>mp4/mkv\" will remux aac to m4a, mov to mp4 and anything else to mkv'))\n    postproc.add_option(\n        '--recode-video',\n        metavar='FORMAT', dest='recodevideo', default=None,\n        help='Re-encode the video into another format if necessary. The syntax and supported formats are the same as --remux-video')\n    postproc.add_option(\n        '--postprocessor-args', '--ppa',\n        metavar='NAME:ARGS', dest='postprocessor_args', default={}, type='str',\n        action='callback', callback=_dict_from_options_callback,\n        callback_kwargs={\n            'allowed_keys': r'\\w+(?:\\+\\w+)?',\n            'default_key': 'default-compat',\n            'process': shlex.split,\n            'multiple_keys': False,\n        }, help=(\n            'Give these arguments to the postprocessors. '\n            'Specify the postprocessor/executable name and the arguments separated by a colon \":\" '\n            'to give the argument to the specified postprocessor/executable. Supported PP are: '\n            'Merger, ModifyChapters, SplitChapters, ExtractAudio, VideoRemuxer, VideoConvertor, '\n            'Metadata, EmbedSubtitle, EmbedThumbnail, SubtitlesConvertor, ThumbnailsConvertor, '\n            'FixupStretched, FixupM4a, FixupM3u8, FixupTimestamp and FixupDuration. '\n            'The supported executables are: AtomicParsley, FFmpeg and FFprobe. '\n            'You can also specify \"PP+EXE:ARGS\" to give the arguments to the specified executable '\n            'only when being used by the specified postprocessor. Additionally, for ffmpeg/ffprobe, '\n            '\"_i\"/\"_o\" can be appended to the prefix optionally followed by a number to pass the argument '\n            'before the specified input/output file, e.g. --ppa \"Merger+ffmpeg_i1:-v quiet\". '\n            'You can use this option multiple times to give different arguments to different '\n            'postprocessors. (Alias: --ppa)'))\n    postproc.add_option(\n        '-k', '--keep-video',\n        action='store_true', dest='keepvideo', default=False,\n        help='Keep the intermediate video file on disk after post-processing')\n    postproc.add_option(\n        '--no-keep-video',\n        action='store_false', dest='keepvideo',\n        help='Delete the intermediate video file after post-processing (default)')\n    postproc.add_option(\n        '--post-overwrites',\n        action='store_false', dest='nopostoverwrites',\n        help='Overwrite post-processed files (default)')\n    postproc.add_option(\n        '--no-post-overwrites',\n        action='store_true', dest='nopostoverwrites', default=False,\n        help='Do not overwrite post-processed files')\n    postproc.add_option(\n        '--embed-subs',\n        action='store_true', dest='embedsubtitles', default=False,\n        help='Embed subtitles in the video (only for mp4, webm and mkv videos)')\n    postproc.add_option(\n        '--no-embed-subs',\n        action='store_false', dest='embedsubtitles',\n        help='Do not embed subtitles (default)')\n    postproc.add_option(\n        '--embed-thumbnail',\n        action='store_true', dest='embedthumbnail', default=False,\n        help='Embed thumbnail in the video as cover art')\n    postproc.add_option(\n        '--no-embed-thumbnail',\n        action='store_false', dest='embedthumbnail',\n        help='Do not embed thumbnail (default)')\n    postproc.add_option(\n        '--embed-metadata', '--add-metadata',\n        action='store_true', dest='addmetadata', default=False,\n        help=(\n            'Embed metadata to the video file. Also embeds chapters/infojson if present '\n            'unless --no-embed-chapters/--no-embed-info-json are used (Alias: --add-metadata)'))\n    postproc.add_option(\n        '--no-embed-metadata', '--no-add-metadata',\n        action='store_false', dest='addmetadata',\n        help='Do not add metadata to file (default) (Alias: --no-add-metadata)')\n    postproc.add_option(\n        '--embed-chapters', '--add-chapters',\n        action='store_true', dest='addchapters', default=None,\n        help='Add chapter markers to the video file (Alias: --add-chapters)')\n    postproc.add_option(\n        '--no-embed-chapters', '--no-add-chapters',\n        action='store_false', dest='addchapters',\n        help='Do not add chapter markers (default) (Alias: --no-add-chapters)')\n    postproc.add_option(\n        '--embed-info-json',\n        action='store_true', dest='embed_infojson', default=None,\n        help='Embed the infojson as an attachment to mkv/mka video files')\n    postproc.add_option(\n        '--no-embed-info-json',\n        action='store_false', dest='embed_infojson',\n        help='Do not embed the infojson as an attachment to the video file')\n    postproc.add_option(\n        '--metadata-from-title',\n        metavar='FORMAT', dest='metafromtitle',\n        help=optparse.SUPPRESS_HELP)\n    postproc.add_option(\n        '--parse-metadata',\n        metavar='[WHEN:]FROM:TO', dest='parse_metadata', **when_prefix('pre_process'),\n        help=(\n            'Parse additional metadata like title/artist from other fields; see \"MODIFYING METADATA\" for details. '\n            'Supported values of \"WHEN\" are the same as that of --use-postprocessor (default: pre_process)'))\n    postproc.add_option(\n        '--replace-in-metadata',\n        dest='parse_metadata', metavar='[WHEN:]FIELDS REGEX REPLACE', nargs=3, **when_prefix('pre_process'),\n        help=(\n            'Replace text in a metadata field using the given regex. This option can be used multiple times. '\n            'Supported values of \"WHEN\" are the same as that of --use-postprocessor (default: pre_process)'))\n    postproc.add_option(\n        '--xattrs', '--xattr',\n        action='store_true', dest='xattrs', default=False,\n        help='Write metadata to the video file\\'s xattrs (using dublin core and xdg standards)')\n    postproc.add_option(\n        '--concat-playlist',\n        metavar='POLICY', dest='concat_playlist', default='multi_video',\n        choices=('never', 'always', 'multi_video'),\n        help=(\n            'Concatenate videos in a playlist. One of \"never\", \"always\", or '\n            '\"multi_video\" (default; only when the videos form a single show). '\n            'All the video files must have same codecs and number of streams to be concatable. '\n            'The \"pl_video:\" prefix can be used with \"--paths\" and \"--output\" to '\n            'set the output filename for the concatenated files. See \"OUTPUT TEMPLATE\" for details'))\n    postproc.add_option(\n        '--fixup',\n        metavar='POLICY', dest='fixup', default=None,\n        choices=('never', 'ignore', 'warn', 'detect_or_warn', 'force'),\n        help=(\n            'Automatically correct known faults of the file. '\n            'One of never (do nothing), warn (only emit a warning), '\n            'detect_or_warn (the default; fix file if we can, warn otherwise), '\n            'force (try fixing even if file already exists)'))\n    postproc.add_option(\n        '--prefer-avconv', '--no-prefer-ffmpeg',\n        action='store_false', dest='prefer_ffmpeg',\n        help=optparse.SUPPRESS_HELP)\n    postproc.add_option(\n        '--prefer-ffmpeg', '--no-prefer-avconv',\n        action='store_true', dest='prefer_ffmpeg', default=True,\n        help=optparse.SUPPRESS_HELP)\n    postproc.add_option(\n        '--ffmpeg-location', '--avconv-location', metavar='PATH',\n        dest='ffmpeg_location',\n        help='Location of the ffmpeg binary; either the path to the binary or its containing directory')\n    postproc.add_option(\n        '--exec',\n        metavar='[WHEN:]CMD', dest='exec_cmd', **when_prefix('after_move'),\n        help=(\n            'Execute a command, optionally prefixed with when to execute it, separated by a \":\". '\n            'Supported values of \"WHEN\" are the same as that of --use-postprocessor (default: after_move). '\n            'Same syntax as the output template can be used to pass any field as arguments to the command. '\n            'If no fields are passed, %(filepath,_filename|)q is appended to the end of the command. '\n            'This option can be used multiple times'))\n    postproc.add_option(\n        '--no-exec',\n        action='store_const', dest='exec_cmd', const={},\n        help='Remove any previously defined --exec')\n    postproc.add_option(\n        '--exec-before-download', metavar='CMD',\n        action='append', dest='exec_before_dl_cmd',\n        help=optparse.SUPPRESS_HELP)\n    postproc.add_option(\n        '--no-exec-before-download',\n        action='store_const', dest='exec_before_dl_cmd', const=None,\n        help=optparse.SUPPRESS_HELP)\n    postproc.add_option(\n        '--convert-subs', '--convert-sub', '--convert-subtitles',\n        metavar='FORMAT', dest='convertsubtitles', default=None,\n        help=(\n            'Convert the subtitles to another format (currently supported: {}) '\n            '(Alias: --convert-subtitles)'.format(', '.join(sorted(FFmpegSubtitlesConvertorPP.SUPPORTED_EXTS)))))\n    postproc.add_option(\n        '--convert-thumbnails',\n        metavar='FORMAT', dest='convertthumbnails', default=None,\n        help=(\n            'Convert the thumbnails to another format '\n            f'(currently supported: {\", \".join(sorted(FFmpegThumbnailsConvertorPP.SUPPORTED_EXTS))}). '\n            'You can specify multiple rules using similar syntax as --remux-video'))\n    postproc.add_option(\n        '--split-chapters', '--split-tracks',\n        dest='split_chapters', action='store_true', default=False,\n        help=(\n            'Split video into multiple files based on internal chapters. '\n            'The \"chapter:\" prefix can be used with \"--paths\" and \"--output\" to '\n            'set the output filename for the split files. See \"OUTPUT TEMPLATE\" for details'))\n    postproc.add_option(\n        '--no-split-chapters', '--no-split-tracks',\n        dest='split_chapters', action='store_false',\n        help='Do not split video based on chapters (default)')\n    postproc.add_option(\n        '--remove-chapters',\n        metavar='REGEX', dest='remove_chapters', action='append',\n        help=(\n            'Remove chapters whose title matches the given regular expression. '\n            'The syntax is the same as --download-sections. This option can be used multiple times'))\n    postproc.add_option(\n        '--no-remove-chapters', dest='remove_chapters', action='store_const', const=None,\n        help='Do not remove any chapters from the file (default)')\n    postproc.add_option(\n        '--force-keyframes-at-cuts',\n        action='store_true', dest='force_keyframes_at_cuts', default=False,\n        help=(\n            'Force keyframes at cuts when downloading/splitting/removing sections. '\n            'This is slow due to needing a re-encode, but the resulting video may have fewer artifacts around the cuts'))\n    postproc.add_option(\n        '--no-force-keyframes-at-cuts',\n        action='store_false', dest='force_keyframes_at_cuts',\n        help='Do not force keyframes around the chapters when cutting/splitting (default)')\n    _postprocessor_opts_parser = lambda key, val='': (\n        *(item.split('=', 1) for item in (val.split(';') if val else [])),\n        ('key', remove_end(key, 'PP')))\n    postproc.add_option(\n        '--use-postprocessor',\n        metavar='NAME[:ARGS]', dest='add_postprocessors', default=[], type='str',\n        action='callback', callback=_list_from_options_callback,\n        callback_kwargs={\n            'delim': None,\n            'process': lambda val: dict(_postprocessor_opts_parser(*val.split(':', 1))),\n        }, help=(\n            'The (case sensitive) name of plugin postprocessors to be enabled, '\n            'and (optionally) arguments to be passed to it, separated by a colon \":\". '\n            'ARGS are a semicolon \";\" delimited list of NAME=VALUE. '\n            'The \"when\" argument determines when the postprocessor is invoked. '\n            'It can be one of \"pre_process\" (after video extraction), \"after_filter\" (after video passes filter), '\n            '\"video\" (after --format; before --print/--output), \"before_dl\" (before each video download), '\n            '\"post_process\" (after each video download; default), '\n            '\"after_move\" (after moving video file to its final locations), '\n            '\"after_video\" (after downloading and processing all formats of a video), '\n            'or \"playlist\" (at end of playlist). '\n            'This option can be used multiple times to add different postprocessors'))\n    sponsorblock = optparse.OptionGroup(parser, 'SponsorBlock Options', description=(\n        'Make chapter entries for, or remove various segments (sponsor, introductions, etc.) '\n        'from downloaded YouTube videos using the SponsorBlock API (https://sponsor.ajay.app)'))\n    sponsorblock.add_option(\n        '--sponsorblock-mark', metavar='CATS',\n        dest='sponsorblock_mark', default=set(), action='callback', type='str',\n        callback=_set_from_options_callback, callback_kwargs={\n            'allowed_values': SponsorBlockPP.CATEGORIES.keys(),\n            'aliases': {'default': ['all']},\n        }, help=(\n            'SponsorBlock categories to create chapters for, separated by commas. '\n            f'Available categories are {\", \".join(SponsorBlockPP.CATEGORIES.keys())}, all and default (=all). '\n            'You can prefix the category with a \"-\" to exclude it. See [1] for description of the categories. '\n            'E.g. --sponsorblock-mark all,-preview [1] https://wiki.sponsor.ajay.app/w/Segment_Categories'))\n    sponsorblock.add_option(\n        '--sponsorblock-remove', metavar='CATS',\n        dest='sponsorblock_remove', default=set(), action='callback', type='str',\n        callback=_set_from_options_callback, callback_kwargs={\n            'allowed_values': set(SponsorBlockPP.CATEGORIES.keys()) - set(SponsorBlockPP.NON_SKIPPABLE_CATEGORIES.keys()),\n            'aliases': {'default': ['all', '-filler']},\n        }, help=(\n            'SponsorBlock categories to be removed from the video file, separated by commas. '\n            'If a category is present in both mark and remove, remove takes precedence. '\n            'The syntax and available categories are the same as for --sponsorblock-mark '\n            'except that \"default\" refers to \"all,-filler\" '\n            f'and {\", \".join(SponsorBlockPP.NON_SKIPPABLE_CATEGORIES.keys())} are not available'))\n    sponsorblock.add_option(\n        '--sponsorblock-chapter-title', metavar='TEMPLATE',\n        default=DEFAULT_SPONSORBLOCK_CHAPTER_TITLE, dest='sponsorblock_chapter_title',\n        help=(\n            'An output template for the title of the SponsorBlock chapters created by --sponsorblock-mark. '\n            'The only available fields are start_time, end_time, category, categories, name, category_names. '\n            'Defaults to \"%default\"'))\n    sponsorblock.add_option(\n        '--no-sponsorblock', default=False,\n        action='store_true', dest='no_sponsorblock',\n        help='Disable both --sponsorblock-mark and --sponsorblock-remove')\n    sponsorblock.add_option(\n        '--sponsorblock-api', metavar='URL',\n        default='https://sponsor.ajay.app', dest='sponsorblock_api',\n        help='SponsorBlock API location, defaults to %default')\n    sponsorblock.add_option(\n        '--sponskrub',\n        action='store_true', dest='sponskrub', default=False,\n        help=optparse.SUPPRESS_HELP)\n    sponsorblock.add_option(\n        '--no-sponskrub',\n        action='store_false', dest='sponskrub',\n        help=optparse.SUPPRESS_HELP)\n    sponsorblock.add_option(\n        '--sponskrub-cut', default=False,\n        action='store_true', dest='sponskrub_cut',\n        help=optparse.SUPPRESS_HELP)\n    sponsorblock.add_option(\n        '--no-sponskrub-cut',\n        action='store_false', dest='sponskrub_cut',\n        help=optparse.SUPPRESS_HELP)\n    sponsorblock.add_option(\n        '--sponskrub-force', default=False,\n        action='store_true', dest='sponskrub_force',\n        help=optparse.SUPPRESS_HELP)\n    sponsorblock.add_option(\n        '--no-sponskrub-force',\n        action='store_true', dest='sponskrub_force',\n        help=optparse.SUPPRESS_HELP)\n    sponsorblock.add_option(\n        '--sponskrub-location', metavar='PATH',\n        dest='sponskrub_path', default='',\n        help=optparse.SUPPRESS_HELP)\n    sponsorblock.add_option(\n        '--sponskrub-args', dest='sponskrub_args', metavar='ARGS',\n        help=optparse.SUPPRESS_HELP)\n    extractor = optparse.OptionGroup(parser, 'Extractor Options')\n    extractor.add_option(\n        '--extractor-retries',\n        dest='extractor_retries', metavar='RETRIES', default=3,\n        help='Number of retries for known extractor errors (default is %default), or \"infinite\"')\n    extractor.add_option(\n        '--allow-dynamic-mpd', '--no-ignore-dynamic-mpd',\n        action='store_true', dest='dynamic_mpd', default=True,\n        help='Process dynamic DASH manifests (default) (Alias: --no-ignore-dynamic-mpd)')\n    extractor.add_option(\n        '--ignore-dynamic-mpd', '--no-allow-dynamic-mpd',\n        action='store_false', dest='dynamic_mpd',\n        help='Do not process dynamic DASH manifests (Alias: --no-allow-dynamic-mpd)')\n    extractor.add_option(\n        '--hls-split-discontinuity',\n        dest='hls_split_discontinuity', action='store_true', default=False,\n        help='Split HLS playlists to different formats at discontinuities such as ad breaks',\n    )\n    extractor.add_option(\n        '--no-hls-split-discontinuity',\n        dest='hls_split_discontinuity', action='store_false',\n        help='Do not split HLS playlists to different formats at discontinuities such as ad breaks (default)')\n    _extractor_arg_parser = lambda key, vals='': (key.strip().lower().replace('-', '_'), [\n        val.replace(r'\\,', ',').strip() for val in re.split(r'(?<!\\\\),', vals)])\n    extractor.add_option(\n        '--extractor-args',\n        metavar='IE_KEY:ARGS', dest='extractor_args', default={}, type='str',\n        action='callback', callback=_dict_from_options_callback,\n        callback_kwargs={\n            'multiple_keys': False,\n            'process': lambda val: dict(\n                _extractor_arg_parser(*arg.split('=', 1)) for arg in val.split(';')),\n        }, help=(\n            'Pass ARGS arguments to the IE_KEY extractor. See \"EXTRACTOR ARGUMENTS\" for details. '\n            'You can use this option multiple times to give arguments for different extractors'))\n    extractor.add_option(\n        '--youtube-include-dash-manifest', '--no-youtube-skip-dash-manifest',\n        action='store_true', dest='youtube_include_dash_manifest', default=True,\n        help=optparse.SUPPRESS_HELP)\n    extractor.add_option(\n        '--youtube-skip-dash-manifest', '--no-youtube-include-dash-manifest',\n        action='store_false', dest='youtube_include_dash_manifest',\n        help=optparse.SUPPRESS_HELP)\n    extractor.add_option(\n        '--youtube-include-hls-manifest', '--no-youtube-skip-hls-manifest',\n        action='store_true', dest='youtube_include_hls_manifest', default=True,\n        help=optparse.SUPPRESS_HELP)\n    extractor.add_option(\n        '--youtube-skip-hls-manifest', '--no-youtube-include-hls-manifest',\n        action='store_false', dest='youtube_include_hls_manifest',\n        help=optparse.SUPPRESS_HELP)\n    parser.add_option_group(general)\n    parser.add_option_group(network)\n    parser.add_option_group(geo)\n    parser.add_option_group(selection)\n    parser.add_option_group(downloader)\n    parser.add_option_group(filesystem)\n    parser.add_option_group(thumbnail)\n    parser.add_option_group(link)\n    parser.add_option_group(verbosity)\n    parser.add_option_group(workarounds)\n    parser.add_option_group(video_format)\n    parser.add_option_group(subtitles)\n    parser.add_option_group(authentication)\n    parser.add_option_group(postproc)\n    parser.add_option_group(sponsorblock)\n    parser.add_option_group(extractor)\n    return parser",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-38519",
        "description": "[{'lang': 'en', 'value': '`yt-dlp` and `youtube-dl` are command-line audio/video downloaders. Prior to the fixed versions,\\xa0`yt-dlp` and `youtube-dl` do not limit the extensions of downloaded files, which could lead to arbitrary filenames being created in the download folder (and path traversal on Windows). Since `yt-dlp` and `youtube-dl` also read config from the working directory (and on Windows executables will be executed from the `yt-dlp` or `youtube-dl` directory), this could lead to arbitrary code being executed.\\n\\n\\n\\n\\n`yt-dlp` version 2024.07.01 fixes this issue by whitelisting the allowed extensions. `youtube-dl` fixes this issue in commit `d42a222` on the `master` branch and in nightly builds tagged 2024-07-03 or later. This might mean some very uncommon extensions might not get downloaded, however it will also limit the possible exploitation surface. In addition to upgrading, have `.%(ext)s` at the end of the output template and make sure the user trusts the websites that they are downloading from. Also, make sure to never download to a directory within PATH or other sensitive locations like one\\'s user directory, `system32`, or other binaries locations. For users who are not able to upgrade, keep the default output template (`-o \"%(title)s [%(id)s].%(ext)s`); make sure the extension of the media to download is a common video/audio/sub/... one; try to avoid the generic extractor; and/or use `--ignore-config --config-location ...` to not load config from common locations.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-70",
      "code": "    def secret_set(\n        self,\n        id: str,\n        *,\n        content: Optional[Dict[str, str]] = None,\n        label: Optional[str] = None,\n        description: Optional[str] = None,\n        expire: Optional[datetime.datetime] = None,\n        rotate: Optional[SecretRotate] = None,\n    ):\n        args = [id]\n        if label is not None:\n            args.extend(['--label', label])\n        if description is not None:\n            args.extend(['--description', description])\n        if expire is not None:\n            args.extend(['--expire', expire.isoformat()])\n        if rotate is not None:\n            args += ['--rotate', rotate.value]\n        if content is not None:\n            for k, v in content.items():\n                args.append(f'{k}={v}')\n        self._run_for_secret('secret-set', *args)\n    def secret_add(\n        self,\n        content: Dict[str, str],\n        *,\n        label: Optional[str] = None,\n        description: Optional[str] = None,\n        expire: Optional[datetime.datetime] = None,\n        rotate: Optional[SecretRotate] = None,\n        owner: Optional[str] = None,\n    ) -> str:\n        args: List[str] = []\n        if label is not None:\n            args.extend(['--label', label])\n        if description is not None:\n            args.extend(['--description', description])\n        if expire is not None:\n            args.extend(['--expire', expire.isoformat()])\n        if rotate is not None:\n            args += ['--rotate', rotate.value]\n        if owner is not None:\n            args += ['--owner', owner]\n        for k, v in content.items():\n            args.append(f'{k}={v}')\n        result = self._run('secret-add', *args, return_output=True)\n        secret_id = typing.cast(str, result)\n        return secret_id.strip()\n    def secret_grant(self, id: str, relation_id: int, *, unit: Optional[str] = None):\n        args = [id, '--relation', str(relation_id)]\n        if unit is not None:\n            args += ['--unit', str(unit)]\n        self._run_for_secret('secret-grant', *args)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-41129",
        "description": "[{'lang': 'en', 'value': 'The ops library is a Python framework for developing and testing Kubernetes and machine charms. The issue here is that ops passes the secret content as one of the args via CLI. This issue may affect any of the charms that are using: Juju (>=3.0), Juju secrets and not correctly capturing and processing `subprocess.CalledProcessError`. This vulnerability is fixed in 2.15.0.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-71",
      "code": "def main():\n    (opts, dummy) = parseArgs()\n    if not os.path.exists(opts.destdir) and not opts.urls:\n        try:\n            os.makedirs(opts.destdir)\n        except OSError, e:\n            print >> sys.stderr, _(\"Error: Cannot create destination dir %s\") % opts.destdir\n            sys.exit(1)\n    if not os.access(opts.destdir, os.W_OK) and not opts.urls:\n        print >> sys.stderr, _(\"Error: Cannot write to  destination dir %s\") % opts.destdir\n        sys.exit(1)\n    my = RepoSync(opts=opts)\n    my.doConfigSetup(fn=opts.config, init_plugins=opts.plugins)\n    if os.getuid() != 0 and not opts.cachedir:\n        opts.tempcache = True\n    if opts.tempcache:\n        if not my.setCacheDir(force=True, reuse=False):\n            print >> sys.stderr, _(\"Error: Could not make cachedir, exiting\")\n            sys.exit(50)\n        my.conf.uid = 1\n    elif opts.cachedir:\n        my.repos.setCacheDir(opts.cachedir)\n    if not opts.cachedir:\n        try:\n            my.doLock()\n        except yum.Errors.LockError, e:\n            print >> sys.stderr, _(\"Error: %s\") % e\n            sys.exit(50)\n    if not opts.quiet:\n        my.repos.setProgressBar(TextMeter(fo=sys.stdout), TextMultiFileMeter(fo=sys.stdout))\n    my.doRepoSetup()\n    if len(opts.repoid) > 0:\n        myrepos = []\n        for glob in opts.repoid:\n            add_repos = my.repos.findRepos(glob)\n            if not add_repos:\n                print >> sys.stderr, _(\"Warning: cannot find repository %s\") % glob\n                continue\n            myrepos.extend(add_repos)\n        if not myrepos:\n            print >> sys.stderr, _(\"No repositories found\")\n            sys.exit(1)\n        for repo in my.repos.repos.values():\n            repo.disable()\n        for repo in myrepos:\n            repo.enable()\n    if len(my.repos.listEnabled()) > 1 and opts.norepopath:\n        print >> sys.stderr, _(\"Error: Can't use --norepopath with multiple repositories\")\n        sys.exit(1)\n    try:\n        arches = rpmUtils.arch.getArchList(opts.arch)\n        if opts.source:\n            arches += ['src']\n        my.doSackSetup(arches)\n    except yum.Errors.RepoError, e:\n        print >> sys.stderr, _(\"Error setting up repositories: %s\") % e\n        sys.exit(1)\n    exit_code = 0\n    for repo in my.repos.listEnabled():\n        reposack = ListPackageSack(my.pkgSack.returnPackages(repoid=repo.id))\n        if opts.newest:\n            download_list = reposack.returnNewestByNameArch()\n        else:\n            download_list = list(reposack)\n        if opts.norepopath:\n            local_repo_path = opts.destdir\n        else:\n            local_repo_path = opts.destdir + '/' + repo.id\n        if opts.delete and os.path.exists(local_repo_path):\n            current_pkgs = localpkgs(local_repo_path)\n            download_set = {}\n            for pkg in download_list:\n                remote = pkg.returnSimple('relativepath')\n                rpmname = os.path.basename(remote)\n                download_set[rpmname] = 1\n            for pkg in current_pkgs:\n                if pkg in download_set:\n                    continue\n                if not opts.quiet:\n                    my.logger.info(\"Removing obsolete %s\", pkg)\n                os.unlink(current_pkgs[pkg]['path'])\n        if opts.downloadcomps or opts.downloadmd:\n            if not os.path.exists(local_repo_path):\n                try:\n                    os.makedirs(local_repo_path)\n                except IOError, e:\n                    my.logger.error(\"Could not make repo subdir: %s\" % e)\n                    my.closeRpmDB()\n                    sys.exit(1)\n            if opts.downloadcomps:\n                wanted_types = ['group']\n            if opts.downloadmd:\n                wanted_types = repo.repoXML.fileTypes()\n            for ftype in repo.repoXML.fileTypes():\n                if ftype in ['primary', 'primary_db', 'filelists',\n                             'filelists_db', 'other', 'other_db']:\n                    continue\n                if ftype not in wanted_types:\n                    continue\n                try:\n                    resultfile = repo.retrieveMD(ftype)\n                    basename = os.path.basename(resultfile)\n                    if ftype == 'group' and opts.downloadcomps:\n                        basename = 'comps.xml'\n                    shutil.copyfile(resultfile, \"%s/%s\" % (local_repo_path, basename))\n                except yum.Errors.RepoMDError, e:\n                    if not opts.quiet:\n                        my.logger.error(\"Unable to fetch metadata: %s\" % e)\n        remote_size = 0\n        if not opts.urls:\n            for pkg in download_list:\n                remote = pkg.returnSimple('relativepath')\n                local = local_repo_path + '/' + remote\n                sz = int(pkg.returnSimple('packagesize'))\n                if os.path.exists(local) and os.path.getsize(local) == sz:\n                    continue\n                remote_size += sz\n        if hasattr(urlgrabber.progress, 'text_meter_total_size'):\n            urlgrabber.progress.text_meter_total_size(remote_size)\n        download_list.sort(key=lambda pkg: pkg.name)\n        if opts.urls:\n            for pkg in download_list:\n                remote = pkg.returnSimple('relativepath')\n                local = os.path.join(local_repo_path, remote)\n                if not (os.path.exists(local) and my.verifyPkg(local, pkg, False)):\n                    print urljoin(pkg.repo.urls[0], pkg.relativepath)\n            continue\n        if not os.path.exists(local_repo_path):\n            os.makedirs(local_repo_path)\n        for pkg in download_list:\n            rpmfn = pkg.remote_path\n            pkg.localpath = os.path.join(local_repo_path, rpmfn)\n            pkg.repo.copy_local = True\n            pkg.repo.cache = 0\n            localdir = os.path.dirname(pkg.localpath)\n            if not os.path.exists(localdir):\n                os.makedirs(localdir)\n        probs = my.downloadPkgs(download_list)\n        if probs:\n            exit_code = 1\n            for key in probs:\n                for error in probs[key]:\n                    my.logger.error('%s: %s', key, error)\n        if opts.gpgcheck:\n            for pkg in download_list:\n                result, error = my.sigCheckPkg(pkg)\n                if result != 0:\n                    rpmfn = os.path.basename(pkg.remote_path)\n                    if result == 1:\n                        my.logger.warning('Removing %s, due to missing GPG key.' % rpmfn)\n                    elif result == 2:\n                        my.logger.warning('Removing %s due to failed signature check.' % rpmfn)\n                    else:\n                        my.logger.warning('Removing %s due to failed signature check: %s' % rpmfn)\n                    os.unlink(pkg.localpath)\n                    exit_code = 1\n                    continue\n    my.closeRpmDB()\n    sys.exit(exit_code)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2018-10897",
        "description": "[{'lang': 'en', 'value': 'A directory traversal issue was found in reposync, a part of yum-utils, where reposync fails to sanitize paths in remote repository configuration files. If an attacker controls a repository, they may be able to copy files outside of the destination directory on the targeted system via path traversal. If reposync is running with heightened privileges on a targeted system, this flaw could potentially result in system compromise via the overwriting of critical system files. Version 1.1.31 and older are believed to be affected.'}]",
        "cwe_number": 59
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-72",
      "code": "def whitelist(allow_guest=False, xss_safe=False, methods=None):\n\t\"\"\"\n\tDecorator for whitelisting a function and making it accessible via HTTP.\n\tStandard request will be `/api/method/[path.to.method]`\n\t:param allow_guest: Allow non logged-in user to access this method.\n\t:param methods: Allowed http method to access the method.\n\tUse as:\n\t\t@frappe.whitelist()\n\t\tdef myfunc(param1, param2):\n\t\t\tpass\n\t\"\"\"\n\tif not methods:\n\t\tmethods = ['GET', 'POST', 'PUT', 'DELETE']\n\tdef innerfn(fn):\n\t\tglobal whitelisted, guest_methods, xss_safe_methods, allowed_http_methods_for_whitelisted_func\n\t\twhitelisted.append(fn)\n\t\tallowed_http_methods_for_whitelisted_func[fn] = methods\n\t\tif allow_guest:\n\t\t\tguest_methods.append(fn)\n\t\t\tif xss_safe:\n\t\t\t\txss_safe_methods.append(fn)\n\t\treturn fn\n\treturn innerfn\ndef execute_cmd(cmd, from_async=False):\n\t\"\"\"execute a request as python module\"\"\"\n\tfor hook in frappe.get_hooks(\"override_whitelisted_methods\", {}).get(cmd, []):\n\t\tcmd = hook\n\t\tbreak\n\tif run_server_script_api(cmd):\n\t\treturn None\n\ttry:\n\t\tmethod = get_attr(cmd)\n\texcept Exception as e:\n\t\tif frappe.local.conf.developer_mode:\n\t\t\traise e\n\t\telse:\n\t\t\tfrappe.respond_as_web_page(title='Invalid Method', html='Method not found',\n\t\t\tindicator_color='red', http_status_code=404)\n\t\treturn\n\tif from_async:\n\t\tmethod = method.queue\n\tis_whitelisted(method)\n\tis_valid_http_method(method)\n\treturn frappe.call(method, **frappe.form_dict)\ndef is_whitelisted(method):\n\tif frappe.session['user'] == 'Guest':\n\t\tif (method not in frappe.guest_methods):\n\t\t\tfrappe.throw(_(\"Not permitted\"), frappe.PermissionError)\n\t\tif method not in frappe.xss_safe_methods:\n\t\t\tfor key, value in frappe.form_dict.items():\n\t\t\t\tif isinstance(value, string_types):\n\t\t\t\t\tfrappe.form_dict[key] = frappe.utils.sanitize_html(value)\n\telse:\n\t\tif not method in frappe.whitelisted:\n\t\t\tfrappe.throw(_(\"Not permitted\"), frappe.PermissionError)\ndef runserverobj(method, docs=None, dt=None, dn=None, arg=None, args=None):\n\tfrappe.desk.form.run_method.runserverobj(method, docs=docs, dt=dt, dn=dn, arg=arg, args=args)\ndef run_custom_method(doctype, name, custom_method):\n\t\"\"\"cmd=run_custom_method&doctype={doctype}&name={name}&custom_method={custom_method}\"\"\"\n\tdoc = frappe.get_doc(doctype, name)\n\tif getattr(doc, custom_method, frappe._dict()).is_whitelisted:\n\t\tfrappe.call(getattr(doc, custom_method), **frappe.local.form_dict)\n\telse:\n\t\tfrappe.throw(_(\"Not permitted\"), frappe.PermissionError)\ndef validate_and_sanitize_search_inputs(fn, instance, args, kwargs):\n\tkwargs.update(dict(zip(fn.__code__.co_varnames, args)))\n\tsanitize_searchfield(kwargs['searchfield'])\n\tkwargs['start'] = cint(kwargs['start'])\n\tkwargs['page_len'] = cint(kwargs['page_len'])\n\tif kwargs['doctype'] and not frappe.db.exists('DocType', kwargs['doctype']):\n\t\treturn []\n\treturn fn(**kwargs)\n\tdef whitelist(f):\n\t\t\"\"\"Decorator: Whitelist method to be called remotely via REST API.\"\"\"\n\t\tf.whitelisted = True\n\t\treturn f\n\tdef is_whitelisted(self, method):\n\t\tfn = getattr(self, method, None)\n\t\tif not fn:\n\t\t\traise NotFound(\"Method {0} not found\".format(method))\n\t\telif not getattr(fn, \"whitelisted\", False):\n\t\t\traise Forbidden(\"Method {0} not whitelisted\".format(method))",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-23058",
        "description": "[{'lang': 'en', 'value': 'ERPNext in versions v12.0.9-v13.0.3 are affected by a stored XSS vulnerability that allows low privileged users to store malicious scripts in the \u2018username\u2019 field in \u2018my settings\u2019 which can lead to full account takeover.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-73",
      "code": "    def check_version(self, version):\n        \"\"\"\n        Determine the client's version and decide whether to continue the\n        handshake.\n        \"\"\"\n        if version == self.VERSION:\n            log.msg(\"Client version %s is valid\" % version.strip())\n            self.transport.write(\"\\x02\\x01\\x02\")\n            return self.select_security_type, 1\n        else:\n            log.err(\"Can't handle VNC version %r\" % version)\n            self.transport.loseConnection()\n    def select_security_type(self, security_type):\n        \"\"\"\n        Choose the security type that the client wants.\n        \"\"\"\n        security_type = ord(security_type)\n        if security_type == 2:\n            self.challenge = urandom(16)\n            self.transport.write(self.challenge)\n            return self.vnc_authentication_result, 16\n        elif security_type == 1:\n            self.authenticated()\n        else:\n            log.err(\"Couldn't agree on an authentication scheme!\")\n            self.transport.loseConnection()",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-36436",
        "description": "[{'lang': 'en', 'value': 'OSU Open Source Lab VNCAuthProxy through 1.1.1 is affected by an vncap/vnc/protocol.py VNCServerAuthenticator authentication-bypass vulnerability that could allow a malicious actor to gain unauthorized access to a VNC session or to disconnect a legitimate user from a VNC session. A remote attacker with network access to the proxy server could leverage this vulnerability to connect to VNC servers protected by the proxy server without providing any authentication credentials. Exploitation of this issue requires that the proxy server is currently accepting connections for the target VNC server.'}]",
        "cwe_number": 287
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-74",
      "code": "    def __init__(self, method, uri, headers, bodyProducer, persistent=False):\n        \"\"\"\n        @param method: The HTTP method for this request, ex: b'GET', b'HEAD',\n            b'POST', etc.\n        @type method: L{bytes}\n        @param uri: The relative URI of the resource to request.  For example,\n            C{b'/foo/bar?baz=quux'}.\n        @type uri: L{bytes}\n        @param headers: Headers to be sent to the server.  It is important to\n            note that this object does not create any implicit headers.  So it\n            is up to the HTTP Client to add required headers such as 'Host'.\n        @type headers: L{twisted.web.http_headers.Headers}\n        @param bodyProducer: L{None} or an L{IBodyProducer} provider which\n            produces the content body to send to the remote HTTP server.\n        @param persistent: Set to C{True} when you use HTTP persistent\n            connection, defaults to C{False}.\n        @type persistent: L{bool}\n        \"\"\"\n        self.method = method\n        self.uri = uri\n        self.headers = headers\n        self.bodyProducer = bodyProducer\n        self.persistent = persistent\n        self._parsedURI = None\n    def _construct(cls, method, uri, headers, bodyProducer, persistent=False,\n                   parsedURI=None):\n        \"\"\"\n        Private constructor.\n        @param method: See L{__init__}.\n        @param uri: See L{__init__}.\n        @param headers: See L{__init__}.\n        @param bodyProducer: See L{__init__}.\n        @param persistent: See L{__init__}.\n        @param parsedURI: See L{Request._parsedURI}.\n        @return: L{Request} instance.\n        \"\"\"\n        request = cls(method, uri, headers, bodyProducer, persistent)\n        request._parsedURI = parsedURI\n        return request\n    def _writeHeaders(self, transport, TEorCL):\n        hosts = self.headers.getRawHeaders(b'host', ())\n        if len(hosts) != 1:\n            raise BadHeaders(u\"Exactly one Host header required\")\n        requestLines = []\n        requestLines.append(b' '.join([self.method, self.uri,\n            b'HTTP/1.1\\r\\n']))\n        if not self.persistent:\n            requestLines.append(b'Connection: close\\r\\n')\n        if TEorCL is not None:\n            requestLines.append(TEorCL)\n        for name, values in self.headers.getAllRawHeaders():\n            requestLines.extend([name + b': ' + v + b'\\r\\n' for v in values])\n        requestLines.append(b'\\r\\n')\n        transport.writeSequence(requestLines)\n    def _writeToBodyProducerChunked(self, transport):\n        \"\"\"\n        Write this request to the given transport using chunked\n        transfer-encoding to frame the body.\n        @param transport: See L{writeTo}.\n        @return: See L{writeTo}.\n        \"\"\"\n        self._writeHeaders(transport, b'Transfer-Encoding: chunked\\r\\n')\n        encoder = ChunkedEncoder(transport)\n        encoder.registerProducer(self.bodyProducer, True)\n        d = self.bodyProducer.startProducing(encoder)\n        def cbProduced(ignored):\n            encoder.unregisterProducer()\n        def ebProduced(err):\n            encoder._allowNoMoreWrites()\n            transport.unregisterProducer()\n            return err\n        d.addCallbacks(cbProduced, ebProduced)\n        return d\n    def _writeToBodyProducerContentLength(self, transport):\n        \"\"\"\n        Write this request to the given transport using content-length to frame\n        the body.\n        @param transport: See L{writeTo}.\n        @return: See L{writeTo}.\n        \"\"\"\n        self._writeHeaders(\n            transport,\n            networkString(\n                'Content-Length: %d\\r\\n' % (self.bodyProducer.length,)))\n        finishedConsuming = Deferred()\n        encoder = LengthEnforcingConsumer(\n            self.bodyProducer, transport, finishedConsuming)\n        transport.registerProducer(self.bodyProducer, True)\n        finishedProducing = self.bodyProducer.startProducing(encoder)\n        def combine(consuming, producing):\n            def cancelConsuming(ign):\n                finishedProducing.cancel()\n            ultimate = Deferred(cancelConsuming)\n            state = [None]\n            def ebConsuming(err):\n                if state == [None]:\n                    state[0] = 1\n                    ultimate.errback(err)\n                else:\n                    self._log.failure(\n                        u\"Buggy state machine in {request}/[{state}]: \"\n                        u\"ebConsuming called\",\n                        failure=err,\n                        request=repr(self),\n                        state=state[0]\n                    )\n            def cbProducing(result):\n                if state == [None]:\n                    state[0] = 2\n                    try:\n                        encoder._noMoreWritesExpected()\n                    except:\n                        ultimate.errback()\n                    else:\n                        ultimate.callback(None)\n            def ebProducing(err):\n                if state == [None]:\n                    state[0] = 3\n                    encoder._allowNoMoreWrites()\n                    ultimate.errback(err)\n                else:\n                    self._log.failure(u\"Producer is buggy\", failure=err)\n            consuming.addErrback(ebConsuming)\n            producing.addCallbacks(cbProducing, ebProducing)\n            return ultimate\n        d = combine(finishedConsuming, finishedProducing)\n        def f(passthrough):\n            transport.unregisterProducer()\n            return passthrough\n        d.addBoth(f)\n        return d\n    def connectionMade(self):\n        method = getattr(self.factory, 'method', b'GET')\n        self.sendCommand(method, self.factory.path)\n        if self.factory.scheme == b'http' and self.factory.port != 80:\n            host = self.factory.host + b':' + intToBytes(self.factory.port)\n        elif self.factory.scheme == b'https' and self.factory.port != 443:\n            host = self.factory.host + b':' + intToBytes(self.factory.port)\n        else:\n            host = self.factory.host\n        self.sendHeader(b'Host', self.factory.headers.get(b\"host\", host))\n        self.sendHeader(b'User-Agent', self.factory.agent)\n        data = getattr(self.factory, 'postdata', None)\n        if data is not None:\n            self.sendHeader(b\"Content-Length\", intToBytes(len(data)))\n        cookieData = []\n        for (key, value) in self.factory.headers.items():\n            if key.lower() not in self._specialHeaders:\n                self.sendHeader(key, value)\n            if key.lower() == b'cookie':\n                cookieData.append(value)\n        for cookie, cookval in self.factory.cookies.items():\n            cookieData.append(cookie + b'=' + cookval)\n        if cookieData:\n            self.sendHeader(b'Cookie', b'; '.join(cookieData))\n        self.endHeaders()\n        self.headers = {}\n        if data is not None:\n            self.transport.write(data)\n    def handleHeader(self, key, value):\n        \"\"\"\n        Called every time a header is received. Stores the header information\n        as key-value pairs in the C{headers} attribute.\n        @type key: C{str}\n        @param key: An HTTP header field name.\n        @type value: C{str}\n        @param value: An HTTP header field value.\n        \"\"\"\n        key = key.lower()\n        l = self.headers.setdefault(key, [])\n        l.append(value)\n    def __init__(self, url, method=b'GET', postdata=None, headers=None,\n                 agent=b\"Twisted PageGetter\", timeout=0, cookies=None,\n                 followRedirect=True, redirectLimit=20,\n                 afterFoundGet=False):\n        self.followRedirect = followRedirect\n        self.redirectLimit = redirectLimit\n        self._redirectCount = 0\n        self.timeout = timeout\n        self.agent = agent\n        self.afterFoundGet = afterFoundGet\n        if cookies is None:\n            cookies = {}\n        self.cookies = cookies\n        if headers is not None:\n            self.headers = InsensitiveDict(headers)\n        else:\n            self.headers = InsensitiveDict()\n        if postdata is not None:\n            self.headers.setdefault(b'Content-Length',\n                                    intToBytes(len(postdata)))\n            self.headers.setdefault(b\"connection\", b\"close\")\n        self.postdata = postdata\n        self.method = method\n        self.setURL(url)\n        self.waiting = 1\n        self._disconnectedDeferred = defer.Deferred()\n        self.deferred = defer.Deferred()\n        self.deferred.addBoth(self._waitForDisconnect)\n        self.response_headers = None\n    def _waitForDisconnect(self, passthrough):\n        \"\"\"\n        Chain onto the _disconnectedDeferred, preserving C{passthrough}, so that\n        the result is only available after the associated connection has been\n        closed.\n        \"\"\"\n        self._disconnectedDeferred.addCallback(lambda ignored: passthrough)\n        return self._disconnectedDeferred\n    def setURL(self, url):\n        self.url = url\n        uri = URI.fromBytes(url)\n        if uri.scheme and uri.host:\n            self.scheme = uri.scheme\n            self.host = uri.host\n            self.port = uri.port\n        self.path = uri.originForm\n    def buildProtocol(self, addr):\n        p = protocol.ClientFactory.buildProtocol(self, addr)\n        p.followRedirect = self.followRedirect\n        p.afterFoundGet = self.afterFoundGet\n        if self.timeout:\n            timeoutCall = reactor.callLater(self.timeout, p.timeout)\n            self.deferred.addBoth(self._cancelTimeout, timeoutCall)\n        return p\ndef _makeGetterFactory(url, factoryFactory, contextFactory=None,\n                       *args, **kwargs):\n    \"\"\"\n    Create and connect an HTTP page getting factory.\n    Any additional positional or keyword arguments are used when calling\n    C{factoryFactory}.\n    @param factoryFactory: Factory factory that is called with C{url}, C{args}\n        and C{kwargs} to produce the getter\n    @param contextFactory: Context factory to use when creating a secure\n        connection, defaulting to L{None}\n    @return: The factory created by C{factoryFactory}\n    \"\"\"\n    uri = URI.fromBytes(url)\n    factory = factoryFactory(url, *args, **kwargs)\n    if uri.scheme == b'https':\n        from twisted.internet import ssl\n        if contextFactory is None:\n            contextFactory = ssl.ClientContextFactory()\n        reactor.connectSSL(\n            nativeString(uri.host), uri.port, factory, contextFactory)\n    else:\n        reactor.connectTCP(nativeString(uri.host), uri.port, factory)\n    return factory\n_GETPAGE_REPLACEMENT_TEXT = \"https://pypi.org/project/treq/ or twisted.web.client.Agent\"\ndef _deprecateGetPageClasses():\n    \"\"\"\n    Mark the protocols and factories associated with L{getPage} and\n    L{downloadPage} as deprecated.\n    \"\"\"\n    for klass in [\n        HTTPPageGetter, HTTPPageDownloader,\n        HTTPClientFactory, HTTPDownloader\n    ]:\n        deprecatedModuleAttribute(\n            Version(\"Twisted\", 16, 7, 0),\n            getDeprecationWarningString(\n                klass,\n                Version(\"Twisted\", 16, 7, 0),\n                replacement=_GETPAGE_REPLACEMENT_TEXT)\n            .split(\"; \")[1],\n            klass.__module__,\n            klass.__name__)\n    def _requestWithEndpoint(self, key, endpoint, method, parsedURI,\n                             headers, bodyProducer, requestPath):\n        \"\"\"\n        Issue a new request, given the endpoint and the path sent as part of\n        the request.\n        \"\"\"\n        if not isinstance(method, bytes):\n            raise TypeError('method={!r} is {}, but must be bytes'.format(\n                    method, type(method)))\n        if headers is None:\n            headers = Headers()\n        if not headers.hasHeader(b'host'):\n            headers = headers.copy()\n            headers.addRawHeader(\n                b'host', self._computeHostValue(parsedURI.scheme,\n                                                parsedURI.host,\n                                                parsedURI.port))\n        d = self._pool.getConnection(key, endpoint)\n        def cbConnected(proto):\n            return proto.request(\n                Request._construct(method, requestPath, headers, bodyProducer,\n                                   persistent=self._pool.persistent,\n                                   parsedURI=parsedURI))\n        d.addCallback(cbConnected)\n        return d\nclass _StandardEndpointFactory(object):\n    def request(self, method, uri, headers=None, bodyProducer=None):\n        \"\"\"\n        Issue a request to the server indicated by the given C{uri}.\n        An existing connection from the connection pool may be used or a new\n        one may be created.\n        I{HTTP} and I{HTTPS} schemes are supported in C{uri}.\n        @see: L{twisted.web.iweb.IAgent.request}\n        \"\"\"\n        parsedURI = URI.fromBytes(uri)\n        try:\n            endpoint = self._getEndpoint(parsedURI)\n        except SchemeNotSupported:\n            return defer.fail(Failure())\n        key = (parsedURI.scheme, parsedURI.host, parsedURI.port)\n        return self._requestWithEndpoint(key, endpoint, method, parsedURI,\n                                         headers, bodyProducer,\n                                         parsedURI.originForm)\nclass ProxyAgent(_AgentBase):",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2019-12387",
        "description": "[{'lang': 'en', 'value': 'In Twisted before 19.2.1, twisted.web did not validate or sanitize URIs or HTTP methods, allowing an attacker to inject invalid characters such as CRLF.'}]",
        "cwe_number": 74
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-75",
      "code": "def _dynarray_make_setter(dst, src):\n    assert isinstance(src.typ, DArrayType)\n    assert isinstance(dst.typ, DArrayType)\n    if src.value == \"~empty\":\n        return IRnode.from_list(STORE(dst, 0))\n    if src.value == \"multi\":\n        ret = [\"seq\"]\n        store_length = STORE(dst, len(src.args))\n        ann = None\n        if src.annotation is not None:\n            ann = f\"len({src.annotation})\"\n        store_length = IRnode.from_list(store_length, annotation=ann)\n        ret.append(store_length)\n        n_items = len(src.args)\n        for i in range(n_items):\n            k = IRnode.from_list(i, typ=\"uint256\")\n            dst_i = get_element_ptr(dst, k, array_bounds_check=False)\n            src_i = get_element_ptr(src, k, array_bounds_check=False)\n            ret.append(make_setter(dst_i, src_i))\n        return ret\n    with src.cache_when_complex(\"darray_src\") as (b1, src):\n        should_loop = (\n            src.encoding in (Encoding.ABI, Encoding.JSON_ABI)\n            and src.typ.subtype.abi_type.is_dynamic()\n        )\n        should_loop |= src.typ.subtype.abi_type.is_dynamic()\n        should_loop |= needs_clamp(src.typ.subtype, src.encoding)\n        with get_dyn_array_count(src).cache_when_complex(\"darray_count\") as (b2, count):\n            ret = [\"seq\"]\n            ret.append(STORE(dst, count))\n            if should_loop:\n                i = IRnode.from_list(_freshname(\"copy_darray_ix\"), typ=\"uint256\")\n                loop_body = make_setter(\n                    get_element_ptr(dst, i, array_bounds_check=False),\n                    get_element_ptr(src, i, array_bounds_check=False),\n                )\n                loop_body.annotation = f\"{dst}[i] = {src}[i]\"\n                ret.append([\"repeat\", i, 0, count, src.typ.count, loop_body])\n            else:\n                element_size = src.typ.subtype.memory_bytes_required\n                n_bytes = _mul(count, element_size)\n                max_bytes = src.typ.count * element_size\n                src_ = dynarray_data_ptr(src)\n                dst_ = dynarray_data_ptr(dst)\n                ret.append(copy_bytes(dst_, src_, n_bytes, max_bytes))\n            return b1.resolve(b2.resolve(ret))\ndef _get_element_ptr_tuplelike(parent, key):\n    typ = parent.typ\n    assert isinstance(typ, TupleLike)\n    if isinstance(typ, StructType):\n        assert isinstance(key, str)\n        subtype = typ.members[key]\n        attrs = list(typ.tuple_keys())\n        index = attrs.index(key)\n        annotation = key\n    else:\n        assert isinstance(key, int)\n        subtype = typ.members[key]\n        attrs = list(range(len(typ.members)))\n        index = key\n        annotation = None\n    if parent.value == \"~empty\":\n        return IRnode.from_list(\"~empty\", typ=subtype)\n    if parent.value == \"multi\":\n        assert parent.encoding != Encoding.ABI, \"no abi-encoded literals\"\n        return parent.args[index]\n    ofst = 0\n    if parent.encoding in (Encoding.ABI, Encoding.JSON_ABI):\n        if parent.location == STORAGE:\n            raise CompilerPanic(\"storage variables should not be abi encoded\")\n        member_t = typ.members[attrs[index]]\n        for i in range(index):\n            member_abi_t = typ.members[attrs[i]].abi_type\n            ofst += member_abi_t.embedded_static_size()\n        return _getelemptr_abi_helper(parent, member_t, ofst)\n    if parent.location.word_addressable:\n        for i in range(index):\n            ofst += typ.members[attrs[i]].storage_size_in_words\n    elif parent.location.byte_addressable:\n        for i in range(index):\n            ofst += typ.members[attrs[i]].memory_bytes_required\n    else:\n        raise CompilerPanic(f\"bad location {parent.location}\")\n    return IRnode.from_list(\n        add_ofst(parent, ofst),\n        typ=subtype,\n        location=parent.location,\n        encoding=parent.encoding,\n        annotation=annotation,\n    )\ndef _get_element_ptr_array(parent, key, array_bounds_check):\n    assert isinstance(parent.typ, ArrayLike)\n    if not is_integer_type(key.typ):\n        raise TypeCheckFailure(f\"{key.typ} used as array index\")\n    subtype = parent.typ.subtype\n    if parent.value == \"~empty\":\n        if array_bounds_check:\n            raise TypeCheckFailure(\"indexing into zero array not allowed\")\n        return IRnode.from_list(\"~empty\", subtype)\n    if parent.value == \"multi\":\n        assert isinstance(key.value, int)\n        return parent.args[key.value]\n    ix = unwrap_location(key)\n    if array_bounds_check:\n        clamp_op = \"uclamplt\"\n        is_darray = isinstance(parent.typ, DArrayType)\n        bound = get_dyn_array_count(parent) if is_darray else parent.typ.count\n        ix = IRnode.from_list([clamp_op, ix, bound], typ=ix.typ)\n    if parent.encoding in (Encoding.ABI, Encoding.JSON_ABI):\n        if parent.location == STORAGE:\n            raise CompilerPanic(\"storage variables should not be abi encoded\")\n        member_abi_t = subtype.abi_type\n        ofst = _mul(ix, member_abi_t.embedded_static_size())\n        return _getelemptr_abi_helper(parent, subtype, ofst)\n    if parent.location.word_addressable:\n        element_size = subtype.storage_size_in_words\n    elif parent.location.byte_addressable:\n        element_size = subtype.memory_bytes_required\n    else:\n        raise CompilerPanic(\"unreachable\")\n    ofst = _mul(ix, element_size)\n    if has_length_word(parent.typ):\n        data_ptr = add_ofst(parent, parent.location.word_scale * DYNAMIC_ARRAY_OVERHEAD)\n    else:\n        data_ptr = parent\n    return IRnode.from_list(add_ofst(data_ptr, ofst), typ=subtype, location=parent.location)\ndef needs_clamp(t, encoding):\n    if encoding not in (Encoding.ABI, Encoding.JSON_ABI):\n        return False\n    if isinstance(t, (ByteArrayLike, DArrayType)):\n        if encoding == Encoding.JSON_ABI:\n            return False\n        return True\n    if isinstance(t, BaseType) and t.typ not in (\"int256\", \"uint256\", \"bytes32\"):\n        return True\n    if isinstance(t, SArrayType):\n        return needs_clamp(t.subtype, encoding)\n    if isinstance(t, TupleLike):\n        return any(needs_clamp(m, encoding) for m in t.tuple_members())\n    return False\ndef _returndata_encoding(contract_sig):\n    if contract_sig.is_from_json:\n        return Encoding.JSON_ABI\n    return Encoding.ABI\ndef _unpack_returndata(buf, contract_sig, skip_contract_check, context):\n    return_t = contract_sig.return_type\n    if return_t is None:\n        return [\"pass\"], 0, 0\n    return_t = calculate_type_for_external_return(return_t)\n    should_unwrap_abi_tuple = return_t != contract_sig.return_type\n    abi_return_t = return_t.abi_type\n    min_return_size = abi_return_t.min_size()\n    max_return_size = abi_return_t.size_bound()\n    assert 0 < min_return_size <= max_return_size\n    ret_ofst = buf\n    ret_len = max_return_size\n    ret = []\n    if not skip_contract_check:\n        ret += [[\"assert\", [\"gt\", \"returndatasize\", min_return_size - 1]]]\n    buf = IRnode(buf, typ=return_t, encoding=_returndata_encoding(contract_sig), location=MEMORY)\n    if should_unwrap_abi_tuple:\n        buf = get_element_ptr(buf, 0, array_bounds_check=False)\n    ret += [buf]\n    return ret, ret_ofst, ret_len\ndef _external_call_helper(\n    contract_address,\n    contract_sig,\n    args_ir,\n    context,\n    value=None,\n    gas=None,\n    skip_contract_check=None,\n    expr=None,\n):\n    if value is None:\n        value = 0\n    if gas is None:\n        gas = \"gas\"\n    if skip_contract_check is None:\n        skip_contract_check = False\n    assert len(contract_sig.base_args) <= len(args_ir) <= len(contract_sig.args)\n    if context.is_constant() and contract_sig.mutability not in (\"view\", \"pure\"):\n        raise StateAccessViolation(\n            f\"May not call state modifying function '{contract_sig.name}' \"\n            f\"within {context.pp_constancy()}.\",\n            expr,\n        )\n    sub = [\"seq\"]\n    buf, arg_packer, args_ofst, args_len = _pack_arguments(contract_sig, args_ir, context)\n    ret_unpacker, ret_ofst, ret_len = _unpack_returndata(\n        buf, contract_sig, skip_contract_check, context\n    )\n    sub += arg_packer\n    if contract_sig.return_type is None and not skip_contract_check:\n        sub.append([\"assert\", [\"extcodesize\", contract_address]])\n    if context.is_constant() or contract_sig.mutability in (\"view\", \"pure\"):\n        call_op = [\"staticcall\", gas, contract_address, args_ofst, args_len, ret_ofst, ret_len]\n    else:\n        call_op = [\"call\", gas, contract_address, value, args_ofst, args_len, ret_ofst, ret_len]\n    sub.append(check_external_call(call_op))\n    if contract_sig.return_type is not None:\n        sub += ret_unpacker\n    ret = IRnode.from_list(\n        sub,\n        typ=contract_sig.return_type,\n        location=MEMORY,\n        encoding=_returndata_encoding(contract_sig),\n    )\n    return ret\ndef _should_decode(typ):\n    if isinstance(typ, BaseType):\n        return typ.typ not in (\"int256\", \"uint256\", \"bytes32\")\n    if isinstance(typ, (ByteArrayLike, DArrayType)):\n        return True\n    if isinstance(typ, SArrayType):\n        return _should_decode(typ.subtype)\n    if isinstance(typ, TupleLike):\n        return any(_should_decode(t) for t in typ.tuple_members())\n    raise CompilerPanic(f\"_should_decode({typ})\")\ndef _register_function_args(context: Context, sig: FunctionSignature) -> List[IRnode]:\n    ret = []\n    base_args_t = TupleType([arg.typ for arg in sig.base_args])\n    if sig.is_init_func:\n        base_args_ofst = IRnode(0, location=DATA, typ=base_args_t, encoding=Encoding.ABI)\n    else:\n        base_args_ofst = IRnode(4, location=CALLDATA, typ=base_args_t, encoding=Encoding.ABI)\n    for i, arg in enumerate(sig.base_args):\n        arg_ir = get_element_ptr(base_args_ofst, i)\n        if _should_decode(arg.typ):\n            p = context.new_variable(arg.name, arg.typ, is_mutable=False)\n            dst = IRnode(p, typ=arg.typ, location=MEMORY)\n            copy_arg = make_setter(dst, arg_ir)\n            copy_arg.source_pos = getpos(arg.ast_source)\n            ret.append(copy_arg)\n        else:\n            context.vars[arg.name] = VariableRecord(\n                name=arg.name,\n                pos=arg_ir,\n                typ=arg.typ,\n                mutable=False,\n                location=arg_ir.location,\n                encoding=Encoding.ABI,\n            )\n    return ret\ndef new_type_to_old_type(typ: new.BasePrimitive) -> old.NodeType:\n    if isinstance(typ, new.BoolDefinition):\n        return old.BaseType(\"bool\")\n    if isinstance(typ, new.AddressDefinition):\n        return old.BaseType(\"address\")\n    if isinstance(typ, new.InterfaceDefinition):\n        return old.InterfaceType(typ._id)\n    if isinstance(typ, new.BytesMDefinition):\n        m = typ._length\n        return old.BaseType(f\"bytes{m}\")\n    if isinstance(typ, new.BytesArrayDefinition):\n        return old.ByteArrayType(typ.length)\n    if isinstance(typ, new.StringDefinition):\n        return old.StringType(typ.length)\n    if isinstance(typ, new.DecimalDefinition):\n        return old.BaseType(\"decimal\")\n    if isinstance(typ, new.SignedIntegerAbstractType):\n        bits = typ._bits\n        return old.BaseType(\"int\" + str(bits))\n    if isinstance(typ, new.UnsignedIntegerAbstractType):\n        bits = typ._bits\n        return old.BaseType(\"uint\" + str(bits))\n    if isinstance(typ, new.ArrayDefinition):\n        return old.SArrayType(new_type_to_old_type(typ.value_type), typ.length)\n    if isinstance(typ, new.DynamicArrayDefinition):\n        return old.DArrayType(new_type_to_old_type(typ.value_type), typ.length)\n    if isinstance(typ, new.TupleDefinition):\n        return old.TupleType(typ.value_type)\n    if isinstance(typ, new.StructDefinition):\n        return old.StructType(\n            {n: new_type_to_old_type(t) for (n, t) in typ.members.items()}, typ._id\n        )\n    raise InvalidType(f\"unknown type {typ}\")",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-24845",
        "description": "[{'lang': 'en', 'value': 'Vyper is a pythonic Smart Contract Language for the ethereum virtual machine. In affected versions, the return of `<iface>.returns_int128()` is not validated to fall within the bounds of `int128`. This issue can result in a misinterpretation of the integer value and lead to incorrect behavior. As of v0.3.0, `<iface>.returns_int128()` is validated in simple expressions, but not complex expressions. Users are advised to upgrade. There is no known workaround for this issue.'}]",
        "cwe_number": 190
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-76",
      "code": "    def put(self, name: str):\n        ''' add new file\n            params in FormData:\n                - file\n                - original_file_name [optional]\n        '''\n        data = {}\n        mindsdb_file_name = name\n        existing_file_names = ca.file_controller.get_files_names()\n        def on_field(field):\n            name = field.field_name.decode()\n            value = field.value.decode()\n            data[name] = value\n        file_object = None\n        def on_file(file):\n            nonlocal file_object\n            data['file'] = file.file_name.decode()\n            file_object = file.file_object\n        temp_dir_path = tempfile.mkdtemp(prefix='mindsdb_file_')\n        if request.headers['Content-Type'].startswith('multipart/form-data'):\n            parser = multipart.create_form_parser(\n                headers=request.headers,\n                on_field=on_field,\n                on_file=on_file,\n                config={\n                    'UPLOAD_DIR': temp_dir_path.encode(),\n                    'UPLOAD_KEEP_FILENAME': True,\n                    'UPLOAD_KEEP_EXTENSIONS': True,\n                    'MAX_MEMORY_FILE_SIZE': 0\n                }\n            )\n            while True:\n                chunk = request.stream.read(8192)\n                if not chunk:\n                    break\n                parser.write(chunk)\n            parser.finalize()\n            parser.close()\n            if file_object is not None and not file_object.closed:\n                file_object.close()\n        else:\n            data = request.json\n        if mindsdb_file_name in existing_file_names:\n            return http_error(\n                400,\n                \"File already exists\",\n                f\"File with name '{data['file']}' already exists\"\n            )\n        if data.get('source_type') == 'url':\n            url = data['source']\n            data['file'] = data['name']\n            config = Config()\n            is_cloud = config.get('cloud', False)\n            if is_cloud is True and ctx.user_class != 1:\n                info = requests.head(url)\n                file_size = info.headers.get('Content-Length')\n                try:\n                    file_size = int(file_size)\n                except Exception:\n                    pass\n                if file_size is None:\n                    return http_error(\n                        400,\n                        \"Error getting file info\",\n                        \"\u0421an't determine remote file size\"\n                    )\n                if file_size > 1024 * 1024 * 100:\n                    return http_error(\n                        400,\n                        \"File is too big\",\n                        \"Upload limit for file is 100Mb\"\n                    )\n            with requests.get(url, stream=True) as r:\n                if r.status_code != 200:\n                    return http_error(\n                        400,\n                        \"Error getting file\",\n                        f\"Got status code: {r.status_code}\"\n                    )\n                file_path = os.path.join(temp_dir_path, data['file'])\n                with open(file_path, 'wb') as f:\n                    for chunk in r.iter_content(chunk_size=8192):\n                        f.write(chunk)\n        original_file_name = data.get('original_file_name')\n        file_path = os.path.join(temp_dir_path, data['file'])\n        lp = file_path.lower()\n        if lp.endswith(('.zip', '.tar.gz')):\n            if lp.endswith('.zip'):\n                with zipfile.ZipFile(file_path) as f:\n                    f.extractall(temp_dir_path)\n            elif lp.endswith('.tar.gz'):\n                with tarfile.open(file_path) as f:\n                    f.extractall(temp_dir_path)\n            os.remove(file_path)\n            files = os.listdir(temp_dir_path)\n            if len(files) != 1:\n                os.rmdir(temp_dir_path)\n                return http_error(400, 'Wrong content.', 'Archive must contain only one data file.')\n            file_path = os.path.join(temp_dir_path, files[0])\n            mindsdb_file_name = files[0]\n            if not os.path.isfile(file_path):\n                os.rmdir(temp_dir_path)\n                return http_error(400, 'Wrong content.', 'Archive must contain data file in root.')\n        ca.file_controller.save_file(mindsdb_file_name, file_path, file_name=original_file_name)\n        os.rmdir(temp_dir_path)\n        return '', 200",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-30620",
        "description": "[{'lang': 'en', 'value': 'mindsdb is a Machine Learning platform to help developers build AI solutions. In affected versions an unsafe extraction is being performed using `tarfile.extractall()` from a remotely retrieved tarball. Which may lead to the writing of the extracted files to an unintended location. Sometimes, the vulnerability is called a TarSlip or a ZipSlip variant. An attacker may leverage this vulnerability to overwrite any local file which the server process has access to. There is no risk of file exposure with this vulnerability. This issue has been addressed in release `23.2.1.0 `. Users are advised to upgrade. There are no known workarounds for this vulnerability.'}]",
        "cwe_number": 22
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-77",
      "code": "    def __init__(\n        self,\n        *,\n        elem_id: str | None = None,\n        elem_classes: list[str] | str | None = None,\n        render: bool = True,\n        visible: bool = True,\n        proxy_url: str | None = None,\n    ):\n        self._id = Context.id\n        Context.id += 1\n        self.visible = visible\n        self.elem_id = elem_id\n        self.elem_classes = (\n            [elem_classes] if isinstance(elem_classes, str) else elem_classes\n        )\n        self.proxy_url = proxy_url\n        self.share_token = secrets.token_urlsafe(32)\n        self.parent: BlockContext | None = None\n        self.is_rendered: bool = False\n        self._constructor_args: list[dict]\n        self.state_session_capacity = 10000\n        self.temp_files: set[str] = set()\n        self.GRADIO_CACHE = str(\n            Path(\n                os.environ.get(\"GRADIO_TEMP_DIR\")\n                or str(Path(tempfile.gettempdir()) / \"gradio\")\n            ).resolve()\n        )\n        if render:\n            self.render()\n    def preprocess_data(\n        self, fn_index: int, inputs: list[Any], state: SessionState | None\n    ):\n        state = state or SessionState(self)\n        block_fn = self.fns[fn_index]\n        dependency = self.dependencies[fn_index]\n        self.validate_inputs(fn_index, inputs)\n        if block_fn.preprocess:\n            processed_input = []\n            for i, input_id in enumerate(dependency[\"inputs\"]):\n                try:\n                    block = self.blocks[input_id]\n                except KeyError as e:\n                    raise InvalidBlockError(\n                        f\"Input component with id {input_id} used in {dependency['trigger']}() event not found in this gr.Blocks context. You are allowed to nest gr.Blocks contexts, but there must be a gr.Blocks context that contains all components and events.\"\n                    ) from e\n                if not isinstance(block, components.Component):\n                    raise InvalidComponentError(\n                        f\"{block.__class__} Component with id {input_id} not a valid input component.\"\n                    )\n                if getattr(block, \"stateful\", False):\n                    processed_input.append(state[input_id])\n                else:\n                    if input_id in state:\n                        block = state[input_id]\n                    inputs_cached = processing_utils.move_files_to_cache(\n                        inputs[i], block, add_urls=True\n                    )\n                    if getattr(block, \"data_model\", None) and inputs_cached is not None:\n                        if issubclass(block.data_model, GradioModel):\n                            inputs_cached = block.data_model(**inputs_cached)\n                        elif issubclass(block.data_model, GradioRootModel):\n                            inputs_cached = block.data_model(root=inputs_cached)\n                    processed_input.append(block.preprocess(inputs_cached))\n        else:\n            processed_input = inputs\n        return processed_input\n    def run_fn_batch(self, fn, batch, fn_index, state):\n        return [fn(fn_index, list(i), state) for i in zip(*batch)]\n    async def process_api(\n        self,\n        fn_index: int,\n        inputs: list[Any],\n        state: SessionState | None = None,\n        request: routes.Request | list[routes.Request] | None = None,\n        iterator: AsyncIterator | None = None,\n        session_hash: str | None = None,\n        event_id: str | None = None,\n        event_data: EventData | None = None,\n        in_event_listener: bool = True,\n    ) -> dict[str, Any]:\n        \"\"\"\n        Processes API calls from the frontend. First preprocesses the data,\n        then runs the relevant function, then postprocesses the output.\n        Parameters:\n            fn_index: Index of function to run.\n            inputs: input data received from the frontend\n            state: data stored from stateful components for session (key is input block id)\n            request: the gr.Request object containing information about the network request (e.g. IP address, headers, query parameters, username)\n            iterators: the in-progress iterators for each generator function (key is function index)\n            event_id: id of event that triggered this API call\n            event_data: data associated with the event trigger itself\n        Returns: None\n        \"\"\"\n        block_fn = self.fns[fn_index]\n        batch = self.dependencies[fn_index][\"batch\"]\n        if batch:\n            max_batch_size = self.dependencies[fn_index][\"max_batch_size\"]\n            batch_sizes = [len(inp) for inp in inputs]\n            batch_size = batch_sizes[0]\n            if inspect.isasyncgenfunction(block_fn.fn) or inspect.isgeneratorfunction(\n                block_fn.fn\n            ):\n                raise ValueError(\"Gradio does not support generators in batch mode.\")\n            if not all(x == batch_size for x in batch_sizes):\n                raise ValueError(\n                    f\"All inputs to a batch function must have the same length but instead have sizes: {batch_sizes}.\"\n                )\n            if batch_size > max_batch_size:\n                raise ValueError(\n                    f\"Batch size ({batch_size}) exceeds the max_batch_size for this function ({max_batch_size})\"\n                )\n            inputs = await anyio.to_thread.run_sync(\n                self.run_fn_batch,\n                self.preprocess_data,\n                inputs,\n                fn_index,\n                state,\n                limiter=self.limiter,\n            )\n            result = await self.call_function(\n                fn_index,\n                list(zip(*inputs)),\n                None,\n                request,\n                event_id,\n                event_data,\n                in_event_listener,\n            )\n            preds = result[\"prediction\"]\n            data = await anyio.to_thread.run_sync(\n                self.run_fn_batch,\n                self.postprocess_data,\n                preds,\n                fn_index,\n                state,\n                limiter=self.limiter,\n            )\n            data = list(zip(*data))\n            is_generating, iterator = None, None\n        else:\n            old_iterator = iterator\n            if old_iterator:\n                inputs = []\n            else:\n                inputs = await anyio.to_thread.run_sync(\n                    self.preprocess_data, fn_index, inputs, state, limiter=self.limiter\n                )\n            was_generating = old_iterator is not None\n            result = await self.call_function(\n                fn_index,\n                inputs,\n                old_iterator,\n                request,\n                event_id,\n                event_data,\n                in_event_listener,\n            )\n            data = await anyio.to_thread.run_sync(\n                self.postprocess_data,\n                fn_index,\n                result[\"prediction\"],\n                state,\n                limiter=self.limiter,\n            )\n            is_generating, iterator = result[\"is_generating\"], result[\"iterator\"]\n            if is_generating or was_generating:\n                run = id(old_iterator) if was_generating else id(iterator)\n                data = self.handle_streaming_outputs(\n                    fn_index,\n                    data,\n                    session_hash=session_hash,\n                    run=run,\n                )\n                data = self.handle_streaming_diffs(\n                    fn_index,\n                    data,\n                    session_hash=session_hash,\n                    run=run,\n                    final=not is_generating,\n                )\n        block_fn.total_runtime += result[\"duration\"]\n        block_fn.total_runs += 1\n        return {\n            \"data\": data,\n            \"is_generating\": is_generating,\n            \"iterator\": iterator,\n            \"duration\": result[\"duration\"],\n            \"average_duration\": block_fn.total_runtime / block_fn.total_runs,\n        }\n    def __init__(self, **kwargs):\n        self.tokens = {}\n        self.auth = None\n        self.blocks: gradio.Blocks | None = None\n        self.state_holder = StateHolder()\n        self.iterators: dict[str, AsyncIterator] = {}\n        self.iterators_to_reset: set[str] = set()\n        self.lock = utils.safe_get_lock()\n        self.cookie_id = secrets.token_urlsafe(32)\n        self.queue_token = secrets.token_urlsafe(32)\n        self.startup_events_triggered = False\n        self.uploaded_file_dir = os.environ.get(\"GRADIO_TEMP_DIR\") or str(\n            (Path(tempfile.gettempdir()) / \"gradio\").resolve()\n        )\n        self.change_event: None | threading.Event = None\n        self._asyncio_tasks: list[asyncio.Task] = []\n        kwargs.setdefault(\"docs_url\", None)\n        kwargs.setdefault(\"redoc_url\", None)\n        super().__init__(**kwargs)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-1728",
        "description": "[{'lang': 'en', 'value': 'gradio-app/gradio is vulnerable to a local file inclusion vulnerability due to improper validation of user-supplied input in the UploadButton component. Attackers can exploit this vulnerability to read arbitrary files on the filesystem, such as private SSH keys, by manipulating the file path in the request to the `/queue/join` endpoint. This issue could potentially lead to remote code execution. The vulnerability is present in the handling of file upload paths, allowing attackers to redirect file uploads to unintended locations on the server.'}]",
        "cwe_number": 22
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-78",
      "code": "    def __init__(self, fobj):\n        \"\"\"\n        fobj is a file-like object as an icns resource\n        \"\"\"\n        self.dct = dct = {}\n        self.fobj = fobj\n        sig, filesize = nextheader(fobj)\n        if sig != b'icns':\n            raise SyntaxError('not an icns file')\n        i = HEADERSIZE\n        while i < filesize:\n            sig, blocksize = nextheader(fobj)\n            i += HEADERSIZE\n            blocksize -= HEADERSIZE\n            dct[sig] = (i, blocksize)\n            fobj.seek(blocksize, 1)\n            i += blocksize\n    def itersizes(self):\n        sizes = []\n        for size, fmts in self.SIZES.items():\n            for (fmt, reader) in fmts:\n                if fmt in self.dct:\n                    sizes.append(size)\n                    break\n        return sizes",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2014-3589",
        "description": "[{'lang': 'en', 'value': 'PIL/IcnsImagePlugin.py in Python Imaging Library (PIL) and Pillow before 2.3.2 and 2.5.x before 2.5.2 allows remote attackers to cause a denial of service via a crafted block size.'}]",
        "cwe_number": 20
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-79",
      "code": "    def _handle_forwarded_room_key_event(\n        self,\n        sender,\n        sender_key,\n        payload,\n    ):\n        event = ForwardedRoomKeyEvent.from_dict(payload, sender, sender_key)\n        if isinstance(event, (BadEvent, UnknownBadEvent)):\n            return event\n        if event.algorithm != \"m.megolm.v1.aes-sha2\":\n            logger.error(\n                f\"Error: unsupported forwarded room key of type {event.algorithm}\"\n            )\n            return None\n        if event.session_id not in self.outgoing_key_requests:\n            logger.info(\n                \"Ignoring session key we have not requested from device {}.\", sender_key\n            )\n            return None\n        key_request = self.outgoing_key_requests[event.session_id]\n        if (\n            event.algorithm != key_request.algorithm\n            or event.room_id != key_request.room_id\n            or event.session_id != key_request.session_id\n        ):\n            logger.info(\n                \"Ignoring session key with mismatched algorithm, room_id, or \"\n                \"session id.\"\n            )\n            return None\n        content = payload[\"content\"]\n        session_sender_key = content[\"sender_key\"]\n        signing_key = content[\"sender_claimed_ed25519_key\"]\n        chain = content[\"forwarding_curve25519_key_chain\"]\n        chain.append(session_sender_key)\n        session = Olm._import_group_session(\n            content[\"session_key\"],\n            signing_key,\n            session_sender_key,\n            event.room_id,\n            chain,\n        )\n        if not session:\n            return None\n        if self.inbound_group_store.add(session):\n            self.save_inbound_group_session(session)\n        key_request = self.outgoing_key_requests.pop(key_request.request_id)\n        self.store.remove_outgoing_key_request(key_request)\n        self.outgoing_to_device_messages.append(\n            key_request.as_cancellation(self.user_id, self.device_id)\n        )\n        return event\n    def _handle_olm_event(\n        self,\n        sender,\n        sender_key,\n        payload,\n    ):\n        logger.info(\n            f\"Received Olm event of type: {payload['type']} from {sender} {sender_key}\"\n        )\n        if payload[\"type\"] == \"m.room_key\":\n            event = self._handle_room_key_event(sender, sender_key, payload)\n            return event\n        elif payload[\"type\"] == \"m.forwarded_room_key\":\n            return self._handle_forwarded_room_key_event(sender, sender_key, payload)\n        elif payload[\"type\"] == \"m.dummy\":\n            return DummyEvent.from_dict(payload, sender, sender_key)\n        else:\n            logger.warn(f\"Received unsupported Olm event of type {payload['type']}\")\n            return None",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-39254",
        "description": "[{'lang': 'en', 'value': 'matrix-nio is a Python Matrix client library, designed according to sans I/O principles. Prior to version 0.20, when a users requests a room key from their devices, the software correctly remember the request. Once they receive a forwarded room key, they accept it without checking who the room key came from. This allows homeservers to try to insert room keys of questionable validity, potentially mounting an impersonation attack. Version 0.20 fixes the issue.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-80",
      "code": "    def _checkPolkitPrivilege(self, sender, conn, privilege):\n        \"\"\"\n        Verify that sender has a given PolicyKit privilege.\n        sender is the sender's (private) D-BUS name, such as \":1:42\"\n        (sender_keyword in @dbus.service.methods). conn is\n        the dbus.Connection object (connection_keyword in\n        @dbus.service.methods). privilege is the PolicyKit privilege string.\n        This method returns if the caller is privileged, and otherwise throws a\n        PermissionDeniedByPolicy exception.\n        \"\"\"\n        if sender is None and conn is None:\n            return\n        if not self.enforce_polkit:\n            return\n        info = SenderInfo(sender, conn)\n        pid = info.connectionPid()\n        self._initPolkit()\n        try:\n            (is_auth, _, details) = self.polkit.CheckAuthorization(\n                    ('unix-process', {'pid': dbus.UInt32(pid, variant_level=1),\n                    'start-time': dbus.UInt64(0, variant_level=1)}),\n                    privilege, {'': ''}, dbus.UInt32(1), '', timeout=3000)\n        except dbus.DBusException as e:\n            if e._dbus_error_name == 'org.freedesktop.DBus.Error.ServiceUnknown':\n                self.polkit = None\n                return self._checkPolkitPrivilege(sender, conn, privilege)\n            else:\n                raise\n        if not is_auth:\n            raise PermissionDeniedByPolicy(privilege)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2017-7572",
        "description": "[{'lang': 'en', 'value': 'The _checkPolkitPrivilege function in serviceHelper.py in Back In Time (aka backintime) 1.1.18 and earlier uses a deprecated polkit authorization method (unix-process) that is subject to a race condition (time of check, time of use). With this authorization method, the owner of a process requesting a polkit operation is checked by polkitd via /proc/<pid>/status, by which time the requesting process may have been replaced by a different process with the same PID that has different privileges then the original requester.'}]",
        "cwe_number": 362
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-81",
      "code": "def register_views(app: Flask) -> None:\n    @app.after_request\n    def add_header(r: Response) -> Response:\n        \"\"\"\n        Disable cache for all auth server requests\n        \"\"\"\n        r.headers[\"Cache-Control\"] = \"no-cache, no-store, must-revalidate\"\n        r.headers[\"Pragma\"] = \"no-cache\"\n        r.headers[\"Expires\"] = \"0\"\n        r.headers[\"Cache-Control\"] = \"public, max-age=0\"\n        return r\n    @app.route(\"/login/server-config\", methods=[\"GET\"])\n    def server_config() -> Response:\n        return jsonify(\n            {\n                \"CLOUD\": app.config.get(\"CLOUD\"),\n                \"CLOUD_URL\": app.config.get(\"CLOUD_URL\"),\n                \"GITHUB_URL\": app.config.get(\"GITHUB_URL\"),\n                \"DOCUMENTATION_URL\": app.config.get(\"DOCUMENTATION_URL\"),\n                \"VIDEOS_URL\": app.config.get(\"VIDEOS_URL\"),\n            }\n        )\n    def is_authenticated(request: Request) -> bool:\n        if not app.config[\"AUTH_ENABLED\"]:\n            return True\n        cookie_token = request.cookies.get(\"auth_token\")\n        username = request.cookies.get(\"auth_username\")\n        token_creation_limit = datetime.datetime.utcnow() - datetime.timedelta(\n            hours=app.config[\"TOKEN_DURATION_HOURS\"]\n        )\n        return db.session.query(\n            db.session.query(Token)\n            .join(User)\n            .filter(\n                Token.token == cookie_token,\n                User.username == username,\n                Token.created > token_creation_limit,\n            )\n            .exists()\n        ).scalar()\n    def serve_static_or_dev(path: PathType) -> Response:\n        file_path = os.path.join(app.config[\"STATIC_DIR\"], path)\n        if os.path.isfile(file_path):\n            return send_from_directory(app.config[\"STATIC_DIR\"], path)\n        else:\n            return send_from_directory(app.config[\"STATIC_DIR\"], \"index.html\")\n    @app.route(\"/login\", defaults={\"path\": \"\"}, methods=[\"GET\"])\n    @app.route(\"/login/<path:path>\", methods=[\"GET\"])\n    def login_static(path: PathType) -> Response:\n        if is_authenticated(request) and path == \"\":\n            return handle_login(redirect_type=\"server\")\n        return serve_static_or_dev(path)\n    @app.route(\"/login/admin\", methods=[\"GET\"])\n    def login_admin() -> Tuple[str, int] | Response:\n        if not is_authenticated(request):\n            return \"\", 401\n        return serve_static_or_dev(\"/admin\")\n    @app.route(\"/auth\", methods=[\"GET\"])\n    def index() -> Tuple[Literal[\"\"], Literal[200]] | Tuple[Literal[\"\"], Literal[401]]:\n        if is_authenticated(request):\n            return \"\", 200\n        else:\n            return \"\", 401\n    @app.route(\"/login/clear\", methods=[\"GET\"])\n    def logout() -> Response | None:\n        resp = redirect_response(\"/\")\n        resp.set_cookie(\"auth_token\", \"\")\n        resp.set_cookie(\"auth_username\", \"\")\n        return resp\n    def redirect_response(url: str, redirect_type: str = \"server\") -> Response:\n        if redirect_type == \"client\":\n            return jsonify({\"redirect\": url})\n        elif redirect_type == \"server\":\n            return redirect(url)\n    @app.route(\"/login/submit\", methods=[\"POST\"])\n    def login() -> Response | Tuple[Response, Literal[401]] | None:\n        return handle_login()\n    @app.route(\"/login\", methods=[\"POST\"])\n    def login_post() -> Response | Tuple[Response, Literal[401]] | None:\n        return handle_login(redirect_type=\"server\")\n    def handle_login(\n        redirect_type: str = \"client\",\n    ) -> Response | Tuple[Response, Literal[401]] | None:\n        request_args = request.args.copy()\n        redirect_url = request_args.pop(\"redirect_url\", \"/\")\n        query_args = \"&\".join(\n            [arg + \"=\" + value for arg, value in request_args.items()]\n        )\n        if query_args:\n            redirect_url += \"?\" + query_args\n        if is_authenticated(request):\n            return redirect_response(redirect_url, redirect_type)\n        if request.method == \"POST\":\n            token_creation_limit = datetime.datetime.utcnow() - datetime.timedelta(\n                hours=app.config[\"TOKEN_DURATION_HOURS\"]\n            )\n            Token.query.filter(Token.created < token_creation_limit).delete()\n            username = request.form.get(\"username\")\n            password = request.form.get(\"password\")\n            token = request.form.get(\"token\")\n            user = User.query.filter(User.username == username).first()\n            invalid_login_msg = \"Username password combination does not exist.\"\n            if user is None:\n                return jsonify({\"error\": invalid_login_msg}), 401\n            else:\n                if password is not None:\n                    can_login = check_password_hash(user.password_hash, password)\n                elif token is not None and user.token_hash is not None:\n                    can_login = check_password_hash(user.token_hash, token)\n                else:\n                    can_login = False\n                if can_login:\n                    token = Token(user=user.uuid, token=str(secrets.token_hex(16)))\n                    db.session.add(token)\n                    db.session.commit()\n                    resp = redirect_response(redirect_url, redirect_type)\n                    resp.set_cookie(\"auth_token\", token.token)\n                    resp.set_cookie(\"auth_username\", username)\n                    return resp\n                else:\n                    return jsonify({\"error\": invalid_login_msg}), 401\n    @app.route(\"/login/users\", methods=[\"DELETE\"])\n    def delete_user() -> Union[\n        Tuple[Literal[\"\"], Literal[401]],\n        Tuple[Response, Literal[500]],\n        Tuple[Response, Literal[405]],\n        Literal[\"\"],\n    ]:\n        if not is_authenticated(request):\n            return \"\", 401\n        self_username = request.cookies.get(\"auth_username\")\n        if \"username\" in request.form:\n            to_delete_username = request.form.get(\"username\")\n            user = User.query.filter(User.username == to_delete_username).first()\n            if user is not None:\n                if user.is_admin:\n                    return jsonify({\"error\": \"Admins cannot be deleted.\"}), 500\n                elif self_username == to_delete_username:\n                    return jsonify({\"error\": \"Deleting own user is not allowed.\"}), 405\n                else:\n                    db.session.delete(user)\n                    db.session.commit()\n                    return \"\"\n            else:\n                return jsonify({\"error\": \"User does not exist.\"}), 500\n        else:\n            return jsonify({\"error\": \"No username supplied.\"}), 500\n    @app.route(\"/login/users\", methods=[\"POST\"])\n    def add_user() -> Union[\n        Tuple[Literal[\"\"], Literal[401]],\n        Tuple[Response, Literal[409]],\n        Tuple[Response, Literal[400]],\n        Literal[\"\"],\n    ]:\n        if not is_authenticated(request):\n            return \"\", 401\n        if \"username\" in request.form:\n            username = request.form.get(\"username\")\n            password = request.form.get(\"password\")\n            if username == app.config.get(\"ORCHEST_CLOUD_RESERVED_USER\"):\n                return jsonify({\"error\": \"User is reserved.\"}), 409\n            if len(password) == 0:\n                return jsonify({\"error\": \"Password cannot be empty.\"}), 400\n            user = User.query.filter(User.username == username).first()\n            if user is not None:\n                return jsonify({\"error\": \"User already exists.\"}), 409\n            user = User(\n                username=username,\n                password_hash=generate_password_hash(password),\n                uuid=str(uuid.uuid4()),\n            )\n            db.session.add(user)\n            db.session.commit()\n            return \"\"\n        else:\n            return jsonify({\"error\": \"No username supplied.\"}), 400\n    @app.route(\"/login/users\", methods=[\"GET\"])\n    def get_users() -> Tuple[Literal[\"\"], Literal[401]] | Tuple[Response, Literal[200]]:\n        if not is_authenticated(request):\n            return \"\", 401\n        data_json: Dict[\n            Literal[\"users\"],\n            List[Dict[Literal[\"username\"], str]],\n        ] = {\"users\": []}\n        users = User.query.all()\n        for user in users:\n            if user.username != app.config.get(\"ORCHEST_CLOUD_RESERVED_USER\"):\n                data_json[\"users\"].append({\"username\": user.username})\n        return jsonify(data_json), 200\n    @app.route(\"/auth/service\", methods=[\"GET\"])\n    def auth_service() -> Union[\n        Tuple[Literal[\"\"], Literal[200]],\n        Tuple[Literal[\"\"], Literal[401]],\n    ]:\n        global _auth_cache, _auth_cache_age\n        if is_authenticated(request):\n            return \"\", 200\n        original_uri = request.headers.get(\"X-Original-URI\")\n        if original_uri is None:\n            return \"\", 401\n        try:\n            components = original_uri.split(\"/\")[1].split(\"_\")[-2].split(\"-\")\n            session_uuid_prefix = components[-1]\n            project_uuid_prefix = components[-2]\n        except Exception:\n            app.logger.error(\"Failed to parse X-Original-URI: %s\" % original_uri)\n            return \"\", 401\n        auth_check = get_auth_cache(\n            project_uuid_prefix, session_uuid_prefix, _auth_cache, _auth_cache_age\n        )\n        if auth_check[\"status\"] == \"available\":\n            if auth_check[\"requires_authentication\"] is False:\n                return \"\", 200\n            else:\n                return \"\", 401\n        else:\n            base_url = \"http://%s/api/services/\" % (app.config[\"ORCHEST_API_ADDRESS\"])\n            service_url = \"%s?project_uuid_prefix=%s&session_uuid_prefix=%s\" % (\n                base_url,\n                project_uuid_prefix,\n                session_uuid_prefix,\n            )\n            try:\n                r = requests.get(service_url)\n                services = r.json().get(\"services\", [])\n                if len(services) == 0:\n                    raise Exception(\"No services found\")\n                if len(services) > 1:\n                    raise Exception(\n                        \"Filtered /api/services endpoint \"\n                        \"should always return a single service\"\n                    )\n                if services[0][\"service\"][\"requires_authentication\"] is False:\n                    set_auth_cache(\n                        project_uuid_prefix, session_uuid_prefix, False, _auth_cache\n                    )\n                    return \"\", 200\n                else:\n                    set_auth_cache(\n                        project_uuid_prefix, session_uuid_prefix, True, _auth_cache\n                    )\n                    raise Exception(\"'requires_authentication' is not set to False\")\n            except Exception as e:\n                app.logger.error(e)\n                return \"\", 401",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-39268",
        "description": "[{'lang': 'en', 'value': \"### Impact In a CSRF attack, an innocent end user is tricked by an attacker into submitting a web request that they did not intend. This may cause actions to be performed on the website that can include inadvertent client or server data leakage, change of session state, or manipulation of an end user's account. ### Patch Upgrade to v2022.09.10 to patch this vulnerability. ### Workarounds Rebuild and redeploy the Orchest `auth-server` with this commit: https://github.com/orchest/orchest/commit/c2587a963cca742c4a2503bce4cfb4161bf64c2d ### References https://en.wikipedia.org/wiki/Cross-site_request_forgery https://cwe.mitre.org/data/definitions/352.html ### For more information If you have any questions or comments about this advisory: * Open an issue in https://github.com/orchest/orchest * Email us at rick@orchest.io\"}]",
        "cwe_number": 352
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-82",
      "code": "def create_project():\n    data = request.json\n    project_name = data.get(\"project_name\")\n    manager.create_project(project_name)\n    return jsonify({\"message\": \"Project created\"})\ndef delete_project():\n    data = request.json\n    project_name = data.get(\"project_name\")\n    manager.delete_project(project_name)\n    AgentState().delete_state(project_name)\n    return jsonify({\"message\": \"Project deleted\"})\ndef download_project():\n    project_name = request.args.get(\"project_name\")\n    manager.project_to_zip(project_name)\n    project_path = manager.get_zip_path(project_name)\n    return send_file(project_path, as_attachment=False)\ndef download_project_pdf():\n    project_name = request.args.get(\"project_name\")\n    pdf_dir = Config().get_pdfs_dir()\n    pdf_path = os.path.join(pdf_dir, f\"{project_name}.pdf\")\n    response = make_response(send_file(pdf_path))\n    response.headers['Content-Type'] = 'project_bplication/pdf'\n    return response",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-5334",
        "description": "[{'lang': 'en', 'value': \"A local file read vulnerability exists in the stitionai/devika repository, affecting the latest version. The vulnerability is due to improper handling of the 'snapshot_path' parameter in the '/api/get-browser-snapshot' endpoint. An attacker can exploit this vulnerability by crafting a request with a malicious 'snapshot_path' parameter, leading to arbitrary file read from the system. This issue impacts the security of the application by allowing unauthorized access to sensitive files on the server.\"}]",
        "cwe_number": 73
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-83",
      "code": "    def __init__(self, cfg):\n        self.cfg = cfg\n        db_uri = self.cfg.database_uri if '://' in self.cfg.database_uri else \"sqlite:///\" + self.cfg.database_uri\n        cherrypy.config.update(\n            {\n                'environment': 'development' if cfg.debug else cfg.environment,\n                'tools.db.uri': db_uri,\n                'tools.db.debug': cfg.debug,\n                'ldap.uri': cfg.ldap_uri,\n                'ldap.base_dn': cfg.ldap_base_dn,\n                'ldap.bind_dn': cfg.ldap_bind_dn,\n                'ldap.bind_password': cfg.ldap_bind_password,\n                'ldap.scope': cfg.ldap_scope,\n                'ldap.tls': cfg.ldap_tls,\n                'ldap.username_attribute': cfg.ldap_username_attribute,\n                'ldap.required_group': cfg.ldap_required_group,\n                'ldap.group_attribute': cfg.ldap_group_attribute,\n                'ldap.group_attribute_is_dn': cfg.ldap_group_attribute_is_dn,\n                'ldap.version': cfg.ldap_version,\n                'ldap.network_timeout': cfg.ldap_network_timeout,\n                'ldap.timeout': cfg.ldap_timeout,\n                'ldap.encoding': cfg.ldap_encoding,\n                'login.add_missing_user': cfg.ldap_add_missing_user,\n                'login.add_user_default_role': cfg.ldap_add_user_default_role,\n                'login.add_user_default_userroot': cfg.ldap_add_user_default_userroot,\n                'smtp.server': cfg.email_host,\n                'smtp.username': cfg.email_username,\n                'smtp.password': cfg.email_password,\n                'smtp.email_from': cfg.email_sender\n                and '%s <%s>'\n                % (\n                    cfg.header_name,\n                    cfg.email_sender,\n                ),\n                'smtp.encryption': cfg.email_encryption,\n                'remove_older.execution_time': self.cfg.remove_older_time,\n                'notification.execution_time': self.cfg.email_notification_time,\n                'notification.send_changed': self.cfg.email_send_changed_notification,\n                'quota.set_quota_cmd': self.cfg.quota_set_cmd,\n                'quota.get_quota_cmd': self.cfg.quota_get_cmd,\n                'quota.get_usage_cmd': self.cfg.quota_used_cmd,\n            }\n        )\n        cherrypy.tools.db.create_all()\n        self.templates = rdw_templating.TemplateManager()\n        rate_limit_storage_class = rdiffweb.tools.ratelimit.RamRateLimit\n        if cfg.rate_limit_dir:\n            rate_limit_storage_class = rdiffweb.tools.ratelimit.FileRateLimit\n        config = {\n            '/': {\n                'request.uri_encoding': 'ISO-8859-1',\n                'tools.i18n.on': True,\n                'tools.i18n.default': 'en_US',\n                'tools.i18n.mo_dir': pkg_resources.resource_filename('rdiffweb', 'locales'),\n                'tools.i18n.domain': 'messages',\n                'tools.encode.on': True,\n                'tools.encode.encoding': 'utf-8',\n                'tools.gzip.on': True,\n                'error_page.default': self.error_page,\n                'tools.sessions.on': True,\n                'tools.sessions.debug': cfg.debug,\n                'tools.sessions.storage_class': DbSession,\n                'tools.sessions.httponly': True,\n                'tools.sessions.timeout': cfg.session_idle_timeout,\n                'tools.sessions.persistent': False,\n                'tools.auth_form.persistent_timeout': cfg.session_persistent_timeout,\n                'tools.auth_form.absolute_timeout': cfg.session_absolute_timeout,\n                'tools.ratelimit.debug': cfg.debug,\n                'tools.ratelimit.delay': 3600,\n                'tools.ratelimit.limit': cfg.rate_limit,\n                'tools.ratelimit.storage_class': rate_limit_storage_class,\n                'tools.ratelimit.storage_path': cfg.rate_limit_dir,\n            },\n        }\n        Application.__init__(self, root=Root(), config=config)\n        self.root.favicon_ico = staticfile(self._favicon)\n        if self._header_logo:\n            self.root.header_logo = staticfile(self._header_logo)\n        if self._tempdir:\n            os.environ[\"TMPDIR\"] = self._tempdir\n        UserObject.create_admin_user(cfg.admin_user, cfg.admin_password)\ndef db_after_create(target, connection, **kw):\n    \"\"\"\n    Called on database creation to update database schema.\n    \"\"\"\n    def exists(column):\n        table_name = column.table.fullname\n        column_name = column.name\n        if 'SQLite' in connection.engine.dialect.__class__.__name__:\n            sql = \"SELECT COUNT(*) FROM pragma_table_info('%s') WHERE LOWER(name)=LOWER('%s')\" % (\n                table_name,\n                column_name,\n            )\n        else:\n            sql = \"SELECT COUNT(*) FROM information_schema.columns WHERE table_name='%s' and column_name='%s'\" % (\n                table_name,\n                column_name,\n            )\n        data = connection.engine.execute(sql).first()\n        return data[0] >= 1\n    def add_column(column):\n        if exists(column):\n            return\n        table_name = column.table.fullname\n        column_name = column.name\n        column_type = column.type.compile(connection.engine.dialect)\n        connection.engine.execute('ALTER TABLE %s ADD COLUMN %s %s' % (table_name, column_name, column_type))\n    if getattr(connection, '_transaction', None):\n        connection._transaction.commit()\n    add_column(RepoObject.__table__.c.Encoding)\n    add_column(RepoObject.__table__.c.keepdays)\n    if not exists(UserObject.__table__.c.role):\n        add_column(UserObject.__table__.c.role)\n        UserObject.query.filter(UserObject._is_admin == 1).update({UserObject.role: UserObject.ADMIN_ROLE})\n    add_column(UserObject.__table__.c.fullname)\n    add_column(UserObject.__table__.c.mfa)\n    if not exists(SessionObject.__table__.c.Number):\n        SessionObject.__table__.drop()\n        SessionObject.__table__.create()\n    if getattr(connection, '_transaction', None):\n        connection._transaction.commit()\n    result = RepoObject.query.all()\n    for row in result:\n        if row.repopath.startswith('/') or row.repopath.endswith('/'):\n            row.repopath = row.repopath.strip('/')\n            row.add()\n        if row.repopath == '.':\n            row.repopath = ''\n            row.add()\n    result = RepoObject.query.order_by(RepoObject.userid, RepoObject.repopath).all()\n    prev_repo = (None, None)\n    for row in result:\n        if prev_repo[0] == row.userid and (prev_repo[1] == row.repopath or row.repopath.startswith(prev_repo[1] + '/')):\n            row.delete()\n        else:\n            prev_repo = (row.userid, row.repopath)\n    def login(self, username, password):\n        \"\"\"\n        Validate username password using database and LDAP.\n        \"\"\"\n        authenticates = self.bus.publish('authenticate', username, password)\n        authenticates = [a for a in authenticates if a]\n        if not authenticates:\n            return None\n        real_username = authenticates[0][0]\n        extra_attrs = authenticates[0][1]\n        fullname = extra_attrs.get('_fullname', None)\n        email = extra_attrs.get('_email', None)\n        userobj = UserObject.query.filter_by(username=username).first()\n        if userobj is None and self.add_missing_user:\n            try:\n                default_user_root = self.add_user_default_userroot and self.add_user_default_userroot.format(\n                    **extra_attrs\n                )\n                default_role = UserObject.ROLES.get(self.add_user_default_role)\n                userobj = UserObject.add_user(\n                    username=real_username,\n                    fullname=fullname,\n                    email=email,\n                    role=default_role,\n                    user_root=default_user_root,\n                ).add()\n            except Exception:\n                logger.error('fail to create new user', exc_info=1)\n        if userobj is None:\n            return None\n        dirty = False\n        if fullname:\n            userobj.fullname = fullname\n            dirty = True\n        if email:\n            userobj.email = email\n            dirty = True\n        if dirty:\n            userobj.add()\n        self.bus.publish('user_login', userobj)\n        return userobj\n    def create_admin_user(cls, default_username, default_password):\n        userobj = UserObject.get_user(default_username)\n        if not userobj:\n            userobj = cls.add_user(default_username, role=UserObject.ADMIN_ROLE, user_root='/backups')\n        if default_password and default_password.startswith('{SSHA}'):\n            userobj.hash_password = default_password\n        elif default_password:\n            userobj.hash_password = hash_password(default_password)\n        else:\n            userobj.hash_password = hash_password('admin123')\n        userobj.add()\n    def add_authorizedkey(self, key, comment=None):\n        \"\"\"\n        Add the given key to the user. Adding the key to his `authorized_keys`\n        file if it exists and adding it to database.\n        \"\"\"\n        assert key\n        key = authorizedkeys.check_publickey(key)\n        key = authorizedkeys.AuthorizedKey(\n            options=None, keytype=key.keytype, key=key.key, comment=comment or key.comment\n        )\n        filename = os.path.join(self.user_root, '.ssh', 'authorized_keys')\n        if os.path.isfile(filename):\n            with open(filename, mode=\"r+\", encoding='utf-8') as fh:\n                if authorizedkeys.exists(fh, key):\n                    raise DuplicateSSHKeyError(_(\"SSH key already exists\"))\n                logger.info(\"add key [%s] to [%s] authorized_keys\", key, self.username)\n                authorizedkeys.add(fh, key)\n        else:\n            logger.info(\"add key [%s] to [%s] database\", key, self.username)\n            try:\n                SshKey(userid=self.userid, fingerprint=key.fingerprint, key=key.getvalue()).add()\n            except IntegrityError:\n                SshKey.session.rollback()\n                raise DuplicateSSHKeyError(\n                    _(\"Duplicate key. This key already exists or is associated to another user.\")\n                )\n        cherrypy.engine.publish('user_attr_changed', self, {'authorizedkeys': True})\n    def add_access_token(self, name, expiration_time=None, length=16):\n        \"\"\"\n        Create a new access token. Return the un-encrypted value of the token.\n        \"\"\"\n        assert name\n        assert length >= 8\n        token = ''.join(secrets.choice(string.ascii_lowercase) for i in range(length))\n        try:\n            obj = Token(userid=self.userid, name=name, hash_token=hash_password(token), expiration_time=expiration_time)\n            obj.add()\n        except IntegrityError:\n            Token.session.rollback()\n            raise ValueError(_(\"Duplicate token name: %s\") % name)\n        cherrypy.engine.publish('access_token_added', self, name)\n        return token\n    def delete(self, *args, **kwargs):\n        cfg = cherrypy.tree.apps[''].cfg\n        if self.username == cfg.admin_user:\n            raise ValueError(_(\"can't delete admin user\"))\n        SshKey.query.filter(SshKey.userid == self.userid).delete()\n        RepoObject.query.filter(RepoObject.userid == self.userid).delete()\n        Token.query.filter(Token.userid == self.userid).delete()\n        Base.delete(self)\n    def set_password(self, password):\n        \"\"\"\n        Change the user's password. Raise a ValueError if the username or\n        the password are invalid.\n        \"\"\"\n        assert isinstance(password, str)\n        if not password:\n            raise ValueError(\"password can't be empty\")\n        cfg = cherrypy.tree.apps[''].cfg\n        if self.username == cfg.admin_user and cfg.admin_password:\n            raise ValueError(_(\"can't update admin-password defined in configuration file\"))\n        if cfg.password_min_length > len(password) or len(password) > cfg.password_max_length:\n            raise ValueError(\n                _('Password must have between %(min)d and %(max)d characters.')\n                % {'min': cfg.password_min_length, 'max': cfg.password_max_length}\n            )\n        stats = zxcvbn(password)\n        if stats.get('score') < cfg.password_score:\n            msg = _('Password too weak.')\n            warning = stats.get('feedback', {}).get('warning')\n            suggestions = stats.get('feedback', {}).get('suggestions')\n            if warning:\n                msg += ' ' + warning\n            if suggestions:\n                msg += ' ' + ' '.join(suggestions)\n            raise ValueError(msg)\n        logger.info(\"updating user password [%s]\", self.username)\n        self.hash_password = hash_password(password)\n    def __eq__(self, other):\n        return type(self) == type(other) and inspect(self).key == inspect(other).key\n    def validates_username(self, key, value):\n        if self.username:\n            raise ValueError('Username cannot be modified.')\n        return value\n    def validate_access_token(self, token):\n        \"\"\"\n        Check if the given token matches.\n        \"\"\"\n        for access_token in Token.query.all():\n            if access_token.is_expired:\n                access_token.delete()\n                continue\n            if check_password(token, access_token.hash_token):\n                access_token.access_time = datetime.datetime.utcnow\n                return True\n        return False\n    def validate_password(self, password):\n        return check_password(password, self.hash_password)\n    def default(self, action=None, **kwargs):\n        form = RevokeSessionForm()\n        if form.is_submitted():\n            if form.validate():\n                session = SessionObject.query.filter(\n                    SessionObject.username == self.app.currentuser.username, SessionObject.number == form.number.data\n                ).first()\n                if not session:\n                    flash(_('The given session cannot be removed because it cannot be found.'), level='warning')\n                elif session.id == cherrypy.session.id:\n                    flash(_('You cannot revoke your current session.'), level='warning')\n                else:\n                    session.delete()\n                    flash(_('The session was successfully revoked.'), level='success')\n            else:\n                flash(form.error_message, level='error')\n        obj_list = SessionObject.query.filter(SessionObject.username == self.app.currentuser.username).all()\n        active_sessions = [\n            {\n                'number': obj.number,\n                'access_time': obj.data.get('access_time', None),\n                'current': cherrypy.session.id == obj.id,\n                'expiration_time': obj.expiration_time,\n                'ip_address': obj.data.get('ip_address', None),\n                'login_time': obj.data.get('login_time', None),\n                'user_agent': obj.data.get('user_agent', None),\n                'username': obj.username,\n            }\n            for obj in obj_list\n        ]\n        return self._compile_template(\"prefs_session.html\", active_sessions=active_sessions)\n    def default(self, action=None, **kwargs):\n        form = RevokeSessionForm()\n        if form.is_submitted():\n            if form.validate():\n                session = SessionObject.query.filter(SessionObject.number == form.number.data).first()\n                if not session:\n                    flash(_('The given session cannot be removed because it cannot be found.'), level='warning')\n                elif session.id == cherrypy.session.id:\n                    flash(_('You cannot revoke your current session.'), level='warning')\n                else:\n                    session.delete()\n                    flash(_('The session was successfully revoked.'), level='success')\n            else:\n                flash(form.error_message, level='error')\n        obj_list = SessionObject.query.filter().all()\n        active_sessions = [\n            {\n                'number': obj.number,\n                'access_time': obj.data.get('access_time', None),\n                'current': cherrypy.session.id == obj.id,\n                'expiration_time': obj.expiration_time,\n                'ip_address': obj.data.get('ip_address', None),\n                'login_time': obj.data.get('login_time', None),\n                'user_agent': obj.data.get('user_agent', None),\n                'username': obj.username,\n            }\n            for obj in obj_list\n        ]\n        return self._compile_template(\"admin_session.html\", active_sessions=active_sessions)\n    def get_repo(cls, name, as_user=None, refresh=False):\n        \"\"\"\n        Return the repository identified as `name`.\n        `name` should be <username>/<repopath>\n        \"\"\"\n        from ._user import UserObject\n        username, repopath = _split_path(name)\n        repopath = os.fsdecode(repopath).strip('/')\n        as_user = as_user or cherrypy.tree.apps[''].currentuser\n        if not as_user:\n            raise AccessDeniedError(\"as_user or current user must be defined\")\n        if username != as_user.username and not as_user.is_admin:\n            raise AccessDeniedError(name)\n        query = RepoObject.query.join(UserObject, UserObject.userid == RepoObject.userid).filter(\n            and_(UserObject.username == username, RepoObject.repopath == repopath)\n        )\n        record = query.first()\n        if refresh and not record:\n            as_user.refresh_repos()\n            record = query.first()\n        if not record:\n            raise DoesNotExistError(username, repopath)\n        return record\n    def delete(self, path=b''):\n        \"\"\"Properly remove the given repository by updating the user's repositories.\"\"\"\n        logger.info(\"deleting repository %s\", self)\n        RdiffRepo.delete(self, path=path)\n        super().delete()\n    def populate_obj(self, userobj):\n        if self.password.data:\n            userobj.set_password(self.password.data)\n        userobj.role = self.role.data\n        userobj.fullname = self.fullname.data or ''\n        userobj.email = self.email.data or ''\n        userobj.user_root = self.user_root.data\n        if self.mfa.data and not userobj.email:\n            flash(_(\"User email is required to enabled Two-Factor Authentication\"), level='error')\n        else:\n            userobj.mfa = self.mfa.data\n        if not userobj.valid_user_root():\n            flash(_(\"User's root directory %s is not accessible!\") % userobj.user_root, level='error')\n            logger.warning(\"user's root directory %s is not accessible\" % userobj.user_root)\n        else:\n            userobj.refresh_repos(delete=True)\n        new_quota = self.disk_quota.data or 0\n        old_quota = humanfriendly.parse_size(humanfriendly.format_size(self.disk_quota.object_data or 0, binary=True))\n        if old_quota != new_quota:\n            userobj.disk_quota = new_quota\n            if userobj.disk_quota != new_quota:\n                flash(_(\"Setting user's quota is not supported\"), level='warning')\n    def _delete_user(self, action, form):\n        assert action == 'delete'\n        assert form\n        if not form.validate():\n            flash(form.error_message, level='error')\n            return\n        if form.username.data == self.app.currentuser.username:\n            flash(_(\"You cannot remove your own account!\"), level='error')\n        else:\n            try:\n                user = UserObject.get_user(form.username.data)\n                if user:\n                    user.delete()\n                    flash(_(\"User account removed.\"))\n                else:\n                    flash(_(\"User doesn't exists!\"), level='warning')\n            except ValueError as e:\n                flash(e, level='error')\nclass DeleteRepoForm(CherryForm):\n    confirm = StringField(_('Confirmation'), validators=[DataRequired()])\n    def default(self, path=b\"\", **kwargs):\n        repo, path = RepoObject.get_repo_path(path)\n        path_obj = repo.fstat(path)\n        is_maintainer()\n        form = DeleteRepoForm()\n        form.expected_confirm = path_obj.display_name\n        if form.is_submitted():\n            if form.validate():\n                RepoObject.session.expunge(repo)\n                cherrypy.engine.publish('schedule_task', repo.delete, path)\n                if path_obj.isroot:\n                    raise cherrypy.HTTPRedirect(url_for('/'))\n                else:\n                    parent_path = repo.fstat(os.path.dirname(path_obj.path))\n                    raise cherrypy.HTTPRedirect(url_for('browse', repo, parent_path))\n            else:\n                raise cherrypy.HTTPError(400, form.error_message)\n        else:\n            raise cherrypy.HTTPError(405)\ndef _checkpassword(realm, username, password):\n    \"\"\"\n    Check basic authentication.\n    \"\"\"\n    userobj = UserObject.get_user(username)\n    if userobj is not None:\n        if userobj.validate_access_token(password):\n            return True\n        if userobj.mfa == UserObject.ENABLED_MFA:\n            cherrypy.tools.ratelimit.hit()\n            return False\n    valid = any(cherrypy.engine.publish('login', username, password))\n    if not valid:\n        cherrypy.tools.ratelimit.hit()\n    return valid\nclass ApiCurrentUser(Controller):\n    def default(self):\n        u = self.app.currentuser\n        u.refresh_repos()\n        return {\n            \"email\": u.email,\n            \"username\": u.username,\n            \"repos\": [\n                {\n                    \"name\": repo_obj.name,\n                    \"maxage\": repo_obj.maxage,\n                    \"keepdays\": repo_obj.keepdays,\n                    \"display_name\": repo_obj.display_name,\n                    \"last_backup_date\": repo_obj.last_backup_date,\n                    \"status\": repo_obj.status[0],\n                    \"encoding\": repo_obj.encoding,\n                }\n                for repo_obj in u.repo_objs\n            ],\n        }\n    def populate_obj(self, userobj):\n        try:\n            token = userobj.add_access_token(self.name.data, self.expiration.data)\n            flash(\n                _(\n                    \"Your new personal access token has been created.\\n\"\n                    \"Make sure to save it - you won't be able to access it again.\\n\"\n                    \"%s\"\n                )\n                % token,\n                level='info',\n            )\n        except ValueError as e:\n            flash(str(e), level='warning')\n        except Exception:\n            logger.exception(\"error adding access token: %s, %s\" % (self.name.data, self.expiration.data))\n            flash(_(\"Unknown error while adding the access token.\"), level='error')\n    def add(self, commit=True):\n        \"\"\"\n        Add current object to session.\n        \"\"\"\n        self.__class__.session.add(self)\n        if commit:\n            self.__class__.session.commit()\n        return self\n    def delete(self, commit=True):\n        \"\"\"\n        Delete current object to session.\n        \"\"\"\n        self.__class__.session.delete(self)\n        if commit:\n            self.__class__.session.commit()\n        return self\n    def expire(self):\n        self.__class__.session.expire(self)\nclass BaseExtensions(DeclarativeMeta):\n    def query(self):\n        return self.session.query(self)\n    def session(self):\n        return cherrypy.tools.db.get_session()\nclass SQLA(cherrypy.Tool):\n    _name = 'sqla'\n    _base = None\n    def __init__(self, **kw):\n        cherrypy.Tool.__init__(self, None, None, priority=20)\n    def _setup(self):\n        cherrypy.request.hooks.attach('on_end_resource', self.on_end_resource)\n    def get_base(self):\n        if self._base is None:\n            self._base = declarative_base(metaclass=BaseExtensions, cls=Base)\n        return self._base\n    def get_session(self):\n        if self._session is None:\n            self._session = scoped_session(sessionmaker(autoflush=False, autocommit=False))\n            self._session.bind = self.get_base().metadata.bind\n        return self._session\n    def on_end_resource(self):\n        if self._session is None:\n            return\n        try:\n            self._session.flush()\n            self._session.commit()\n        except Exception:\n            logger.exception('error trying to flush and commit session')\n            self._session.rollback()\n            self._session.expunge_all()\n        finally:\n            self._session.remove()\n    def index(self):\n        self.app.currentuser.refresh_repos()\n        params = {\n            \"repos\": self.app.currentuser.repo_objs,\n            \"disk_usage\": self.app.currentuser.disk_usage,\n            \"disk_quota\": self.app.currentuser.disk_quota,\n        }\n        return self._compile_template(\"locations.html\", **params)\n    def _save(self, expiration_time):\n        session = SessionObject.query.filter(SessionObject.id == self.id).first()\n        if not session:\n            session = SessionObject(id=self.id)\n        session.data = self._data\n        session.data['_timeout'] = self.timeout\n        session.expiration_time = expiration_time\n        session.add()\n    def _delete(self):\n        SessionObject.query.filter(SessionObject.id == self.id).delete()\n    def clean_up(self):\n        \"\"\"Clean up expired sessions.\"\"\"\n        try:\n            now = self.now()\n            SessionObject.query.filter(SessionObject.expiration_time < now).delete()\n        except Exception:\n            logger.error('fail to clean-up sessions', exc_info=1)\n        finally:\n            cherrypy.tools.db.on_end_resource()\n    def __len__(self):\n        \"\"\"Return the number of active sessions.\"\"\"\n        return SessionObject.query.count()\n    def populate_obj(self, userobj):\n        if self.enable_mfa.data:\n            userobj.mfa = UserObject.ENABLED_MFA\n            flash(_(\"Two-Factor authentication enabled successfully.\"), level='success')\n        elif self.disable_mfa.data:\n            userobj.mfa = UserObject.DISABLED_MFA\n            flash(_(\"Two-Factor authentication disabled successfully.\"), level='success')\n    def validate_code(self, field):\n        if self.enable_mfa.data or self.disable_mfa.data:\n            if not self.code.data:\n                raise ValueError(_(\"Enter the verification code to continue.\"))\n            if not cherrypy.tools.auth_mfa.verify_code(self.code.data, False):\n                raise ValueError(_(\"Invalid verification code.\"))\n    def populate_obj(self, userobj):\n        try:\n            userobj.add_authorizedkey(key=self.key.data, comment=self.title.data)\n        except DuplicateSSHKeyError as e:\n            flash(str(e), level='error')\n        except Exception:\n            flash(_(\"Unknown error while adding the SSH Key\"), level='error')\n            _logger.warning(\"error adding ssh key\", exc_info=1)\n    def populate_obj(self, userobj):\n        for repo in userobj.repo_objs:\n            if repo.display_name in self:\n                repo.maxage = self[repo.display_name].data\n    def populate_obj(self, user):\n        user.fullname = self.fullname.data\n        user.email = self.email.data\n        user.add()\nclass WebCase(helper.CPWebCase):\n    interactive = False\n    login = False",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-3362",
        "description": "[{'lang': 'en', 'value': 'Insufficient Session Expiration in GitHub repository ikus060/rdiffweb prior to 2.5.0.'}]",
        "cwe_number": 613
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-84",
      "code": "    def _load_templates(self, config):\n        \"\"\" load templates\n        @dict: configuration of ldapcherry\n        \"\"\"\n        self.template_dir = self._get_param(\n            'resources',\n            'templates.dir',\n            config\n            )\n        cherrypy.log.error(\n            msg=\"loading templates from dir '%(dir)s'\" %\n                {'dir': self.template_dir},\n            severity=logging.DEBUG\n        )\n        self.temp_lookup = lookup.TemplateLookup(\n            directories=self.template_dir, input_encoding='utf-8'\n            )\n        self.temp = {}\n        for t in ('index.tmpl', 'error.tmpl', 'login.tmpl', '404.tmpl',\n                  'searchadmin.tmpl', 'searchuser.tmpl', 'adduser.tmpl',\n                  'roles.tmpl', 'groups.tmpl', 'form.tmpl', 'selfmodify.tmpl',\n                  'modify.tmpl', 'service_unavailable.tmpl'\n                  ):\n            self.temp[t] = self.temp_lookup.get_template(t)\n    def _check_auth(self, must_admin, redir_login=True):\n        \"\"\" check if a user is autheticated and, optionnaly an administrator\n        if user not authentifaced -> redirection to login page (with base64\n            of the originaly requested page (redirection after login)\n        if user authenticated, not admin and must_admin enabled -> 403 error\n        @boolean must_admin: flag \"user must be an administrator to access\n            this page\"\n        @rtype str: login of the user\n        \"\"\"\n        if self.auth_mode == 'none':\n            return 'anonymous'\n        username = self._check_session()\n        if cherrypy.request.query_string == '':\n            qs = ''\n        else:\n            qs = '?' + cherrypy.request.query_string\n        b64requrl = base64.b64encode(cherrypy.url() + qs)\n        if not username:\n            if redir_login:\n                raise cherrypy.HTTPRedirect(\n                    \"/signin?url=%(url)s\" % {'url': b64requrl},\n                    )\n            else:\n                raise cherrypy.HTTPError(\n                    \"403 Forbidden\",\n                    \"You must be logged in to access this ressource.\",\n                    )\n        if 'connected' not in cherrypy.session \\\n                or not cherrypy.session['connected']:\n            if redir_login:\n                raise cherrypy.HTTPRedirect(\n                    \"/signin?url=%(url)s\" % {'url': b64requrl},\n                    )\n            else:\n                raise cherrypy.HTTPError(\n                    \"403 Forbidden\",\n                    \"You must be logged in to access this ressource.\",\n                    )\n        if cherrypy.session['connected'] and \\\n                not cherrypy.session['isadmin']:\n            if must_admin:\n                raise cherrypy.HTTPError(\n                    \"403 Forbidden\",\n                    \"You are not allowed to access this resource.\",\n                    )\n            else:\n                return username\n        if cherrypy.session['connected'] and \\\n                cherrypy.session['isadmin']:\n            return username\n        else:\n            if redir_login:\n                raise cherrypy.HTTPRedirect(\n                    \"/signin?url=%(url)s\" % {'url': b64requrl},\n                    )\n            else:\n                raise cherrypy.HTTPError(\n                    \"403 Forbidden\",\n                    \"You must be logged in to access this ressource.\",\n                    )\n    def login(self, login, password, url=None):\n        \"\"\"login page\n        \"\"\"\n        auth = self._auth(login, password)\n        cherrypy.session['isadmin'] = auth['isadmin']\n        cherrypy.session['connected'] = auth['connected']\n        if auth['connected']:\n            if auth['isadmin']:\n                message = \\\n                    \"login success for user '%(user)s' as administrator\" % {\n                        'user': login\n                    }\n            else:\n                message = \\\n                    \"login success for user '%(user)s' as normal user\" % {\n                        'user': login\n                    }\n            cherrypy.log.error(\n                msg=message,\n                severity=logging.INFO\n            )\n            cherrypy.session[SESSION_KEY] = cherrypy.request.login = login\n            if url is None:\n                redirect = \"/\"\n            else:\n                redirect = base64.b64decode(url)\n            raise cherrypy.HTTPRedirect(redirect)\n        else:\n            message = \"login failed for user '%(user)s'\" % {\n                'user': login\n            }\n            cherrypy.log.error(\n                msg=message,\n                severity=logging.WARNING\n            )\n            if url is None:\n                qs = ''\n            else:\n                qs = '?url=' + url\n            raise cherrypy.HTTPRedirect(\"/signin\" + qs)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2019-25095",
        "description": "[{'lang': 'en', 'value': 'A vulnerability, which was classified as problematic, was found in kakwa LdapCherry up to 0.x. Affected is an unknown function of the component URL Handler. The manipulation leads to cross site scripting. It is possible to launch the attack remotely. Upgrading to version 1.0.0 is able to address this issue. The patch is identified as 6f98076281e9452fdb1adcd1bcbb70a6f968ade9. It is recommended to upgrade the affected component. VDB-217434 is the identifier assigned to this vulnerability.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-85",
      "code": "def _export_annotations(\n    db_instance: models.Project | models.Task | models.Job,\n    rq_id: str,\n    request: HttpRequest,\n    format_name: str,\n    action: str,\n    callback: Callable[[int, Optional[str], Optional[str]], str],\n    filename: Optional[str],\n    location_conf: Dict[str, Any]\n):\n    if action not in {\"\", \"download\"}:\n        raise serializers.ValidationError(\n            \"Unexpected action specified for the request\")\n    format_desc = {f.DISPLAY_NAME: f\n        for f in dm.views.get_export_formats()}.get(format_name)\n    if format_desc is None:\n        raise serializers.ValidationError(\n            \"Unknown format specified for the request\")\n    elif not format_desc.ENABLED:\n        return Response(status=status.HTTP_405_METHOD_NOT_ALLOWED)\n    queue = django_rq.get_queue(settings.CVAT_QUEUES.EXPORT_DATA.value)\n    rq_job = queue.fetch_job(rq_id)\n    location = location_conf.get('location')\n    if location not in Location.list():\n        raise serializers.ValidationError(\n            f\"Unexpected location {location} specified for the request\"\n        )\n    cache_ttl = dm.views.get_export_cache_ttl(db_instance)\n    instance_update_time = timezone.localtime(db_instance.updated_date)\n    if isinstance(db_instance, Project):\n        tasks_update = list(map(lambda db_task: timezone.localtime(db_task.updated_date), db_instance.tasks.all()))\n        instance_update_time = max(tasks_update + [instance_update_time])\n    instance_timestamp = datetime.strftime(instance_update_time, \"%Y_%m_%d_%H_%M_%S\")\n    is_annotation_file = rq_id.startswith('export:annotations')\n    if rq_job:\n        rq_request = rq_job.meta.get('request', None)\n        request_time = rq_request.get('timestamp', None) if rq_request else None\n        if request_time is None or request_time < instance_update_time:\n            rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)\n            rq_job.delete()\n        else:\n            if rq_job.is_finished:\n                if location == Location.CLOUD_STORAGE:\n                    rq_job.delete()\n                    return Response(status=status.HTTP_200_OK)\n                elif location == Location.LOCAL:\n                    file_path = rq_job.return_value()\n                    if not file_path:\n                        return Response(\n                            'A result for exporting job was not found for finished RQ job',\n                            status=status.HTTP_500_INTERNAL_SERVER_ERROR\n                        )\n                    with dm.util.get_export_cache_lock(\n                        file_path, ttl=60,\n                    ):\n                        if action == \"download\":\n                            if not osp.exists(file_path):\n                                return Response(\n                                    \"The exported file has expired, please retry exporting\",\n                                    status=status.HTTP_404_NOT_FOUND\n                                )\n                            filename = filename or \\\n                                build_annotations_file_name(\n                                    class_name=db_instance.__class__.__name__,\n                                    identifier=db_instance.name if isinstance(db_instance, (Task, Project)) else db_instance.id,\n                                    timestamp=instance_timestamp,\n                                    format_name=format_name,\n                                    is_annotation_file=is_annotation_file,\n                                    extension=osp.splitext(file_path)[1]\n                                )\n                            rq_job.delete()\n                            return sendfile(request, file_path, attachment=True, attachment_filename=filename)\n                        else:\n                            if osp.exists(file_path):\n                                os.utime(file_path, None)\n                                return Response(status=status.HTTP_201_CREATED)\n                            else:\n                                rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)\n                                rq_job.delete()\n                else:\n                    raise NotImplementedError(f\"Export to {location} location is not implemented yet\")\n            elif rq_job.is_failed:\n                exc_info = rq_job.meta.get('formatted_exception', str(rq_job.exc_info))\n                rq_job.delete()\n                return Response(exc_info, status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n            elif rq_job.is_deferred and rq_id not in queue.deferred_job_registry.get_job_ids():\n                rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)\n                rq_job.delete()\n            else:\n                return Response(status=status.HTTP_202_ACCEPTED)\n    try:\n        if request.scheme:\n            server_address = request.scheme + '://'\n        server_address += request.get_host()\n    except Exception:\n        server_address = None\n    user_id = request.user.id\n    func = callback if location == Location.LOCAL else export_resource_to_cloud_storage\n    func_args = (db_instance.id, format_name, server_address)\n    if location == Location.CLOUD_STORAGE:\n        try:\n            storage_id = location_conf['storage_id']\n        except KeyError:\n            raise serializers.ValidationError(\n                'Cloud storage location was selected as the destination,'\n                ' but cloud storage id was not specified')\n        db_storage = get_cloud_storage_for_import_or_export(\n            storage_id=storage_id, request=request,\n            is_default=location_conf['is_default'])\n        filename_pattern = build_annotations_file_name(\n            class_name=db_instance.__class__.__name__,\n            identifier=db_instance.name if isinstance(db_instance, (Task, Project)) else db_instance.id,\n            timestamp=instance_timestamp,\n            format_name=format_name,\n            is_annotation_file=is_annotation_file,\n        )\n        func_args = (db_storage, filename, filename_pattern, callback) + func_args\n    else:\n        db_storage = None\n    with get_rq_lock_by_user(queue, user_id):\n        queue.enqueue_call(\n            func=func,\n            args=func_args,\n            job_id=rq_id,\n            meta=get_rq_job_meta(request=request, db_obj=db_instance),\n            depends_on=define_dependent_job(queue, user_id, rq_id=rq_id),\n            result_ttl=cache_ttl.total_seconds(),\n            failure_ttl=cache_ttl.total_seconds(),\n        )\n    handle_dataset_export(db_instance,\n        format_name=format_name, cloud_storage=db_storage, save_images=not is_annotation_file)\n    return Response(status=status.HTTP_202_ACCEPTED)\ndef export(db_instance, request, queue_name):\n    action = request.query_params.get('action', None)\n    filename = request.query_params.get('filename', None)\n    if action not in (None, 'download'):\n        raise serializers.ValidationError(\n            \"Unexpected action specified for the request\")\n    if isinstance(db_instance, Task):\n        obj_type = 'task'\n        logger = slogger.task[db_instance.pk]\n        Exporter = TaskExporter\n        cache_ttl = TASK_CACHE_TTL\n        use_target_storage_conf = request.query_params.get('use_default_location', True)\n    elif isinstance(db_instance, Project):\n        obj_type = 'project'\n        logger = slogger.project[db_instance.pk]\n        Exporter = ProjectExporter\n        cache_ttl = PROJECT_CACHE_TTL\n        use_target_storage_conf = request.query_params.get('use_default_location', True)\n    else:\n        raise Exception(\n            \"Unexpected type of db_instance: {}\".format(type(db_instance)))\n    use_settings = to_bool(use_target_storage_conf)\n    obj = db_instance if use_settings else request.query_params\n    location_conf = get_location_configuration(\n        obj=obj,\n        use_settings=use_settings,\n        field_name=StorageType.TARGET\n    )\n    queue = django_rq.get_queue(queue_name)\n    rq_id = f\"export:{obj_type}.id{db_instance.pk}-by-{request.user}\"\n    rq_job = queue.fetch_job(rq_id)\n    last_instance_update_time = timezone.localtime(db_instance.updated_date)\n    timestamp = datetime.strftime(last_instance_update_time, \"%Y_%m_%d_%H_%M_%S\")\n    location = location_conf.get('location')\n    if rq_job:\n        rq_request = rq_job.meta.get('request', None)\n        request_time = rq_request.get(\"timestamp\", None) if rq_request else None\n        if request_time is None or request_time < last_instance_update_time:\n            rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)\n            rq_job.delete()\n        else:\n            if rq_job.is_finished:\n                if location == Location.LOCAL:\n                    file_path = rq_job.return_value()\n                    if not file_path:\n                        return Response('A result for exporting job was not found for finished RQ job', status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n                    elif not os.path.exists(file_path):\n                        return Response('The result file does not exist in export cache', status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n                    filename = filename or build_backup_file_name(\n                        class_name=obj_type,\n                        identifier=db_instance.name,\n                        timestamp=timestamp,\n                        extension=os.path.splitext(file_path)[1]\n                    )\n                    if action == \"download\":\n                        rq_job.delete()\n                        return sendfile(request, file_path, attachment=True,\n                            attachment_filename=filename)\n                    return Response(status=status.HTTP_201_CREATED)\n                elif location == Location.CLOUD_STORAGE:\n                    rq_job.delete()\n                    return Response(status=status.HTTP_200_OK)\n                else:\n                    raise NotImplementedError()\n            elif rq_job.is_failed:\n                exc_info = rq_job.meta.get('formatted_exception', str(rq_job.exc_info))\n                rq_job.delete()\n                return Response(exc_info,\n                    status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n            else:\n                return Response(status=status.HTTP_202_ACCEPTED)\n    ttl = dm.views.PROJECT_CACHE_TTL.total_seconds()\n    user_id = request.user.id\n    func = _create_backup if location == Location.LOCAL else export_resource_to_cloud_storage\n    func_args = (db_instance, Exporter, '{}_backup.zip'.format(obj_type), logger, cache_ttl)\n    if location == Location.CLOUD_STORAGE:\n        try:\n            storage_id = location_conf['storage_id']\n        except KeyError:\n            raise serializers.ValidationError(\n                'Cloud storage location was selected as the destination,'\n                ' but cloud storage id was not specified')\n        db_storage = get_cloud_storage_for_import_or_export(\n            storage_id=storage_id, request=request,\n            is_default=location_conf['is_default'])\n        filename_pattern = build_backup_file_name(\n            class_name=obj_type,\n            identifier=db_instance.name,\n            timestamp=timestamp,\n        )\n        func_args = (db_storage, filename, filename_pattern, _create_backup) + func_args\n    with get_rq_lock_by_user(queue, user_id):\n        queue.enqueue_call(\n            func=func,\n            args=func_args,\n            job_id=rq_id,\n            meta=get_rq_job_meta(request=request, db_obj=db_instance),\n            depends_on=define_dependent_job(queue, user_id, rq_id=rq_id),\n            result_ttl=ttl,\n            failure_ttl=ttl,\n        )\n    return Response(status=status.HTTP_202_ACCEPTED)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-37306",
        "description": "[{'lang': 'en', 'value': 'Computer Vision Annotation Tool (CVAT) is an interactive video and image annotation tool for computer vision. Starting in version 2.2.0 and prior to version 2.14.3, if an attacker can trick a logged-in CVAT user into visiting a malicious URL, they can initiate a dataset export or a backup from a project, task or job that the victim user has permission to export into a cloud storage that the victim user has access to. The name of the resulting file can be chosen by the attacker. This implies that the attacker can overwrite arbitrary files in any cloud storage that the victim can access and, if the attacker has read access to the cloud storage used in the attack, they can obtain media files, annotations, settings and other information from any projects, tasks or jobs that the victim has permission to export. Version 2.14.3 contains a fix for the issue. No known workarounds are available.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-86",
      "code": "    def _fork(self, path, href, text, fetch=False):\n        if fetch:\n            tmppath = urllib.parse.urlparse(href).path\n            fname = os.path.basename(tmppath)\n            tmpdir = tempfile.mkdtemp(prefix=\"canto-\")\n            tmpnam = tmpdir + '/' + fname\n            on_hook(\"curses_exit\", lambda : (os.unlink(tmpnam)))\n            on_hook(\"curses_exit\", lambda : (os.rmdir(tmpdir)))\n        pid = os.fork()\n        if pid:\n            return pid\n        if fetch:\n            tmp = open(tmpnam, 'w+b')\n            response = urllib.request.urlopen(href)\n            while True:\n                r = response.read(1024)\n                if not r:\n                    break\n                tmp.write(r)\n            response.close()\n            tmp.close()\n            href = tmpnam\n        fd = os.open(\"/dev/null\", os.O_RDWR)\n        os.dup2(fd, sys.stderr.fileno())\n        if not text:\n            os.setpgid(os.getpid(), os.getpid())\n            os.dup2(fd, sys.stdout.fileno())\n        if \"%u\" in path:\n            path = path.replace(\"%u\", href)\n        elif href:\n            path = path + \" \" + href\n        os.execv(\"/bin/sh\", [\"/bin/sh\", \"-c\", path])\n        sys.exit(0)\n    def _edit(self, text):\n        if not self.editor:\n            self.editor = os.getenv(\"EDITOR\")\n        if not self.editor:\n            self.editor = self.input(\"editor: \")\n        if not self.editor:\n            return text\n        self.callbacks[\"pause_interface\"]()\n        fd, path = tempfile.mkstemp(text=True)\n        f = os.fdopen(fd, \"w\")\n        f.write(text)\n        f.close()\n        logging.info(\"Invoking editor on %s\" % path)\n        pid = self._fork(self.editor + \" %u\", path, True)\n        pid, status = os.waitpid(pid, 0)\n        if status == 0:\n            f = open(path, \"r\")\n            r = f.read()\n            f.close()\n        else:\n            self.callbacks[\"set_var\"](\"error_msg\",\n                    \"Editor failed! Status = %d\" % (status,))\n            r = text\n        os.unlink(path)\n        self.callbacks[\"unpause_interface\"]()\n        return r",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2013-7416",
        "description": "[{'lang': 'en', 'value': 'canto_curses/guibase.py in Canto Curses before 0.9.0 allows remote feed servers to execute arbitrary commands via shell metacharacters in a URL in a feed.'}]",
        "cwe_number": 77
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-87",
      "code": "def get(image_file, domain, title, singer, album):\n    import ast\n    import base64\n    import json\n    import os\n    from html import unescape\n    import requests\n    api = f\"http://{domain}:7873/bGVhdmVfcmlnaHRfbm93\"\n    with open(image_file, \"rb\") as f:\n        im_bytes = f.read()\n        f.close()\n    im_b64 = base64.b64encode(im_bytes).decode(\"utf8\")\n    headers = {\"Content-type\": \"application/json\", \"Accept\": \"text/plain\"}\n    status = try_get_cached(domain, {\"title\": title, \"singer\": singer, \"album\": album})\n    status = ast.literal_eval(str(status))\n    if status is None:\n        print(\"Cached version not found. Uploading image with song metadata.\")\n        payload = json.dumps(\n            {\"image\": im_b64, \"title\": title, \"singer\": singer, \"album\": album}\n        )\n        response = requests.post(api, data=payload, headers=headers)\n        data = unescape(response.text)\n        print(data)\n        data = ast.literal_eval(data)[\"entry\"]\n        print(data)\n    else:\n        data = status\n    cmd = \"del \" + image_file\n    os.system(cmd)\n    return data",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-23611",
        "description": "[{'lang': 'en', 'value': 'iTunesRPC-Remastered is a Discord Rich Presence for iTunes on Windows utility. In affected versions iTunesRPC-Remastered did not properly sanitize image file paths leading to OS level command injection. This issue has been patched in commit cdcd48b. Users are advised to upgrade.'}]",
        "cwe_number": 78
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-88",
      "code": "    def request(\n        self,\n        uri,\n        method=\"GET\",\n        body=None,\n        headers=None,\n        redirections=DEFAULT_MAX_REDIRECTS,\n        connection_type=None,\n    ):\n        \"\"\" Performs a single HTTP request.\n        The 'uri' is the URI of the HTTP resource and can begin with either\n        'http' or 'https'. The value of 'uri' must be an absolute URI.\n        The 'method' is the HTTP method to perform, such as GET, POST, DELETE,\n        etc. There is no restriction on the methods allowed.\n        The 'body' is the entity body to be sent with the request. It is a\n        string object.\n        Any extra headers that are to be sent with the request should be\n        provided in the 'headers' dictionary.\n        The maximum number of redirect to follow before raising an\n        exception is 'redirections. The default is 5.\n        The return value is a tuple of (response, content), the first\n        being and instance of the 'Response' class, the second being\n        a string that contains the response entity body.\n        \"\"\"\n        conn_key = ''\n        try:\n            if headers is None:\n                headers = {}\n            else:\n                headers = self._normalize_headers(headers)\n            if \"user-agent\" not in headers:\n                headers[\"user-agent\"] = \"Python-httplib2/%s (gzip)\" % __version__\n            uri = iri2uri(uri)\n            (scheme, authority, request_uri, defrag_uri) = urlnorm(uri)\n            proxy_info = self._get_proxy_info(scheme, authority)\n            conn_key = scheme + \":\" + authority\n            conn = self.connections.get(conn_key)\n            if conn is None:\n                if not connection_type:\n                    connection_type = SCHEME_TO_CONNECTION[scheme]\n                certs = list(self.certificates.iter(authority))\n                if scheme == \"https\":\n                    if certs:\n                        conn = self.connections[conn_key] = connection_type(\n                            authority,\n                            key_file=certs[0][0],\n                            cert_file=certs[0][1],\n                            timeout=self.timeout,\n                            proxy_info=proxy_info,\n                            ca_certs=self.ca_certs,\n                            disable_ssl_certificate_validation=self.disable_ssl_certificate_validation,\n                            ssl_version=self.ssl_version,\n                            key_password=certs[0][2],\n                        )\n                    else:\n                        conn = self.connections[conn_key] = connection_type(\n                            authority,\n                            timeout=self.timeout,\n                            proxy_info=proxy_info,\n                            ca_certs=self.ca_certs,\n                            disable_ssl_certificate_validation=self.disable_ssl_certificate_validation,\n                            ssl_version=self.ssl_version,\n                        )\n                else:\n                    conn = self.connections[conn_key] = connection_type(\n                        authority, timeout=self.timeout, proxy_info=proxy_info\n                    )\n                conn.set_debuglevel(debuglevel)\n            if \"range\" not in headers and \"accept-encoding\" not in headers:\n                headers[\"accept-encoding\"] = \"gzip, deflate\"\n            info = email.Message.Message()\n            cachekey = None\n            cached_value = None\n            if self.cache:\n                cachekey = defrag_uri.encode(\"utf-8\")\n                cached_value = self.cache.get(cachekey)\n                if cached_value:\n                    try:\n                        info, content = cached_value.split(\"\\r\\n\\r\\n\", 1)\n                        feedparser = email.FeedParser.FeedParser()\n                        feedparser.feed(info)\n                        info = feedparser.close()\n                        feedparser._parse = None\n                    except (IndexError, ValueError):\n                        self.cache.delete(cachekey)\n                        cachekey = None\n                        cached_value = None\n            if (\n                method in self.optimistic_concurrency_methods\n                and self.cache\n                and \"etag\" in info\n                and not self.ignore_etag\n                and \"if-match\" not in headers\n            ):\n                headers[\"if-match\"] = info[\"etag\"]\n            if self.cache and cachekey and method not in self.safe_methods:\n                self.cache.delete(cachekey)\n            if method in self.safe_methods and \"vary\" in info:\n                vary = info[\"vary\"]\n                vary_headers = vary.lower().replace(\" \", \"\").split(\",\")\n                for header in vary_headers:\n                    key = \"-varied-%s\" % header\n                    value = info[key]\n                    if headers.get(header, None) != value:\n                        cached_value = None\n                        break\n            if (\n                self.cache\n                and cached_value\n                and (method in self.safe_methods or info[\"status\"] == \"308\")\n                and \"range\" not in headers\n            ):\n                redirect_method = method\n                if info[\"status\"] not in (\"307\", \"308\"):\n                    redirect_method = \"GET\"\n                if \"-x-permanent-redirect-url\" in info:\n                    if redirections <= 0:\n                        raise RedirectLimit(\n                            \"Redirected more times than rediection_limit allows.\",\n                            {},\n                            \"\",\n                        )\n                    (response, new_content) = self.request(\n                        info[\"-x-permanent-redirect-url\"],\n                        method=redirect_method,\n                        headers=headers,\n                        redirections=redirections - 1,\n                    )\n                    response.previous = Response(info)\n                    response.previous.fromcache = True\n                else:\n                    entry_disposition = _entry_disposition(info, headers)\n                    if entry_disposition == \"FRESH\":\n                        if not cached_value:\n                            info[\"status\"] = \"504\"\n                            content = \"\"\n                        response = Response(info)\n                        if cached_value:\n                            response.fromcache = True\n                        return (response, content)\n                    if entry_disposition == \"STALE\":\n                        if (\n                            \"etag\" in info\n                            and not self.ignore_etag\n                            and not \"if-none-match\" in headers\n                        ):\n                            headers[\"if-none-match\"] = info[\"etag\"]\n                        if \"last-modified\" in info and not \"last-modified\" in headers:\n                            headers[\"if-modified-since\"] = info[\"last-modified\"]\n                    elif entry_disposition == \"TRANSPARENT\":\n                        pass\n                    (response, new_content) = self._request(\n                        conn,\n                        authority,\n                        uri,\n                        request_uri,\n                        method,\n                        body,\n                        headers,\n                        redirections,\n                        cachekey,\n                    )\n                if response.status == 304 and method == \"GET\":\n                    for key in _get_end2end_headers(response):\n                        info[key] = response[key]\n                    merged_response = Response(info)\n                    if hasattr(response, \"_stale_digest\"):\n                        merged_response._stale_digest = response._stale_digest\n                    _updateCache(\n                        headers, merged_response, content, self.cache, cachekey\n                    )\n                    response = merged_response\n                    response.status = 200\n                    response.fromcache = True\n                elif response.status == 200:\n                    content = new_content\n                else:\n                    self.cache.delete(cachekey)\n                    content = new_content\n            else:\n                cc = _parse_cache_control(headers)\n                if \"only-if-cached\" in cc:\n                    info[\"status\"] = \"504\"\n                    response = Response(info)\n                    content = \"\"\n                else:\n                    (response, content) = self._request(\n                        conn,\n                        authority,\n                        uri,\n                        request_uri,\n                        method,\n                        body,\n                        headers,\n                        redirections,\n                        cachekey,\n                    )\n        except Exception as e:\n            is_timeout = isinstance(e, socket.timeout)\n            if is_timeout:\n                conn = self.connections.pop(conn_key, None)\n                if conn:\n                    conn.close()\n            if self.force_exception_to_status_code:\n                if isinstance(e, HttpLib2ErrorWithResponse):\n                    response = e.response\n                    content = e.content\n                    response.status = 500\n                    response.reason = str(e)\n                elif is_timeout:\n                    content = \"Request Timeout\"\n                    response = Response(\n                        {\n                            \"content-type\": \"text/plain\",\n                            \"status\": \"408\",\n                            \"content-length\": len(content),\n                        }\n                    )\n                    response.reason = \"Request Timeout\"\n                else:\n                    content = str(e)\n                    response = Response(\n                        {\n                            \"content-type\": \"text/plain\",\n                            \"status\": \"400\",\n                            \"content-length\": len(content),\n                        }\n                    )\n                    response.reason = \"Bad Request\"\n            else:\n                raise\n        return (response, content)\n    def _get_proxy_info(self, scheme, authority):\n        \"\"\"Return a ProxyInfo instance (or None) based on the scheme\n        and authority.\n        \"\"\"\n        hostname, port = urllib.splitport(authority)\n        proxy_info = self.proxy_info\n        if callable(proxy_info):\n            proxy_info = proxy_info(scheme)\n        if hasattr(proxy_info, \"applies_to\") and not proxy_info.applies_to(hostname):\n            proxy_info = None\n        return proxy_info\n    def request(\n        self,\n        uri,\n        method=\"GET\",\n        body=None,\n        headers=None,\n        redirections=DEFAULT_MAX_REDIRECTS,\n        connection_type=None,\n    ):\n        \"\"\" Performs a single HTTP request.\nThe 'uri' is the URI of the HTTP resource and can begin\nwith either 'http' or 'https'. The value of 'uri' must be an absolute URI.\nThe 'method' is the HTTP method to perform, such as GET, POST, DELETE, etc.\nThere is no restriction on the methods allowed.\nThe 'body' is the entity body to be sent with the request. It is a string\nobject.\nAny extra headers that are to be sent with the request should be provided in the\n'headers' dictionary.\nThe maximum number of redirect to follow before raising an\nexception is 'redirections. The default is 5.\nThe return value is a tuple of (response, content), the first\nbeing and instance of the 'Response' class, the second being\na string that contains the response entity body.\n        \"\"\"\n        conn_key = ''\n        try:\n            if headers is None:\n                headers = {}\n            else:\n                headers = self._normalize_headers(headers)\n            if \"user-agent\" not in headers:\n                headers[\"user-agent\"] = \"Python-httplib2/%s (gzip)\" % __version__\n            uri = iri2uri(uri)\n            (scheme, authority, request_uri, defrag_uri) = urlnorm(uri)\n            conn_key = scheme + \":\" + authority\n            conn = self.connections.get(conn_key)\n            if conn is None:\n                if not connection_type:\n                    connection_type = SCHEME_TO_CONNECTION[scheme]\n                certs = list(self.certificates.iter(authority))\n                if issubclass(connection_type, HTTPSConnectionWithTimeout):\n                    if certs:\n                        conn = self.connections[conn_key] = connection_type(\n                            authority,\n                            key_file=certs[0][0],\n                            cert_file=certs[0][1],\n                            timeout=self.timeout,\n                            proxy_info=self.proxy_info,\n                            ca_certs=self.ca_certs,\n                            disable_ssl_certificate_validation=self.disable_ssl_certificate_validation,\n                            tls_maximum_version=self.tls_maximum_version,\n                            tls_minimum_version=self.tls_minimum_version,\n                            key_password=certs[0][2],\n                        )\n                    else:\n                        conn = self.connections[conn_key] = connection_type(\n                            authority,\n                            timeout=self.timeout,\n                            proxy_info=self.proxy_info,\n                            ca_certs=self.ca_certs,\n                            disable_ssl_certificate_validation=self.disable_ssl_certificate_validation,\n                            tls_maximum_version=self.tls_maximum_version,\n                            tls_minimum_version=self.tls_minimum_version,\n                        )\n                else:\n                    conn = self.connections[conn_key] = connection_type(\n                        authority, timeout=self.timeout, proxy_info=self.proxy_info\n                    )\n                conn.set_debuglevel(debuglevel)\n            if \"range\" not in headers and \"accept-encoding\" not in headers:\n                headers[\"accept-encoding\"] = \"gzip, deflate\"\n            info = email.message.Message()\n            cachekey = None\n            cached_value = None\n            if self.cache:\n                cachekey = defrag_uri\n                cached_value = self.cache.get(cachekey)\n                if cached_value:\n                    try:\n                        info, content = cached_value.split(b\"\\r\\n\\r\\n\", 1)\n                        info = email.message_from_bytes(info)\n                        for k, v in info.items():\n                            if v.startswith(\"=?\") and v.endswith(\"?=\"):\n                                info.replace_header(\n                                    k, str(*email.header.decode_header(v)[0])\n                                )\n                    except (IndexError, ValueError):\n                        self.cache.delete(cachekey)\n                        cachekey = None\n                        cached_value = None\n            if (\n                method in self.optimistic_concurrency_methods\n                and self.cache\n                and \"etag\" in info\n                and not self.ignore_etag\n                and \"if-match\" not in headers\n            ):\n                headers[\"if-match\"] = info[\"etag\"]\n            if self.cache and cachekey and method not in self.safe_methods:\n                self.cache.delete(cachekey)\n            if method in self.safe_methods and \"vary\" in info:\n                vary = info[\"vary\"]\n                vary_headers = vary.lower().replace(\" \", \"\").split(\",\")\n                for header in vary_headers:\n                    key = \"-varied-%s\" % header\n                    value = info[key]\n                    if headers.get(header, None) != value:\n                        cached_value = None\n                        break\n            if (\n                self.cache\n                and cached_value\n                and (method in self.safe_methods or info[\"status\"] == \"308\")\n                and \"range\" not in headers\n            ):\n                redirect_method = method\n                if info[\"status\"] not in (\"307\", \"308\"):\n                    redirect_method = \"GET\"\n                if \"-x-permanent-redirect-url\" in info:\n                    if redirections <= 0:\n                        raise RedirectLimit(\n                            \"Redirected more times than redirection_limit allows.\",\n                            {},\n                            \"\",\n                        )\n                    (response, new_content) = self.request(\n                        info[\"-x-permanent-redirect-url\"],\n                        method=redirect_method,\n                        headers=headers,\n                        redirections=redirections - 1,\n                    )\n                    response.previous = Response(info)\n                    response.previous.fromcache = True\n                else:\n                    entry_disposition = _entry_disposition(info, headers)\n                    if entry_disposition == \"FRESH\":\n                        if not cached_value:\n                            info[\"status\"] = \"504\"\n                            content = b\"\"\n                        response = Response(info)\n                        if cached_value:\n                            response.fromcache = True\n                        return (response, content)\n                    if entry_disposition == \"STALE\":\n                        if (\n                            \"etag\" in info\n                            and not self.ignore_etag\n                            and not \"if-none-match\" in headers\n                        ):\n                            headers[\"if-none-match\"] = info[\"etag\"]\n                        if \"last-modified\" in info and not \"last-modified\" in headers:\n                            headers[\"if-modified-since\"] = info[\"last-modified\"]\n                    elif entry_disposition == \"TRANSPARENT\":\n                        pass\n                    (response, new_content) = self._request(\n                        conn,\n                        authority,\n                        uri,\n                        request_uri,\n                        method,\n                        body,\n                        headers,\n                        redirections,\n                        cachekey,\n                    )\n                if response.status == 304 and method == \"GET\":\n                    for key in _get_end2end_headers(response):\n                        info[key] = response[key]\n                    merged_response = Response(info)\n                    if hasattr(response, \"_stale_digest\"):\n                        merged_response._stale_digest = response._stale_digest\n                    _updateCache(\n                        headers, merged_response, content, self.cache, cachekey\n                    )\n                    response = merged_response\n                    response.status = 200\n                    response.fromcache = True\n                elif response.status == 200:\n                    content = new_content\n                else:\n                    self.cache.delete(cachekey)\n                    content = new_content\n            else:\n                cc = _parse_cache_control(headers)\n                if \"only-if-cached\" in cc:\n                    info[\"status\"] = \"504\"\n                    response = Response(info)\n                    content = b\"\"\n                else:\n                    (response, content) = self._request(\n                        conn,\n                        authority,\n                        uri,\n                        request_uri,\n                        method,\n                        body,\n                        headers,\n                        redirections,\n                        cachekey,\n                    )\n        except Exception as e:\n            is_timeout = isinstance(e, socket.timeout)\n            if is_timeout:\n                conn = self.connections.pop(conn_key, None)\n                if conn:\n                    conn.close()\n            if self.force_exception_to_status_code:\n                if isinstance(e, HttpLib2ErrorWithResponse):\n                    response = e.response\n                    content = e.content\n                    response.status = 500\n                    response.reason = str(e)\n                elif isinstance(e, socket.timeout):\n                    content = b\"Request Timeout\"\n                    response = Response(\n                        {\n                            \"content-type\": \"text/plain\",\n                            \"status\": \"408\",\n                            \"content-length\": len(content),\n                        }\n                    )\n                    response.reason = \"Request Timeout\"\n                else:\n                    content = str(e).encode(\"utf-8\")\n                    response = Response(\n                        {\n                            \"content-type\": \"text/plain\",\n                            \"status\": \"400\",\n                            \"content-length\": len(content),\n                        }\n                    )\n                    response.reason = \"Bad Request\"\n            else:\n                raise\n        return (response, content)\nclass Response(dict):",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2020-11078",
        "description": "[{'lang': 'en', 'value': 'In httplib2 before version 0.18.0, an attacker controlling unescaped part of uri for `httplib2.Http.request()` could change request headers and body, send additional hidden requests to same server. This vulnerability impacts software that uses httplib2 with uri constructed by string concatenation, as opposed to proper urllib building with escaping. This has been fixed in 0.18.0.'}]",
        "cwe_number": 93
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-89",
      "code": "    def download_check_files(self, filelist):\n        if not cherrypy.session['admin']:\n            uo = self.useroptions.forUser(self.getUserId())\n            if not uo.getOptionValue('media.may_download'):\n                return 'not_permitted'\n        for f in filelist:\n            if '/../' in f:\n                return 'invalid_file'\n        size_limit = cherry.config['media.maximum_download_size']\n        try:\n            if self.model.file_size_within_limit(filelist, size_limit):\n                return 'ok'\n            else:\n                return 'too_big'\n        except OSError as e:\n            return str(e)\n    def api_downloadcheck(self, filelist):\n        status = self.download_check_files(filelist)\n        if status == 'not_permitted':\n            return \"\"\"You are not allowed to download files.\"\"\"\n        elif status == 'invalid_file':\n            return \"Error: invalid filename found in {list}\".format(list=filelist)\n        elif status == 'too_big':\n            size_limit = cherry.config['media.maximum_download_size']\n            return \"\"\"Can't download: Playlist is bigger than {maxsize} mB.\n                        The server administrator can change this configuration.\n                        \"\"\".format(maxsize=size_limit/1024/1024)\n        elif status == 'ok':\n            return status\n        else:\n            message = \"Error status check for download: {status!r}\".format(status=status)\n            log.e(message)\n            return message",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2015-8309",
        "description": "[{'lang': 'en', 'value': 'Directory traversal vulnerability in Cherry Music before 0.36.0 allows remote authenticated users to read arbitrary files via the \"value\" parameter to \"download.\"'}]",
        "cwe_number": 22
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-90",
      "code": "def is_local_uri(uri, is_tracking_or_registry_uri=True):\n    \"\"\"\n    Returns true if the specified URI is a local file path (/foo or file:/foo).\n    :param uri: The URI.\n    :param is_tracking_uri: Whether or not the specified URI is an MLflow Tracking or MLflow\n                            Model Registry URI. Examples of other URIs are MLflow artifact URIs,\n                            filesystem paths, etc.\n    \"\"\"\n    if uri == \"databricks\" and is_tracking_or_registry_uri:\n        return False\n    if is_windows() and uri.startswith(\"\\\\\\\\\"):\n        return False\n    parsed_uri = urllib.parse.urlparse(uri)\n    if parsed_uri.hostname and not (\n        parsed_uri.hostname == \".\"\n        or parsed_uri.hostname.startswith(\"localhost\")\n        or parsed_uri.hostname.startswith(\"127.0.0.1\")\n    ):\n        return False\n    scheme = parsed_uri.scheme\n    if scheme == \"\" or scheme == \"file\":\n        return True\n    if is_windows() and len(scheme) == 1 and scheme.lower() == pathlib.Path(uri).drive.lower()[0]:\n        return True\n    return False\ndef is_file_uri(uri):\n    return urllib.parse.urlparse(uri).scheme == \"file\"",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-6977",
        "description": "[{'lang': 'en', 'value': 'This vulnerability enables malicious users to read sensitive files on the server.'}]",
        "cwe_number": 29
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-91",
      "code": "if __name__ == \"__main__\":\n  test.main()",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-36003",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. When `RandomPoissonV2` receives large input shape and rates, it gives a `CHECK` fail that can trigger a denial of service attack. We have patched the issue in GitHub commit 552bfced6ce4809db5f3ca305f60ff80dd40c5a3. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue.'}]",
        "cwe_number": 617
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-92",
      "code": "    def post(self):\n        password = self.get_argument(\"password\", \"\")\n        if hashlib.sha512(password).hexdigest() == PASSWORD:\n            self.set_secure_cookie(COOKIE_NAME, str(time.time()))\n            self.redirect(\"/\")\n        else:\n            time.sleep(1)\n            self.redirect(u\"/login?error\")",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-37109",
        "description": "[{'lang': 'en', 'value': 'patrickfuller camp up to and including commit bbd53a256ed70e79bd8758080936afbf6d738767 is vulnerable to Incorrect Access Control. Access to the password.txt file is not properly restricted as it is in the root directory served by StaticFileHandler and the Tornado rule to throw a 403 error when password.txt is accessed can be bypassed. Furthermore, it is not necessary to crack the password hash to authenticate with the application because the password hash is also used as the cookie secret, so an attacker can generate his own authentication cookie.'}]",
        "cwe_number": 522
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-93",
      "code": "    def login(self, flag=True) -> WerkzeugResponse:\n        @self.appbuilder.sm.oid.loginhandler\n        def login_handler(self):\n            if g.user is not None and g.user.is_authenticated:\n                return redirect(self.appbuilder.get_url_for_index)\n            form = LoginForm_oid()\n            if form.validate_on_submit():\n                session[\"remember_me\"] = form.remember_me.data\n                return self.appbuilder.sm.oid.try_login(\n                    form.openid.data,\n                    ask_for=self.oid_ask_for,\n                    ask_for_optional=self.oid_ask_for_optional,\n                )\n            return self.render_template(\n                self.login_template,\n                title=self.title,\n                form=form,\n                providers=self.appbuilder.sm.openid_providers,\n                appbuilder=self.appbuilder,\n            )\n        @self.appbuilder.sm.oid.after_login\n        def after_login(resp):\n            if resp.email is None or resp.email == \"\":\n                flash(as_unicode(self.invalid_login_message), \"warning\")\n                return redirect(self.appbuilder.get_url_for_login)\n            user = self.appbuilder.sm.auth_user_oid(resp.email)\n            if user is None:\n                flash(as_unicode(self.invalid_login_message), \"warning\")\n                return redirect(self.appbuilder.get_url_for_login)\n            remember_me = False\n            if \"remember_me\" in session:\n                remember_me = session[\"remember_me\"]\n                session.pop(\"remember_me\", None)\n            login_user(user, remember=remember_me)\n            next_url = request.args.get(\"next\", \"\")\n            return redirect(get_safe_redirect(next_url))\n        return login_handler(self)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-25128",
        "description": "[{'lang': 'en', 'value': 'Flask-AppBuilder is an application development framework, built on top of Flask. When Flask-AppBuilder is set to AUTH_TYPE AUTH_OID, it allows an attacker to forge an HTTP request, that could deceive the backend into using any requested OpenID service. This vulnerability could grant an attacker unauthorised privilege access if a custom OpenID service is deployed by the attacker and accessible by the backend. This vulnerability is only exploitable when the application is using the OpenID 2.0 authorization protocol. Upgrade to Flask-AppBuilder 4.3.11 to fix the vulnerability.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-94",
      "code": "def _export_annotations(\n    db_instance: models.Project | models.Task | models.Job,\n    rq_id: str,\n    request: HttpRequest,\n    format_name: str,\n    action: str,\n    callback: Callable[[int, Optional[str], Optional[str]], str],\n    filename: Optional[str],\n    location_conf: Dict[str, Any]\n):\n    if action not in {\"\", \"download\"}:\n        raise serializers.ValidationError(\n            \"Unexpected action specified for the request\")\n    format_desc = {f.DISPLAY_NAME: f\n        for f in dm.views.get_export_formats()}.get(format_name)\n    if format_desc is None:\n        raise serializers.ValidationError(\n            \"Unknown format specified for the request\")\n    elif not format_desc.ENABLED:\n        return Response(status=status.HTTP_405_METHOD_NOT_ALLOWED)\n    queue = django_rq.get_queue(settings.CVAT_QUEUES.EXPORT_DATA.value)\n    rq_job = queue.fetch_job(rq_id)\n    location = location_conf.get('location')\n    if location not in Location.list():\n        raise serializers.ValidationError(\n            f\"Unexpected location {location} specified for the request\"\n        )\n    cache_ttl = dm.views.get_export_cache_ttl(db_instance)\n    instance_update_time = timezone.localtime(db_instance.updated_date)\n    if isinstance(db_instance, Project):\n        tasks_update = list(map(lambda db_task: timezone.localtime(db_task.updated_date), db_instance.tasks.all()))\n        instance_update_time = max(tasks_update + [instance_update_time])\n    instance_timestamp = datetime.strftime(instance_update_time, \"%Y_%m_%d_%H_%M_%S\")\n    is_annotation_file = rq_id.startswith('export:annotations')\n    if rq_job:\n        rq_request = rq_job.meta.get('request', None)\n        request_time = rq_request.get('timestamp', None) if rq_request else None\n        if request_time is None or request_time < instance_update_time:\n            rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)\n            rq_job.delete()\n        else:\n            if rq_job.is_finished:\n                if location == Location.CLOUD_STORAGE:\n                    rq_job.delete()\n                    return Response(status=status.HTTP_200_OK)\n                elif location == Location.LOCAL:\n                    file_path = rq_job.return_value()\n                    if not file_path:\n                        return Response(\n                            'A result for exporting job was not found for finished RQ job',\n                            status=status.HTTP_500_INTERNAL_SERVER_ERROR\n                        )\n                    with dm.util.get_export_cache_lock(\n                        file_path, ttl=60,\n                    ):\n                        if action == \"download\":\n                            if not osp.exists(file_path):\n                                return Response(\n                                    \"The exported file has expired, please retry exporting\",\n                                    status=status.HTTP_404_NOT_FOUND\n                                )\n                            filename = filename or \\\n                                build_annotations_file_name(\n                                    class_name=db_instance.__class__.__name__,\n                                    identifier=db_instance.name if isinstance(db_instance, (Task, Project)) else db_instance.id,\n                                    timestamp=instance_timestamp,\n                                    format_name=format_name,\n                                    is_annotation_file=is_annotation_file,\n                                    extension=osp.splitext(file_path)[1]\n                                )\n                            rq_job.delete()\n                            return sendfile(request, file_path, attachment=True, attachment_filename=filename)\n                        else:\n                            if osp.exists(file_path):\n                                os.utime(file_path, None)\n                                return Response(status=status.HTTP_201_CREATED)\n                            else:\n                                rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)\n                                rq_job.delete()\n                else:\n                    raise NotImplementedError(f\"Export to {location} location is not implemented yet\")\n            elif rq_job.is_failed:\n                exc_info = rq_job.meta.get('formatted_exception', str(rq_job.exc_info))\n                rq_job.delete()\n                return Response(exc_info, status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n            elif rq_job.is_deferred and rq_id not in queue.deferred_job_registry.get_job_ids():\n                rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)\n                rq_job.delete()\n            else:\n                return Response(status=status.HTTP_202_ACCEPTED)\n    try:\n        if request.scheme:\n            server_address = request.scheme + '://'\n        server_address += request.get_host()\n    except Exception:\n        server_address = None\n    user_id = request.user.id\n    func = callback if location == Location.LOCAL else export_resource_to_cloud_storage\n    func_args = (db_instance.id, format_name, server_address)\n    if location == Location.CLOUD_STORAGE:\n        try:\n            storage_id = location_conf['storage_id']\n        except KeyError:\n            raise serializers.ValidationError(\n                'Cloud storage location was selected as the destination,'\n                ' but cloud storage id was not specified')\n        db_storage = get_cloud_storage_for_import_or_export(\n            storage_id=storage_id, request=request,\n            is_default=location_conf['is_default'])\n        filename_pattern = build_annotations_file_name(\n            class_name=db_instance.__class__.__name__,\n            identifier=db_instance.name if isinstance(db_instance, (Task, Project)) else db_instance.id,\n            timestamp=instance_timestamp,\n            format_name=format_name,\n            is_annotation_file=is_annotation_file,\n        )\n        func_args = (db_storage, filename, filename_pattern, callback) + func_args\n    else:\n        db_storage = None\n    with get_rq_lock_by_user(queue, user_id):\n        queue.enqueue_call(\n            func=func,\n            args=func_args,\n            job_id=rq_id,\n            meta=get_rq_job_meta(request=request, db_obj=db_instance),\n            depends_on=define_dependent_job(queue, user_id, rq_id=rq_id),\n            result_ttl=cache_ttl.total_seconds(),\n            failure_ttl=cache_ttl.total_seconds(),\n        )\n    handle_dataset_export(db_instance,\n        format_name=format_name, cloud_storage=db_storage, save_images=not is_annotation_file)\n    return Response(status=status.HTTP_202_ACCEPTED)\ndef _import_project_dataset(request, rq_id_template, rq_func, db_obj, format_name, filename=None, conv_mask_to_poly=True, location_conf=None):\n    format_desc = {f.DISPLAY_NAME: f\n        for f in dm.views.get_import_formats()}.get(format_name)\n    if format_desc is None:\n        raise serializers.ValidationError(\n            \"Unknown input format '{}'\".format(format_name))\n    elif not format_desc.ENABLED:\n        return Response(status=status.HTTP_405_METHOD_NOT_ALLOWED)\n    rq_id = rq_id_template.format(db_obj.pk, request.user)\n    queue = django_rq.get_queue(settings.CVAT_QUEUES.IMPORT_DATA.value)\n    rq_job = queue.fetch_job(rq_id)\n    if not rq_job or rq_job.is_finished or rq_job.is_failed:\n        if rq_job and (rq_job.is_finished or rq_job.is_failed):\n            rq_job.delete()\n        location = location_conf.get('location') if location_conf else None\n        db_storage = None\n        if not filename and location != Location.CLOUD_STORAGE:\n            serializer = DatasetFileSerializer(data=request.data)\n            if serializer.is_valid(raise_exception=True):\n                dataset_file = serializer.validated_data['dataset_file']\n                with NamedTemporaryFile(\n                    prefix='cvat_{}'.format(db_obj.pk),\n                    dir=settings.TMP_FILES_ROOT,\n                    delete=False) as tf:\n                    filename = tf.name\n                    for chunk in dataset_file.chunks():\n                        tf.write(chunk)\n        elif location == Location.CLOUD_STORAGE:\n            assert filename, 'The filename was not specified'\n            try:\n                storage_id = location_conf['storage_id']\n            except KeyError:\n                raise serializers.ValidationError(\n                    'Cloud storage location was selected as the source,'\n                    ' but cloud storage id was not specified')\n            db_storage = get_cloud_storage_for_import_or_export(\n                storage_id=storage_id, request=request,\n                is_default=location_conf['is_default'])\n            key = filename\n            with NamedTemporaryFile(\n                prefix='cvat_{}'.format(db_obj.pk),\n                dir=settings.TMP_FILES_ROOT,\n                delete=False) as tf:\n                filename = tf.name\n        func = import_resource_with_clean_up_after\n        func_args = (rq_func, filename, db_obj.pk, format_name, conv_mask_to_poly)\n        if location == Location.CLOUD_STORAGE:\n            func_args = (db_storage, key, func) + func_args\n            func = import_resource_from_cloud_storage\n        user_id = request.user.id\n        with get_rq_lock_by_user(queue, user_id):\n            rq_job = queue.enqueue_call(\n                func=func,\n                args=func_args,\n                job_id=rq_id,\n                meta={\n                    'tmp_file': filename,\n                    **get_rq_job_meta(request=request, db_obj=db_obj),\n                },\n                depends_on=define_dependent_job(queue, user_id, rq_id=rq_id),\n                result_ttl=settings.IMPORT_CACHE_SUCCESS_TTL.total_seconds(),\n                failure_ttl=settings.IMPORT_CACHE_FAILED_TTL.total_seconds()\n            )\n        handle_dataset_import(db_obj, format_name=format_name, cloud_storage=db_storage)\n    else:\n        return Response(status=status.HTTP_409_CONFLICT, data='Import job already exists')\n    serializer = RqIdSerializer(data={'rq_id': rq_id})\n    serializer.is_valid(raise_exception=True)\n    return Response(serializer.data, status=status.HTTP_202_ACCEPTED)\ndef export(db_instance, request, queue_name):\n    action = request.query_params.get('action', None)\n    filename = request.query_params.get('filename', None)\n    if action not in (None, 'download'):\n        raise serializers.ValidationError(\n            \"Unexpected action specified for the request\")\n    if isinstance(db_instance, Task):\n        obj_type = 'task'\n        logger = slogger.task[db_instance.pk]\n        Exporter = TaskExporter\n        cache_ttl = TASK_CACHE_TTL\n        use_target_storage_conf = request.query_params.get('use_default_location', True)\n    elif isinstance(db_instance, Project):\n        obj_type = 'project'\n        logger = slogger.project[db_instance.pk]\n        Exporter = ProjectExporter\n        cache_ttl = PROJECT_CACHE_TTL\n        use_target_storage_conf = request.query_params.get('use_default_location', True)\n    else:\n        raise Exception(\n            \"Unexpected type of db_instance: {}\".format(type(db_instance)))\n    use_settings = to_bool(use_target_storage_conf)\n    obj = db_instance if use_settings else request.query_params\n    location_conf = get_location_configuration(\n        obj=obj,\n        use_settings=use_settings,\n        field_name=StorageType.TARGET\n    )\n    queue = django_rq.get_queue(queue_name)\n    rq_id = f\"export:{obj_type}.id{db_instance.pk}-by-{request.user}\"\n    rq_job = queue.fetch_job(rq_id)\n    last_instance_update_time = timezone.localtime(db_instance.updated_date)\n    timestamp = datetime.strftime(last_instance_update_time, \"%Y_%m_%d_%H_%M_%S\")\n    location = location_conf.get('location')\n    if rq_job:\n        rq_request = rq_job.meta.get('request', None)\n        request_time = rq_request.get(\"timestamp\", None) if rq_request else None\n        if request_time is None or request_time < last_instance_update_time:\n            rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)\n            rq_job.delete()\n        else:\n            if rq_job.is_finished:\n                if location == Location.LOCAL:\n                    file_path = rq_job.return_value()\n                    if not file_path:\n                        return Response('A result for exporting job was not found for finished RQ job', status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n                    elif not os.path.exists(file_path):\n                        return Response('The result file does not exist in export cache', status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n                    filename = filename or build_backup_file_name(\n                        class_name=obj_type,\n                        identifier=db_instance.name,\n                        timestamp=timestamp,\n                        extension=os.path.splitext(file_path)[1]\n                    )\n                    if action == \"download\":\n                        rq_job.delete()\n                        return sendfile(request, file_path, attachment=True,\n                            attachment_filename=filename)\n                    return Response(status=status.HTTP_201_CREATED)\n                elif location == Location.CLOUD_STORAGE:\n                    rq_job.delete()\n                    return Response(status=status.HTTP_200_OK)\n                else:\n                    raise NotImplementedError()\n            elif rq_job.is_failed:\n                exc_info = rq_job.meta.get('formatted_exception', str(rq_job.exc_info))\n                rq_job.delete()\n                return Response(exc_info,\n                    status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n            else:\n                return Response(status=status.HTTP_202_ACCEPTED)\n    ttl = dm.views.PROJECT_CACHE_TTL.total_seconds()\n    user_id = request.user.id\n    func = _create_backup if location == Location.LOCAL else export_resource_to_cloud_storage\n    func_args = (db_instance, Exporter, '{}_backup.zip'.format(obj_type), logger, cache_ttl)\n    if location == Location.CLOUD_STORAGE:\n        try:\n            storage_id = location_conf['storage_id']\n        except KeyError:\n            raise serializers.ValidationError(\n                'Cloud storage location was selected as the destination,'\n                ' but cloud storage id was not specified')\n        db_storage = get_cloud_storage_for_import_or_export(\n            storage_id=storage_id, request=request,\n            is_default=location_conf['is_default'])\n        filename_pattern = build_backup_file_name(\n            class_name=obj_type,\n            identifier=db_instance.name,\n            timestamp=timestamp,\n        )\n        func_args = (db_storage, filename, filename_pattern, _create_backup) + func_args\n    with get_rq_lock_by_user(queue, user_id):\n        queue.enqueue_call(\n            func=func,\n            args=func_args,\n            job_id=rq_id,\n            meta=get_rq_job_meta(request=request, db_obj=db_instance),\n            depends_on=define_dependent_job(queue, user_id, rq_id=rq_id),\n            result_ttl=ttl,\n            failure_ttl=ttl,\n        )\n    return Response(status=status.HTTP_202_ACCEPTED)\ndef _import(importer, request, queue, rq_id, Serializer, file_field_name, location_conf, filename=None):\n    rq_job = queue.fetch_job(rq_id)\n    if (user_id_from_meta := getattr(rq_job, 'meta', {}).get('user', {}).get('id')) and user_id_from_meta != request.user.id:\n        return Response(status=status.HTTP_403_FORBIDDEN)\n    if not rq_job:\n        org_id = getattr(request.iam_context['organization'], 'id', None)\n        location = location_conf.get('location')\n        if location == Location.LOCAL:\n            if not filename:\n                serializer = Serializer(data=request.data)\n                serializer.is_valid(raise_exception=True)\n                payload_file = serializer.validated_data[file_field_name]\n                with NamedTemporaryFile(\n                    prefix='cvat_',\n                    dir=settings.TMP_FILES_ROOT,\n                    delete=False) as tf:\n                    filename = tf.name\n                    for chunk in payload_file.chunks():\n                        tf.write(chunk)\n        else:\n            file_name = request.query_params.get('filename')\n            assert file_name, \"The filename wasn't specified\"\n            try:\n                storage_id = location_conf['storage_id']\n            except KeyError:\n                raise serializers.ValidationError(\n                    'Cloud storage location was selected as the source,'\n                    ' but cloud storage id was not specified')\n            db_storage = get_cloud_storage_for_import_or_export(\n                storage_id=storage_id, request=request,\n                is_default=location_conf['is_default'])\n            key = filename\n            with NamedTemporaryFile(prefix='cvat_', dir=settings.TMP_FILES_ROOT, delete=False) as tf:\n                filename = tf.name\n        func = import_resource_with_clean_up_after\n        func_args = (importer, filename, request.user.id, org_id)\n        if location == Location.CLOUD_STORAGE:\n            func_args = (db_storage, key, func) + func_args\n            func = import_resource_from_cloud_storage\n        user_id = request.user.id\n        with get_rq_lock_by_user(queue, user_id):\n            rq_job = queue.enqueue_call(\n                func=func,\n                args=func_args,\n                job_id=rq_id,\n                meta={\n                    'tmp_file': filename,\n                    **get_rq_job_meta(request=request, db_obj=None)\n                },\n                depends_on=define_dependent_job(queue, user_id),\n                result_ttl=settings.IMPORT_CACHE_SUCCESS_TTL.total_seconds(),\n                failure_ttl=settings.IMPORT_CACHE_FAILED_TTL.total_seconds()\n            )\n    else:\n        if rq_job.is_finished:\n            project_id = rq_job.return_value()\n            rq_job.delete()\n            return Response({'id': project_id}, status=status.HTTP_201_CREATED)\n        elif rq_job.is_failed:\n            exc_info = process_failed_job(rq_job)\n            import_error_prefix = '{}.{}'.format(\n                CvatImportError.__module__, CvatImportError.__name__)\n            if exc_info.startswith(import_error_prefix):\n                exc_info = exc_info.replace(import_error_prefix + ': ', '')\n                return Response(data=exc_info,\n                    status=status.HTTP_400_BAD_REQUEST)\n            else:\n                return Response(data=exc_info,\n                    status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n    serializer = RqIdSerializer(data={'rq_id': rq_id})\n    serializer.is_valid(raise_exception=True)\n    return Response(serializer.data, status=status.HTTP_202_ACCEPTED)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-37306",
        "description": "[{'lang': 'en', 'value': 'Computer Vision Annotation Tool (CVAT) is an interactive video and image annotation tool for computer vision. Starting in version 2.2.0 and prior to version 2.14.3, if an attacker can trick a logged-in CVAT user into visiting a malicious URL, they can initiate a dataset export or a backup from a project, task or job that the victim user has permission to export into a cloud storage that the victim user has access to. The name of the resulting file can be chosen by the attacker. This implies that the attacker can overwrite arbitrary files in any cloud storage that the victim can access and, if the attacker has read access to the cloud storage used in the attack, they can obtain media files, annotations, settings and other information from any projects, tasks or jobs that the victim has permission to export. Version 2.14.3 contains a fix for the issue. No known workarounds are available.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-95",
      "code": "def login():\n    form = forms.UserForm()\n    if form.validate_on_submit():\n        db = get_db()\n        user = db.search(\n            (Query().username == form.username.data) & (Query().type == \"user\")\n        )\n        if user and check_password_hash(user[0][\"hashed_password\"], form.password.data):\n            user = User.from_db(user[0])\n            login_user(user, remember=True)\n            flash(\"Login successful!\", \"success\")\n            next_url = request.args.get(\"next\")\n            return redirect(next_url or \"/\")\n        flash(\"Invalid credentials\", \"error\")\n        return redirect(\"/login\")\n    return render_template(\"users/login.html\", form=form, title=\"Login\")",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-0697",
        "description": "[{'lang': 'en', 'value': 'Open Redirect in GitHub repository archivy/archivy prior to 1.7.0.'}]",
        "cwe_number": 601
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-96",
      "code": "    def authorize_security_group_ingress(self, context, group_name=None,\n                                         group_id=None, **kwargs):\n        if not group_name and not group_id:\n            err = _(\"Not enough parameters, need group_name or group_id\")\n            raise exception.EC2APIError(err)\n        self.compute_api.ensure_default_security_group(context)\n        notfound = exception.SecurityGroupNotFound\n        if group_name:\n            security_group = db.security_group_get_by_name(context,\n                                                           context.project_id,\n                                                           group_name)\n            if not security_group:\n                raise notfound(security_group_id=group_name)\n        if group_id:\n            security_group = db.security_group_get(context, group_id)\n            if not security_group:\n                raise notfound(security_group_id=group_id)\n        msg = _(\"Authorize security group ingress %s\")\n        LOG.audit(msg, security_group['name'], context=context)\n        prevalues = []\n        try:\n            prevalues = kwargs['ip_permissions']\n        except KeyError:\n            prevalues.append(kwargs)\n        postvalues = []\n        for values in prevalues:\n            rulesvalues = self._rule_args_to_dict(context, values)\n            if not rulesvalues:\n                err = _(\"%s Not enough parameters to build a valid rule\")\n                raise exception.EC2APIError(err % rulesvalues)\n            for values_for_rule in rulesvalues:\n                values_for_rule['parent_group_id'] = security_group.id\n                if self._security_group_rule_exists(security_group,\n                                                    values_for_rule):\n                    err = _('%s - This rule already exists in group')\n                    raise exception.EC2APIError(err % values_for_rule)\n                postvalues.append(values_for_rule)\n        rule_ids = []\n        for values_for_rule in postvalues:\n            security_group_rule = db.security_group_rule_create(\n                    context,\n                    values_for_rule)\n            rule_ids.append(security_group_rule['id'])\n        if postvalues:\n            self.compute_api.trigger_security_group_rules_refresh(\n                    context,\n                    security_group_id=security_group['id'])\n            self.sgh.trigger_security_group_rule_create_refresh(\n                    context, rule_ids)\n            return True\n        raise exception.EC2APIError(_(\"No rule for the specified parameters.\"))\n    def _get_source_project_id(self, context, source_security_group_owner_id):\n        if source_security_group_owner_id:\n            source_parts = source_security_group_owner_id.split(':')\n            if len(source_parts) == 2:\n                source_project_id = source_parts[1]\n            else:\n                source_project_id = source_parts[0]\n        else:\n            source_project_id = context.project_id\n        return source_project_id\n    def create_security_group(self, context, group_name, group_description):\n        if not re.match('^[a-zA-Z0-9_\\- ]+$', str(group_name)):\n            err = _(\"Value (%s) for parameter GroupName is invalid.\"\n                    \" Content limited to Alphanumeric characters, \"\n                    \"spaces, dashes, and underscores.\") % group_name\n            raise exception.InvalidParameterValue(err=err)\n        if len(str(group_name)) > 255:\n            err = _(\"Value (%s) for parameter GroupName is invalid.\"\n                    \" Length exceeds maximum of 255.\") % group_name\n            raise exception.InvalidParameterValue(err=err)\n        LOG.audit(_(\"Create Security Group %s\"), group_name, context=context)\n        self.compute_api.ensure_default_security_group(context)\n        if db.security_group_exists(context, context.project_id, group_name):\n            msg = _('group %s already exists')\n            raise exception.EC2APIError(msg % group_name)\n        group = {'user_id': context.user_id,\n                 'project_id': context.project_id,\n                 'name': group_name,\n                 'description': group_description}\n        group_ref = db.security_group_create(context, group)\n        self.sgh.trigger_security_group_create_refresh(context, group)\n        return {'securityGroupSet': [self._format_security_group(context,\n                                                                 group_ref)]}\n    def delete_security_group(self, context, group_name=None, group_id=None,\n                              **kwargs):\n        if not group_name and not group_id:\n            err = _(\"Not enough parameters, need group_name or group_id\")\n            raise exception.EC2APIError(err)\n        notfound = exception.SecurityGroupNotFound\n        if group_name:\n            security_group = db.security_group_get_by_name(context,\n                                                           context.project_id,\n                                                           group_name)\n            if not security_group:\n                raise notfound(security_group_id=group_name)\n        elif group_id:\n            security_group = db.security_group_get(context, group_id)\n            if not security_group:\n                raise notfound(security_group_id=group_id)\n        if db.security_group_in_use(context, security_group.id):\n            raise exception.InvalidGroup(reason=\"In Use\")\n        LOG.audit(_(\"Delete security group %s\"), group_name, context=context)\n        db.security_group_destroy(context, security_group.id)\n        self.sgh.trigger_security_group_destroy_refresh(context,\n                                                        security_group.id)\n        return True\n    def create(self, req, body):\n        \"\"\"Creates a new security group.\"\"\"\n        context = req.environ['nova.context']\n        authorize(context)\n        if not body:\n            raise exc.HTTPUnprocessableEntity()\n        security_group = body.get('security_group', None)\n        if security_group is None:\n            raise exc.HTTPUnprocessableEntity()\n        group_name = security_group.get('name', None)\n        group_description = security_group.get('description', None)\n        self._validate_security_group_property(group_name, \"name\")\n        self._validate_security_group_property(group_description,\n                                               \"description\")\n        group_name = group_name.strip()\n        group_description = group_description.strip()\n        LOG.audit(_(\"Create Security Group %s\"), group_name, context=context)\n        self.compute_api.ensure_default_security_group(context)\n        if db.security_group_exists(context, context.project_id, group_name):\n            msg = _('Security group %s already exists') % group_name\n            raise exc.HTTPBadRequest(explanation=msg)\n        group = {'user_id': context.user_id,\n                 'project_id': context.project_id,\n                 'name': group_name,\n                 'description': group_description}\n        group_ref = db.security_group_create(context, group)\n        self.sgh.trigger_security_group_create_refresh(context, group)\n        return {'security_group': self._format_security_group(context,\n                                                                 group_ref)}\n    def _validate_security_group_property(self, value, typ):\n        \"\"\" typ will be either 'name' or 'description',\n            depending on the caller\n        \"\"\"\n        try:\n            val = value.strip()\n        except AttributeError:\n            msg = _(\"Security group %s is not a string or unicode\") % typ\n            raise exc.HTTPBadRequest(explanation=msg)\n        if not val:\n            msg = _(\"Security group %s cannot be empty.\") % typ\n            raise exc.HTTPBadRequest(explanation=msg)\n        if len(val) > 255:\n            msg = _(\"Security group %s should not be greater \"\n                            \"than 255 characters.\") % typ\n            raise exc.HTTPBadRequest(explanation=msg)\ndef _get_default_quotas():\n    defaults = {\n        'instances': FLAGS.quota_instances,\n        'cores': FLAGS.quota_cores,\n        'ram': FLAGS.quota_ram,\n        'volumes': FLAGS.quota_volumes,\n        'gigabytes': FLAGS.quota_gigabytes,\n        'floating_ips': FLAGS.quota_floating_ips,\n        'metadata_items': FLAGS.quota_metadata_items,\n        'injected_files': FLAGS.quota_injected_files,\n        'injected_file_content_bytes':\n            FLAGS.quota_injected_file_content_bytes,\n    }\n    return defaults\ndef get_class_quotas(context, quota_class, defaults=None):\n    \"\"\"Update defaults with the quota class values.\"\"\"\n    if not defaults:\n        defaults = _get_default_quotas()\n    quota = db.quota_class_get_all_by_name(context, quota_class)\n    for key in defaults.keys():\n        if key in quota:\n            defaults[key] = quota[key]\n    return defaults\ndef allowed_metadata_items(context, requested_metadata_items):\n    \"\"\"Return the number of metadata items allowed.\"\"\"\n    return _calculate_simple_quota(context, 'metadata_items',\n                                   requested_metadata_items)\ndef allowed_injected_files(context, requested_injected_files):\n    \"\"\"Return the number of injected files allowed.\"\"\"\n    return _calculate_simple_quota(context, 'injected_files',\n                                   requested_injected_files)\ndef allowed_injected_file_content_bytes(context, requested_bytes):\n    \"\"\"Return the number of bytes allowed per injected file content.\"\"\"\n    resource = 'injected_file_content_bytes'\n    return _calculate_simple_quota(context, resource, requested_bytes)\ndef allowed_injected_file_path_bytes(context):\n    \"\"\"Return the number of bytes allowed in an injected file path.\"\"\"\n    return FLAGS.quota_injected_file_path_bytes\ndef _security_group_rule_get_query(context, session=None):\n    return model_query(context, models.SecurityGroupIngressRule,\n                       session=session)\ndef provider_fw_rule_create(context, rule):\n    fw_rule_ref = models.ProviderFirewallRule()\n    fw_rule_ref.update(rule)\n    fw_rule_ref.save()\n    return fw_rule_ref\ndef provider_fw_rule_get_all(context):\n    return model_query(context, models.ProviderFirewallRule).all()\ndef provider_fw_rule_create(context, rule):\n    \"\"\"Add a firewall rule at the provider level (all hosts & instances).\"\"\"\n    return IMPL.provider_fw_rule_create(context, rule)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2012-2101",
        "description": "[{'lang': 'en', 'value': 'Openstack Compute (Nova) Folsom, 2012.1, and 2011.3 does not limit the number of security group rules, which allows remote authenticated users with certain permissions to cause a denial of service (CPU and hard drive consumption) via a network request that triggers a large number of iptables rules.'}]",
        "cwe_number": 264
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-97",
      "code": "    def __init__(self, *args, env=None, text=False, **kwargs):\n        if env is None:\n            env = os.environ.copy()\n        self._fix_pyinstaller_ld_path(env)\n        self.__text_mode = kwargs.get('encoding') or kwargs.get('errors') or text or kwargs.get('universal_newlines')\n        if text is True:\n            kwargs['universal_newlines'] = True\n            kwargs.setdefault('encoding', 'utf-8')\n            kwargs.setdefault('errors', 'replace')\n        super().__init__(*args, env=env, **kwargs, startupinfo=self._startupinfo)\ncompat_os_name = os._name if os.name == 'java' else os.name\nif compat_os_name == 'nt':\n    def compat_shlex_quote(s):\n        import re\n        return s if re.match(r'^[-_\\w./]+$', s) else '\"%s\"' % s.replace('\"', '\\\\\"')\nelse:\n    from shlex import quote as compat_shlex_quote\n    def run(self, info):\n        for tmpl in self.exec_cmd:\n            cmd = self.parse_cmd(tmpl, info)\n            self.to_screen('Executing command: %s' % cmd)\n            retCode = subprocess.call(encodeArgument(cmd), shell=True)\n            if retCode != 0:\n                raise PostProcessingError('Command returned error code %d' % retCode)\n        return [], info",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-40581",
        "description": "[{'lang': 'en', 'value': 'yt-dlp is a youtube-dl fork with additional features and fixes. yt-dlp allows the user to provide shell command lines to be executed at various stages in its download steps through the `--exec` flag. This flag allows output template expansion in its argument, so that metadata values may be used in the shell commands. The metadata fields can be combined with the `%q` conversion, which is intended to quote/escape these values so they can be safely passed to the shell. However, the escaping used for `cmd` (the shell used by Python\\'s `subprocess` on Windows) does not properly escape special characters, which can allow for remote code execution if `--exec` is used directly with maliciously crafted remote data. This vulnerability only impacts `yt-dlp` on Windows, and the vulnerability is present regardless of whether `yt-dlp` is run from `cmd` or from `PowerShell`. Support for output template expansion in `--exec`, along with this vulnerable behavior, was added to `yt-dlp` in version 2021.04.11. yt-dlp version 2023.09.24 fixes this issue by properly escaping each special character. `\\\\n` will be replaced by `\\\\r` as no way of escaping it has been found. It is recommended to upgrade yt-dlp to version 2023.09.24 as soon as possible. Also, always be careful when using --exec, because while this specific vulnerability has been patched, using unvalidated input in shell commands is inherently dangerous. For Windows users who are not able to upgrade: 1. Avoid using any output template expansion in --exec other than {} (filepath). 2. If expansion in --exec is needed, verify the fields you are using do not contain \", | or &. 3. Instead of using --exec, write the info json and load the fields from it instead.\\n'}]",
        "cwe_number": 78
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-98",
      "code": "    def _require_verified_user(self, request):\n        user = request.user\n        if not settings.WAGTAIL_2FA_REQUIRED:\n            return False\n        if not user.is_authenticated:\n            return False\n        if not (\n            user.is_staff\n            or user.is_superuser\n            or user.has_perms([\"wagtailadmin.access_admin\"])\n        ):\n            return False\n        if request.path in self._allowed_paths:\n            return False\n        return True\n    def _allowed_paths(self):\n        \"\"\"Return the paths the user may visit when not verified via otp\n        This result cannot be cached since we want to be compatible with the\n        django-hosts package. Django-hosts alters the urlconf based on the\n        hostname in the request, so the urls might exist for admin.<domain> but\n        not for www.<domain>.\n        \"\"\"\n        results = []\n        for route_name in self._allowed_url_names:\n            try:\n                results.append(settings.WAGTAIL_MOUNT_PATH + reverse(route_name))\n            except NoReverseMatch:\n                pass\n        return results",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2019-16766",
        "description": "[{'lang': 'en', 'value': \"When using wagtail-2fa before 1.3.0, if someone gains access to someone's Wagtail login credentials, they can log into the CMS and bypass the 2FA check by changing the URL. They can then add a new device and gain full access to the CMS. This problem has been patched in version 1.3.0.\"}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-99",
      "code": "def _dynarray_make_setter(dst, src):\n    assert isinstance(src.typ, DArrayType)\n    assert isinstance(dst.typ, DArrayType)\n    if src.value == \"~empty\":\n        return IRnode.from_list(STORE(dst, 0))\n    if src.value == \"multi\":\n        ret = [\"seq\"]\n        store_length = STORE(dst, len(src.args))\n        ann = None\n        if src.annotation is not None:\n            ann = f\"len({src.annotation})\"\n        store_length = IRnode.from_list(store_length, annotation=ann)\n        ret.append(store_length)\n        n_items = len(src.args)\n        for i in range(n_items):\n            k = IRnode.from_list(i, typ=\"uint256\")\n            dst_i = get_element_ptr(dst, k, array_bounds_check=False)\n            src_i = get_element_ptr(src, k, array_bounds_check=False)\n            ret.append(make_setter(dst_i, src_i))\n        return ret\n    with src.cache_when_complex(\"darray_src\") as (b1, src):\n        should_loop = (\n            src.encoding in (Encoding.ABI, Encoding.JSON_ABI)\n            and src.typ.subtype.abi_type.is_dynamic()\n        )\n        should_loop |= src.typ.subtype.abi_type.is_dynamic()\n        should_loop |= needs_clamp(src.typ.subtype, src.encoding)\n        with get_dyn_array_count(src).cache_when_complex(\"darray_count\") as (b2, count):\n            ret = [\"seq\"]\n            ret.append(STORE(dst, count))\n            if should_loop:\n                i = IRnode.from_list(_freshname(\"copy_darray_ix\"), typ=\"uint256\")\n                loop_body = make_setter(\n                    get_element_ptr(dst, i, array_bounds_check=False),\n                    get_element_ptr(src, i, array_bounds_check=False),\n                )\n                loop_body.annotation = f\"{dst}[i] = {src}[i]\"\n                ret.append([\"repeat\", i, 0, count, src.typ.count, loop_body])\n            else:\n                element_size = src.typ.subtype.memory_bytes_required\n                n_bytes = _mul(count, element_size)\n                max_bytes = src.typ.count * element_size\n                src_ = dynarray_data_ptr(src)\n                dst_ = dynarray_data_ptr(dst)\n                ret.append(copy_bytes(dst_, src_, n_bytes, max_bytes))\n            return b1.resolve(b2.resolve(ret))\ndef _get_element_ptr_tuplelike(parent, key):\n    typ = parent.typ\n    assert isinstance(typ, TupleLike)\n    if isinstance(typ, StructType):\n        assert isinstance(key, str)\n        subtype = typ.members[key]\n        attrs = list(typ.tuple_keys())\n        index = attrs.index(key)\n        annotation = key\n    else:\n        assert isinstance(key, int)\n        subtype = typ.members[key]\n        attrs = list(range(len(typ.members)))\n        index = key\n        annotation = None\n    if parent.value == \"~empty\":\n        return IRnode.from_list(\"~empty\", typ=subtype)\n    if parent.value == \"multi\":\n        assert parent.encoding != Encoding.ABI, \"no abi-encoded literals\"\n        return parent.args[index]\n    ofst = 0\n    if parent.encoding in (Encoding.ABI, Encoding.JSON_ABI):\n        if parent.location == STORAGE:\n            raise CompilerPanic(\"storage variables should not be abi encoded\")\n        member_t = typ.members[attrs[index]]\n        for i in range(index):\n            member_abi_t = typ.members[attrs[i]].abi_type\n            ofst += member_abi_t.embedded_static_size()\n        return _getelemptr_abi_helper(parent, member_t, ofst)\n    if parent.location.word_addressable:\n        for i in range(index):\n            ofst += typ.members[attrs[i]].storage_size_in_words\n    elif parent.location.byte_addressable:\n        for i in range(index):\n            ofst += typ.members[attrs[i]].memory_bytes_required\n    else:\n        raise CompilerPanic(f\"bad location {parent.location}\")\n    return IRnode.from_list(\n        add_ofst(parent, ofst),\n        typ=subtype,\n        location=parent.location,\n        encoding=parent.encoding,\n        annotation=annotation,\n    )\ndef _get_element_ptr_array(parent, key, array_bounds_check):\n    assert isinstance(parent.typ, ArrayLike)\n    if not is_integer_type(key.typ):\n        raise TypeCheckFailure(f\"{key.typ} used as array index\")\n    subtype = parent.typ.subtype\n    if parent.value == \"~empty\":\n        if array_bounds_check:\n            raise TypeCheckFailure(\"indexing into zero array not allowed\")\n        return IRnode.from_list(\"~empty\", subtype)\n    if parent.value == \"multi\":\n        assert isinstance(key.value, int)\n        return parent.args[key.value]\n    ix = unwrap_location(key)\n    if array_bounds_check:\n        clamp_op = \"uclamplt\"\n        is_darray = isinstance(parent.typ, DArrayType)\n        bound = get_dyn_array_count(parent) if is_darray else parent.typ.count\n        ix = IRnode.from_list([clamp_op, ix, bound], typ=ix.typ)\n    if parent.encoding in (Encoding.ABI, Encoding.JSON_ABI):\n        if parent.location == STORAGE:\n            raise CompilerPanic(\"storage variables should not be abi encoded\")\n        member_abi_t = subtype.abi_type\n        ofst = _mul(ix, member_abi_t.embedded_static_size())\n        return _getelemptr_abi_helper(parent, subtype, ofst)\n    if parent.location.word_addressable:\n        element_size = subtype.storage_size_in_words\n    elif parent.location.byte_addressable:\n        element_size = subtype.memory_bytes_required\n    else:\n        raise CompilerPanic(\"unreachable\")\n    ofst = _mul(ix, element_size)\n    if has_length_word(parent.typ):\n        data_ptr = add_ofst(parent, parent.location.word_scale * DYNAMIC_ARRAY_OVERHEAD)\n    else:\n        data_ptr = parent\n    return IRnode.from_list(add_ofst(data_ptr, ofst), typ=subtype, location=parent.location)\ndef needs_clamp(t, encoding):\n    if encoding not in (Encoding.ABI, Encoding.JSON_ABI):\n        return False\n    if isinstance(t, (ByteArrayLike, DArrayType)):\n        if encoding == Encoding.JSON_ABI:\n            return False\n        return True\n    if isinstance(t, BaseType) and t.typ not in (\"int256\", \"uint256\", \"bytes32\"):\n        return True\n    if isinstance(t, SArrayType):\n        return needs_clamp(t.subtype, encoding)\n    if isinstance(t, TupleLike):\n        return any(needs_clamp(m, encoding) for m in t.tuple_members())\n    return False\ndef _returndata_encoding(contract_sig):\n    if contract_sig.is_from_json:\n        return Encoding.JSON_ABI\n    return Encoding.ABI\ndef _unpack_returndata(buf, contract_sig, skip_contract_check, context):\n    return_t = contract_sig.return_type\n    if return_t is None:\n        return [\"pass\"], 0, 0\n    return_t = calculate_type_for_external_return(return_t)\n    should_unwrap_abi_tuple = return_t != contract_sig.return_type\n    abi_return_t = return_t.abi_type\n    min_return_size = abi_return_t.min_size()\n    max_return_size = abi_return_t.size_bound()\n    assert 0 < min_return_size <= max_return_size\n    ret_ofst = buf\n    ret_len = max_return_size\n    ret = []\n    if not skip_contract_check:\n        ret += [[\"assert\", [\"gt\", \"returndatasize\", min_return_size - 1]]]\n    buf = IRnode(buf, typ=return_t, encoding=_returndata_encoding(contract_sig), location=MEMORY)\n    if should_unwrap_abi_tuple:\n        buf = get_element_ptr(buf, 0, array_bounds_check=False)\n    ret += [buf]\n    return ret, ret_ofst, ret_len\ndef _external_call_helper(\n    contract_address,\n    contract_sig,\n    args_ir,\n    context,\n    value=None,\n    gas=None,\n    skip_contract_check=None,\n    expr=None,\n):\n    if value is None:\n        value = 0\n    if gas is None:\n        gas = \"gas\"\n    if skip_contract_check is None:\n        skip_contract_check = False\n    assert len(contract_sig.base_args) <= len(args_ir) <= len(contract_sig.args)\n    if context.is_constant() and contract_sig.mutability not in (\"view\", \"pure\"):\n        raise StateAccessViolation(\n            f\"May not call state modifying function '{contract_sig.name}' \"\n            f\"within {context.pp_constancy()}.\",\n            expr,\n        )\n    sub = [\"seq\"]\n    buf, arg_packer, args_ofst, args_len = _pack_arguments(contract_sig, args_ir, context)\n    ret_unpacker, ret_ofst, ret_len = _unpack_returndata(\n        buf, contract_sig, skip_contract_check, context\n    )\n    sub += arg_packer\n    if contract_sig.return_type is None and not skip_contract_check:\n        sub.append([\"assert\", [\"extcodesize\", contract_address]])\n    if context.is_constant() or contract_sig.mutability in (\"view\", \"pure\"):\n        call_op = [\"staticcall\", gas, contract_address, args_ofst, args_len, ret_ofst, ret_len]\n    else:\n        call_op = [\"call\", gas, contract_address, value, args_ofst, args_len, ret_ofst, ret_len]\n    sub.append(check_external_call(call_op))\n    if contract_sig.return_type is not None:\n        sub += ret_unpacker\n    ret = IRnode.from_list(\n        sub,\n        typ=contract_sig.return_type,\n        location=MEMORY,\n        encoding=_returndata_encoding(contract_sig),\n    )\n    return ret\ndef _should_decode(typ):\n    if isinstance(typ, BaseType):\n        return typ.typ not in (\"int256\", \"uint256\", \"bytes32\")\n    if isinstance(typ, (ByteArrayLike, DArrayType)):\n        return True\n    if isinstance(typ, SArrayType):\n        return _should_decode(typ.subtype)\n    if isinstance(typ, TupleLike):\n        return any(_should_decode(t) for t in typ.tuple_members())\n    raise CompilerPanic(f\"_should_decode({typ})\")\ndef _register_function_args(context: Context, sig: FunctionSignature) -> List[IRnode]:\n    ret = []\n    base_args_t = TupleType([arg.typ for arg in sig.base_args])\n    if sig.is_init_func:\n        base_args_ofst = IRnode(0, location=DATA, typ=base_args_t, encoding=Encoding.ABI)\n    else:\n        base_args_ofst = IRnode(4, location=CALLDATA, typ=base_args_t, encoding=Encoding.ABI)\n    for i, arg in enumerate(sig.base_args):\n        arg_ir = get_element_ptr(base_args_ofst, i)\n        if _should_decode(arg.typ):\n            p = context.new_variable(arg.name, arg.typ, is_mutable=False)\n            dst = IRnode(p, typ=arg.typ, location=MEMORY)\n            copy_arg = make_setter(dst, arg_ir)\n            copy_arg.source_pos = getpos(arg.ast_source)\n            ret.append(copy_arg)\n        else:\n            context.vars[arg.name] = VariableRecord(\n                name=arg.name,\n                pos=arg_ir,\n                typ=arg.typ,\n                mutable=False,\n                location=arg_ir.location,\n                encoding=Encoding.ABI,\n            )\n    return ret\ndef new_type_to_old_type(typ: new.BasePrimitive) -> old.NodeType:\n    if isinstance(typ, new.BoolDefinition):\n        return old.BaseType(\"bool\")\n    if isinstance(typ, new.AddressDefinition):\n        return old.BaseType(\"address\")\n    if isinstance(typ, new.InterfaceDefinition):\n        return old.InterfaceType(typ._id)\n    if isinstance(typ, new.BytesMDefinition):\n        m = typ._length\n        return old.BaseType(f\"bytes{m}\")\n    if isinstance(typ, new.BytesArrayDefinition):\n        return old.ByteArrayType(typ.length)\n    if isinstance(typ, new.StringDefinition):\n        return old.StringType(typ.length)\n    if isinstance(typ, new.DecimalDefinition):\n        return old.BaseType(\"decimal\")\n    if isinstance(typ, new.SignedIntegerAbstractType):\n        bits = typ._bits\n        return old.BaseType(\"int\" + str(bits))\n    if isinstance(typ, new.UnsignedIntegerAbstractType):\n        bits = typ._bits\n        return old.BaseType(\"uint\" + str(bits))\n    if isinstance(typ, new.ArrayDefinition):\n        return old.SArrayType(new_type_to_old_type(typ.value_type), typ.length)\n    if isinstance(typ, new.DynamicArrayDefinition):\n        return old.DArrayType(new_type_to_old_type(typ.value_type), typ.length)\n    if isinstance(typ, new.TupleDefinition):\n        return old.TupleType(typ.value_type)\n    if isinstance(typ, new.StructDefinition):\n        return old.StructType(\n            {n: new_type_to_old_type(t) for (n, t) in typ.members.items()}, typ._id\n        )\n    raise InvalidType(f\"unknown type {typ}\")",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-24788",
        "description": "[{'lang': 'en', 'value': 'Vyper is a pythonic Smart Contract Language for the ethereum virtual machine. Versions of vyper prior to 0.3.2 suffer from a potential buffer overrun. Importing a function from a JSON interface which returns `bytes` generates bytecode which does not clamp bytes length, potentially resulting in a buffer overrun. Users are advised to upgrade. There are no known workarounds for this issue.'}]",
        "cwe_number": 119
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-100",
      "code": "def gravatar(context, user, size=None):\n    \"\"\"\n    Outputs the HTML for displaying a user's gravatar.\n    This can take an optional size of the image (defaults to 80 if not\n    specified).\n    This is also influenced by the following settings:\n        GRAVATAR_SIZE    - Default size for gravatars\n        GRAVATAR_RATING  - Maximum allowed rating (g, pg, r, x)\n        GRAVATAR_DEFAULT - Default image set to show if the user hasn't\n                           specified a gravatar (identicon, monsterid, wavatar)\n    See http://www.gravatar.com/ for more information.\n    \"\"\"\n    url = get_gravatar_url(context['request'], user, size)\n    if url:\n        return ('<img src=\"%s\" width=\"%s\" height=\"%s\" alt=\"%s\" '\n                '     class=\"gravatar\"/>' %\n                (url, size, size, user.get_full_name() or user.username))\n    else:\n        return ''",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2014-3995",
        "description": "[{'lang': 'en', 'value': 'Cross-site scripting (XSS) vulnerability in gravatars/templatetags/gravatars.py in Djblets before 0.7.30 and 0.8.x before 0.8.3 for Django allows remote attackers to inject arbitrary web script or HTML via a user display name.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-101",
      "code": "    def rebuild_proxies(self, prepared_request, proxies):\n        \"\"\"This method re-evaluates the proxy configuration by considering the\n        environment variables. If we are redirected to a URL covered by\n        NO_PROXY, we strip the proxy configuration. Otherwise, we set missing\n        proxy keys for this URL (in case they were stripped by a previous\n        redirect).\n        This method also replaces the Proxy-Authorization header where\n        necessary.\n        :rtype: dict\n        \"\"\"\n        headers = prepared_request.headers\n        scheme = urlparse(prepared_request.url).scheme\n        new_proxies = resolve_proxies(prepared_request, proxies, self.trust_env)\n        if \"Proxy-Authorization\" in headers:\n            del headers[\"Proxy-Authorization\"]\n        try:\n            username, password = get_auth_from_url(new_proxies[scheme])\n        except KeyError:\n            username, password = None, None\n        if username and password:\n            headers[\"Proxy-Authorization\"] = _basic_auth_str(username, password)\n        return new_proxies\n    def rebuild_method(self, prepared_request, response):\n        \"\"\"When being redirected we may want to change the method of the request\n        based on certain specs or browser behavior.\n        \"\"\"\n        method = prepared_request.method\n        if response.status_code == codes.see_other and method != \"HEAD\":\n            method = \"GET\"\n        if response.status_code == codes.found and method != \"HEAD\":\n            method = \"GET\"\n        if response.status_code == codes.moved and method == \"POST\":\n            method = \"GET\"\n        prepared_request.method = method",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-32681",
        "description": "[{'lang': 'en', 'value': 'Requests is a HTTP library. Since Requests 2.3.0, Requests has been leaking Proxy-Authorization headers to destination servers when redirected to an HTTPS endpoint. This is a product of how we use `rebuild_proxies` to reattach the `Proxy-Authorization` header to requests. For HTTP connections sent through the tunnel, the proxy will identify the header in the request itself and remove it prior to forwarding to the destination server. However when sent over HTTPS, the `Proxy-Authorization` header must be sent in the CONNECT request as the proxy has no visibility into the tunneled request. This results in Requests forwarding proxy credentials to the destination server unintentionally, allowing a malicious actor to potentially exfiltrate sensitive information. This issue has been patched in version 2.31.0.\\n\\n'}]",
        "cwe_number": 200
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-102",
      "code": "    def _setup_app(self, app):\n        from octoprint.server.util.flask import (\n            OctoPrintFlaskRequest,\n            OctoPrintFlaskResponse,\n            OctoPrintJsonEncoder,\n            OctoPrintSessionInterface,\n            PrefixAwareJinjaEnvironment,\n            ReverseProxiedEnvironment,\n        )\n        app.jinja_environment = PrefixAwareJinjaEnvironment\n        app.config[\"TEMPLATES_AUTO_RELOAD\"] = True\n        app.config[\"JSONIFY_PRETTYPRINT_REGULAR\"] = False\n        app.config[\"REMEMBER_COOKIE_HTTPONLY\"] = True\n        app.debug = self._debug\n        app.json_encoder = OctoPrintJsonEncoder\n        s = settings()\n        secret_key = s.get([\"server\", \"secretKey\"])\n        if not secret_key:\n            import string\n            from random import choice\n            chars = string.ascii_lowercase + string.ascii_uppercase + string.digits\n            secret_key = \"\".join(choice(chars) for _ in range(32))\n            s.set([\"server\", \"secretKey\"], secret_key)\n            s.save()\n        app.secret_key = secret_key\n        reverse_proxied = ReverseProxiedEnvironment(\n            header_prefix=s.get([\"server\", \"reverseProxy\", \"prefixHeader\"]),\n            header_scheme=s.get([\"server\", \"reverseProxy\", \"schemeHeader\"]),\n            header_host=s.get([\"server\", \"reverseProxy\", \"hostHeader\"]),\n            header_server=s.get([\"server\", \"reverseProxy\", \"serverHeader\"]),\n            header_port=s.get([\"server\", \"reverseProxy\", \"portHeader\"]),\n            prefix=s.get([\"server\", \"reverseProxy\", \"prefixFallback\"]),\n            scheme=s.get([\"server\", \"reverseProxy\", \"schemeFallback\"]),\n            host=s.get([\"server\", \"reverseProxy\", \"hostFallback\"]),\n            server=s.get([\"server\", \"reverseProxy\", \"serverFallback\"]),\n            port=s.get([\"server\", \"reverseProxy\", \"portFallback\"]),\n        )\n        OctoPrintFlaskRequest.environment_wrapper = reverse_proxied\n        app.request_class = OctoPrintFlaskRequest\n        app.response_class = OctoPrintFlaskResponse\n        app.session_interface = OctoPrintSessionInterface()\n        @app.before_request\n        def before_request():\n            g.locale = self._get_locale()\n            g.start_time = time.monotonic()\n            if self._debug and \"perfprofile\" in request.args:\n                try:\n                    from pyinstrument import Profiler\n                    g.perfprofiler = Profiler()\n                    g.perfprofiler.start()\n                except ImportError:\n                    pass\n        @app.after_request\n        def after_request(response):\n            if request.method == \"POST\":\n                response.cache_control.no_cache = True\n            response.headers.add(\"X-Clacks-Overhead\", \"GNU Terry Pratchett\")\n            if hasattr(g, \"perfprofiler\"):\n                g.perfprofiler.stop()\n                output_html = g.perfprofiler.output_html()\n                return make_response(output_html)\n            if hasattr(g, \"start_time\"):\n                end_time = time.monotonic()\n                duration_ms = int((end_time - g.start_time) * 1000)\n                response.headers.add(\"Server-Timing\", f\"app;dur={duration_ms}\")\n            return response\n        from octoprint.util.jinja import MarkdownFilter\n        MarkdownFilter(app)\n    def _setup_i18n(self, app):\n        global babel\n        global LOCALES\n        global LANGUAGES\n        babel = Babel(app)\n        def get_available_locale_identifiers(locales):\n            result = set()\n            for locale in locales:\n                result.add(locale.language)\n                if locale.territory:\n                    result.add(f\"{locale.language}_{locale.territory}\")\n            return result\n        LOCALES = babel.list_translations()\n        LANGUAGES = get_available_locale_identifiers(LOCALES)\n        @babel.localeselector\n        def get_locale():\n            return self._get_locale()",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-2822",
        "description": "[{'lang': 'en', 'value': 'An attacker can freely brute force username and password and can takeover any account. An attacker could easily guess user passwords and gain access to user and administrative accounts.'}]",
        "cwe_number": 307
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-103",
      "code": "def load_metadata_preview(request, c_type, c_id, conn=None, share_id=None, **kwargs):\n    \"\"\"\n    This is the image 'Preview' tab for the right-hand panel.\n    \"\"\"\n    context = {}\n    index = getIntOrDefault(request, \"index\", 0)\n    manager = BaseContainer(conn, **{str(c_type): long(c_id)})\n    if share_id:\n        context[\"share\"] = BaseShare(conn, share_id)\n    if c_type == \"well\":\n        manager.image = manager.well.getImage(index)\n    allRdefs = manager.image.getAllRenderingDefs()\n    rdefs = {}\n    rdefId = manager.image.getRenderingDefId()\n    for r in allRdefs:\n        ownerId = r[\"owner\"][\"id\"]\n        r[\"current\"] = r[\"id\"] == rdefId\n        if ownerId not in rdefs or rdefs[ownerId][\"id\"] < r[\"id\"]:\n            rdefs[ownerId] = r\n    rdefs = rdefs.values()\n    rdefQueries = []\n    for r in rdefs:\n        chs = []\n        for i, c in enumerate(r[\"c\"]):\n            act = \"-\"\n            if c[\"active\"]:\n                act = \"\"\n            color = c[\"lut\"] if \"lut\" in c else c[\"color\"]\n            reverse = \"r\" if c[\"inverted\"] else \"-r\"\n            chs.append(\n                \"%s%s|%s:%s%s$%s\" % (act, i + 1, c[\"start\"], c[\"end\"], reverse, color)\n            )\n        rdefQueries.append(\n            {\n                \"id\": r[\"id\"],\n                \"owner\": r[\"owner\"],\n                \"c\": \",\".join(chs),\n                \"m\": r[\"model\"] == \"greyscale\" and \"g\" or \"c\",\n            }\n        )\n    max_w, max_h = conn.getMaxPlaneSize()\n    size_x = manager.image.getSizeX()\n    size_y = manager.image.getSizeY()\n    context[\"tiledImage\"] = (size_x * size_y) > (max_w * max_h)\n    context[\"manager\"] = manager\n    context[\"rdefsJson\"] = json.dumps(rdefQueries)\n    context[\"rdefs\"] = rdefs\n    context[\"template\"] = \"webclient/annotations/metadata_preview.html\"\n    return context",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-41132",
        "description": "[{'lang': 'en', 'value': 'OMERO.web provides a web based client and plugin infrastructure. In versions prior to 5.11.0, a variety of templates do not perform proper sanitization through HTML escaping. Due to the lack of sanitization and use of ``jQuery.html()``, there are a whole host of cross-site scripting possibilities with specially crafted input to a variety of fields. This issue is patched in version 5.11.0. There are no known workarounds aside from upgrading.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-104",
      "code": "def setup_shadow_filesystem(virtual_documents_uri: str):\n    if not virtual_documents_uri.startswith(\"file:/\"):\n        raise ShadowFilesystemError(\n            'Virtual documents URI has to start with \"file:/\", got '\n            + virtual_documents_uri\n        )\n    initialized = False\n    failures: List[Exception] = []\n    shadow_filesystem = Path(file_uri_to_path(virtual_documents_uri))\n    @lsp_message_listener(\"client\")\n    async def shadow_virtual_documents(scope, message, language_server, manager):\n        \"\"\"Intercept a message with document contents creating a shadow file for it.\n        Only create the shadow file if the URI matches the virtual documents URI.\n        Returns the path on filesystem where the content was stored.\n        \"\"\"\n        nonlocal initialized\n        server_spec = manager.language_servers[language_server]\n        if not server_spec.get(\"requires_documents_on_disk\", True):\n            return\n        if not message.get(\"method\") in WRITE_ONE:\n            return\n        document = extract_or_none(message, [\"params\", \"textDocument\"])\n        if document is None:\n            raise ShadowFilesystemError(\n                \"Could not get textDocument from: {}\".format(message)\n            )\n        uri = extract_or_none(document, [\"uri\"])\n        if not uri:\n            raise ShadowFilesystemError(\"Could not get URI from: {}\".format(message))\n        if not uri.startswith(virtual_documents_uri):\n            return\n        if not initialized:\n            if len(failures) == 3:\n                return\n            try:\n                shadow_filesystem.mkdir(parents=True, exist_ok=True)\n                rmtree(str(shadow_filesystem))\n                shadow_filesystem.mkdir(parents=True, exist_ok=True)\n            except (OSError, PermissionError, FileNotFoundError) as e:\n                failures.append(e)\n                if len(failures) == 3:\n                    manager.log.warn(\n                        \"[lsp] initialization of shadow filesystem failed three times\"\n                        \" check if the path set by `LanguageServerManager.virtual_documents_dir`\"\n                        \" or `JP_LSP_VIRTUAL_DIR` is correct; if this is happening with a server\"\n                        \" for which you control (or wish to override) jupyter-lsp specification\"\n                        \" you can try switching `requires_documents_on_disk` off. The errors were: %s\",\n                        failures,\n                    )\n                return\n            initialized = True\n        path = file_uri_to_path(uri)\n        editable_file = EditableFile(path)\n        await editable_file.read()\n        text = extract_or_none(document, [\"text\"])\n        if text is not None:\n            changes = [{\"text\": text}]\n        else:\n            if message[\"method\"] != \"textDocument/didChange\":\n                return\n            if \"contentChanges\" not in message[\"params\"]:\n                raise ShadowFilesystemError(\n                    \"textDocument/didChange is missing contentChanges\"\n                )\n            changes = message[\"params\"][\"contentChanges\"]\n        if len(changes) > 1:\n            manager.log.warn(\n                \"LSP warning: up to one change supported for textDocument/didChange\"\n            )\n        for change in changes[:1]:\n            change_range = change.get(\"range\", editable_file.full_range)\n            editable_file.apply_change(change[\"text\"], **change_range)\n        await editable_file.write()\n        return path\n    return shadow_virtual_documents\ndef load_jupyter_server_extension(nbapp):\n    \"\"\"create a LanguageServerManager and add handlers\"\"\"\n    nbapp.add_traits(language_server_manager=traitlets.Instance(LanguageServerManager))\n    manager = nbapp.language_server_manager = LanguageServerManager(parent=nbapp)\n    contents = nbapp.contents_manager\n    page_config = nbapp.web_app.settings.setdefault(\"page_config_data\", {})\n    root_uri = \"\"\n    virtual_documents_uri = \"\"\n    if hasattr(contents, \"root_dir\"):\n        root_uri = normalized_uri(contents.root_dir)\n        nbapp.log.debug(\"[lsp] rootUri will be %s\", root_uri)\n        virtual_documents_uri = normalized_uri(\n            Path(contents.root_dir) / manager.virtual_documents_dir\n        )\n        nbapp.log.debug(\"[lsp] virtualDocumentsUri will be %s\", virtual_documents_uri)\n    else:\n        nbapp.log.warn(\n            \"[lsp] %s did not appear to have a root_dir, could not set rootUri\",\n            contents,\n        )\n    page_config.update(rootUri=root_uri, virtualDocumentsUri=virtual_documents_uri)\n    add_handlers(nbapp)\n    nbapp.io_loop.call_later(0, initialize, nbapp, virtual_documents_uri)\n    manager = None\n    def initialize(self, manager: LanguageServerManager):\n        self.manager = manager\n    def initialize(self, *args, **kwargs):\n        super().initialize(*args, **kwargs)\n    async def get(self):\n        \"\"\"finish with the JSON representations of the sessions\"\"\"\n        await self.manager.ready()\n        response = {\n            \"version\": 2,\n            \"sessions\": {\n                language_server: session.to_json()\n                for language_server, session in self.manager.sessions.items()\n            },\n            \"specs\": {\n                key: censored_spec(spec)\n                for key, spec in self.manager.all_language_servers.items()\n            },\n        }\n        errors = list(self.validator.iter_errors(response))\n        if errors:\n            self.log.warning(\"{} validation errors: {}\".format(len(errors), errors))\n        self.finish(response)\ndef normalized_uri(root_dir):\n    \"\"\"Attempt to make an LSP rootUri from a ContentsManager root_dir\n    Special care must be taken around windows paths: the canonical form of\n    windows drives and UNC paths is lower case\n    \"\"\"\n    root_uri = pathlib.Path(root_dir).expanduser().resolve().as_uri()\n    root_uri = re.sub(\n        RE_PATH_ANCHOR, lambda m: \"file://{}\".format(m.group(1).lower()), root_uri\n    )\n    return root_uri",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-22415",
        "description": "[{'lang': 'en', 'value': 'jupyter-lsp is a coding assistance tool for JupyterLab (code navigation + hover suggestions + linters + autocompletion + rename) using Language Server Protocol. Installations of jupyter-lsp running in environments without configured file system access control (on the operating system level), and with jupyter-server instances exposed to non-trusted network are vulnerable to unauthorised access and modification of file system beyond the jupyter root directory. This issue has been patched in version 2.2.2 and all users are advised to upgrade. Users unable to upgrade should uninstall jupyter-lsp.'}]",
        "cwe_number": 22
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-105",
      "code": "def config():\n    config_disabled = (\n            app.config['CONFIG_DISABLE'] or\n            not valid_user_session(session))\n    if request.method == 'GET':\n        return json.dumps(g.user_config.__dict__)\n    elif request.method == 'PUT' and not config_disabled:\n        if 'name' in request.args:\n            config_pkl = os.path.join(\n                app.config['CONFIG_PATH'],\n                request.args.get('name'))\n            session['config'] = (pickle.load(open(config_pkl, 'rb'))\n                                 if os.path.exists(config_pkl)\n                                 else session['config'])\n            return json.dumps(session['config'])\n        else:\n            return json.dumps({})\n    elif not config_disabled:\n        config_data = request.form.to_dict()\n        if 'url' not in config_data or not config_data['url']:\n            config_data['url'] = g.user_config.url\n        if 'name' in request.args:\n            pickle.dump(\n                config_data,\n                open(os.path.join(\n                    app.config['CONFIG_PATH'],\n                    request.args.get('name')), 'wb'))\n        session['config'] = config_data\n        return redirect(config_data['url'])\n    else:\n        return redirect(url_for('.index'), code=403)\ndef element():\n    element_url = src_url = request.args.get('url')\n    if element_url.startswith('gAAAAA'):\n        try:\n            cipher_suite = Fernet(g.session_key)\n            src_url = cipher_suite.decrypt(element_url.encode()).decode()\n        except (InvalidSignature, InvalidToken) as e:\n            return render_template(\n                'error.html',\n                error_message=str(e)), 401\n    src_type = request.args.get('type')\n    try:\n        file_data = g.user_request.send(base_url=src_url).content\n        tmp_mem = io.BytesIO()\n        tmp_mem.write(file_data)\n        tmp_mem.seek(0)\n        return send_file(tmp_mem, mimetype=src_type)\n    except exceptions.RequestException:\n        pass\n    empty_gif = base64.b64decode(\n        'R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==')\n    return send_file(io.BytesIO(empty_gif), mimetype='image/gif')",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-22205",
        "description": "[{'lang': 'en', 'value': 'Whoogle Search is a self-hosted metasearch engine. In versions 0.8.3 and prior, the `window` endpoint does not sanitize user-supplied input from the `location` variable and passes it to the `send` method which sends a `GET` request on lines 339-343 in `request.py,` which leads to a server-side request forgery. This issue allows for crafting GET requests to internal and external resources on behalf of the server. For example, this issue would allow for accessing resources on the internal network that the server has access to, even though these resources may not be accessible on the internet. This issue is fixed in version 0.8.4.\\n\\n'}]",
        "cwe_number": 918
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-106",
      "code": "def choose_tls_port_and_get_bind_sock(config, options):\n    if \"tlsport\" in options:\n        ports_to_try = [int(options[\"tlsport\"])]\n    else:\n        lower_bound, upper_bound = get_tls_port_range(config)\n        ports_to_try = list(range(lower_bound, upper_bound))\n        random.shuffle(ports_to_try)\n    if \"netns\" not in options:\n        tls_port_sock = find_tls_port_in_range_and_get_bind_sock(ports_to_try)\n    else:\n        with NetNS(nspath=options[\"netns\"]):\n            tls_port_sock = find_tls_port_in_range_and_get_bind_sock(ports_to_try)\n    if tls_port_sock:\n        return tls_port_sock\n    if \"tlsport\" in options:\n        fatal_error(\n            \"Specified port [%s] is unavailable. Try selecting a different port.\"\n            % options[\"tlsport\"]\n        )\n    else:\n        fatal_error(\n            \"Failed to locate an available port in the range [%d, %d], try specifying a different port range in %s\"\n            % (lower_bound, upper_bound, CONFIG_FILE)\n        )\ndef find_tls_port_in_range_and_get_bind_sock(ports_to_try):\n    sock = socket.socket()\n    for tls_port in ports_to_try:\n        try:\n            logging.info(\"binding %s\", tls_port)\n            sock.bind((\"localhost\", tls_port))\n            return sock\n        except socket.error as e:\n            logging.info(e)\n            continue\n    sock.close()\n    return None\ndef is_ocsp_enabled(config, options):\n    if \"ocsp\" in options:\n        return True\n    elif \"noocsp\" in options:\n        return False\n    else:\n        return get_boolean_config_item_value(\n            config, CONFIG_SECTION, \"stunnel_check_cert_validity\", default_value=False\n        )\ndef get_mount_specific_filename(fs_id, mountpoint, tls_port):\n    return \"%s.%s.%d\" % (\n        fs_id,\n        os.path.abspath(mountpoint).replace(os.sep, \".\").lstrip(\".\"),\n        tls_port,\n    )\ndef serialize_stunnel_config(config, header=None):\n    lines = []\n    if header:\n        lines.append(\"[%s]\" % header)\n    for k, v in config.items():\n        if type(v) is list:\n            for item in v:\n                lines.append(\"%s = %s\" % (k, item))\n        else:\n            lines.append(\"%s = %s\" % (k, v))\n    return lines\ndef get_init_system(comm_file=\"/proc/1/comm\"):\n    init_system = DEFAULT_UNKNOWN_VALUE\n    if not check_if_platform_is_mac():\n        try:\n            with open(comm_file) as f:\n                init_system = f.read().strip()\n        except IOError:\n            logging.warning(\"Unable to read %s\", comm_file)\n    else:\n        init_system = \"launchd\"\n    logging.debug(\"Identified init system: %s\", init_system)\n    return init_system\ndef check_network_target(fs_id):\n    with open(os.devnull, \"w\") as devnull:\n        if not check_if_platform_is_mac():\n            rc = subprocess.call(\n                [\"systemctl\", \"is-active\", \"network.target\"],\n                stdout=devnull,\n                stderr=devnull,\n                close_fds=True,\n            )\n        else:\n            rc = subprocess.call(\n                [\"sudo\", \"ifconfig\", \"en0\"],\n                stdout=devnull,\n                stderr=devnull,\n                close_fds=True,\n            )\n    if rc != 0:\n        fatal_error(\n            'Failed to mount %s because the network was not yet available, add \"_netdev\" to your mount options'\n            % fs_id,\n            exit_code=0,\n        )\ndef bootstrap_tls(\n    config,\n    init_system,\n    dns_name,\n    fs_id,\n    mountpoint,\n    options,\n    state_file_dir=STATE_FILE_DIR,\n    fallback_ip_address=None,\n):\n    tls_port_sock = choose_tls_port_and_get_bind_sock(config, options)\n    tls_port = get_tls_port_from_sock(tls_port_sock)\n    try:\n        options[\"tlsport\"] = tls_port\n        use_iam = \"iam\" in options\n        ap_id = options.get(\"accesspoint\")\n        cert_details = {}\n        security_credentials = None\n        client_info = get_client_info(config)\n        region = get_target_region(config)\n        if use_iam:\n            aws_creds_uri = options.get(\"awscredsuri\")\n            if aws_creds_uri:\n                kwargs = {\"aws_creds_uri\": aws_creds_uri}\n            else:\n                kwargs = {\"awsprofile\": get_aws_profile(options, use_iam)}\n            security_credentials, credentials_source = get_aws_security_credentials(\n                config, use_iam, region, **kwargs\n            )\n            if credentials_source:\n                cert_details[\"awsCredentialsMethod\"] = credentials_source\n        if ap_id:\n            cert_details[\"accessPoint\"] = ap_id\n        cert_details[\"mountStateDir\"] = (\n            get_mount_specific_filename(fs_id, mountpoint, tls_port) + \"+\"\n        )\n        cert_details[\"commonName\"] = socket.gethostname()[0:64]\n        region = get_target_region(config)\n        cert_details[\"region\"] = region\n        cert_details[\"certificateCreationTime\"] = create_certificate(\n            config,\n            cert_details[\"mountStateDir\"],\n            cert_details[\"commonName\"],\n            cert_details[\"region\"],\n            fs_id,\n            security_credentials,\n            ap_id,\n            client_info,\n            base_path=state_file_dir,\n        )\n        cert_details[\"certificate\"] = os.path.join(\n            state_file_dir, cert_details[\"mountStateDir\"], \"certificate.pem\"\n        )\n        cert_details[\"privateKey\"] = get_private_key_path()\n        cert_details[\"fsId\"] = fs_id\n        start_watchdog(init_system)\n        if not os.path.exists(state_file_dir):\n            create_required_directory(config, state_file_dir)\n        verify_level = int(options.get(\"verify\", DEFAULT_STUNNEL_VERIFY_LEVEL))\n        ocsp_enabled = is_ocsp_enabled(config, options)\n        stunnel_config_file = write_stunnel_config_file(\n            config,\n            state_file_dir,\n            fs_id,\n            mountpoint,\n            tls_port,\n            dns_name,\n            verify_level,\n            ocsp_enabled,\n            options,\n            region,\n            cert_details=cert_details,\n            fallback_ip_address=fallback_ip_address,\n        )\n        tunnel_args = [_stunnel_bin(), stunnel_config_file]\n        if \"netns\" in options:\n            tunnel_args = [\"nsenter\", \"--net=\" + options[\"netns\"]] + tunnel_args\n    finally:\n        logging.debug(\"Closing socket used to choose TLS port %s.\", tls_port)\n        tls_port_sock.close()\n    logging.info('Starting TLS tunnel: \"%s\"', \" \".join(tunnel_args))\n    tunnel_proc = subprocess.Popen(\n        tunnel_args,\n        stdout=subprocess.DEVNULL,\n        stderr=subprocess.PIPE,\n        preexec_fn=os.setsid,\n        close_fds=True,\n    )\n    logging.info(\"Started TLS tunnel, pid: %d\", tunnel_proc.pid)\n    temp_tls_state_file = write_tls_tunnel_state_file(\n        fs_id,\n        mountpoint,\n        tls_port,\n        tunnel_proc.pid,\n        tunnel_args,\n        [stunnel_config_file],\n        state_file_dir,\n        cert_details=cert_details,\n    )\n    if \"netns\" not in options:\n        test_tlsport(options[\"tlsport\"])\n    else:\n        with NetNS(nspath=options[\"netns\"]):\n            test_tlsport(options[\"tlsport\"])\n    try:\n        yield tunnel_proc\n    finally:\n        os.rename(\n            os.path.join(state_file_dir, temp_tls_state_file),\n            os.path.join(state_file_dir, temp_tls_state_file[1:]),\n        )\ndef test_tlsport(tlsport):\n    retry_times = 5\n    while not verify_tlsport_can_be_connected(tlsport) and retry_times > 0:\n        logging.debug(\n            \"The tlsport %s cannot be connected yet, sleep %s(s), %s retry time(s) left\",\n            tlsport,\n            DEFAULT_TIMEOUT,\n            retry_times,\n        )\n        time.sleep(DEFAULT_TIMEOUT)\n        retry_times -= 1\ndef check_if_nfsvers_is_compatible_with_macos(options):\n    if (\n        (\"nfsvers\" in options and options[\"nfsvers\"] == \"4.1\")\n        or (\"vers\" in options and options[\"vers\"] == \"4.1\")\n        or (\"minorversion\" in options and options[\"minorversion\"] == 1)\n    ):\n        fatal_error(\"NFSv4.1 is not supported on MacOS, please switch to NFSv4.0\")\ndef get_nfs_mount_options(options):\n    if \"nfsvers\" not in options and \"vers\" not in options:\n        options[\"nfsvers\"] = \"4.1\" if not check_if_platform_is_mac() else \"4.0\"\n    if check_if_platform_is_mac():\n        check_if_nfsvers_is_compatible_with_macos(options)\n    if \"rsize\" not in options:\n        options[\"rsize\"] = \"1048576\"\n    if \"wsize\" not in options:\n        options[\"wsize\"] = \"1048576\"\n    if \"soft\" not in options and \"hard\" not in options:\n        options[\"hard\"] = None\n    if \"timeo\" not in options:\n        options[\"timeo\"] = \"600\"\n    if \"retrans\" not in options:\n        options[\"retrans\"] = \"2\"\n    if \"noresvport\" not in options:\n        options[\"noresvport\"] = None\n    if check_if_platform_is_mac():\n        options[\"mountport\"] = \"2049\"\n    if \"tls\" in options:\n        options[\"port\"] = options[\"tlsport\"]\n    def to_nfs_option(k, v):\n        if v is None:\n            return k\n        return \"%s=%s\" % (str(k), str(v))\n    nfs_options = [\n        to_nfs_option(k, v) for k, v in options.items() if k not in EFS_ONLY_OPTIONS\n    ]\n    return \",\".join(nfs_options)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-46174",
        "description": "[{'lang': 'en', 'value': 'efs-utils is a set of Utilities for Amazon Elastic File System (EFS). A potential race condition issue exists within the Amazon EFS mount helper in efs-utils versions v1.34.3 and below. When using TLS to mount file systems, the mount helper allocates a local port for stunnel to receive NFS connections prior to applying the TLS tunnel. In affected versions, concurrent mount operations can allocate the same local port, leading to either failed mount operations or an inappropriate mapping from an EFS customer\u2019s local mount points to that customer\u2019s EFS file systems. This issue is patched in version v1.34.4. There is no recommended work around. We recommend affected users update the installed version of efs-utils to v1.34.4 or later.'}]",
        "cwe_number": 362
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-107",
      "code": "def _generate_kwarg_handlers(context: Context, sig: FunctionSignature) -> List[Any]:\n    def handler_for(calldata_kwargs, default_kwargs):\n        calldata_args = sig.base_args + calldata_kwargs\n        calldata_args_t = TupleT(list(arg.typ for arg in calldata_args))\n        abi_sig = sig.abi_signature_for_kwargs(calldata_kwargs)\n        method_id = _annotated_method_id(abi_sig)\n        calldata_kwargs_ofst = IRnode(\n            4, location=CALLDATA, typ=calldata_args_t, encoding=Encoding.ABI\n        )\n        ret = [\"seq\"]\n        args_abi_t = calldata_args_t.abi_type\n        calldata_min_size = args_abi_t.min_size() + 4\n        ret.append([\"assert\", [\"ge\", \"calldatasize\", calldata_min_size]])\n        n_base_args = len(sig.base_args)\n        for i, arg_meta in enumerate(calldata_kwargs):\n            k = n_base_args + i\n            dst = context.lookup_var(arg_meta.name).pos\n            lhs = IRnode(dst, location=MEMORY, typ=arg_meta.typ)\n            rhs = get_element_ptr(calldata_kwargs_ofst, k, array_bounds_check=False)\n            copy_arg = make_setter(lhs, rhs)\n            copy_arg.source_pos = getpos(arg_meta.ast_source)\n            ret.append(copy_arg)\n        for x in default_kwargs:\n            dst = context.lookup_var(x.name).pos\n            lhs = IRnode(dst, location=MEMORY, typ=x.typ)\n            lhs.source_pos = getpos(x.ast_source)\n            kw_ast_val = sig.default_values[x.name]\n            rhs = Expr(kw_ast_val, context).ir_node\n            copy_arg = make_setter(lhs, rhs)\n            copy_arg.source_pos = getpos(x.ast_source)\n            ret.append(copy_arg)\n        ret.append([\"goto\", sig.external_function_base_entry_label])\n        ret = [\"if\", [\"eq\", \"_calldata_method_id\", method_id], ret]\n        return ret\n    ret = [\"seq\"]\n    keyword_args = sig.default_args\n    for arg in keyword_args:\n        context.new_variable(arg.name, arg.typ, is_mutable=False)\n    for i, _ in enumerate(keyword_args):\n        calldata_kwargs = keyword_args[:i]\n        default_kwargs = keyword_args[i:]\n        ret.append(handler_for(calldata_kwargs, default_kwargs))\n    ret.append(handler_for(keyword_args, []))\n    return ret\ndef generate_ir_for_external_function(code, sig, context, skip_nonpayable_check):\n    \"\"\"Return the IR for an external function. Includes code to inspect the method_id,\n    enter the function (nonpayable and reentrancy checks), handle kwargs and exit\n    the function (clean up reentrancy storage variables)\n    \"\"\"\n    func_type = code._metadata[\"type\"]\n    nonreentrant_pre, nonreentrant_post = get_nonreentrant_lock(func_type)\n    handle_base_args = _register_function_args(context, sig)\n    kwarg_handlers = _generate_kwarg_handlers(context, sig)\n    body = [\"seq\"]\n    body += handle_base_args\n    if sig.mutability != \"payable\" and not skip_nonpayable_check:\n        body += [[\"assert\", [\"iszero\", \"callvalue\"]]]\n    body += nonreentrant_pre\n    body += [parse_body(code.body, context, ensure_terminated=True)]\n    body = [\"label\", sig.external_function_base_entry_label, [\"var_list\"], body]\n    exit_sequence = [\"seq\"] + nonreentrant_post\n    if sig.is_init_func:\n        pass\n    elif context.return_type is None:\n        exit_sequence += [[\"stop\"]]\n    else:\n        exit_sequence += [[\"return\", \"ret_ofst\", \"ret_len\"]]\n    exit_sequence_args = [\"var_list\"]\n    if context.return_type is not None:\n        exit_sequence_args += [\"ret_ofst\", \"ret_len\"]\n    exit = [\"label\", sig.exit_sequence_label, exit_sequence_args, exit_sequence]\n    func_common_ir = [\"seq\", body, exit]\n    if sig.is_default_func or sig.is_init_func:\n        ret = [\"seq\"]\n        ret.append([\"goto\", sig.external_function_base_entry_label])\n        ret.append(func_common_ir)\n    else:\n        ret = kwarg_handlers\n        ret[-1][-1].append(func_common_ir)\n    return IRnode.from_list(ret, source_pos=getpos(sig.func_ast_code))",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-32675",
        "description": "[{'lang': 'en', 'value': 'Vyper is a pythonic Smart Contract Language for the ethereum virtual machine. In contracts with more than one regular nonpayable function, it is possible to send funds to the default function, even if the default function is marked `nonpayable`. This applies to contracts compiled with vyper versions prior to 0.3.8. This issue was fixed by the removal of the global `calldatasize` check in commit `02339dfda`. Users are advised to upgrade to version 0.3.8. Users unable to upgrade should avoid use of nonpayable default functions.'}]",
        "cwe_number": 670
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-108",
      "code": "    async def on_exchange_third_party_invite_request(\n        self, room_id: str, event_dict: JsonDict\n    ) -> None:\n        \"\"\"Handle an exchange_third_party_invite request from a remote server\n        The remote server will call this when it wants to turn a 3pid invite\n        into a normal m.room.member invite.\n        Args:\n            room_id: The ID of the room.\n            event_dict (dict[str, Any]): Dictionary containing the event body.\n        \"\"\"\n        room_version = await self.store.get_room_version_id(room_id)\n        builder = self.event_builder_factory.new(room_version, event_dict)\n        event, context = await self.event_creation_handler.create_new_client_event(\n            builder=builder\n        )\n        event, context = await self.add_display_name_to_third_party_invite(\n            room_version, event_dict, event, context\n        )\n        try:\n            await self.auth.check_from_context(room_version, event, context)\n        except AuthError as e:\n            logger.warning(\"Denying third party invite %r because %s\", event, e)\n            raise e\n        await self._check_signature(event, context)\n        event.internal_metadata.send_on_behalf_of = get_domain_from_id(event.sender)\n        member_handler = self.hs.get_room_member_handler()\n        await member_handler.send_membership_event(None, event, context)\n    async def on_context_state_request(\n        self, origin: str, room_id: str, event_id: str\n    ) -> Tuple[int, Dict[str, Any]]:\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, room_id)\n        in_room = await self.auth.check_host_in_room(room_id, origin)\n        if not in_room:\n            raise AuthError(403, \"Host not in room.\")\n        with (await self._server_linearizer.queue((origin, room_id))):\n            resp = dict(\n                await self._state_resp_cache.wrap(\n                    (room_id, event_id),\n                    self._on_context_state_request_compute,\n                    room_id,\n                    event_id,\n                )\n            )\n        room_version = await self.store.get_room_version_id(room_id)\n        resp[\"room_version\"] = room_version\n        return 200, resp\n    async def on_send_join_request(\n        self, origin: str, content: JsonDict, room_id: str\n    ) -> Dict[str, Any]:\n        logger.debug(\"on_send_join_request: content: %s\", content)\n        room_version = await self.store.get_room_version(room_id)\n        pdu = event_from_pdu_json(content, room_version)\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, pdu.room_id)\n        logger.debug(\"on_send_join_request: pdu sigs: %s\", pdu.signatures)\n        pdu = await self._check_sigs_and_hash(room_version, pdu)\n        res_pdus = await self.handler.on_send_join_request(origin, pdu)\n        time_now = self._clock.time_msec()\n        return {\n            \"state\": [p.get_pdu_json(time_now) for p in res_pdus[\"state\"]],\n            \"auth_chain\": [p.get_pdu_json(time_now) for p in res_pdus[\"auth_chain\"]],\n        }\n    async def on_make_leave_request(\n        self, origin: str, room_id: str, user_id: str\n    ) -> Dict[str, Any]:\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, room_id)\n        pdu = await self.handler.on_make_leave_request(origin, room_id, user_id)\n        room_version = await self.store.get_room_version_id(room_id)\n        time_now = self._clock.time_msec()\n        return {\"event\": pdu.get_pdu_json(time_now), \"room_version\": room_version}\n    async def on_send_leave_request(\n        self, origin: str, content: JsonDict, room_id: str\n    ) -> dict:\n        logger.debug(\"on_send_leave_request: content: %s\", content)\n        room_version = await self.store.get_room_version(room_id)\n        pdu = event_from_pdu_json(content, room_version)\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, pdu.room_id)\n        logger.debug(\"on_send_leave_request: pdu sigs: %s\", pdu.signatures)\n        pdu = await self._check_sigs_and_hash(room_version, pdu)\n        await self.handler.on_send_leave_request(origin, pdu)\n        return {}\n    async def on_exchange_third_party_invite_request(\n        self, room_id: str, event_dict: Dict\n    ):\n        ret = await self.handler.on_exchange_third_party_invite_request(\n            room_id, event_dict\n        )\n        return ret\n    async def on_GET(self, origin, content, query, context):\n        return await self.handler.on_context_state_request(\n            origin,\n            context,\n            parse_string_from_args(query, \"event_id\", None, required=False),\n        )\n    async def on_GET(self, origin, _content, query, context, user_id):\n        \"\"\"\n        Args:\n            origin (unicode): The authenticated server_name of the calling server\n            _content (None): (GETs don't have bodies)\n            query (dict[bytes, list[bytes]]): Query params from the request.\n            **kwargs (dict[unicode, unicode]): the dict mapping keys to path\n                components as specified in the path match regexp.\n        Returns:\n            Tuple[int, object]: (response code, response object)\n        \"\"\"\n        versions = query.get(b\"ver\")\n        if versions is not None:\n            supported_versions = [v.decode(\"utf-8\") for v in versions]\n        else:\n            supported_versions = [\"1\"]\n        content = await self.handler.on_make_join_request(\n            origin, context, user_id, supported_versions=supported_versions\n        )\n        return 200, content\n    async def on_GET(self, origin, content, query, context, user_id):\n        content = await self.handler.on_make_leave_request(origin, context, user_id)\n        return 200, content\n    async def on_PUT(self, origin, content, query, room_id, event_id):\n        content = await self.handler.on_send_leave_request(origin, content, room_id)\n        return 200, (200, content)\n    async def on_GET(self, origin, content, query, context, event_id):\n        return await self.handler.on_event_auth(origin, context, event_id)\n    async def on_PUT(self, origin, content, query, room_id):\n        content = await self.handler.on_exchange_third_party_invite_request(\n            room_id, content\n        )\n        return 200, content",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2020-26257",
        "description": "[{'lang': 'en', 'value': 'Matrix is an ecosystem for open federated Instant Messaging and VoIP. Synapse is a reference \"homeserver\" implementation of Matrix. A malicious or poorly-implemented homeserver can inject malformed events into a room by specifying a different room id in the path of a `/send_join`, `/send_leave`, `/invite` or `/exchange_third_party_invite` request. This can lead to a denial of service in which future events will not be correctly sent to other servers over federation. This affects any server which accepts federation requests from untrusted servers. The Matrix Synapse reference implementation before version 1.23.1 the implementation is vulnerable to this injection attack. Issue is fixed in version 1.23.1. As a workaround homeserver administrators could limit access to the federation API to trusted servers (for example via `federation_domain_whitelist`).'}]",
        "cwe_number": 400
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-109",
      "code": "    def get_events(self, uuid):\n        \"\"\"\n        Lists occured events, may be affected to changes in future.\n        :param uuid:\n        :return: list of `Events`\n        \"\"\"\n        events = self.pyload.event_manager.get_events(uuid)\n        new_events = []\n        def conv_dest(d):\n            return (Destination.QUEUE if d == \"queue\" else Destination.COLLECTOR).value\n        for e in events:\n            event = EventInfo()\n            event.eventname = e[0]\n            if e[0] in (\"update\", \"remove\", \"insert\"):\n                event.id = e[3]\n                event.type = (\n                    ElementType.PACKAGE if e[2] == \"pack\" else ElementType.FILE\n                ).value\n                event.destination = conv_dest(e[1])\n            elif e[0] == \"order\":\n                if e[1]:\n                    event.id = e[1]\n                    event.type = (\n                        ElementType.PACKAGE if e[2] == \"pack\" else ElementType.FILE\n                    )\n                    event.destination = conv_dest(e[3])\n            elif e[0] == \"reload\":\n                event.destination = conv_dest(e[1])\n            new_events.append(event)\n        return new_events\ndef is_authenticated(session=flask.session):\n    return session.get(\"name\") and session.get(\n        \"authenticated\"\n    )",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-0227",
        "description": "[{'lang': 'en', 'value': 'Insufficient Session Expiration in GitHub repository pyload/pyload prior to 0.5.0b3.dev36.'}]",
        "cwe_number": 613
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-110",
      "code": "if __name__ == \"__main__\":\n  googletest.main()",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-35993",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. When `SetSize` receives an input `set_shape` that is not a 1D tensor, it gives a `CHECK` fails that can be used to trigger a denial of service attack. We have patched the issue in GitHub commit cf70b79d2662c0d3c6af74583641e345fc939467. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue.'}]",
        "cwe_number": 617
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-111",
      "code": "    def _expand_user_properties(self, template):\n        safe_chars = set(string.ascii_lowercase + string.digits)\n        if self.name:\n            servername = '--{}'.format(self.name)\n            safe_servername = '--{}'.format(escapism.escape(self.name, safe=safe_chars, escape_char='-').lower())\n        else:\n            servername = ''\n            safe_servername = ''\n        legacy_escaped_username = ''.join([s if s in safe_chars else '-' for s in self.user.name.lower()])\n        safe_username = escapism.escape(self.user.name, safe=safe_chars, escape_char='-').lower()\n        return template.format(\n            userid=self.user.id,\n            username=safe_username,\n            unescaped_username=self.user.name,\n            legacy_escape_username=legacy_escaped_username,\n            servername=safe_servername,\n            unescaped_servername=servername,\n        )\n    def _expand_all(self, src):\n        if isinstance(src, list):\n            return [self._expand_all(i) for i in src]\n        elif isinstance(src, dict):\n            return {k: self._expand_all(v) for k, v in src.items()}\n        elif isinstance(src, str):\n            return self._expand_user_properties(src)\n        else:\n            return src\n    def _build_common_labels(self, extra_labels):\n        labels = {}\n        labels.update(extra_labels)\n        labels.update(self.common_labels)\n        return labels",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2020-15110",
        "description": "[{'lang': 'en', 'value': 'In jupyterhub-kubespawner before 0.12, certain usernames will be able to craft particular server names which will grant them access to the default server of other users who have matching usernames. This has been fixed in 0.12.'}]",
        "cwe_number": 863
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-112",
      "code": "    def __init__(\n        self,\n        template: str,\n        eos_token: str,\n        bos_token: str,\n        add_generation_prompt: bool = True,\n        stop_token_ids: Optional[List[int]] = None,\n    ):\n        \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"\n        self.template = template\n        self.eos_token = eos_token\n        self.bos_token = bos_token\n        self.add_generation_prompt = add_generation_prompt\n        self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None\n        self._environment = jinja2.Environment(\n            loader=jinja2.BaseLoader(),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        ).from_string(self.template)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-34359",
        "description": "[{'lang': 'en', 'value': \"llama-cpp-python is the Python bindings for llama.cpp. `llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to remote code execution by a carefully constructed payload.\"}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-113",
      "code": "def train_model(\n    headless,\n    print_only,\n    pretrained_model_name_or_path,\n    v2,\n    v_parameterization,\n    sdxl_checkbox,\n    train_dir,\n    image_folder,\n    output_dir,\n    dataset_config,\n    logging_dir,\n    max_resolution,\n    min_bucket_reso,\n    max_bucket_reso,\n    batch_size,\n    flip_aug,\n    masked_loss,\n    caption_metadata_filename,\n    latent_metadata_filename,\n    full_path,\n    learning_rate,\n    lr_scheduler,\n    lr_warmup,\n    dataset_repeats,\n    train_batch_size,\n    epoch,\n    save_every_n_epochs,\n    mixed_precision,\n    save_precision,\n    seed,\n    num_cpu_threads_per_process,\n    learning_rate_te,\n    learning_rate_te1,\n    learning_rate_te2,\n    train_text_encoder,\n    full_bf16,\n    generate_caption_database,\n    generate_image_buckets,\n    save_model_as,\n    caption_extension,\n    xformers,\n    clip_skip,\n    num_processes,\n    num_machines,\n    multi_gpu,\n    gpu_ids,\n    main_process_port,\n    save_state,\n    save_state_on_train_end,\n    resume,\n    gradient_checkpointing,\n    gradient_accumulation_steps,\n    block_lr,\n    mem_eff_attn,\n    shuffle_caption,\n    output_name,\n    max_token_length,\n    max_train_epochs,\n    max_train_steps,\n    max_data_loader_n_workers,\n    full_fp16,\n    color_aug,\n    model_list,\n    cache_latents,\n    cache_latents_to_disk,\n    use_latent_files,\n    keep_tokens,\n    persistent_data_loader_workers,\n    bucket_no_upscale,\n    random_crop,\n    bucket_reso_steps,\n    v_pred_like_loss,\n    caption_dropout_every_n_epochs,\n    caption_dropout_rate,\n    optimizer,\n    optimizer_args,\n    lr_scheduler_args,\n    noise_offset_type,\n    noise_offset,\n    noise_offset_random_strength,\n    adaptive_noise_scale,\n    multires_noise_iterations,\n    multires_noise_discount,\n    ip_noise_gamma,\n    ip_noise_gamma_random_strength,\n    sample_every_n_steps,\n    sample_every_n_epochs,\n    sample_sampler,\n    sample_prompts,\n    additional_parameters,\n    loss_type,\n    huber_schedule,\n    huber_c,\n    vae_batch_size,\n    min_snr_gamma,\n    weighted_captions,\n    save_every_n_steps,\n    save_last_n_steps,\n    save_last_n_steps_state,\n    use_wandb,\n    wandb_api_key,\n    wandb_run_name,\n    log_tracker_name,\n    log_tracker_config,\n    scale_v_pred_loss_like_noise_pred,\n    sdxl_cache_text_encoder_outputs,\n    sdxl_no_half_vae,\n    min_timestep,\n    max_timestep,\n    extra_accelerate_launch_args,\n):\n    parameters = list(locals().items())\n    log.debug(f\"headless = {headless} ; print_only = {print_only}\")\n    log.info(f\"Start Finetuning...\")\n    if train_dir != \"\" and not os.path.exists(train_dir):\n        os.mkdir(train_dir)\n    if not validate_paths(\n        output_dir=output_dir,\n        pretrained_model_name_or_path=pretrained_model_name_or_path,\n        finetune_image_folder=image_folder,\n        headless=headless,\n        logging_dir=logging_dir,\n        log_tracker_config=log_tracker_config,\n        resume=resume,\n        dataset_config=dataset_config,\n    ):\n        return\n    if not print_only and check_if_model_exist(\n        output_name, output_dir, save_model_as, headless\n    ):\n        return\n    if dataset_config:\n        log.info(\n            \"Dataset config toml file used, skipping caption json file, image buckets, total_steps, train_batch_size, gradient_accumulation_steps, epoch, reg_factor, max_train_steps creation...\"\n        )\n    else:\n        if generate_caption_database:\n            run_cmd = rf'\"{PYTHON}\" \"{scriptdir}/sd-scripts/finetune/merge_captions_to_metadata.py\"'\n            if caption_extension == \"\":\n                run_cmd += f' --caption_extension=\".caption\"'\n            else:\n                run_cmd += f\" --caption_extension={caption_extension}\"\n            run_cmd += rf' \"{image_folder}\"'\n            run_cmd += rf' \"{train_dir}/{caption_metadata_filename}\"'\n            if full_path:\n                run_cmd += f\" --full_path\"\n            log.info(run_cmd)\n            env = os.environ.copy()\n            env[\"PYTHONPATH\"] = (\n                rf\"{scriptdir}{os.pathsep}{scriptdir}/sd-scripts{os.pathsep}{env.get('PYTHONPATH', '')}\"\n            )\n            if not print_only:\n                subprocess.run(run_cmd, shell=True, env=env)\n        if generate_image_buckets:\n            run_cmd = rf'\"{PYTHON}\" \"{scriptdir}/sd-scripts/finetune/prepare_buckets_latents.py\"'\n            run_cmd += rf' \"{image_folder}\"'\n            run_cmd += rf' \"{train_dir}/{caption_metadata_filename}\"'\n            run_cmd += rf' \"{train_dir}/{latent_metadata_filename}\"'\n            run_cmd += rf' \"{pretrained_model_name_or_path}\"'\n            run_cmd += f\" --batch_size={batch_size}\"\n            run_cmd += f\" --max_resolution={max_resolution}\"\n            run_cmd += f\" --min_bucket_reso={min_bucket_reso}\"\n            run_cmd += f\" --max_bucket_reso={max_bucket_reso}\"\n            run_cmd += f\" --mixed_precision={mixed_precision}\"\n            if full_path:\n                run_cmd += f\" --full_path\"\n            if sdxl_checkbox and sdxl_no_half_vae:\n                log.info(\n                    \"Using mixed_precision = no because no half vae is selected...\"\n                )\n                run_cmd += f' --mixed_precision=\"no\"'\n            log.info(run_cmd)\n            env = os.environ.copy()\n            env[\"PYTHONPATH\"] = (\n                rf\"{scriptdir}{os.pathsep}{scriptdir}/sd-scripts{os.pathsep}{env.get('PYTHONPATH', '')}\"\n            )\n            if not print_only:\n                subprocess.run(run_cmd, shell=True, env=env)\n        image_num = len(\n            [\n                f\n                for f, lower_f in (\n                    (file, file.lower()) for file in os.listdir(image_folder)\n                )\n                if lower_f.endswith((\".jpg\", \".jpeg\", \".png\", \".webp\"))\n            ]\n        )\n        log.info(f\"image_num = {image_num}\")\n        repeats = int(image_num) * int(dataset_repeats)\n        log.info(f\"repeats = {str(repeats)}\")\n        max_train_steps = int(\n            math.ceil(\n                float(repeats)\n                / int(train_batch_size)\n                / int(gradient_accumulation_steps)\n                * int(epoch)\n            )\n        )\n        if flip_aug and max_train_steps:\n            max_train_steps = int(math.ceil(float(max_train_steps) / 2))\n    if max_train_steps != \"\":\n        log.info(f\"max_train_steps = {max_train_steps}\")\n        lr_warmup_steps = round(float(int(lr_warmup) * int(max_train_steps) / 100))\n    else:\n        lr_warmup_steps = 0\n    log.info(f\"lr_warmup_steps = {lr_warmup_steps}\")\n    run_cmd = \"accelerate launch\"\n    run_cmd += AccelerateLaunch.run_cmd(\n        num_processes=num_processes,\n        num_machines=num_machines,\n        multi_gpu=multi_gpu,\n        gpu_ids=gpu_ids,\n        main_process_port=main_process_port,\n        num_cpu_threads_per_process=num_cpu_threads_per_process,\n        mixed_precision=mixed_precision,\n        extra_accelerate_launch_args=extra_accelerate_launch_args,\n    )\n    if sdxl_checkbox:\n        run_cmd += rf' \"{scriptdir}/sd-scripts/sdxl_train.py\"'\n    else:\n        run_cmd += rf' \"{scriptdir}/sd-scripts/fine_tune.py\"'\n    in_json = (\n        rf\"{train_dir}/{latent_metadata_filename}\"\n        if use_latent_files == \"Yes\"\n        else rf\"{train_dir}/{caption_metadata_filename}\"\n    )\n    cache_text_encoder_outputs = sdxl_checkbox and sdxl_cache_text_encoder_outputs\n    no_half_vae = sdxl_checkbox and sdxl_no_half_vae\n    kwargs_for_training = {\n        \"adaptive_noise_scale\": adaptive_noise_scale,\n        \"block_lr\": block_lr,\n        \"bucket_no_upscale\": bucket_no_upscale,\n        \"bucket_reso_steps\": bucket_reso_steps,\n        \"cache_latents\": cache_latents,\n        \"cache_latents_to_disk\": cache_latents_to_disk,\n        \"caption_dropout_every_n_epochs\": caption_dropout_every_n_epochs,\n        \"caption_dropout_rate\": caption_dropout_rate,\n        \"caption_extension\": caption_extension,\n        \"clip_skip\": clip_skip,\n        \"color_aug\": color_aug,\n        \"dataset_config\": dataset_config,\n        \"dataset_repeats\": dataset_repeats,\n        \"enable_bucket\": True,\n        \"flip_aug\": flip_aug,\n        \"masked_loss\": masked_loss,\n        \"full_bf16\": full_bf16,\n        \"full_fp16\": full_fp16,\n        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n        \"gradient_checkpointing\": gradient_checkpointing,\n        \"in_json\": in_json,\n        \"ip_noise_gamma\": ip_noise_gamma,\n        \"ip_noise_gamma_random_strength\": ip_noise_gamma_random_strength,\n        \"keep_tokens\": keep_tokens,\n        \"learning_rate\": learning_rate,\n        \"logging_dir\": logging_dir,\n        \"log_tracker_name\": log_tracker_name,\n        \"log_tracker_config\": log_tracker_config,\n        \"lr_scheduler\": lr_scheduler,\n        \"lr_scheduler_args\": lr_scheduler_args,\n        \"lr_warmup_steps\": lr_warmup_steps,\n        \"max_bucket_reso\": max_bucket_reso,\n        \"max_data_loader_n_workers\": max_data_loader_n_workers,\n        \"max_resolution\": max_resolution,\n        \"max_timestep\": max_timestep,\n        \"max_token_length\": max_token_length,\n        \"max_train_epochs\": max_train_epochs,\n        \"max_train_steps\": max_train_steps,\n        \"mem_eff_attn\": mem_eff_attn,\n        \"min_bucket_reso\": min_bucket_reso,\n        \"min_snr_gamma\": min_snr_gamma,\n        \"min_timestep\": min_timestep,\n        \"mixed_precision\": mixed_precision,\n        \"multires_noise_discount\": multires_noise_discount,\n        \"multires_noise_iterations\": multires_noise_iterations,\n        \"noise_offset\": noise_offset,\n        \"noise_offset_random_strength\": noise_offset_random_strength,\n        \"noise_offset_type\": noise_offset_type,\n        \"optimizer\": optimizer,\n        \"optimizer_args\": optimizer_args,\n        \"output_dir\": output_dir,\n        \"output_name\": output_name,\n        \"persistent_data_loader_workers\": persistent_data_loader_workers,\n        \"pretrained_model_name_or_path\": pretrained_model_name_or_path,\n        \"random_crop\": random_crop,\n        \"resume\": resume,\n        \"save_every_n_epochs\": save_every_n_epochs,\n        \"save_every_n_steps\": save_every_n_steps,\n        \"save_last_n_steps\": save_last_n_steps,\n        \"save_last_n_steps_state\": save_last_n_steps_state,\n        \"save_model_as\": save_model_as,\n        \"save_precision\": save_precision,\n        \"save_state\": save_state,\n        \"save_state_on_train_end\": save_state_on_train_end,\n        \"scale_v_pred_loss_like_noise_pred\": scale_v_pred_loss_like_noise_pred,\n        \"seed\": seed,\n        \"shuffle_caption\": shuffle_caption,\n        \"train_batch_size\": train_batch_size,\n        \"train_data_dir\": image_folder,\n        \"train_text_encoder\": train_text_encoder,\n        \"use_wandb\": use_wandb,\n        \"v2\": v2,\n        \"v_parameterization\": v_parameterization,\n        \"v_pred_like_loss\": v_pred_like_loss,\n        \"vae_batch_size\": vae_batch_size,\n        \"wandb_api_key\": wandb_api_key,\n        \"wandb_run_name\": wandb_run_name,\n        \"weighted_captions\": weighted_captions,\n        \"xformers\": xformers,\n        \"additional_parameters\": additional_parameters,\n        \"loss_type\": loss_type,\n        \"huber_schedule\": huber_schedule,\n        \"huber_c\": huber_c,\n    }\n    if sdxl_checkbox:\n        kwargs_for_training[\"cache_text_encoder_outputs\"] = cache_text_encoder_outputs\n        kwargs_for_training[\"learning_rate_te1\"] = learning_rate_te1\n        kwargs_for_training[\"learning_rate_te2\"] = learning_rate_te2\n        kwargs_for_training[\"no_half_vae\"] = no_half_vae\n    else:\n        kwargs_for_training[\"learning_rate_te\"] = learning_rate_te\n    run_cmd += run_cmd_advanced_training(**kwargs_for_training)\n    run_cmd += run_cmd_sample(\n        sample_every_n_steps,\n        sample_every_n_epochs,\n        sample_sampler,\n        sample_prompts,\n        output_dir,\n    )\n    if print_only:\n        log.warning(\n            \"Here is the trainer command as a reference. It will not be executed:\\n\"\n        )\n        print(run_cmd)\n        save_to_file(run_cmd)\n    else:\n        current_datetime = datetime.now()\n        formatted_datetime = current_datetime.strftime(\"%Y%m%d-%H%M%S\")\n        file_path = os.path.join(output_dir, f\"{output_name}_{formatted_datetime}.json\")\n        log.info(f\"Saving training config to {file_path}...\")\n        SaveConfigFile(\n            parameters=parameters,\n            file_path=file_path,\n            exclusion=[\"file_path\", \"save_as\", \"headless\", \"print_only\"],\n        )\n        log.info(run_cmd)\n        env = os.environ.copy()\n        env[\"PYTHONPATH\"] = (\n            rf\"{scriptdir}{os.pathsep}{scriptdir}/sd-scripts{os.pathsep}{env.get('PYTHONPATH', '')}\"\n        )\n        env[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n        executor.execute_command(run_cmd=run_cmd, env=env)\ndef caption_images(\n    caption_text: str,\n    images_dir: str,\n    overwrite: bool,\n    caption_ext: str,\n    prefix: str,\n    postfix: str,\n    find_text: str,\n    replace_text: str,\n):\n    \"\"\"\n    Captions images in a given directory with a given caption text.\n    Args:\n        caption_text (str): The text to be used as the caption.\n        images_dir (str): The directory containing the images to be captioned.\n        overwrite (bool): Whether to overwrite existing captions.\n        caption_ext (str): The file extension for the caption files.\n        prefix (str): Text to be added before the caption text.\n        postfix (str): Text to be added after the caption text.\n        find_text (str): Text to be replaced in the caption files.\n        replace_text (str): Text to replace the found text in the caption files.\n    Returns:\n        None\n    \"\"\"\n    if not images_dir:\n        msgbox(\n            \"Image folder is missing. Please provide the directory containing the images to caption.\"\n        )\n        return\n    if not caption_ext:\n        msgbox(\"Please provide an extension for the caption files.\")\n        return\n    if caption_text:\n        log.info(f\"Captioning files in {images_dir} with {caption_text}...\")\n        run_cmd = rf'\"{PYTHON}\" \"{scriptdir}/tools/caption.py\"'\n        run_cmd += f' --caption_text=\"{caption_text}\"'\n        if overwrite:\n            run_cmd += f\" --overwrite\"\n        if caption_ext:\n            run_cmd += f' --caption_file_ext=\"{caption_ext}\"'\n        run_cmd += f' \"{images_dir}\"'\n        log.info(run_cmd)\n        env = os.environ.copy()\n        env[\"PYTHONPATH\"] = (\n            rf\"{scriptdir}{os.pathsep}{scriptdir}/tools{os.pathsep}{env.get('PYTHONPATH', '')}\"\n        )\n        subprocess.run(run_cmd, shell=True, env=env)\n    if overwrite:\n        if prefix or postfix:\n            add_pre_postfix(\n                folder=images_dir,\n                caption_file_ext=caption_ext,\n                prefix=prefix,\n                postfix=postfix,\n            )\n        if find_text:\n            find_replace(\n                folder_path=images_dir,\n                caption_file_ext=caption_ext,\n                search_text=find_text,\n                replace_text=replace_text,\n            )\n    else:\n        if prefix or postfix:\n            msgbox(\n                'Could not modify caption files with requested change because the \"Overwrite existing captions in folder\" option is not selected.'\n            )\n    log.info(\"Captioning done.\")\ndef caption_images(\n    train_data_dir: str,\n    caption_extension: str,\n    batch_size: int,\n    general_threshold: float,\n    character_threshold: float,\n    repo_id: str,\n    recursive: bool,\n    max_data_loader_n_workers: int,\n    debug: bool,\n    undesired_tags: str,\n    frequency_tags: bool,\n    always_first_tags: str,\n    onnx: bool,\n    append_tags: bool,\n    force_download: bool,\n    caption_separator: str,\n    tag_replacement: bool,\n    character_tag_expand: str,\n    use_rating_tags: bool,\n    use_rating_tags_as_last_tag: bool,\n    remove_underscore: bool,\n    thresh: float,\n) -> None:\n    if train_data_dir == \"\":\n        msgbox(\"Image folder is missing...\")\n        return\n    if caption_extension == \"\":\n        msgbox(\"Please provide an extension for the caption files.\")\n        return\n    log.info(f\"Captioning files in {train_data_dir}...\")\n    run_cmd = rf'accelerate launch \"{scriptdir}/sd-scripts/finetune/tag_images_by_wd14_tagger.py\"'\n    if append_tags:\n        run_cmd += f\" --append_tags\"\n    run_cmd += f\" --batch_size={int(batch_size)}\"\n    run_cmd += f' --caption_extension=\"{caption_extension}\"'\n    run_cmd += f' --caption_separator=\"{caption_separator}\"'\n    if character_tag_expand:\n        run_cmd += f\" --character_tag_expand\"\n    if not character_threshold == 0.35:\n        run_cmd += f\" --character_threshold={character_threshold}\"\n    if debug:\n        run_cmd += f\" --debug\"\n    if force_download:\n        run_cmd += f\" --force_download\"\n    if frequency_tags:\n        run_cmd += f\" --frequency_tags\"\n    if not general_threshold == 0.35:\n        run_cmd += f\" --general_threshold={general_threshold}\"\n    run_cmd += f' --max_data_loader_n_workers=\"{int(max_data_loader_n_workers)}\"'\n    if onnx:\n        run_cmd += f\" --onnx\"\n    if recursive:\n        run_cmd += f\" --recursive\"\n    if remove_underscore:\n        run_cmd += f\" --remove_underscore\"\n    run_cmd += f' --repo_id=\"{repo_id}\"'\n    if not tag_replacement == \"\":\n        run_cmd += f\" --tag_replacement={tag_replacement}\"\n    if not thresh == 0.35:\n        run_cmd += f\" --thresh={thresh}\"\n    if not undesired_tags == \"\":\n        run_cmd += f' --undesired_tags=\"{undesired_tags}\"'\n    if use_rating_tags:\n        run_cmd += f\" --use_rating_tags\"\n    if use_rating_tags_as_last_tag:\n        run_cmd += f\" --use_rating_tags_as_last_tag\"\n    run_cmd += rf' \"{train_data_dir}\"'\n    log.info(run_cmd)\n    env = os.environ.copy()\n    env[\"PYTHONPATH\"] = (\n        rf\"{scriptdir}{os.pathsep}{scriptdir}/sd-scripts{os.pathsep}{env.get('PYTHONPATH', '')}\"\n    )\n    env[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n    subprocess.run(run_cmd, shell=True, env=env)\n    add_pre_postfix(\n        folder=train_data_dir,\n        caption_file_ext=caption_extension,\n        prefix=always_first_tags,\n    )\n    log.info(\"...captioning done\")\ndef extract_lycoris_locon(\n    db_model,\n    base_model,\n    output_name,\n    device,\n    is_sdxl,\n    is_v2,\n    mode,\n    linear_dim,\n    conv_dim,\n    linear_threshold,\n    conv_threshold,\n    linear_ratio,\n    conv_ratio,\n    linear_quantile,\n    conv_quantile,\n    use_sparse_bias,\n    sparsity,\n    disable_cp,\n):\n    if db_model == \"\":\n        msgbox(\"Invalid finetuned model file\")\n        return\n    if base_model == \"\":\n        msgbox(\"Invalid base model file\")\n        return\n    if not os.path.isfile(db_model):\n        msgbox(\"The provided finetuned model is not a file\")\n        return\n    if not os.path.isfile(base_model):\n        msgbox(\"The provided base model is not a file\")\n        return\n    if os.path.dirname(output_name) == \"\":\n        output_name = os.path.join(os.path.dirname(db_model), output_name)\n    if os.path.isdir(output_name):\n        output_name = os.path.join(output_name, \"lora.safetensors\")\n    if os.path.normpath(db_model) == os.path.normpath(output_name):\n        path, ext = os.path.splitext(output_name)\n        output_name = f\"{path}_tmp{ext}\"\n    run_cmd = rf'\"{PYTHON}\" \"{scriptdir}/tools/lycoris_locon_extract.py\"'\n    if is_sdxl:\n        run_cmd += f\" --is_sdxl\"\n    if is_v2:\n        run_cmd += f\" --is_v2\"\n    run_cmd += f\" --device {device}\"\n    run_cmd += f\" --mode {mode}\"\n    run_cmd += f\" --safetensors\"\n    if mode == \"fixed\":\n        run_cmd += f\" --linear_dim {linear_dim}\"\n        run_cmd += f\" --conv_dim {conv_dim}\"\n    if mode == \"threshold\":\n        run_cmd += f\" --linear_threshold {linear_threshold}\"\n        run_cmd += f\" --conv_threshold {conv_threshold}\"\n    if mode == \"ratio\":\n        run_cmd += f\" --linear_ratio {linear_ratio}\"\n        run_cmd += f\" --conv_ratio {conv_ratio}\"\n    if mode == \"quantile\":\n        run_cmd += f\" --linear_quantile {linear_quantile}\"\n        run_cmd += f\" --conv_quantile {conv_quantile}\"\n    if use_sparse_bias:\n        run_cmd += f\" --use_sparse_bias\"\n    run_cmd += f\" --sparsity {sparsity}\"\n    if disable_cp:\n        run_cmd += f\" --disable_cp\"\n    run_cmd += rf' \"{base_model}\"'\n    run_cmd += rf' \"{db_model}\"'\n    run_cmd += rf' \"{output_name}\"'\n    log.info(run_cmd)\n    env = os.environ.copy()\n    env[\"PYTHONPATH\"] = (\n        rf\"{scriptdir}{os.pathsep}{scriptdir}/sd-scripts{os.pathsep}{env.get('PYTHONPATH', '')}\"\n    )\n    subprocess.run(run_cmd, shell=True, env=env)\n    log.info(\"Done extracting...\")\ndef convert_model(\n    source_model_input,\n    source_model_type,\n    target_model_folder_input,\n    target_model_name_input,\n    target_model_type,\n    target_save_precision_type,\n    unet_use_linear_projection,\n):\n    if source_model_type == \"\":\n        msgbox(\"Invalid source model type\")\n        return\n    if os.path.isfile(source_model_input):\n        log.info(\"The provided source model is a file\")\n    elif os.path.isdir(source_model_input):\n        log.info(\"The provided model is a folder\")\n    else:\n        msgbox(\"The provided source model is neither a file nor a folder\")\n        return\n    if os.path.isdir(target_model_folder_input):\n        log.info(\"The provided model folder exist\")\n    else:\n        msgbox(\"The provided target folder does not exist\")\n        return\n    run_cmd = (\n        rf'\"{PYTHON}\" \"{scriptdir}/sd-scripts/tools/convert_diffusers20_original_sd.py\"'\n    )\n    v1_models = [\n        \"runwayml/stable-diffusion-v1-5\",\n        \"CompVis/stable-diffusion-v1-4\",\n    ]\n    if str(source_model_type) in v1_models:\n        log.info(\"SD v1 model specified. Setting --v1 parameter\")\n        run_cmd += \" --v1\"\n    else:\n        log.info(\"SD v2 model specified. Setting --v2 parameter\")\n        run_cmd += \" --v2\"\n    if not target_save_precision_type == \"unspecified\":\n        run_cmd += f\" --{target_save_precision_type}\"\n    if target_model_type == \"diffuser\" or target_model_type == \"diffuser_safetensors\":\n        run_cmd += f' --reference_model=\"{source_model_type}\"'\n    if target_model_type == \"diffuser_safetensors\":\n        run_cmd += \" --use_safetensors\"\n    if unet_use_linear_projection:\n        run_cmd += \" --unet_use_linear_projection\"\n    run_cmd += f' \"{source_model_input}\"'\n    if target_model_type == \"diffuser\" or target_model_type == \"diffuser_safetensors\":\n        target_model_path = os.path.join(\n            target_model_folder_input, target_model_name_input\n        )\n        run_cmd += f' \"{target_model_path}\"'\n    else:\n        target_model_path = os.path.join(\n            target_model_folder_input,\n            f\"{target_model_name_input}.{target_model_type}\",\n        )\n        run_cmd += f' \"{target_model_path}\"'\n    log.info(run_cmd)\n    env = os.environ.copy()\n    env[\"PYTHONPATH\"] = (\n        rf\"{scriptdir}{os.pathsep}{scriptdir}/sd-scripts{os.pathsep}{env.get('PYTHONPATH', '')}\"\n    )\n    subprocess.run(run_cmd, shell=True, env=env)\ndef extract_lora(\n    model_tuned,\n    model_org,\n    save_to,\n    save_precision,\n    dim,\n    v2,\n    sdxl,\n    conv_dim,\n    clamp_quantile,\n    min_diff,\n    device,\n    load_original_model_to,\n    load_tuned_model_to,\n    load_precision,\n):\n    if model_tuned == \"\":\n        log.info(\"Invalid finetuned model file\")\n        return\n    if model_org == \"\":\n        log.info(\"Invalid base model file\")\n        return\n    if not os.path.isfile(model_tuned):\n        log.info(\"The provided finetuned model is not a file\")\n        return\n    if not os.path.isfile(model_org):\n        log.info(\"The provided base model is not a file\")\n        return\n    if os.path.dirname(save_to) == \"\":\n        save_to = os.path.join(os.path.dirname(model_tuned), save_to)\n    if os.path.isdir(save_to):\n        save_to = os.path.join(save_to, \"lora.safetensors\")\n    if os.path.normpath(model_tuned) == os.path.normpath(save_to):\n        path, ext = os.path.splitext(save_to)\n        save_to = f\"{path}_tmp{ext}\"\n    if not is_file_writable(save_to):\n        return\n    run_cmd = (\n        rf'\"{PYTHON}\" \"{scriptdir}/sd-scripts/networks/extract_lora_from_models.py\"'\n    )\n    run_cmd += f\" --load_precision {load_precision}\"\n    run_cmd += f\" --save_precision {save_precision}\"\n    run_cmd += rf' --save_to \"{save_to}\"'\n    run_cmd += rf' --model_org \"{model_org}\"'\n    run_cmd += rf' --model_tuned \"{model_tuned}\"'\n    run_cmd += f\" --dim {dim}\"\n    run_cmd += f\" --device {device}\"\n    if conv_dim > 0:\n        run_cmd += f\" --conv_dim {conv_dim}\"\n    if v2:\n        run_cmd += f\" --v2\"\n    if sdxl:\n        run_cmd += f\" --sdxl\"\n    run_cmd += f\" --clamp_quantile {clamp_quantile}\"\n    run_cmd += f\" --min_diff {min_diff}\"\n    if sdxl:\n        run_cmd += f\" --load_original_model_to {load_original_model_to}\"\n        run_cmd += f\" --load_tuned_model_to {load_tuned_model_to}\"\n    log.info(run_cmd)\n    env = os.environ.copy()\n    env[\"PYTHONPATH\"] = (\n        rf\"{scriptdir}{os.pathsep}{scriptdir}/sd-scripts{os.pathsep}{env.get('PYTHONPATH', '')}\"\n    )\n    subprocess.run(run_cmd, shell=True, env=env)\ndef caption_images(\n    train_data_dir: str,\n    caption_file_ext: str,\n    batch_size: int,\n    num_beams: int,\n    top_p: float,\n    max_length: int,\n    min_length: int,\n    beam_search: bool,\n    prefix: str = \"\",\n    postfix: str = \"\",\n) -> None:\n    \"\"\"\n    Automatically generates captions for images in the specified directory using the BLIP model.\n    This function prepares and executes a command-line script to process images in batches, applying advanced\n    NLP techniques for caption generation. It supports customization of the captioning process through various\n    parameters like batch size, beam search, and more. Optionally, prefixes and postfixes can be added to captions.\n    Args:\n        train_data_dir (str): The directory containing the images to be captioned.\n        caption_file_ext (str): The extension for the caption files.\n        batch_size (int): The batch size for the captioning process.\n        num_beams (int): The number of beams to use in the captioning process.\n        top_p (float): The top p value to use in the captioning process.\n        max_length (int): The maximum length of the captions.\n        min_length (int): The minimum length of the captions.\n        beam_search (bool): Whether to use beam search in the captioning process.\n        prefix (str): The prefix to add to the captions.\n        postfix (str): The postfix to add to the captions.\n    \"\"\"\n    if not train_data_dir:\n        msgbox(\"Image folder is missing...\")\n        return\n    if not caption_file_ext:\n        msgbox(\"Please provide an extension for the caption files.\")\n        return\n    log.info(f\"Captioning files in {train_data_dir}...\")\n    run_cmd = rf'\"{PYTHON}\" \"{scriptdir}/sd-scripts/finetune/make_captions.py\"'\n    run_cmd += f' --batch_size=\"{int(batch_size)}\"'\n    run_cmd += f' --num_beams=\"{int(num_beams)}\"'\n    run_cmd += f' --top_p=\"{top_p}\"'\n    run_cmd += f' --max_length=\"{int(max_length)}\"'\n    run_cmd += f' --min_length=\"{int(min_length)}\"'\n    if beam_search:\n        run_cmd += f\" --beam_search\"\n    if caption_file_ext:\n        run_cmd += f' --caption_extension=\"{caption_file_ext}\"'\n    run_cmd += f' \"{train_data_dir}\"'\n    run_cmd += f' --caption_weights=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth\"'\n    log.info(run_cmd)\n    env = os.environ.copy()\n    env[\"PYTHONPATH\"] = (\n        f\"{scriptdir}{os.pathsep}{scriptdir}/sd-scripts{os.pathsep}{env.get('PYTHONPATH', '')}\"\n    )\n    subprocess.run(run_cmd, shell=True, env=env, cwd=f\"{scriptdir}/sd-scripts\")\n    add_pre_postfix(\n        folder=train_data_dir,\n        caption_file_ext=caption_file_ext,\n        prefix=prefix,\n        postfix=postfix,\n    )\n    log.info(\"...captioning done\")\ndef svd_merge_lora(\n    lora_a_model,\n    lora_b_model,\n    lora_c_model,\n    lora_d_model,\n    ratio_a,\n    ratio_b,\n    ratio_c,\n    ratio_d,\n    save_to,\n    precision,\n    save_precision,\n    new_rank,\n    new_conv_rank,\n    device,\n):\n    if os.path.isfile(save_to):\n        print(f\"Output file '{save_to}' already exists. Aborting.\")\n        return\n    total_ratio = ratio_a + ratio_b + ratio_c + ratio_d\n    if total_ratio != 1:\n        ratio_a /= total_ratio\n        ratio_b /= total_ratio\n        ratio_c /= total_ratio\n        ratio_d /= total_ratio\n    run_cmd = rf'\"{PYTHON}\" \"{scriptdir}/sd-scripts/networks/svd_merge_lora.py\"'\n    run_cmd += f\" --save_precision {save_precision}\"\n    run_cmd += f\" --precision {precision}\"\n    run_cmd += rf' --save_to \"{save_to}\"'\n    run_cmd_models = \" --models\"\n    run_cmd_ratios = \" --ratios\"\n    if lora_a_model:\n        if not os.path.isfile(lora_a_model):\n            msgbox(\"The provided model A is not a file\")\n            return\n        run_cmd_models += rf' \"{lora_a_model}\"'\n        run_cmd_ratios += f\" {ratio_a}\"\n    if lora_b_model:\n        if not os.path.isfile(lora_b_model):\n            msgbox(\"The provided model B is not a file\")\n            return\n        run_cmd_models += rf' \"{lora_b_model}\"'\n        run_cmd_ratios += f\" {ratio_b}\"\n    if lora_c_model:\n        if not os.path.isfile(lora_c_model):\n            msgbox(\"The provided model C is not a file\")\n            return\n        run_cmd_models += rf' \"{lora_c_model}\"'\n        run_cmd_ratios += f\" {ratio_c}\"\n    if lora_d_model:\n        if not os.path.isfile(lora_d_model):\n            msgbox(\"The provided model D is not a file\")\n            return\n        run_cmd_models += rf' \"{lora_d_model}\"'\n        run_cmd_ratios += f\" {ratio_d}\"\n    run_cmd += run_cmd_models\n    run_cmd += run_cmd_ratios\n    run_cmd += f\" --device {device}\"\n    run_cmd += f' --new_rank \"{new_rank}\"'\n    run_cmd += f' --new_conv_rank \"{new_conv_rank}\"'\n    log.info(run_cmd)\n    env = os.environ.copy()\n    env[\"PYTHONPATH\"] = (\n        rf\"{scriptdir}{os.pathsep}{scriptdir}/sd-scripts{os.pathsep}{env.get('PYTHONPATH', '')}\"\n    )\n    subprocess.run(run_cmd, shell=True, env=env)\n    def merge_lora(\n        self,\n        sd_model,\n        sdxl_model,\n        lora_a_model,\n        lora_b_model,\n        lora_c_model,\n        lora_d_model,\n        ratio_a,\n        ratio_b,\n        ratio_c,\n        ratio_d,\n        save_to,\n        precision,\n        save_precision,\n    ):\n        log.info(\"Merge model...\")\n        models = [\n            sd_model,\n            lora_a_model,\n            lora_b_model,\n            lora_c_model,\n            lora_d_model,\n        ]\n        lora_models = models[1:]\n        ratios = [ratio_a, ratio_b, ratio_c, ratio_d]\n        if not verify_conditions(sd_model, lora_models):\n            log.info(\n                \"Warning: Either provide at least one LoRa model along with the sd_model or at least two LoRa models if no sd_model is provided.\"\n            )\n            return\n        for model in models:\n            if not check_model(model):\n                return\n        if not sdxl_model:\n            run_cmd = rf'\"{PYTHON}\" \"{scriptdir}/sd-scripts/networks/merge_lora.py\"'\n        else:\n            run_cmd = (\n                rf'\"{PYTHON}\" \"{scriptdir}/sd-scripts/networks/sdxl_merge_lora.py\"'\n            )\n        if sd_model:\n            run_cmd += rf' --sd_model \"{sd_model}\"'\n        run_cmd += f\" --save_precision {save_precision}\"\n        run_cmd += f\" --precision {precision}\"\n        run_cmd += rf' --save_to \"{save_to}\"'\n        models_cmd = \" \".join([rf'\"{model}\"' for model in lora_models if model])\n        valid_ratios = [ratios[i] for i, model in enumerate(lora_models) if model]\n        ratios_cmd = \" \".join([str(ratio) for ratio in valid_ratios])\n        if models_cmd:\n            run_cmd += f\" --models {models_cmd}\"\n            run_cmd += f\" --ratios {ratios_cmd}\"\n        log.info(run_cmd)\n        env = os.environ.copy()\n        env[\"PYTHONPATH\"] = (\n            rf\"{scriptdir}{os.pathsep}{scriptdir}/sd-scripts{os.pathsep}{env.get('PYTHONPATH', '')}\"\n        )\n        subprocess.run(run_cmd, shell=True, env=env)\n        log.info(\"Done merging...\")\ndef resize_lora(\n    model,\n    new_rank,\n    save_to,\n    save_precision,\n    device,\n    dynamic_method,\n    dynamic_param,\n    verbose,\n):\n    if model == \"\":\n        msgbox(\"Invalid model file\")\n        return\n    if not os.path.isfile(model):\n        msgbox(\"The provided model is not a file\")\n        return\n    if dynamic_method == \"sv_ratio\":\n        if float(dynamic_param) < 2:\n            msgbox(f\"Dynamic parameter for {dynamic_method} need to be 2 or greater...\")\n            return\n    if dynamic_method == \"sv_fro\" or dynamic_method == \"sv_cumulative\":\n        if float(dynamic_param) < 0 or float(dynamic_param) > 1:\n            msgbox(\n                f\"Dynamic parameter for {dynamic_method} need to be between 0 and 1...\"\n            )\n            return\n    if not save_to.endswith((\".pt\", \".safetensors\")):\n        save_to += \".safetensors\"\n    if device == \"\":\n        device = \"cuda\"\n    run_cmd = rf'\"{PYTHON}\" \"{scriptdir}/sd-scripts/networks/resize_lora.py\"'\n    run_cmd += f\" --save_precision {save_precision}\"\n    run_cmd += rf' --save_to \"{save_to}\"'\n    run_cmd += rf' --model \"{model}\"'\n    run_cmd += f\" --new_rank {new_rank}\"\n    run_cmd += f\" --device {device}\"\n    if not dynamic_method == \"None\":\n        run_cmd += f\" --dynamic_method {dynamic_method}\"\n        run_cmd += f\" --dynamic_param {dynamic_param}\"\n    if verbose:\n        run_cmd += f\" --verbose\"\n    log.info(run_cmd)\n    env = os.environ.copy()\n    env[\"PYTHONPATH\"] = (\n        rf\"{scriptdir}{os.pathsep}{scriptdir}/sd-scripts{os.pathsep}{env.get('PYTHONPATH', '')}\"\n    )\n    subprocess.run(run_cmd, shell=True, env=env)\n    log.info(\"Done resizing...\")\ndef caption_images(\n    train_data_dir,\n    caption_ext,\n    batch_size,\n    max_data_loader_n_workers,\n    max_length,\n    model_id,\n    prefix,\n    postfix,\n):\n    if train_data_dir == \"\":\n        msgbox(\"Image folder is missing...\")\n        return\n    if caption_ext == \"\":\n        msgbox(\"Please provide an extension for the caption files.\")\n        return\n    log.info(f\"GIT captioning files in {train_data_dir}...\")\n    run_cmd = rf'\"{PYTHON}\" \"{scriptdir}/sd-scripts/finetune/make_captions_by_git.py\"'\n    if not model_id == \"\":\n        run_cmd += f' --model_id=\"{model_id}\"'\n    run_cmd += f' --batch_size=\"{int(batch_size)}\"'\n    run_cmd += f' --max_data_loader_n_workers=\"{int(max_data_loader_n_workers)}\"'\n    run_cmd += f' --max_length=\"{int(max_length)}\"'\n    if caption_ext != \"\":\n        run_cmd += f' --caption_extension=\"{caption_ext}\"'\n    run_cmd += f' \"{train_data_dir}\"'\n    log.info(run_cmd)\n    env = os.environ.copy()\n    env[\"PYTHONPATH\"] = (\n        rf\"{scriptdir}{os.pathsep}{scriptdir}/sd-scripts{os.pathsep}{env.get('PYTHONPATH', '')}\"\n    )\n    subprocess.run(run_cmd, shell=True, env=env)\n    add_pre_postfix(\n        folder=train_data_dir,\n        caption_file_ext=caption_ext,\n        prefix=prefix,\n        postfix=postfix,\n    )\n    log.info(\"...captioning done\")\ndef extract_dylora(\n    model,\n    save_to,\n    unit,\n):\n    if model == \"\":\n        msgbox(\"Invalid DyLoRA model file\")\n        return\n    if not os.path.isfile(model):\n        msgbox(\"The provided DyLoRA model is not a file\")\n        return\n    if os.path.dirname(save_to) == \"\":\n        save_to = os.path.join(os.path.dirname(model), save_to)\n    if os.path.isdir(save_to):\n        save_to = os.path.join(save_to, \"lora.safetensors\")\n    if os.path.normpath(model) == os.path.normpath(save_to):\n        path, ext = os.path.splitext(save_to)\n        save_to = f\"{path}_tmp{ext}\"\n    run_cmd = (\n        rf'\"{PYTHON}\" \"{scriptdir}/sd-scripts/networks/extract_lora_from_dylora.py\"'\n    )\n    run_cmd += rf' --save_to \"{save_to}\"'\n    run_cmd += rf' --model \"{model}\"'\n    run_cmd += f\" --unit {unit}\"\n    log.info(run_cmd)\n    env = os.environ.copy()\n    env[\"PYTHONPATH\"] = (\n        rf\"{scriptdir}{os.pathsep}{scriptdir}/sd-scripts{os.pathsep}{env.get('PYTHONPATH', '')}\"\n    )\n    subprocess.run(run_cmd, shell=True, env=env)\n    log.info(\"Done extracting DyLoRA...\")\ndef group_images(\n    input_folder,\n    output_folder,\n    group_size,\n    include_subfolders,\n    do_not_copy_other_files,\n    generate_captions,\n    caption_ext,\n):\n    if input_folder == \"\":\n        msgbox(\"Input folder is missing...\")\n        return\n    if output_folder == \"\":\n        msgbox(\"Please provide an output folder.\")\n        return\n    log.info(f\"Grouping images in {input_folder}...\")\n    run_cmd = rf'\"{PYTHON}\" \"{scriptdir}/tools/group_images.py\"'\n    run_cmd += f' \"{input_folder}\"'\n    run_cmd += f' \"{output_folder}\"'\n    run_cmd += f\" {(group_size)}\"\n    if include_subfolders:\n        run_cmd += f\" --include_subfolders\"\n    if do_not_copy_other_files:\n        run_cmd += f\" --do_not_copy_other_files\"\n    if generate_captions:\n        run_cmd += f\" --caption\"\n        if caption_ext:\n            run_cmd += f\" --caption_ext={caption_ext}\"\n    log.info(run_cmd)\n    env = os.environ.copy()\n    env[\"PYTHONPATH\"] = (\n        rf\"{scriptdir}{os.pathsep}{scriptdir}/sd-scripts{os.pathsep}{env.get('PYTHONPATH', '')}\"\n    )\n    subprocess.run(run_cmd, shell=True, env=env)\n    log.info(\"...grouping done\")\ndef convert_lcm(name, model_path, lora_scale, model_type):\n    run_cmd = rf'\"{PYTHON}\" \"{scriptdir}/tools/lcm_convert.py\"'\n    if not os.path.isfile(model_path):\n        log.error(\"The provided DyLoRA model is not a file\")\n        return\n    if os.path.dirname(name) == \"\":\n        name = os.path.join(os.path.dirname(model_path), name)\n    if os.path.isdir(name):\n        name = os.path.join(name, \"lcm.safetensors\")\n    if os.path.normpath(model_path) == os.path.normpath(name):\n        path, ext = os.path.splitext(save_to)\n        save_to = f\"{path}_lcm{ext}\"\n    run_cmd += f\" --lora-scale {lora_scale}\"\n    run_cmd += f' --model \"{model_path}\"'\n    run_cmd += f' --name \"{name}\"'\n    if model_type == \"SDXL\":\n        run_cmd += f\" --sdxl\"\n    if model_type == \"SSD-1B\":\n        run_cmd += f\" --ssd-1b\"\n    log.info(run_cmd)\n    env = os.environ.copy()\n    env[\"PYTHONPATH\"] = (\n        rf\"{scriptdir}{os.pathsep}{scriptdir}/sd-scripts{os.pathsep}{env.get('PYTHONPATH', '')}\"\n    )\n    subprocess.run(run_cmd, shell=True, env=env)\n    log.info(\"Done extracting...\")\ndef merge_lycoris(\n    base_model,\n    lycoris_model,\n    weight,\n    output_name,\n    dtype,\n    device,\n    is_sdxl,\n    is_v2,\n):\n    log.info(\"Merge model...\")\n    run_cmd = rf'\"{PYTHON}\" \"{scriptdir}/tools/merge_lycoris.py\"'\n    run_cmd += rf' \"{base_model}\"'\n    run_cmd += rf' \"{lycoris_model}\"'\n    run_cmd += rf' \"{output_name}\"'\n    run_cmd += f\" --weight {weight}\"\n    run_cmd += f\" --device {device}\"\n    run_cmd += f\" --dtype {dtype}\"\n    if is_sdxl:\n        run_cmd += f\" --is_sdxl\"\n    if is_v2:\n        run_cmd += f\" --is_v2\"\n    log.info(run_cmd)\n    env = os.environ.copy()\n    env[\"PYTHONPATH\"] = (\n        rf\"{scriptdir}{os.pathsep}{scriptdir}/sd-scripts{os.pathsep}{env.get('PYTHONPATH', '')}\"\n    )\n    subprocess.run(run_cmd, shell=True, env=env)\n    log.info(\"Done merging...\")\n    def execute_command(self, run_cmd: str, **kwargs):\n        \"\"\"\n        Execute a command if no other command is currently running.\n        Parameters:\n        - run_cmd (str): The command to execute.\n        - **kwargs: Additional keyword arguments to pass to subprocess.Popen.\n        \"\"\"\n        if self.process and self.process.poll() is None:\n            log.info(\"The command is already running. Please wait for it to finish.\")\n        else:\n            self.process = subprocess.Popen(run_cmd, shell=True, **kwargs)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-32027",
        "description": "[{'lang': 'en', 'value': \"Kohya_ss is a GUI for Kohya's Stable Diffusion trainers. Kohya_ss v22.6.1 is vulnerable to command injection in `finetune_gui.py` This vulnerability is fixed in 23.1.5.\"}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-114",
      "code": "    def sql_execute(self, sentence):\n        self.cursor.execute(sentence)\n        return self.cursor.fetchall()\n    def sql_one_row(self, sentence, column):\n        self.cursor.execute(sentence)\n        return self.cursor.fetchone()[column]\n    def sql_insert(self, sentence):\n        self.cursor.execute(sentence)\n        self.conn.commit()\n        return True\n    def prop_sentences_stats(self, type, vId = None):\n        return {\n            'get_data' : \"SELECT victims.*, geo.*, victims.ip AS ip_local, COUNT(clicks.id) FROM victims INNER JOIN geo ON victims.id = geo.id LEFT JOIN clicks ON clicks.id = victims.id GROUP BY victims.id ORDER BY victims.time DESC\",\n            'all_networks' : \"SELECT networks.* FROM networks ORDER BY id\",\n            'get_preview' : \"SELECT victims.*, geo.*, victims.ip AS ip_local FROM victims INNER JOIN geo ON victims.id = geo.id WHERE victims.id = '%s'\" % (vId),\n            'id_networks' : \"SELECT networks.* FROM networks WHERE id = '%s'\" % (vId),\n            'get_requests' : \"SELECT requests.*, geo.ip FROM requests INNER JOIN geo on geo.id = requests.user_id ORDER BY requests.date DESC, requests.id \",\n            'get_sessions' : \"SELECT COUNT(*) AS Total FROM networks\",\n            'get_clicks' : \"SELECT COUNT(*) AS Total FROM clicks\",\n            'get_online' : \"SELECT COUNT(*) AS Total FROM victims WHERE status = '%s'\" % ('online')\n        }.get(type, False)\n    def sentences_stats(self, type, vId = None):\n        return self.sql_execute(self.prop_sentences_stats(type, vId))\n    def prop_sentences_victim(self, type, data = None):\n        if type == 'count_victim':\n            return \"SELECT COUNT(*) AS C FROM victims WHERE id = '%s'\" % (data)\n        elif type == 'count_times':\n            return \"SELECT COUNT(*) AS C FROM clicks WHERE id = '%s'\" % (data)\n        elif type == 'update_victim':\n            return \"UPDATE victims SET ip = '%s', date = '%s', bVersion = '%s', browser = '%s', device = '%s', ports = '%s', time = '%s', cpu = '%s', status = '%s' WHERE id = '%s'\" % (data[0].ip, data[0].date, data[0].version, data[0].browser, data[0].device, data[0].ports, data[2], data[0].cpu, 'online', data[1])\n        elif type == 'update_victim_geo':\n            return \"UPDATE geo SET city = '%s', country_code = '%s', country_name = '%s', ip = '%s', latitude = '%s', longitude = '%s', metro_code = '%s', region_code = '%s', region_name = '%s', time_zone = '%s', zip_code = '%s', isp = '%s', ua='%s' WHERE id = '%s'\" % (data[0].city, data[0].country_code, data[0].country_name, data[0].ip, data[0].latitude, data[0].longitude, data[0].metro_code, data[0].region_code, data[0].region_name, data[0].time_zone, data[0].zip_code, data[0].isp, data[0].ua, data[1])\n        elif type == 'insert_victim':\n            return \"INSERT INTO victims(id, ip, date, bVersion, browser, device, ports, time, cpu, status) VALUES('%s','%s', '%s','%s', '%s','%s', '%s', '%s', '%s', '%s')\" % (data[1], data[0].ip, data[0].date, data[0].version, data[0].browser, data[0].device, data[0].ports, data[2], data[0].cpu, 'online')\n        elif type == 'insert_victim_geo':\n            return \"INSERT INTO geo(id, city, country_code, country_name, ip, latitude, longitude, metro_code, region_code, region_name, time_zone, zip_code, isp, ua) VALUES('%s', '%s', '%s', '%s', '%s', '%s', '%s', '%s', '%s', '%s', '%s', '%s', '%s', '%s')\"  % (data[1], data[0].city, data[0].country_code, data[0].country_name, data[0].ip, data[0].latitude, data[0].longitude, data[0].metro_code, data[0].region_code, data[0].region_name, data[0].time_zone, data[0].zip_code, data[0].isp, data[0].ua)\n        elif type == 'count_victim_network':\n            return \"SELECT COUNT(*) AS C FROM networks WHERE id = '%s' AND network = '%s'\" % (data[0], data[1])\n        elif type == 'delete_networks':\n            return \"DELETE FROM networks WHERE id = '%s'\" % (data[0])\n        elif type == 'update_network':\n            return \"UPDATE networks SET date = '%s' WHERE id = '%s' AND network = '%s'\" % (data[2], data[0], data[1])\n        elif type == 'insert_networks':\n            return \"INSERT INTO networks(id, public_ip, ip, network, date) VALUES('%s','%s', '%s', '%s','%s')\" % (data[0], data[1], data[2], data[3], data[4])\n        elif type == 'insert_requests':\n            return \"INSERT INTO requests(id, user_id, site, fid, name, value, date) VALUES('%s', '%s','%s', '%s', '%s','%s', '%s')\" % (data[0].sId, data[0].id, data[0].site, data[0].fid, data[0].name, data[0].value, data[1])\n        elif type == 'insert_click':\n            return \"INSERT INTO clicks(id, site, date) VALUES('%s', '%s','%s')\" % (data[0], data[1], data[2])\n        elif type == 'report_online':\n            return \"UPDATE victims SET status = '%s' WHERE id = '%s'\" % ('online', data[0])\n        elif type == 'clean_online':\n            return \"UPDATE victims SET status = '%s' \" % ('offline')\n        elif type == 'disconnect_victim':\n            return \"UPDATE victims SET status = '%s' WHERE id = '%s'\" % ('offline', data)\n        else:\n            return False\n    def sentences_victim(self, type, data = None, sRun = 1, column = 0):\n        if sRun == 2:\n            return self.sql_insert(self.prop_sentences_victim(type, data))\n        elif sRun == 3:\n            return self.sql_one_row(self.prop_sentences_victim(type, data), column)\n        else:\n            return self.sql_execute(self.prop_sentences_victim(type, data))\n    def __del__(self):\n        self.conn.close()\ndef home_get_dat():\n    d = db.sentences_stats('get_data')\n    n = db.sentences_stats('all_networks')\n    ('clean_online')\n    rows = db.sentences_stats('get_clicks')\n    c = rows[0][0]\n    rows = db.sentences_stats('get_sessions')\n    s = rows[0][0]\n    rows = db.sentences_stats('get_online')\n    o = rows[0][0]\n    return json.dumps({'status' : 'OK', 'd' : d, 'n' : n, 'c' : c, 's' : s, 'o' : o});\ndef home_get_preview():\n    vId = request.form['vId']\n    d = db.sentences_stats('get_preview', vId)\n    n = db.sentences_stats('id_networks', vId)\n    return json.dumps({'status' : 'OK', 'vId' : vId, 'd' : d, 'n' : n});\n    def receivePing():\n        vrequest = request.form['id']\n        db.sentences_victim('report_online', [vrequest])\n        return json.dumps({'status' : 'OK', 'vId' : vrequest});",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2017-17714",
        "description": "[{'lang': 'en', 'value': 'Trape before 2017-11-05 has XSS via the /nr red parameter, the /nr vId parameter, the /register User-Agent HTTP header, the /register country parameter, the /register countryCode parameter, the /register cpu parameter, the /register isp parameter, the /register lat parameter, the /register lon parameter, the /register org parameter, the /register query parameter, the /register region parameter, the /register regionName parameter, the /register timezone parameter, the /register vId parameter, the /register zip parameter, or the /tping id parameter.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-115",
      "code": "def useruid(s, login):\n    \"\"\"Connect to a LDAP and check the uid matching the given field data\"\"\"\n    uid = False\n    c = Connection(s, config.LDAPACC,\n                   password=config.LDAPPASS, auto_bind=True)\n    if c.result[\"description\"] != \"success\":\n        app.logger.error(\"Error connecting to the LDAP with the service account\")\n        return False\n    if not c.search(config.LDAPBASE,\n                    \"(\" + config.LDAPFIELD + \"=\" + login + \")\") :\n        app.logger.error(\"Error: Connection to the LDAP with service account failed\")\n    else:\n        if len(c.entries) >= 1 :\n            if len(c.entries) > 1 :\n                app.logger.error(\"Error: multiple entries with this login. \"+ \\\n                          \"Trying first entry...\")\n            uid = c.entries[0].entry_dn\n        else:\n            app.logger.error(\"Error: Login not found\")\n        c.unbind()\n    return uid\ndef try_ldap_login(login, password):\n    \"\"\" Connect to a LDAP directory to verify user login/passwords\"\"\"\n    result = \"Wrong login/password\"\n    s = Server(config.LDAPURI, port=config.LDAPPORT,\n               use_ssl=False, get_info=ALL)\n    uid = useruid(s, login)\n    if uid:\n        c = Connection(s, user = uid , password = password, auto_bind = True)\n        c.open()\n        c.bind()\n        result =  c.result[\"description\"]\n        c.unbind()\n    return result",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-3027",
        "description": "[{'lang': 'en', 'value': 'app/views_mod/user/user.py in LibrIT PaSSHport through 2.5 is affected by LDAP Injection. There is an information leak through the crafting of special queries, escaping the provided search filter because user input gets no sanitization.'}]",
        "cwe_number": 74
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-116",
      "code": "def main(argv=None, stdout=None, stdin=None):\n    \"\"\"Entry-point of the executable.\"\"\"\n    parser = argparse.ArgumentParser(\n        description='Convert SVG files to other formats')\n    parser.add_argument('input', default='-', help='input filename or URL')\n    parser.add_argument(\n        '-v', '--version', action='version', version=VERSION)\n    parser.add_argument(\n        '-f', '--format', help='output format',\n        choices=sorted([surface.lower() for surface in SURFACES]))\n    parser.add_argument(\n        '-d', '--dpi', default=96, type=float,\n        help='ratio between 1 inch and 1 pixel')\n    parser.add_argument(\n        '-W', '--width', default=None, type=float,\n        help='width of the parent container in pixels')\n    parser.add_argument(\n        '-H', '--height', default=None, type=float,\n        help='height of the parent container in pixels')\n    parser.add_argument(\n        '-s', '--scale', default=1, type=float, help='output scaling factor')\n    parser.add_argument(\n        '-b', '--background', metavar='COLOR', help='output background color')\n    parser.add_argument(\n        '-n', '--negate-colors', action='store_true',\n        help='replace every vector color with its complement')\n    parser.add_argument(\n        '-i', '--invert-images', action='store_true',\n        help='replace every raster pixel with its complementary color')\n    parser.add_argument(\n        '-u', '--unsafe', action='store_true',\n        help='resolve XML entities and allow very large files '\n             '(WARNING: vulnerable to XXE attacks and various DoS)')\n    parser.add_argument(\n        '--output-width', default=None, type=float,\n        help='desired output width in pixels')\n    parser.add_argument(\n        '--output-height', default=None, type=float,\n        help='desired output height in pixels')\n    parser.add_argument('-o', '--output', default='-', help='output filename')\n    options = parser.parse_args(argv)\n    kwargs = {\n        'parent_width': options.width, 'parent_height': options.height,\n        'dpi': options.dpi, 'scale': options.scale, 'unsafe': options.unsafe,\n        'background_color': options.background,\n        'negate_colors': options.negate_colors,\n        'invert_images': options.invert_images,\n        'output_width': options.output_width,\n        'output_height': options.output_height}\n    stdin = stdin or sys.stdin\n    stdout = stdout or sys.stdout\n    kwargs['write_to'] = (\n        stdout.buffer if options.output == '-' else options.output)\n    if options.input == '-':\n        kwargs['file_obj'] = stdin.buffer\n    else:\n        kwargs['url'] = options.input\n    output_format = (\n        options.format or\n        os.path.splitext(options.output)[1].lstrip('.') or\n        'pdf').upper()\n    SURFACES[output_format.upper()].convert(**kwargs)\n    def convert(cls, bytestring=None, *, file_obj=None, url=None, dpi=96,\n                parent_width=None, parent_height=None, scale=1, unsafe=False,\n                background_color=None, negate_colors=False,\n                invert_images=False, write_to=None, output_width=None,\n                output_height=None, **kwargs):\n        \"\"\"Convert an SVG document to the format for this class.\n        Specify the input by passing one of these:\n        :param bytestring: The SVG source as a byte-string.\n        :param file_obj: A file-like object.\n        :param url: A filename.\n        Give some options:\n        :param dpi: The ratio between 1 inch and 1 pixel.\n        :param parent_width: The width of the parent container in pixels.\n        :param parent_height: The height of the parent container in pixels.\n        :param scale: The ouptut scaling factor.\n        :param unsafe: A boolean allowing XML entities and very large files\n                       (WARNING: vulnerable to XXE attacks and various DoS).\n        Specifiy the output with:\n        :param write_to: The filename of file-like object where to write the\n                         output. If None or not provided, return a byte string.\n        Only ``bytestring`` can be passed as a positional argument, other\n        parameters are keyword-only.\n        \"\"\"\n        tree = Tree(\n            bytestring=bytestring, file_obj=file_obj, url=url, unsafe=unsafe,\n            **kwargs)\n        output = write_to or io.BytesIO()\n        instance = cls(\n            tree, output, dpi, None, parent_width, parent_height, scale,\n            output_width, output_height, background_color,\n            map_rgba=negate_color if negate_colors else None,\n            map_image=invert_image if invert_images else None)\n        instance.finish()\n        if write_to is None:\n            return output.getvalue()",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-27586",
        "description": "[{'lang': 'en', 'value': \"CairoSVG is an SVG converter based on Cairo, a 2D graphics library. Prior to version 2.7.0, Cairo can send requests to external hosts when processing SVG files. A malicious actor could send a specially crafted SVG file that allows them to perform a server-side request forgery or denial of service. Version 2.7.0 disables CairoSVG's ability to access other files online by default.\"}]",
        "cwe_number": 918
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-117",
      "code": "if __name__ == \"__main__\":\n  for index, (input_size_, filter_size_, output_size_, stride_,\n              padding_) in enumerate(GetShrunkInceptionShapes()):\n    setattr(Conv2DTest, \"testInceptionFwd_\" + str(index),\n            test_util.run_in_graph_and_eager_modes(\n                GetInceptionFwdTest(input_size_, filter_size_, stride_,\n                                    padding_)))\n    setattr(\n        Conv2DTest, \"testInceptionFwdDilatedConv_\" + str(index),\n        test_util.run_in_graph_and_eager_modes(GetInceptionFwdDilatedConvTest(\n            input_size_, filter_size_, stride_, padding_)))\n    setattr(Conv2DTest, \"testInceptionBackInput_\" + str(index),\n            test_util.run_in_graph_and_eager_modes(\n                GetInceptionBackInputTest(input_size_, filter_size_,\n                                          output_size_, stride_, padding_)))\n    setattr(Conv2DTest, \"testInceptionBackFilter_\" + str(index),\n            test_util.run_in_graph_and_eager_modes(\n                GetInceptionBackFilterTest(input_size_, filter_size_,\n                                           output_size_, [stride_, stride_],\n                                           padding_)))\n  ishape = [1, 400, 400, 1]\n  fshape = [1, 1, 1, 256]\n  oshape = [1, 400, 400, 256]\n  setattr(Conv2DTest, \"testInceptionFwd_No_Winograd_Nonfused\",\n          test_util.run_in_graph_and_eager_modes(\n              GetInceptionFwdTest(ishape, fshape, 1, \"SAME\", gpu_only=True)))\n  setattr(Conv2DTest, \"testInceptionFwdDilatedConv_No_Winograd_Nonfused\",\n          test_util.run_in_graph_and_eager_modes(\n              GetInceptionFwdDilatedConvTest(ishape, fshape, 1, \"SAME\")))\n  setattr(Conv2DTest, \"testInceptionBackInput_No_Winograd_Nonfused\",\n          test_util.run_in_graph_and_eager_modes(\n              GetInceptionBackInputTest(ishape, fshape, oshape, 1, \"SAME\",\n                                        gpu_only=True)))\n  setattr(Conv2DTest, \"testInceptionBackFilter_No_Winograd_Nonfused\",\n          test_util.run_in_graph_and_eager_modes(\n              GetInceptionBackFilterTest(ishape, fshape, oshape, [1, 1], \"SAME\",\n                                         gpu_only=True)))\n  test.main()",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-41885",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. When `tf.raw_ops.FusedResizeAndPadConv2D` is given a large tensor shape, it overflows. We have patched the issue in GitHub commit d66e1d568275e6a2947de97dca7a102a211e01ce. The fix will be included in TensorFlow 2.11. We will also cherrypick this commit on TensorFlow 2.10.1, 2.9.3, and TensorFlow 2.8.4, as these are also affected and still in supported range.'}]",
        "cwe_number": 131
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-118",
      "code": "def gradio_manual_caption_gui_tab(headless=False, default_images_dir=None):\n    from .common_gui import create_refresh_button\n    default_images_dir = (\n        default_images_dir\n        if default_images_dir is not None\n        else os.path.join(scriptdir, \"data\")\n    )\n    current_images_dir = default_images_dir\n    def list_images_dirs(path):\n        nonlocal current_images_dir\n        current_images_dir = path\n        return list(list_dirs(path))\n    with gr.Tab(\"Manual Captioning\"):\n        gr.Markdown(\"This utility allows quick captioning and tagging of images.\")\n        page = gr.Number(value=-1, visible=False)\n        max_page = gr.Number(value=1, visible=False)\n        loaded_images_dir = gr.Text(visible=False)\n        with gr.Group(), gr.Row():\n            images_dir = gr.Dropdown(\n                label=\"Image folder to caption (containing the images to caption)\",\n                choices=[\"\"] + list_images_dirs(default_images_dir),\n                value=\"\",\n                interactive=True,\n                allow_custom_value=True,\n            )\n            create_refresh_button(\n                images_dir,\n                lambda: None,\n                lambda: {\"choices\": list_images_dirs(current_images_dir)},\n                \"open_folder_small\",\n            )\n            folder_button = gr.Button(\n                \"\ud83d\udcc2\",\n                elem_id=\"open_folder_small\",\n                elem_classes=[\"tool\"],\n                visible=(not headless),\n            )\n            folder_button.click(\n                get_folder_path,\n                outputs=images_dir,\n                show_progress=False,\n            )\n            load_images_button = gr.Button(\"Load\", elem_id=\"open_folder\")\n            caption_ext = gr.Textbox(\n                label=\"Caption file extension\",\n                placeholder=\"Extension for caption file (e.g., .caption, .txt)\",\n                value=\".txt\",\n                interactive=True,\n            )\n            auto_save = gr.Checkbox(\n                label=\"Autosave\", info=\"Options\", value=True, interactive=True\n            )\n            images_dir.change(\n                fn=lambda path: gr.Dropdown(choices=[\"\"] + list_images_dirs(path)),\n                inputs=images_dir,\n                outputs=images_dir,\n                show_progress=False,\n            )\n        with gr.Group(), gr.Row():\n            quick_tags_text = gr.Textbox(\n                label=\"Quick Tags\",\n                placeholder=\"Comma separated list of tags\",\n                interactive=True,\n            )\n            import_tags_button = gr.Button(\"Import\", elem_id=\"open_folder\")\n            ignore_load_tags_word_count = gr.Slider(\n                minimum=1,\n                maximum=100,\n                value=3,\n                step=1,\n                label=\"Ignore Imported Tags Above Word Count\",\n                interactive=True,\n            )\n        def render_pagination():\n            gr.Button(\"< Prev\", elem_id=\"open_folder\").click(\n                paginate,\n                inputs=[page, max_page, gr.Number(value=-1, visible=False)],\n                outputs=[page],\n            )\n            page_count = gr.Label(\"Page 1\", label=\"Page\")\n            page_goto_text = gr.Textbox(\n                label=\"Goto page\",\n                placeholder=\"Page Number\",\n                interactive=True,\n            )\n            gr.Button(\"Go >\", elem_id=\"open_folder\").click(\n                paginate_go,\n                inputs=[page_goto_text, max_page],\n                outputs=[page],\n            )\n            gr.Button(\"Next >\", elem_id=\"open_folder\").click(\n                paginate,\n                inputs=[page, max_page, gr.Number(value=1, visible=False)],\n                outputs=[page],\n            )\n            return page_count\n        with gr.Row(visible=False) as pagination_row1:\n            page_count1 = render_pagination()\n        image_rows = []\n        image_files = []\n        image_images = []\n        image_caption_texts = []\n        image_tag_checks = []\n        save_buttons = []\n        for _ in range(IMAGES_TO_SHOW):\n            with gr.Row(visible=False) as row:\n                image_file = gr.Text(visible=False)\n                image_files.append(image_file)\n                image_image = gr.Image(type=\"filepath\")\n                image_images.append(image_image)\n                image_caption_text = gr.TextArea(\n                    label=\"Captions\",\n                    placeholder=\"Input captions\",\n                    interactive=True,\n                )\n                image_caption_texts.append(image_caption_text)\n                tag_checkboxes = gr.CheckboxGroup([], label=\"Tags\", interactive=True)\n                save_button = gr.Button(\n                    \"\ud83d\udcbe\",\n                    elem_id=\"open_folder_small\",\n                    elem_classes=[\"tool\"],\n                    visible=False,\n                )\n                save_buttons.append(save_button)\n                image_caption_text.input(\n                    update_image_caption,\n                    inputs=[\n                        quick_tags_text,\n                        image_caption_text,\n                        image_file,\n                        loaded_images_dir,\n                        caption_ext,\n                        auto_save,\n                    ],\n                    outputs=tag_checkboxes,\n                )\n                tag_checkboxes.input(\n                    update_image_tags,\n                    inputs=[\n                        quick_tags_text,\n                        tag_checkboxes,\n                        image_file,\n                        loaded_images_dir,\n                        caption_ext,\n                        auto_save,\n                    ],\n                    outputs=[image_caption_text],\n                )\n                save_button.click(\n                    save_caption,\n                    inputs=[\n                        image_caption_text,\n                        caption_ext,\n                        image_file,\n                        images_dir,\n                    ],\n                )\n                image_tag_checks.append(tag_checkboxes)\n                image_rows.append(row)\n        with gr.Row(visible=False) as pagination_row2:\n            page_count2 = render_pagination()\n        quick_tags_text.change(\n            update_quick_tags,\n            inputs=[quick_tags_text] + image_caption_texts,\n            outputs=image_tag_checks,\n        )\n        import_tags_button.click(\n            import_tags_from_captions,\n            inputs=[\n                loaded_images_dir,\n                caption_ext,\n                quick_tags_text,\n                ignore_load_tags_word_count,\n            ],\n            outputs=quick_tags_text,\n        )\n        load_images_button.click(\n            load_images,\n            inputs=[\n                images_dir,\n                caption_ext,\n                loaded_images_dir,\n                page,\n                max_page,\n            ],\n            outputs=[loaded_images_dir, page, max_page],\n        )\n        image_update_key = gr.Text(visible=False)\n        image_update_key.change(\n            update_images,\n            inputs=[loaded_images_dir, caption_ext, quick_tags_text, page],\n            outputs=image_rows\n            + image_files\n            + image_images\n            + image_caption_texts\n            + image_tag_checks\n            + [pagination_row1, pagination_row2],\n            show_progress=False,\n        )\n        listener_kwargs = {\n            \"fn\": lambda p, i: f\"{p}-{i}\",\n            \"inputs\": [page, loaded_images_dir],\n            \"outputs\": image_update_key,\n        }\n        page.change(**listener_kwargs)\n        loaded_images_dir.change(**listener_kwargs)\n        auto_save.change(\n            lambda auto_save: [gr.Button(visible=not auto_save)] * IMAGES_TO_SHOW,\n            inputs=auto_save,\n            outputs=save_buttons,\n        )\n        page.change(\n            lambda page, max_page: [f\"Page {int(page)} / {int(max_page)}\"] * 2,\n            inputs=[page, max_page],\n            outputs=[page_count1, page_count2],\n            show_progress=False,\n        )\ndef gradio_wd14_caption_gui_tab(\n    headless=False, default_train_dir=None, config: KohyaSSGUIConfig = {}\n):\n    from .common_gui import create_refresh_button\n    default_train_dir = (\n        default_train_dir\n        if default_train_dir is not None\n        else os.path.join(scriptdir, \"data\")\n    )\n    current_train_dir = default_train_dir\n    def list_train_dirs(path):\n        nonlocal current_train_dir\n        current_train_dir = path\n        return list(list_dirs(path))\n    with gr.Tab(\"WD14 Captioning\"):\n        gr.Markdown(\n            \"This utility will use WD14 to caption files for each images in a folder.\"\n        )\n        with gr.Group(), gr.Row():\n            train_data_dir = gr.Dropdown(\n                label=\"Image folder to caption (containing the images to caption)\",\n                choices=[config.get(\"wd14_caption.train_data_dir\", \"\")]\n                + list_train_dirs(default_train_dir),\n                value=config.get(\"wd14_caption.train_data_dir\", \"\"),\n                interactive=True,\n                allow_custom_value=True,\n            )\n            create_refresh_button(\n                train_data_dir,\n                lambda: None,\n                lambda: {\"choices\": list_train_dirs(current_train_dir)},\n                \"open_folder_small\",\n            )\n            button_train_data_dir_input = gr.Button(\n                \"\ud83d\udcc2\",\n                elem_id=\"open_folder_small\",\n                elem_classes=[\"tool\"],\n                visible=(not headless),\n            )\n            button_train_data_dir_input.click(\n                get_folder_path,\n                outputs=train_data_dir,\n                show_progress=False,\n            )\n            repo_id = gr.Dropdown(\n                label=\"Repo ID\",\n                choices=[\n                    \"SmilingWolf/wd-v1-4-convnext-tagger-v2\",\n                    \"SmilingWolf/wd-v1-4-convnextv2-tagger-v2\",\n                    \"SmilingWolf/wd-v1-4-vit-tagger-v2\",\n                    \"SmilingWolf/wd-v1-4-swinv2-tagger-v2\",\n                    \"SmilingWolf/wd-v1-4-moat-tagger-v2\",\n                    \"SmilingWolf/wd-swinv2-tagger-v3\",\n                    \"SmilingWolf/wd-vit-tagger-v3\",\n                    \"SmilingWolf/wd-convnext-tagger-v3\",\n                ],\n                value=config.get(\n                    \"wd14_caption.repo_id\", \"SmilingWolf/wd-v1-4-convnextv2-tagger-v2\"\n                ),\n                show_label=\"Repo id for wd14 tagger on Hugging Face\",\n            )\n            force_download = gr.Checkbox(\n                label=\"Force model re-download\",\n                value=config.get(\"wd14_caption.force_download\", False),\n                info=\"Useful to force model re download when switching to onnx\",\n            )\n        with gr.Row():\n            caption_extension = gr.Textbox(\n                label=\"Caption file extension\",\n                placeholder=\"Extension for caption file (e.g., .caption, .txt)\",\n                value=config.get(\"wd14_caption.caption_extension\", \".txt\"),\n                interactive=True,\n            )\n            caption_separator = gr.Textbox(\n                label=\"Caption Separator\",\n                value=config.get(\"wd14_caption.caption_separator\", \", \"),\n                interactive=True,\n            )\n        with gr.Row():\n            tag_replacement = gr.Textbox(\n                label=\"Tag replacement\",\n                info=\"tag replacement in the format of `source1,target1;source2,target2; ...`. Escape `,` and `;` with `\\`. e.g. `tag1,tag2;tag3,tag4`\",\n                value=config.get(\"wd14_caption.tag_replacement\", \"\"),\n                interactive=True,\n            )\n            character_tag_expand = gr.Checkbox(\n                label=\"Character tag expand\",\n                info=\"expand tag tail parenthesis to another tag for character tags. `chara_name_(series)` becomes `chara_name, series`\",\n                value=config.get(\"wd14_caption.character_tag_expand\", False),\n                interactive=True,\n            )\n        undesired_tags = gr.Textbox(\n            label=\"Undesired tags\",\n            placeholder=\"(Optional) Separate `undesired_tags` with comma `(,)` if you want to remove multiple tags, e.g. `1girl,solo,smile`.\",\n            interactive=True,\n            value=config.get(\"wd14_caption.undesired_tags\", \"\"),\n        )\n        with gr.Row():\n            always_first_tags = gr.Textbox(\n                label=\"Prefix to add to WD14 caption\",\n                info=\"comma-separated list of tags to always put at the beginning, e.g. 1girl, 1boy, \",\n                placeholder=\"(Optional)\",\n                interactive=True,\n                value=config.get(\"wd14_caption.always_first_tags\", \"\"),\n            )\n        with gr.Row():\n            onnx = gr.Checkbox(\n                label=\"Use onnx\",\n                value=config.get(\"wd14_caption.onnx\", True),\n                interactive=True,\n                info=\"https://github.com/onnx/onnx\",\n            )\n            append_tags = gr.Checkbox(\n                label=\"Append TAGs\",\n                value=config.get(\"wd14_caption.append_tags\", False),\n                interactive=True,\n                info=\"This option appends the tags to the existing tags, instead of replacing them.\",\n            )\n            use_rating_tags = gr.Checkbox(\n                label=\"Use rating tags\",\n                value=config.get(\"wd14_caption.use_rating_tags\", False),\n                interactive=True,\n                info=\"Adds rating tags as the first tag\",\n            )\n            use_rating_tags_as_last_tag = gr.Checkbox(\n                label=\"Use rating tags as last tag\",\n                value=config.get(\"wd14_caption.use_rating_tags_as_last_tag\", False),\n                interactive=True,\n                info=\"Adds rating tags as the last tag\",\n            )\n        with gr.Row():\n            recursive = gr.Checkbox(\n                label=\"Recursive\",\n                value=config.get(\"wd14_caption.recursive\", False),\n                info=\"Tag subfolders images as well\",\n            )\n            remove_underscore = gr.Checkbox(\n                label=\"Remove underscore\",\n                value=config.get(\"wd14_caption.remove_underscore\", True),\n                info=\"replace underscores with spaces in the output tags\",\n            )\n            debug = gr.Checkbox(\n                label=\"Debug\",\n                value=config.get(\"wd14_caption.debug\", True),\n                info=\"Debug mode\",\n            )\n            frequency_tags = gr.Checkbox(\n                label=\"Show tags frequency\",\n                value=config.get(\"wd14_caption.frequency_tags\", True),\n                info=\"Show frequency of tags for images.\",\n            )\n        with gr.Row():\n            thresh = gr.Slider(\n                value=config.get(\"wd14_caption.thresh\", 0.35),\n                label=\"Threshold\",\n                info=\"threshold of confidence to add a tag\",\n                minimum=0,\n                maximum=1,\n                step=0.05,\n            )\n            general_threshold = gr.Slider(\n                value=config.get(\"wd14_caption.general_threshold\", 0.35),\n                label=\"General threshold\",\n                info=\"Adjust `general_threshold` for pruning tags (less tags, less flexible)\",\n                minimum=0,\n                maximum=1,\n                step=0.05,\n            )\n            character_threshold = gr.Slider(\n                value=config.get(\"wd14_caption.character_threshold\", 0.35),\n                label=\"Character threshold\",\n                minimum=0,\n                maximum=1,\n                step=0.05,\n            )\n        with gr.Row():\n            batch_size = gr.Number(\n                value=config.get(\"wd14_caption.batch_size\", 1),\n                label=\"Batch size\",\n                interactive=True,\n            )\n            max_data_loader_n_workers = gr.Number(\n                value=config.get(\"wd14_caption.max_data_loader_n_workers\", 2),\n                label=\"Max dataloader workers\",\n                interactive=True,\n            )\n        caption_button = gr.Button(\"Caption images\")\n        caption_button.click(\n            caption_images,\n            inputs=[\n                train_data_dir,\n                caption_extension,\n                batch_size,\n                general_threshold,\n                character_threshold,\n                repo_id,\n                recursive,\n                max_data_loader_n_workers,\n                debug,\n                undesired_tags,\n                frequency_tags,\n                always_first_tags,\n                onnx,\n                append_tags,\n                force_download,\n                caption_separator,\n                tag_replacement,\n                character_tag_expand,\n                use_rating_tags,\n                use_rating_tags_as_last_tag,\n                remove_underscore,\n                thresh,\n            ],\n            show_progress=False,\n        )\n        train_data_dir.change(\n            fn=lambda path: gr.Dropdown(choices=[\"\"] + list_train_dirs(path)),\n            inputs=train_data_dir,\n            outputs=train_data_dir,\n            show_progress=False,\n        )\ndef gradio_basic_caption_gui_tab(headless=False, default_images_dir=None):\n    \"\"\"\n    Creates a Gradio tab for basic image captioning.\n    Args:\n        headless (bool, optional): If True, the GUI will be headless (no visible elements). Defaults to False.\n        default_images_dir (str, optional): The default directory to use for image selection. If not provided,\n            it defaults to the 'data' directory in the script directory.\n    Returns:\n        None\n    \"\"\"\n    from .common_gui import create_refresh_button\n    default_images_dir = (\n        default_images_dir\n        if default_images_dir is not None\n        else os.path.join(scriptdir, \"data\")\n    )\n    current_images_dir = default_images_dir\n    def list_images_dirs(path):\n        \"\"\"\n        Lists directories within a specified path and updates the current image directory.\n        Parameters:\n            path (str): The directory path to list image directories from.\n        Returns:\n            list: A list of directories within the specified path.\n        \"\"\"\n        nonlocal current_images_dir\n        current_images_dir = path\n        return list(list_dirs(path))\n    with gr.Tab(\"Basic Captioning\"):\n        gr.Markdown(\n            \"This utility allows you to create simple caption files for each image in a folder.\"\n        )\n        with gr.Group(), gr.Row():\n            images_dir = gr.Dropdown(\n                label=\"Image folder to caption (containing the images to caption)\",\n                choices=[\"\"] + list_images_dirs(default_images_dir),\n                value=\"\",\n                interactive=True,\n                allow_custom_value=True,\n            )\n            create_refresh_button(\n                images_dir,\n                lambda: None,\n                lambda: {\"choices\": list_images_dirs(current_images_dir)},\n                \"open_folder_small\",\n            )\n            folder_button = gr.Button(\n                \"\ud83d\udcc2\",\n                elem_id=\"open_folder_small\",\n                elem_classes=[\"tool\"],\n                visible=(not headless),\n            )\n            folder_button.click(\n                get_folder_path,\n                outputs=images_dir,\n                show_progress=False,\n            )\n            caption_ext = gr.Textbox(\n                label=\"Caption file extension\",\n                placeholder=\"Extension for caption file (e.g., .caption, .txt)\",\n                value=\".txt\",\n                interactive=True,\n            )\n            overwrite = gr.Checkbox(\n                label=\"Overwrite existing captions in folder\",\n                interactive=True,\n                value=False,\n            )\n        with gr.Row():\n            prefix = gr.Textbox(\n                label=\"Prefix to add to caption\",\n                placeholder=\"(Optional)\",\n                interactive=True,\n            )\n            caption_text = gr.Textbox(\n                label=\"Caption text\",\n                placeholder='e.g., \"by some artist\". Leave empty if you only want to add a prefix or postfix.',\n                interactive=True,\n                lines=2,\n            )\n            postfix = gr.Textbox(\n                label=\"Postfix to add to caption\",\n                placeholder=\"(Optional)\",\n                interactive=True,\n            )\n        with gr.Group(), gr.Row():\n            find_text = gr.Textbox(\n                label=\"Find text\",\n                placeholder='e.g., \"by some artist\". Leave empty if you only want to add a prefix or postfix.',\n                interactive=True,\n                lines=2,\n            )\n            replace_text = gr.Textbox(\n                label=\"Replacement text\",\n                placeholder='e.g., \"by some artist\". Leave empty if you want to replace with nothing.',\n                interactive=True,\n                lines=2,\n            )\n            caption_button = gr.Button(\"Caption images\")\n            caption_button.click(\n                caption_images,\n                inputs=[\n                    caption_text,\n                    images_dir,\n                    overwrite,\n                    caption_ext,\n                    prefix,\n                    postfix,\n                    find_text,\n                    replace_text,\n                ],\n                show_progress=False,\n            )\n        images_dir.change(\n            fn=lambda path: gr.Dropdown(choices=[\"\"] + list_images_dirs(path)),\n            inputs=images_dir,\n            outputs=images_dir,\n            show_progress=False,\n        )\ndef gradio_group_images_gui_tab(headless=False):\n    from .common_gui import create_refresh_button\n    current_input_folder = os.path.join(scriptdir, \"data\")\n    current_output_folder = os.path.join(scriptdir, \"data\")\n    def list_input_dirs(path):\n        nonlocal current_input_folder\n        current_input_folder = path\n        return list(list_dirs(path))\n    def list_output_dirs(path):\n        nonlocal current_output_folder\n        current_output_folder = path\n        return list(list_dirs(path))\n    with gr.Tab(\"Group Images\"):\n        gr.Markdown(\n            \"This utility will group images in a folder based on their aspect ratio.\"\n        )\n        with gr.Group(), gr.Row():\n            input_folder = gr.Dropdown(\n                label=\"Input folder (containing the images to group)\",\n                interactive=True,\n                choices=[\"\"] + list_input_dirs(current_input_folder),\n                value=\"\",\n                allow_custom_value=True,\n            )\n            create_refresh_button(\n                input_folder,\n                lambda: None,\n                lambda: {\"choices\": list_input_dirs(current_input_folder)},\n                \"open_folder_small\",\n            )\n            button_input_folder = gr.Button(\n                \"\ud83d\udcc2\",\n                elem_id=\"open_folder_small\",\n                elem_classes=[\"tool\"],\n                visible=(not headless),\n            )\n            button_input_folder.click(\n                get_folder_path,\n                outputs=input_folder,\n                show_progress=False,\n            )\n            output_folder = gr.Dropdown(\n                label=\"Output folder (where the grouped images will be stored)\",\n                interactive=True,\n                choices=[\"\"] + list_output_dirs(current_output_folder),\n                value=\"\",\n                allow_custom_value=True,\n            )\n            create_refresh_button(\n                output_folder,\n                lambda: None,\n                lambda: {\"choices\": list_output_dirs(current_output_folder)},\n                \"open_folder_small\",\n            )\n            button_output_folder = gr.Button(\n                \"\ud83d\udcc2\",\n                elem_id=\"open_folder_small\",\n                elem_classes=[\"tool\"],\n                visible=(not headless),\n            )\n            button_output_folder.click(\n                get_folder_path,\n                outputs=output_folder,\n                show_progress=False,\n            )\n            input_folder.change(\n                fn=lambda path: gr.Dropdown(choices=[\"\"] + list_input_dirs(path)),\n                inputs=input_folder,\n                outputs=input_folder,\n                show_progress=False,\n            )\n            output_folder.change(\n                fn=lambda path: gr.Dropdown(choices=[\"\"] + list_output_dirs(path)),\n                inputs=output_folder,\n                outputs=output_folder,\n                show_progress=False,\n            )\n        with gr.Row():\n            group_size = gr.Slider(\n                label=\"Group size\",\n                info=\"Number of images to group together\",\n                value=4,\n                minimum=1,\n                maximum=64,\n                step=1,\n                interactive=True,\n            )\n            include_subfolders = gr.Checkbox(\n                label=\"Include Subfolders\",\n                value=False,\n                info=\"Include images in subfolders as well\",\n            )\n            do_not_copy_other_files = gr.Checkbox(\n                label=\"Do not copy other files\",\n                value=False,\n                info=\"Do not copy other files in the input folder to the output folder\",\n            )\n            generate_captions = gr.Checkbox(\n                label=\"Generate Captions\",\n                value=False,\n                info=\"Generate caption files for the grouped images based on their folder name\",\n            )\n            caption_ext = gr.Textbox(\n                label=\"Caption Extension\",\n                placeholder=\"Caption file extension (e.g., .txt)\",\n                value=\".txt\",\n                interactive=True,\n            )\n        group_images_button = gr.Button(\"Group images\")\n        group_images_button.click(\n            group_images,\n            inputs=[\n                input_folder,\n                output_folder,\n                group_size,\n                include_subfolders,\n                do_not_copy_other_files,\n                generate_captions,\n                caption_ext,\n            ],\n            show_progress=False,\n        )\ndef gradio_blip_caption_gui_tab(headless=False, default_train_dir=None):\n    from .common_gui import create_refresh_button\n    default_train_dir = (\n        default_train_dir\n        if default_train_dir is not None\n        else os.path.join(scriptdir, \"data\")\n    )\n    current_train_dir = default_train_dir\n    def list_train_dirs(path):\n        nonlocal current_train_dir\n        current_train_dir = path\n        return list(list_dirs(path))\n    with gr.Tab(\"BLIP Captioning\"):\n        gr.Markdown(\n            \"This utility uses BLIP to caption files for each image in a folder.\"\n        )\n        with gr.Group(), gr.Row():\n            train_data_dir = gr.Dropdown(\n                label=\"Image folder to caption (containing the images to caption)\",\n                choices=[\"\"] + list_train_dirs(default_train_dir),\n                value=\"\",\n                interactive=True,\n                allow_custom_value=True,\n            )\n            create_refresh_button(\n                train_data_dir,\n                lambda: None,\n                lambda: {\"choices\": list_train_dirs(current_train_dir)},\n                \"open_folder_small\",\n            )\n            button_train_data_dir_input = gr.Button(\n                \"\ud83d\udcc2\",\n                elem_id=\"open_folder_small\",\n                elem_classes=[\"tool\"],\n                visible=(not headless),\n            )\n            button_train_data_dir_input.click(\n                get_folder_path,\n                outputs=train_data_dir,\n                show_progress=False,\n            )\n        with gr.Row():\n            caption_file_ext = gr.Textbox(\n                label=\"Caption file extension\",\n                placeholder=\"Extension for caption file (e.g., .caption, .txt)\",\n                value=\".txt\",\n                interactive=True,\n            )\n            prefix = gr.Textbox(\n                label=\"Prefix to add to BLIP caption\",\n                placeholder=\"(Optional)\",\n                interactive=True,\n            )\n            postfix = gr.Textbox(\n                label=\"Postfix to add to BLIP caption\",\n                placeholder=\"(Optional)\",\n                interactive=True,\n            )\n            batch_size = gr.Number(value=1, label=\"Batch size\", interactive=True)\n        with gr.Row():\n            beam_search = gr.Checkbox(\n                label=\"Use beam search\", interactive=True, value=True\n            )\n            num_beams = gr.Number(value=1, label=\"Number of beams\", interactive=True)\n            top_p = gr.Number(value=0.9, label=\"Top p\", interactive=True)\n            max_length = gr.Number(value=75, label=\"Max length\", interactive=True)\n            min_length = gr.Number(value=5, label=\"Min length\", interactive=True)\n        caption_button = gr.Button(\"Caption images\")\n        caption_button.click(\n            caption_images,\n            inputs=[\n                train_data_dir,\n                caption_file_ext,\n                batch_size,\n                num_beams,\n                top_p,\n                max_length,\n                min_length,\n                beam_search,\n                prefix,\n                postfix,\n            ],\n            show_progress=False,\n        )\n        train_data_dir.change(\n            fn=lambda path: gr.Dropdown(choices=[\"\"] + list_train_dirs(path)),\n            inputs=train_data_dir,\n            outputs=train_data_dir,\n            show_progress=False,\n        )\ndef gradio_git_caption_gui_tab(headless=False, default_train_dir=None):\n    from .common_gui import create_refresh_button\n    default_train_dir = (\n        default_train_dir\n        if default_train_dir is not None\n        else os.path.join(scriptdir, \"data\")\n    )\n    current_train_dir = default_train_dir\n    def list_train_dirs(path):\n        nonlocal current_train_dir\n        current_train_dir = path\n        return list(list_dirs(path))\n    with gr.Tab(\"GIT Captioning\"):\n        gr.Markdown(\n            \"This utility will use GIT to caption files for each images in a folder.\"\n        )\n        with gr.Group(), gr.Row():\n            train_data_dir = gr.Dropdown(\n                label=\"Image folder to caption (containing the images to caption)\",\n                choices=[\"\"] + list_train_dirs(default_train_dir),\n                value=\"\",\n                interactive=True,\n                allow_custom_value=True,\n            )\n            create_refresh_button(\n                train_data_dir,\n                lambda: None,\n                lambda: {\"choices\": list_train_dirs(current_train_dir)},\n                \"open_folder_small\",\n            )\n            button_train_data_dir_input = gr.Button(\n                \"\ud83d\udcc2\",\n                elem_id=\"open_folder_small\",\n                elem_classes=[\"tool\"],\n                visible=(not headless),\n            )\n            button_train_data_dir_input.click(\n                get_folder_path,\n                outputs=train_data_dir,\n                show_progress=False,\n            )\n        with gr.Row():\n            caption_ext = gr.Textbox(\n                label=\"Caption file extension\",\n                placeholder=\"Extension for caption file (e.g., .caption, .txt)\",\n                value=\".txt\",\n                interactive=True,\n            )\n            prefix = gr.Textbox(\n                label=\"Prefix to add to GIT caption\",\n                placeholder=\"(Optional)\",\n                interactive=True,\n            )\n            postfix = gr.Textbox(\n                label=\"Postfix to add to GIT caption\",\n                placeholder=\"(Optional)\",\n                interactive=True,\n            )\n            batch_size = gr.Number(value=1, label=\"Batch size\", interactive=True)\n        with gr.Row():\n            max_data_loader_n_workers = gr.Number(\n                value=2, label=\"Number of workers\", interactive=True\n            )\n            max_length = gr.Number(value=75, label=\"Max length\", interactive=True)\n            model_id = gr.Textbox(\n                label=\"Model\",\n                placeholder=\"(Optional) model id for GIT in Hugging Face\",\n                interactive=True,\n            )\n        caption_button = gr.Button(\"Caption images\")\n        caption_button.click(\n            caption_images,\n            inputs=[\n                train_data_dir,\n                caption_ext,\n                batch_size,\n                max_data_loader_n_workers,\n                max_length,\n                model_id,\n                prefix,\n                postfix,\n            ],\n            show_progress=False,\n        )\n        train_data_dir.change(\n            fn=lambda path: gr.Dropdown(choices=[\"\"] + list_train_dirs(path)),\n            inputs=train_data_dir,\n            outputs=train_data_dir,\n            show_progress=False,\n        )\ndef find_replace(\n    folder_path: str = \"\",\n    caption_file_ext: str = \".caption\",\n    search_text: str = \"\",\n    replace_text: str = \"\",\n) -> None:\n    \"\"\"\n    Efficiently finds and replaces specified text across all caption files in a given folder.\n    This function iterates through each caption file matching the specified extension within the given folder path, replacing all occurrences of the search text with the replacement text. It ensures that the operation only proceeds if the search text is provided and there are caption files to process.\n    Args:\n        folder_path (str, optional): The directory path where caption files are located. Defaults to an empty string, which implies the current directory.\n        caption_file_ext (str, optional): The file extension for caption files. Defaults to \".caption\".\n        search_text (str, optional): The text to search for within the caption files. Defaults to an empty string.\n        replace_text (str, optional): The text to use as a replacement. Defaults to an empty string.\n    \"\"\"\n    log.info(\"Running caption find/replace\")\n    if not search_text or not has_ext_files(folder_path, caption_file_ext):\n        msgbox(\n            f\"No files with extension {caption_file_ext} were found in {folder_path}...\"\n        )\n        log.warning(\n            \"No files with extension {caption_file_ext} were found in {folder_path}...\"\n        )\n        return\n    caption_files = [f for f in os.listdir(folder_path) if f.endswith(caption_file_ext)]\n    for caption_file in caption_files:\n        file_path = os.path.join(folder_path, caption_file)\n        with open(file_path, \"r\", errors=\"ignore\") as f:\n            content = f.read().replace(search_text, replace_text)\n        with open(file_path, \"w\") as f:\n            f.write(content)\ndef color_aug_changed(color_aug):\n    \"\"\"\n    Handles the change in color augmentation checkbox.\n    This function is called when the color augmentation checkbox is toggled.\n    If color augmentation is enabled, it disables the cache latent checkbox\n    and returns a new checkbox with the value set to False and interactive set to False.\n    If color augmentation is disabled, it returns a new checkbox with interactive set to True.\n    Args:\n        color_aug (bool): The new state of the color augmentation checkbox.\n    Returns:\n        gr.Checkbox: A new checkbox with the appropriate settings based on the color augmentation state.\n    \"\"\"\n    if color_aug:\n        msgbox(\n            'Disabling \"Cache latent\" because \"Color augmentation\" has been selected...'\n        )\n        return gr.Checkbox(value=False, interactive=False)\n    else:\n        return gr.Checkbox(interactive=True)\n    def init_training_controls(self) -> None:\n        \"\"\"\n        Initializes the training controls for the model.\n        \"\"\"\n        with gr.Row():\n            self.train_batch_size = gr.Slider(\n                minimum=1, maximum=64, label=\"Train batch size\", value=1, step=self.config.get(\"basic.train_batch_size\", 1),\n            )\n            self.epoch = gr.Number(label=\"Epoch\", value=self.config.get(\"basic.epoch\", 1), precision=0)\n            self.max_train_epochs = gr.Textbox(\n                label=\"Max train epoch\",\n                placeholder=\"(Optional) Enforce\n                value=self.config.get(\"basic.max_train_epochs\", \"\"),\n            )\n            self.max_train_steps = gr.Textbox(\n                label=\"Max train steps\",\n                placeholder=\"(Optional) Enforce\n                value=self.config.get(\"basic.max_train_steps\", \"\"),\n            )\n            self.save_every_n_epochs = gr.Number(\n                label=\"Save every N epochs\", value=self.config.get(\"basic.save_every_n_epochs\", 1), precision=0\n            )\n            self.caption_extension = gr.Textbox(\n                label=\"Caption Extension\",\n                placeholder=\"(Optional) default: .caption\",\n                value=self.config.get(\"basic.caption_extension\", \"\"),\n            )",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-32023",
        "description": "[{'lang': 'en', 'value': \"Kohya_ss is a GUI for Kohya's Stable Diffusion trainers. Kohya_ss is vulnerable to a path injection in the `common_gui.py` `find_and_replace` function. This vulnerability is fixed in 23.1.5.\"}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-119",
      "code": "def info():\n    api = flask.current_app.config[\"PYLOAD_API\"]\n    conf = api.get_config_dict()\n    extra = os.uname() if hasattr(os, \"uname\") else tuple()\n    context = {\n        \"python\": sys.version,\n        \"os\": \" \".join((os.name, sys.platform) + extra),\n        \"version\": api.get_server_version(),\n        \"folder\": PKGDIR,\n        \"config\": api.get_userdir(),\n        \"download\": conf[\"general\"][\"storage_folder\"][\"value\"],\n        \"freespace\": format.size(api.free_space()),\n        \"webif\": conf[\"webui\"][\"port\"][\"value\"],\n        \"language\": conf[\"general\"][\"language\"][\"value\"],\n    }\n    return render_template(\"info.html\", **context)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-21644",
        "description": "[{'lang': 'en', 'value': 'pyLoad is the free and open-source Download Manager written in pure Python. Any unauthenticated user can browse to a specific URL to expose the Flask config, including the `SECRET_KEY` variable. This issue has been patched in version 0.5.0b3.dev77.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-120",
      "code": "  def testEmptyTensorListInvalidShape(self):\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                r\"Shape must be at most rank 1 but is rank 2\"):\n      t = gen_list_ops.EmptyTensorList(\n          element_shape=array_ops.ones(dtype=dtypes.int32, shape=[1, 0]),\n          max_num_elements=constant_op.constant(1),\n          element_dtype=dtypes.int32)\n      self.evaluate(t)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-41891",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. If `tf.raw_ops.TensorListConcat` is given `element_shape=[]`, it results segmentation fault which can be used to trigger a denial of service attack. We have patched the issue in GitHub commit fc33f3dc4c14051a83eec6535b608abe1d355fde. The fix will be included in TensorFlow 2.11. We will also cherrypick this commit on TensorFlow 2.10.1, 2.9.3, and TensorFlow 2.8.4, as these are also affected and still in supported range.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-121",
      "code": "    def get_field_options(self, field):\n        options = {}\n        options['label'] = field.label\n        options['help_text'] = field.help_text\n        options['required'] = field.required\n        options['initial'] = field.default_value\n        return options\n    def get_form_class(self):\n        return type(str('WagtailForm'), (BaseForm,), self.formfields)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2020-15118",
        "description": "[{'lang': 'en', 'value': \"In Wagtail before versions 2.7.4 and 2.9.3, when a form page type is made available to Wagtail editors through the `wagtail.contrib.forms` app, and the page template is built using Django's standard form rendering helpers such as form.as_p, any HTML tags used within a form field's help text will be rendered unescaped in the page. Allowing HTML within help text is an intentional design decision by Django; however, as a matter of policy Wagtail does not allow editors to insert arbitrary HTML by default, as this could potentially be used to carry out cross-site scripting attacks, including privilege escalation. This functionality should therefore not have been made available to editor-level users. The vulnerability is not exploitable by an ordinary site visitor without access to the Wagtail admin. Patched versions have been released as Wagtail 2.7.4 (for the LTS 2.7 branch) and Wagtail 2.9.3 (for the current 2.9 branch). In these versions, help text will be escaped to prevent the inclusion of HTML tags. Site owners who wish to re-enable the use of HTML within help text (and are willing to accept the risk of this being exploited by editors) may set WAGTAILFORMS_HELP_TEXT_ALLOW_HTML = True in their configuration settings. Site owners who are unable to upgrade to the new versions can secure their form page templates by rendering forms field-by-field as per Django's documentation, but omitting the |safe filter when outputting the help text.\"}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-122",
      "code": "    def __render_string(self, value):\n        orig = last = value\n        max_recursion = self.__dict__.get('jinja_max_recursion', 5)\n        for _ in range(max_recursion):\n            template = jinja2.Template(value, keep_trailing_newline=True)\n            value = _to_native(template.render(self.__dict__))\n            if value == last:\n                return value\n            last = value\n        raise ValueError(\"too deep jinja re-evaluation on '{}'\".format(orig))",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-6395",
        "description": "[{'lang': 'en', 'value': 'The Mock software contains a vulnerability wherein an attacker could potentially exploit privilege escalation, enabling the execution of arbitrary code with root user privileges. This weakness stems from the absence of proper sandboxing during the expansion and execution of Jinja2 templates, which may be included in certain configuration parameters. While the Mock documentation advises treating users added to the mock group as privileged, certain build systems invoking mock on behalf of users might inadvertently permit less privileged users to define configuration tags. These tags could then be passed as parameters to mock during execution, potentially leading to the utilization of Jinja2 templates for remote privilege escalation and the execution of arbitrary code as the root user on the build server.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-123",
      "code": "    def create_new_client_event(self, builder, requester=None,\n                                prev_events_and_hashes=None):\n        \"\"\"Create a new event for a local client\n        Args:\n            builder (EventBuilder):\n            requester (synapse.types.Requester|None):\n            prev_events_and_hashes (list[(str, dict[str, str], int)]|None):\n                the forward extremities to use as the prev_events for the\n                new event. For each event, a tuple of (event_id, hashes, depth)\n                where *hashes* is a map from algorithm to hash.\n                If None, they will be requested from the database.\n        Returns:\n            Deferred[(synapse.events.EventBase, synapse.events.snapshot.EventContext)]\n        \"\"\"\n        if prev_events_and_hashes is not None:\n            assert len(prev_events_and_hashes) <= 10, \\\n                \"Attempting to create an event with %i prev_events\" % (\n                    len(prev_events_and_hashes),\n            )\n        else:\n            prev_events_and_hashes = \\\n                yield self.store.get_prev_events_for_room(builder.room_id)\n        if prev_events_and_hashes:\n            depth = max([d for _, _, d in prev_events_and_hashes]) + 1\n        else:\n            depth = 1\n        prev_events = [\n            (event_id, prev_hashes)\n            for event_id, prev_hashes, _ in prev_events_and_hashes\n        ]\n        builder.prev_events = prev_events\n        builder.depth = depth\n        context = yield self.state.compute_event_context(builder)\n        if requester:\n            context.app_service = requester.app_service\n        if builder.is_state():\n            builder.prev_state = yield self.store.add_event_hashes(\n                context.prev_state_events\n            )\n        yield self.auth.add_auth_events(builder, context)\n        signing_key = self.hs.config.signing_key[0]\n        add_hashes_and_signatures(\n            builder, self.server_name, signing_key\n        )\n        event = builder.build()\n        logger.debug(\n            \"Created event %s with state: %s\",\n            event.event_id, context.prev_state_ids,\n        )\n        defer.returnValue(\n            (event, context,)\n        )\n    def handle_new_client_event(\n        self,\n        requester,\n        event,\n        context,\n        ratelimit=True,\n        extra_users=[],\n    ):\n        \"\"\"Processes a new event. This includes checking auth, persisting it,\n        notifying users, sending to remote servers, etc.\n        If called from a worker will hit out to the master process for final\n        processing.\n        Args:\n            requester (Requester)\n            event (FrozenEvent)\n            context (EventContext)\n            ratelimit (bool)\n            extra_users (list(UserID)): Any extra users to notify about event\n        \"\"\"\n        try:\n            yield self.auth.check_from_context(event, context)\n        except AuthError as err:\n            logger.warn(\"Denying new event %r because %s\", event, err)\n            raise err\n        try:\n            dump = frozendict_json_encoder.encode(event.content)\n            simplejson.loads(dump)\n        except Exception:\n            logger.exception(\"Failed to encode content: %r\", event.content)\n            raise\n        yield self.action_generator.handle_push_actions_for_event(\n            event, context\n        )\n        try:\n            if self.config.worker_app:\n                yield send_event_to_master(\n                    self.http_client,\n                    host=self.config.worker_replication_host,\n                    port=self.config.worker_replication_http_port,\n                    requester=requester,\n                    event=event,\n                    context=context,\n                    ratelimit=ratelimit,\n                    extra_users=extra_users,\n                )\n                return\n            yield self.persist_and_notify_client_event(\n                requester,\n                event,\n                context,\n                ratelimit=ratelimit,\n                extra_users=extra_users,\n            )\n        except:\n            preserve_fn(self.store.remove_push_actions_from_staging)(event.event_id)\n            raise\ndef event_from_pdu_json(pdu_json, outlier=False):\n    \"\"\"Construct a FrozenEvent from an event json received over federation\n    Args:\n        pdu_json (object): pdu as received over federation\n        outlier (bool): True to mark this event as an outlier\n    Returns:\n        FrozenEvent\n    Raises:\n        SynapseError: if the pdu is missing required fields\n    \"\"\"\n    assert_params_in_request(pdu_json, ('event_id', 'type'))\n    event = FrozenEvent(\n        pdu_json\n    )\n    event.internal_metadata.outlier = outlier\n    return event",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2018-10657",
        "description": "[{'lang': 'en', 'value': 'Matrix Synapse before 0.28.1 is prone to a denial of service flaw where malicious events injected with depth = 2^63 - 1 render rooms unusable, related to federation/federation_base.py and handlers/message.py, as exploited in the wild in April 2018.'}]",
        "cwe_number": 20
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-124",
      "code": "    def render_prefs_panel(self, panelid, action=None, **kwargs):\n        form = SshForm()\n        if action == \"add\":\n            self._add_key(action, form)\n        elif action == 'delete':\n            self._delete_key(action, DeleteSshForm())\n        params = {'form': form}\n        try:\n            params[\"sshkeys\"] = [\n                {'title': key.comment or (key.keytype + ' ' + key.key[:18]), 'fingerprint': key.fingerprint}\n                for key in self.app.currentuser.authorizedkeys\n            ]\n        except IOError:\n            params[\"sshkeys\"] = []\n            flash(_(\"Failed to get SSH keys\"), level='error')\n            _logger.warning(\"error reading SSH keys\", exc_info=1)\n        return \"prefs_sshkeys.html\", params",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-3221",
        "description": "[{'lang': 'en', 'value': 'Cross-Site Request Forgery (CSRF) in GitHub repository ikus060/rdiffweb prior to 2.4.3.'}]",
        "cwe_number": 352
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-125",
      "code": "    def __init__(\n        self,\n        *tables: t.Union[t.Type[Table], TableConfig],\n        forms: t.List[FormConfig] = [],\n        auth_table: t.Type[BaseUser] = BaseUser,\n        session_table: t.Type[SessionsBase] = SessionsBase,\n        session_expiry: timedelta = timedelta(hours=1),\n        max_session_expiry: timedelta = timedelta(days=7),\n        increase_expiry: t.Optional[timedelta] = timedelta(minutes=20),\n        page_size: int = 15,\n        read_only: bool = False,\n        rate_limit_provider: t.Optional[RateLimitProvider] = None,\n        production: bool = False,\n        site_name: str = \"Piccolo Admin\",\n        default_language_code: str = \"auto\",\n        translations: t.Optional[t.List[Translation]] = None,\n        allowed_hosts: t.Sequence[str] = [],\n        debug: bool = False,\n        sidebar_links: t.Dict[str, str] = {},\n    ) -> None:\n        super().__init__(\n            title=site_name,\n            description=f\"{site_name} documentation\",\n            middleware=[\n                Middleware(CSRFMiddleware, allowed_hosts=allowed_hosts)\n            ],\n            debug=debug,\n            exception_handlers={500: log_error},\n            docs_url=None,\n            redoc_url=None,\n        )\n        table_configs: t.List[TableConfig] = []\n        for table in tables:\n            if isinstance(table, TableConfig):\n                table_configs.append(table)\n            else:\n                table_configs.append(TableConfig(table_class=table))\n        self.table_configs = sorted(\n            table_configs,\n            key=lambda table_config: table_config.table_class._meta.tablename,\n        )\n        self.table_config_map = {\n            table_config.table_class._meta.tablename: table_config\n            for table_config in self.table_configs\n        }\n        for table_config in table_configs:\n            table_class = table_config.table_class\n            for column in table_class._meta.columns:\n                if column._meta.secret and column._meta.required:\n                    message = (\n                        f\"{table_class._meta.tablename}.\"\n                        f\"{column._meta._name} is using `secret` and \"\n                        f\"`required` column args which are incompatible. \"\n                        f\"You may encounter unexpected behavior when using \"\n                        f\"this table within Piccolo Admin.\"\n                    )\n                    colored_warning(message, level=Level.high)\n        media_storage = [\n            i\n            for i in itertools.chain(\n                *[\n                    table_config.media_storage or []\n                    for table_config in table_configs\n                ]\n            )\n        ]\n        if len(media_storage) != len(set(media_storage)):\n            raise ValueError(\n                \"Media storage is misconfigured - multiple columns are saving \"\n                \"to the same location.\"\n            )\n        self.default_language_code = default_language_code\n        self.translations_map = {\n            translation.language_code.lower(): translation\n            for translation in (translations or TRANSLATIONS)\n        }\n        self.auth_table = auth_table\n        self.site_name = site_name\n        self.forms = forms\n        self.read_only = read_only\n        self.sidebar_links = sidebar_links\n        self.form_config_map = {form.slug: form for form in self.forms}\n        with open(os.path.join(ASSET_PATH, \"index.html\")) as f:\n            self.template = f.read()\n        private_app = FastAPI(\n            docs_url=None,\n            redoc_url=None,\n            debug=debug,\n            exception_handlers={500: log_error},\n        )\n        private_app.mount(\"/docs/\", swagger_ui(schema_url=\"../openapi.json\"))\n        for table_config in table_configs:\n            table_class = table_config.table_class\n            visible_column_names = table_config.get_visible_column_names()\n            visible_filter_names = table_config.get_visible_filter_names()\n            rich_text_columns_names = (\n                table_config.get_rich_text_columns_names()\n            )\n            media_columns_names = table_config.get_media_columns_names()\n            link_column_name = table_config.get_link_column()._meta.name\n            order_by = table_config.get_order_by()\n            time_resolution = table_config.get_time_resolution()\n            validators = table_config.validators\n            if table_class in (auth_table, session_table):\n                validators = validators or Validators()\n                validators.every = [superuser_validators, *validators.every]\n            FastAPIWrapper(\n                root_url=f\"/tables/{table_class._meta.tablename}/\",\n                fastapi_app=private_app,\n                piccolo_crud=PiccoloCRUD(\n                    table=table_class,\n                    read_only=read_only,\n                    page_size=page_size,\n                    schema_extra={\n                        \"visible_column_names\": visible_column_names,\n                        \"visible_filter_names\": visible_filter_names,\n                        \"rich_text_columns\": rich_text_columns_names,\n                        \"media_columns\": media_columns_names,\n                        \"link_column_name\": link_column_name,\n                        \"order_by\": tuple(i.to_dict() for i in order_by),\n                        \"time_resolution\": time_resolution,\n                    },\n                    validators=validators,\n                    hooks=table_config.hooks,\n                ),\n                fastapi_kwargs=FastAPIKwargs(\n                    all_routes={\n                        \"tags\": [f\"{table_class._meta.tablename.capitalize()}\"]\n                    },\n                ),\n            )\n        private_app.add_api_route(\n            path=\"/tables/\",\n            endpoint=self.get_table_list,\n            methods=[\"GET\"],\n            response_model=t.List[str],\n            tags=[\"Tables\"],\n        )\n        private_app.add_api_route(\n            path=\"/tables/grouped/\",\n            endpoint=self.get_table_list_grouped,\n            methods=[\"GET\"],\n            response_model=GroupedTableNamesResponseModel,\n            tags=[\"Tables\"],\n        )\n        private_app.add_api_route(\n            path=\"/links/\",\n            endpoint=self.get_sidebar_links,\n            methods=[\"GET\"],\n            tags=[\"Links\"],\n        )\n        private_app.add_api_route(\n            path=\"/forms/\",\n            endpoint=self.get_forms,\n            methods=[\"GET\"],\n            tags=[\"Forms\"],\n            response_model=t.List[FormConfigResponseModel],\n        )\n        private_app.add_api_route(\n            path=\"/forms/{form_slug:str}/\",\n            endpoint=self.get_single_form,\n            methods=[\"GET\"],\n            tags=[\"Forms\"],\n        )\n        private_app.add_api_route(\n            path=\"/forms/{form_slug:str}/schema/\",\n            endpoint=self.get_single_form_schema,\n            methods=[\"GET\"],\n            tags=[\"Forms\"],\n        )\n        private_app.add_api_route(\n            path=\"/forms/{form_slug:str}/\",\n            endpoint=self.post_single_form,\n            methods=[\"POST\"],\n            tags=[\"Forms\"],\n        )\n        private_app.add_api_route(\n            path=\"/user/\",\n            endpoint=self.get_user,\n            methods=[\"GET\"],\n            tags=[\"User\"],\n            response_model=UserResponseModel,\n        )\n        private_app.add_route(\n            path=\"/change-password/\",\n            route=change_password(\n                login_url=\"./../../public/login/\",\n                session_table=session_table,\n                read_only=read_only,\n            ),\n            methods=[\"POST\"],\n        )\n        private_app.add_api_route(\n            path=\"/media/\",\n            endpoint=self.store_file,\n            methods=[\"POST\"],\n            tags=[\"Media\"],\n            response_model=StoreFileResponseModel,\n        )\n        private_app.add_api_route(\n            path=\"/media/generate-file-url/\",\n            endpoint=self.generate_file_url,\n            methods=[\"POST\"],\n            tags=[\"Media\"],\n            response_model=GenerateFileURLResponseModel,\n        )\n        for table_config in self.table_configs:\n            if table_config.media_columns:\n                for (\n                    column,\n                    media_storage,\n                ) in table_config.media_columns.items():\n                    if isinstance(media_storage, LocalMediaStorage):\n                        private_app.mount(\n                            path=f\"/media-files/{column._meta.table._meta.tablename}/{column._meta.name}/\",\n                            app=StaticFiles(\n                                directory=media_storage.media_path\n                            ),\n                        )\n        public_app = FastAPI(\n            redoc_url=None,\n            docs_url=None,\n            debug=debug,\n            exception_handlers={500: log_error},\n        )\n        public_app.mount(\"/docs/\", swagger_ui(schema_url=\"../openapi.json\"))\n        if not rate_limit_provider:\n            rate_limit_provider = InMemoryLimitProvider(\n                limit=100, timespan=300\n            )\n        public_app.mount(\n            path=\"/login/\",\n            app=RateLimitingMiddleware(\n                app=session_login(\n                    auth_table=self.auth_table,\n                    session_table=session_table,\n                    session_expiry=session_expiry,\n                    max_session_expiry=max_session_expiry,\n                    redirect_to=None,\n                    production=production,\n                ),\n                provider=rate_limit_provider,\n            ),\n        )\n        public_app.add_route(\n            path=\"/logout/\",\n            route=session_logout(session_table=session_table),\n            methods=[\"POST\"],\n        )\n        public_app.add_api_route(\n            \"/meta/\", endpoint=self.get_meta, tags=[\"Meta\"]\n        )\n        public_app.add_api_route(\n            \"/translations/\",\n            endpoint=self.get_translation_list,\n            methods=[\"GET\"],\n            tags=[\"Translations\"],\n            response_model=TranslationListResponse,\n        )\n        public_app.add_api_route(\n            \"/translations/{language_code:str}/\",\n            endpoint=self.get_translation,\n            methods=[\"GET\"],\n            tags=[\"Translations\"],\n            response_model=Translation,\n        )\n        self.router.add_route(\n            path=\"/\", endpoint=self.get_root, methods=[\"GET\"]\n        )\n        self.mount(\n            path=\"/assets\",\n            app=StaticFiles(directory=os.path.join(ASSET_PATH, \"assets\")),\n        )\n        auth_middleware = partial(\n            AuthenticationMiddleware,\n            backend=SessionsAuthBackend(\n                auth_table=auth_table,\n                session_table=session_table,\n                admin_only=True,\n                increase_expiry=increase_expiry,\n            ),\n            on_error=handle_auth_exception,\n        )\n        self.mount(path=\"/api\", app=auth_middleware(private_app))\n        self.mount(path=\"/public\", app=public_app)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-30248",
        "description": "[{'lang': 'en', 'value': \"Piccolo Admin is an admin interface/content management system for Python, built on top of Piccolo. Piccolo's admin panel allows media files to be uploaded. As a default, SVG is an allowed file type for upload. An attacker can upload an SVG which when loaded can allow arbitrary access to the admin page. This vulnerability was patched in version 1.3.2.\"}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-126",
      "code": "    def __init__(self, repository, key, manifest, path=None, sync=True):\n        self.timestamp = None\n        self.txn_active = False\n        self.repository = repository\n        self.key = key\n        self.manifest = manifest\n        self.path = path or os.path.join(get_cache_dir(), hexlify(repository.id).decode('ascii'))\n        if not os.path.exists(self.path):\n            self.create()\n        self.open()\n        if sync and self.manifest.id != self.manifest_id:\n            if self.timestamp and self.timestamp > manifest.timestamp:\n                raise self.RepositoryReplay()\n            self.sync()\n            self.commit()\n    def __del__(self):\n        self.close()\n    def create(self):\n        \"\"\"Create a new empty cache at `path`\n        \"\"\"\n        os.makedirs(self.path)\n        with open(os.path.join(self.path, 'README'), 'w') as fd:\n            fd.write('This is an Attic cache')\n        config = RawConfigParser()\n        config.add_section('cache')\n        config.set('cache', 'version', '1')\n        config.set('cache', 'repository', hexlify(self.repository.id).decode('ascii'))\n        config.set('cache', 'manifest', '')\n        with open(os.path.join(self.path, 'config'), 'w') as fd:\n            config.write(fd)\n        ChunkIndex().write(os.path.join(self.path, 'chunks').encode('utf-8'))\n        with open(os.path.join(self.path, 'files'), 'w') as fd:\n            pass\n    def _read_files(self):\n        self.files = {}\n        self._newest_mtime = 0\n        with open(os.path.join(self.path, 'files'), 'rb') as fd:\n            u = msgpack.Unpacker(use_list=True)\n            while True:\n                data = fd.read(64 * 1024)\n                if not data:\n                    break\n                u.feed(data)\n                for path_hash, item in u:\n                    item[0] += 1\n                    self.files[path_hash] = msgpack.packb(item)\n    def begin_txn(self):\n        txn_dir = os.path.join(self.path, 'txn.tmp')\n        os.mkdir(txn_dir)\n        shutil.copy(os.path.join(self.path, 'config'), txn_dir)\n        shutil.copy(os.path.join(self.path, 'chunks'), txn_dir)\n        shutil.copy(os.path.join(self.path, 'files'), txn_dir)\n        os.rename(os.path.join(self.path, 'txn.tmp'),\n                  os.path.join(self.path, 'txn.active'))\n        self.txn_active = True\n    def rollback(self):\n        \"\"\"Roll back partial and aborted transactions\n        \"\"\"\n        if os.path.exists(os.path.join(self.path, 'txn.tmp')):\n            shutil.rmtree(os.path.join(self.path, 'txn.tmp'))\n        txn_dir = os.path.join(self.path, 'txn.active')\n        if os.path.exists(txn_dir):\n            shutil.copy(os.path.join(txn_dir, 'config'), self.path)\n            shutil.copy(os.path.join(txn_dir, 'chunks'), self.path)\n            shutil.copy(os.path.join(txn_dir, 'files'), self.path)\n            os.rename(txn_dir, os.path.join(self.path, 'txn.tmp'))\n            if os.path.exists(os.path.join(self.path, 'txn.tmp')):\n                shutil.rmtree(os.path.join(self.path, 'txn.tmp'))\n        self.txn_active = False\n    def sync(self):\n        \"\"\"Initializes cache by fetching and reading all archive indicies\n        \"\"\"\n        def add(id, size, csize):\n            try:\n                count, size, csize = self.chunks[id]\n                self.chunks[id] = count + 1, size, csize\n            except KeyError:\n                self.chunks[id] = 1, size, csize\n        self.begin_txn()\n        print('Initializing cache...')\n        self.chunks.clear()\n        unpacker = msgpack.Unpacker()\n        repository = cache_if_remote(self.repository)\n        for name, info in self.manifest.archives.items():\n            archive_id = info[b'id']\n            cdata = repository.get(archive_id)\n            data = self.key.decrypt(archive_id, cdata)\n            add(archive_id, len(data), len(cdata))\n            archive = msgpack.unpackb(data)\n            if archive[b'version'] != 1:\n                raise Exception('Unknown archive metadata version')\n            decode_dict(archive, (b'name',))\n            print('Analyzing archive:', archive[b'name'])\n            for key, chunk in zip(archive[b'items'], repository.get_many(archive[b'items'])):\n                data = self.key.decrypt(key, chunk)\n                add(key, len(data), len(chunk))\n                unpacker.feed(data)\n                for item in unpacker:\n                    if b'chunks' in item:\n                        for chunk_id, size, csize in item[b'chunks']:\n                            add(chunk_id, size, csize)\n    def setUp(self):\n        os.environ['ATTIC_CHECK_I_KNOW_WHAT_I_AM_DOING'] = '1'\n        self.archiver = Archiver()\n        self.tmpdir = tempfile.mkdtemp()\n        self.repository_path = os.path.join(self.tmpdir, 'repository')\n        self.repository_location = self.prefix + self.repository_path\n        self.input_path = os.path.join(self.tmpdir, 'input')\n        self.output_path = os.path.join(self.tmpdir, 'output')\n        self.keys_path = os.path.join(self.tmpdir, 'keys')\n        self.cache_path = os.path.join(self.tmpdir, 'cache')\n        self.exclude_file_path = os.path.join(self.tmpdir, 'excludes')\n        os.environ['ATTIC_KEYS_DIR'] = self.keys_path\n        os.environ['ATTIC_CACHE_DIR'] = self.cache_path\n        os.mkdir(self.input_path)\n        os.mkdir(self.output_path)\n        os.mkdir(self.keys_path)\n        os.mkdir(self.cache_path)\n        with open(self.exclude_file_path, 'wb') as fd:\n            fd.write(b'input/file2\\n\n        self._old_wd = os.getcwd()\n        os.chdir(self.tmpdir)\n    def test_strip_components(self):\n        self.attic('init', self.repository_location)\n        self.create_regular_file('dir/file')\n        self.attic('create', self.repository_location + '::test', 'input')\n        with changedir('output'):\n            self.attic('extract', self.repository_location + '::test', '--strip-components', '3')\n            self.assert_true(not os.path.exists('file'))\n            with self.assert_creates_file('file'):\n                self.attic('extract', self.repository_location + '::test', '--strip-components', '2')\n            with self.assert_creates_file('dir/file'):\n                self.attic('extract', self.repository_location + '::test', '--strip-components', '1')\n            with self.assert_creates_file('input/dir/file'):\n                self.attic('extract', self.repository_location + '::test', '--strip-components', '0')\n    def test_extract_include_exclude(self):\n        self.attic('init', self.repository_location)\n        self.create_regular_file('file1', size=1024 * 80)\n        self.create_regular_file('file2', size=1024 * 80)\n        self.create_regular_file('file3', size=1024 * 80)\n        self.create_regular_file('file4', size=1024 * 80)\n        self.attic('create', '--exclude=input/file4', self.repository_location + '::test', 'input')\n        with changedir('output'):\n            self.attic('extract', self.repository_location + '::test', 'input/file1', )\n        self.assert_equal(sorted(os.listdir('output/input')), ['file1'])\n        with changedir('output'):\n            self.attic('extract', '--exclude=input/file2', self.repository_location + '::test')\n        self.assert_equal(sorted(os.listdir('output/input')), ['file1', 'file3'])\n        with changedir('output'):\n            self.attic('extract', '--exclude-from=' + self.exclude_file_path, self.repository_location + '::test')\n        self.assert_equal(sorted(os.listdir('output/input')), ['file1', 'file3'])\n    def test_exclude_caches(self):\n        self.attic('init', self.repository_location)\n        self.create_regular_file('file1', size=1024 * 80)\n        self.create_regular_file('cache1/CACHEDIR.TAG', contents = b'Signature: 8a477f597d28d172789f06886806bc55 extra stuff')\n        self.create_regular_file('cache2/CACHEDIR.TAG', contents = b'invalid signature')\n        self.attic('create', '--exclude-caches', self.repository_location + '::test', 'input')\n        with changedir('output'):\n            self.attic('extract', self.repository_location + '::test')\n        self.assert_equal(sorted(os.listdir('output/input')), ['cache2', 'file1'])\n        self.assert_equal(sorted(os.listdir('output/input/cache2')), ['CACHEDIR.TAG'])\n    def do_init(self, args):\n        \"\"\"Initialize an empty repository\"\"\"\n        print('Initializing repository at \"%s\"' % args.repository.orig)\n        repository = self.open_repository(args.repository, create=True, exclusive=True)\n        key = key_creator(repository, args)\n        manifest = Manifest(key, repository)\n        manifest.key = key\n        manifest.write()\n        repository.commit()\n        return self.exit_code",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2015-4082",
        "description": "[{'lang': 'en', 'value': 'attic before 0.15 does not confirm unencrypted backups with the user, which allows remote attackers with read and write privileges for the encrypted repository to obtain potentially sensitive information by changing the manifest type byte of the repository to \"unencrypted / without key file\".'}]",
        "cwe_number": 264
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-127",
      "code": "    def create_app(\n        blocks: gradio.Blocks, app_kwargs: Dict[str, Any] | None = None\n    ) -> App:\n        app_kwargs = app_kwargs or {}\n        app_kwargs.setdefault(\"default_response_class\", ORJSONResponse)\n        app = App(**app_kwargs)\n        app.configure_app(blocks)\n        if not wasm_utils.IS_WASM:\n            app.add_middleware(\n                CORSMiddleware,\n                allow_origins=[\"*\"],\n                allow_methods=[\"*\"],\n                allow_headers=[\"*\"],\n            )\n        @app.get(\"/user\")\n        @app.get(\"/user/\")\n        def get_current_user(request: fastapi.Request) -> Optional[str]:\n            token = request.cookies.get(\n                f\"access-token-{app.cookie_id}\"\n            ) or request.cookies.get(f\"access-token-unsecure-{app.cookie_id}\")\n            return app.tokens.get(token)\n        @app.get(\"/login_check\")\n        @app.get(\"/login_check/\")\n        def login_check(user: str = Depends(get_current_user)):\n            if app.auth is None or user is not None:\n                return\n            raise HTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Not authenticated\"\n            )\n        @app.get(\"/token\")\n        @app.get(\"/token/\")\n        def get_token(request: fastapi.Request) -> dict:\n            token = request.cookies.get(f\"access-token-{app.cookie_id}\")\n            return {\"token\": token, \"user\": app.tokens.get(token)}\n        @app.get(\"/app_id\")\n        @app.get(\"/app_id/\")\n        def app_id(request: fastapi.Request) -> dict:\n            return {\"app_id\": app.get_blocks().app_id}\n        @app.get(\"/dev/reload\", dependencies=[Depends(login_check)])\n        async def notify_changes(\n            request: fastapi.Request,\n        ):\n            async def reload_checker(request: fastapi.Request):\n                heartbeat_rate = 15\n                check_rate = 0.05\n                last_heartbeat = time.perf_counter()\n                while True:\n                    if await request.is_disconnected():\n                        return\n                    if app.change_event and app.change_event.is_set():\n                        app.change_event.clear()\n                        yield \"\"\"data: CHANGE\\n\\n\"\"\"\n                    await asyncio.sleep(check_rate)\n                    if time.perf_counter() - last_heartbeat > heartbeat_rate:\n                        yield \"\"\"data: HEARTBEAT\\n\\n\"\"\"\n                        last_heartbeat = time.time()\n            return StreamingResponse(\n                reload_checker(request),\n                media_type=\"text/event-stream\",\n            )\n        @app.post(\"/login\")\n        @app.post(\"/login/\")\n        def login(form_data: OAuth2PasswordRequestForm = Depends()):\n            username, password = form_data.username.strip(), form_data.password\n            if app.auth is None:\n                return RedirectResponse(url=\"/\", status_code=status.HTTP_302_FOUND)\n            if (\n                not callable(app.auth)\n                and username in app.auth\n                and compare_passwords_securely(password, app.auth[username])\n            ) or (callable(app.auth) and app.auth.__call__(username, password)):\n                token = secrets.token_urlsafe(16)\n                app.tokens[token] = username\n                response = JSONResponse(content={\"success\": True})\n                response.set_cookie(\n                    key=f\"access-token-{app.cookie_id}\",\n                    value=token,\n                    httponly=True,\n                    samesite=\"none\",\n                    secure=True,\n                )\n                response.set_cookie(\n                    key=f\"access-token-unsecure-{app.cookie_id}\",\n                    value=token,\n                    httponly=True,\n                )\n                return response\n            else:\n                raise HTTPException(status_code=400, detail=\"Incorrect credentials.\")\n        if app.blocks is not None and app.blocks.expects_oauth:\n            attach_oauth(app)\n        @app.head(\"/\", response_class=HTMLResponse)\n        @app.get(\"/\", response_class=HTMLResponse)\n        def main(request: fastapi.Request, user: str = Depends(get_current_user)):\n            mimetypes.add_type(\"application/javascript\", \".js\")\n            blocks = app.get_blocks()\n            root = route_utils.get_root_url(\n                request=request, route_path=\"/\", root_path=app.root_path\n            )\n            if app.auth is None or user is not None:\n                config = app.get_blocks().config\n                config = route_utils.update_root_in_config(config, root)\n            else:\n                config = {\n                    \"auth_required\": True,\n                    \"auth_message\": blocks.auth_message,\n                    \"space_id\": app.get_blocks().space_id,\n                    \"root\": root,\n                }\n            try:\n                template = (\n                    \"frontend/share.html\" if blocks.share else \"frontend/index.html\"\n                )\n                return templates.TemplateResponse(\n                    template,\n                    {\"request\": request, \"config\": config},\n                )\n            except TemplateNotFound as err:\n                if blocks.share:\n                    raise ValueError(\n                        \"Did you install Gradio from source files? Share mode only \"\n                        \"works when Gradio is installed through the pip package.\"\n                    ) from err\n                else:\n                    raise ValueError(\n                        \"Did you install Gradio from source files? You need to build \"\n                        \"the frontend by running /scripts/build_frontend.sh\"\n                    ) from err\n        @app.get(\"/info/\", dependencies=[Depends(login_check)])\n        @app.get(\"/info\", dependencies=[Depends(login_check)])\n        def api_info():\n            return app.get_blocks().get_api_info()\n        @app.get(\"/config/\", dependencies=[Depends(login_check)])\n        @app.get(\"/config\", dependencies=[Depends(login_check)])\n        def get_config(request: fastapi.Request):\n            config = app.get_blocks().config\n            root = route_utils.get_root_url(\n                request=request, route_path=\"/config\", root_path=app.root_path\n            )\n            config = route_utils.update_root_in_config(config, root)\n            return ORJSONResponse(content=config)\n        @app.get(\"/static/{path:path}\")\n        def static_resource(path: str):\n            static_file = safe_join(STATIC_PATH_LIB, path)\n            return FileResponse(static_file)\n        @app.get(\"/custom_component/{id}/{type}/{file_name}\")\n        def custom_component_path(id: str, type: str, file_name: str):\n            config = app.get_blocks().config\n            components = config[\"components\"]\n            location = next(\n                (item for item in components if item[\"component_class_id\"] == id), None\n            )\n            if location is None:\n                raise HTTPException(status_code=404, detail=\"Component not found.\")\n            component_instance = app.get_blocks().get_component(location[\"id\"])\n            module_name = component_instance.__class__.__module__\n            module_path = sys.modules[module_name].__file__\n            if module_path is None or component_instance is None:\n                raise HTTPException(status_code=404, detail=\"Component not found.\")\n            return FileResponse(\n                safe_join(\n                    str(Path(module_path).parent),\n                    f\"{component_instance.__class__.TEMPLATE_DIR}/{type}/{file_name}\",\n                )\n            )\n        @app.get(\"/assets/{path:path}\")\n        def build_resource(path: str):\n            build_file = safe_join(BUILD_PATH_LIB, path)\n            return FileResponse(build_file)\n        @app.get(\"/favicon.ico\")\n        async def favicon():\n            blocks = app.get_blocks()\n            if blocks.favicon_path is None:\n                return static_resource(\"img/logo.svg\")\n            else:\n                return FileResponse(blocks.favicon_path)\n        @app.head(\"/proxy={url_path:path}\", dependencies=[Depends(login_check)])\n        @app.get(\"/proxy={url_path:path}\", dependencies=[Depends(login_check)])\n        async def reverse_proxy(url_path: str):\n            try:\n                rp_req = app.build_proxy_request(url_path)\n            except PermissionError as err:\n                raise HTTPException(status_code=400, detail=str(err)) from err\n            rp_resp = await client.send(rp_req, stream=True)\n            return StreamingResponse(\n                rp_resp.aiter_raw(),\n                status_code=rp_resp.status_code,\n                headers=rp_resp.headers,\n                background=BackgroundTask(rp_resp.aclose),\n            )\n        @app.head(\"/file={path_or_url:path}\", dependencies=[Depends(login_check)])\n        @app.get(\"/file={path_or_url:path}\", dependencies=[Depends(login_check)])\n        async def file(path_or_url: str, request: fastapi.Request):\n            blocks = app.get_blocks()\n            if client_utils.is_http_url_like(path_or_url):\n                return RedirectResponse(\n                    url=path_or_url, status_code=status.HTTP_302_FOUND\n                )\n            if route_utils.starts_with_protocol(path_or_url):\n                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")\n            abs_path = utils.abspath(path_or_url)\n            in_blocklist = any(\n                utils.is_in_or_equal(abs_path, blocked_path)\n                for blocked_path in blocks.blocked_paths\n            )\n            is_dir = abs_path.is_dir()\n            if in_blocklist or is_dir:\n                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")\n            created_by_app = False\n            for temp_file_set in blocks.temp_file_sets:\n                if abs_path in temp_file_set:\n                    created_by_app = True\n                    break\n            in_allowlist = any(\n                utils.is_in_or_equal(abs_path, allowed_path)\n                for allowed_path in blocks.allowed_paths\n            )\n            was_uploaded = utils.is_in_or_equal(abs_path, app.uploaded_file_dir)\n            is_cached_example = utils.is_in_or_equal(\n                abs_path, utils.abspath(utils.get_cache_folder())\n            )\n            if not (\n                created_by_app or in_allowlist or was_uploaded or is_cached_example\n            ):\n                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")\n            if not abs_path.exists():\n                raise HTTPException(404, f\"File not found: {path_or_url}.\")\n            range_val = request.headers.get(\"Range\", \"\").strip()\n            if range_val.startswith(\"bytes=\") and \"-\" in range_val:\n                range_val = range_val[6:]\n                start, end = range_val.split(\"-\")\n                if start.isnumeric() and end.isnumeric():\n                    start = int(start)\n                    end = int(end)\n                    response = ranged_response.RangedFileResponse(\n                        abs_path,\n                        ranged_response.OpenRange(start, end),\n                        dict(request.headers),\n                        stat_result=os.stat(abs_path),\n                    )\n                    return response\n            return FileResponse(abs_path, headers={\"Accept-Ranges\": \"bytes\"})\n        @app.get(\n            \"/stream/{session_hash}/{run}/{component_id}\",\n            dependencies=[Depends(login_check)],\n        )\n        async def stream(\n            session_hash: str,\n            run: int,\n            component_id: int,\n            request: fastapi.Request,\n        ):\n            stream: list = (\n                app.get_blocks()\n                .pending_streams[session_hash]\n                .get(run, {})\n                .get(component_id, None)\n            )\n            if stream is None:\n                raise HTTPException(404, \"Stream not found.\")\n            def stream_wrapper():\n                check_stream_rate = 0.01\n                max_wait_time = 120\n                wait_time = 0\n                while True:\n                    if len(stream) == 0:\n                        if wait_time > max_wait_time:\n                            return\n                        wait_time += check_stream_rate\n                        time.sleep(check_stream_rate)\n                        continue\n                    wait_time = 0\n                    next_stream = stream.pop(0)\n                    if next_stream is None:\n                        return\n                    yield next_stream\n            return StreamingResponse(stream_wrapper())\n        @app.get(\"/file/{path:path}\", dependencies=[Depends(login_check)])\n        async def file_deprecated(path: str, request: fastapi.Request):\n            return await file(path, request)\n        @app.post(\"/reset/\")\n        @app.post(\"/reset\")\n        async def reset_iterator(body: ResetBody):\n            if body.event_id not in app.iterators:\n                return {\"success\": False}\n            async with app.lock:\n                del app.iterators[body.event_id]\n                app.iterators_to_reset.add(body.event_id)\n                await app.get_blocks()._queue.clean_events(event_id=body.event_id)\n            return {\"success\": True}\n        @app.post(\"/run/{api_name}\", dependencies=[Depends(login_check)])\n        @app.post(\"/run/{api_name}/\", dependencies=[Depends(login_check)])\n        @app.post(\"/api/{api_name}\", dependencies=[Depends(login_check)])\n        @app.post(\"/api/{api_name}/\", dependencies=[Depends(login_check)])\n        async def predict(\n            api_name: str,\n            body: PredictBody,\n            request: fastapi.Request,\n            username: str = Depends(get_current_user),\n        ):\n            fn_index_inferred = route_utils.infer_fn_index(\n                app=app, api_name=api_name, body=body\n            )\n            if not app.get_blocks().api_open and app.get_blocks().queue_enabled_for_fn(\n                fn_index_inferred\n            ):\n                raise HTTPException(\n                    detail=\"This API endpoint does not accept direct HTTP POST requests. Please join the queue to use this API.\",\n                    status_code=status.HTTP_404_NOT_FOUND,\n                )\n            gr_request = route_utils.compile_gr_request(\n                app,\n                body,\n                fn_index_inferred=fn_index_inferred,\n                username=username,\n                request=request,\n            )\n            try:\n                output = await route_utils.call_process_api(\n                    app=app,\n                    body=body,\n                    gr_request=gr_request,\n                    fn_index_inferred=fn_index_inferred,\n                )\n            except BaseException as error:\n                show_error = app.get_blocks().show_error or isinstance(error, Error)\n                traceback.print_exc()\n                return JSONResponse(\n                    content={\"error\": str(error) if show_error else None},\n                    status_code=500,\n                )\n            root_path = route_utils.get_root_url(\n                request=request, route_path=f\"/api/{api_name}\", root_path=app.root_path\n            )\n            output = add_root_url(output, root_path, None)\n            return output\n        @app.get(\"/queue/data\", dependencies=[Depends(login_check)])\n        async def queue_data(\n            request: fastapi.Request,\n            session_hash: str,\n        ):\n            blocks = app.get_blocks()\n            root_path = route_utils.get_root_url(\n                request=request, route_path=\"/queue/data\", root_path=app.root_path\n            )\n            async def sse_stream(request: fastapi.Request):\n                try:\n                    last_heartbeat = time.perf_counter()\n                    while True:\n                        if await request.is_disconnected():\n                            await blocks._queue.clean_events(session_hash=session_hash)\n                            return\n                        if (\n                            session_hash\n                            not in blocks._queue.pending_messages_per_session\n                        ):\n                            raise HTTPException(\n                                status_code=status.HTTP_404_NOT_FOUND,\n                                detail=\"Session not found.\",\n                            )\n                        heartbeat_rate = 15\n                        check_rate = 0.05\n                        message = None\n                        try:\n                            messages = blocks._queue.pending_messages_per_session[\n                                session_hash\n                            ]\n                            message = messages.get_nowait()\n                        except EmptyQueue:\n                            await asyncio.sleep(check_rate)\n                            if time.perf_counter() - last_heartbeat > heartbeat_rate:\n                                message = {\n                                    \"msg\": ServerMessage.heartbeat,\n                                }\n                                last_heartbeat = time.perf_counter()\n                        if blocks._queue.stopped:\n                            message = {\n                                \"msg\": \"unexpected_error\",\n                                \"message\": \"Server stopped unexpectedly.\",\n                                \"success\": False,\n                            }\n                        if message:\n                            add_root_url(message, root_path, None)\n                            yield f\"data: {json.dumps(message)}\\n\\n\"\n                            if message[\"msg\"] == ServerMessage.process_completed:\n                                blocks._queue.pending_event_ids_session[\n                                    session_hash\n                                ].remove(message[\"event_id\"])\n                                if message[\"msg\"] == ServerMessage.server_stopped or (\n                                    message[\"msg\"] == ServerMessage.process_completed\n                                    and (\n                                        len(\n                                            blocks._queue.pending_event_ids_session[\n                                                session_hash\n                                            ]\n                                        )\n                                        == 0\n                                    )\n                                ):\n                                    return\n                except BaseException as e:\n                    message = {\n                        \"msg\": \"unexpected_error\",\n                        \"success\": False,\n                        \"message\": str(e),\n                    }\n                    yield f\"data: {json.dumps(message)}\\n\\n\"\n                    if isinstance(e, asyncio.CancelledError):\n                        del blocks._queue.pending_messages_per_session[session_hash]\n                        await blocks._queue.clean_events(session_hash=session_hash)\n                    raise e\n            return StreamingResponse(\n                sse_stream(request),\n                media_type=\"text/event-stream\",\n            )\n        @app.post(\"/queue/join\", dependencies=[Depends(login_check)])\n        async def queue_join(\n            body: PredictBody,\n            request: fastapi.Request,\n            username: str = Depends(get_current_user),\n        ):\n            blocks = app.get_blocks()\n            if blocks._queue.server_app is None:\n                blocks._queue.set_server_app(app)\n            if blocks._queue.stopped:\n                raise HTTPException(\n                    status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n                    detail=\"Queue is stopped.\",\n                )\n            success, event_id = await blocks._queue.push(body, request, username)\n            if not success:\n                status_code = (\n                    status.HTTP_503_SERVICE_UNAVAILABLE\n                    if \"Queue is full.\" in event_id\n                    else status.HTTP_400_BAD_REQUEST\n                )\n                raise HTTPException(status_code=status_code, detail=event_id)\n            return {\"event_id\": event_id}\n        @app.post(\"/component_server\", dependencies=[Depends(login_check)])\n        @app.post(\"/component_server/\", dependencies=[Depends(login_check)])\n        def component_server(body: ComponentServerBody):\n            state = app.state_holder[body.session_hash]\n            component_id = body.component_id\n            block: Block\n            if component_id in state:\n                block = state[component_id]\n            else:\n                block = app.get_blocks().blocks[component_id]\n            fn = getattr(block, body.fn_name, None)\n            if fn is None or not getattr(fn, \"_is_server_fn\", False):\n                raise HTTPException(\n                    status_code=status.HTTP_404_NOT_FOUND,\n                    detail=\"Function not found.\",\n                )\n            return fn(body.data)\n        @app.get(\n            \"/queue/status\",\n            dependencies=[Depends(login_check)],\n            response_model=Estimation,\n        )\n        async def get_queue_status():\n            return app.get_blocks()._queue.get_status()\n        @app.get(\"/upload_progress\")\n        def get_upload_progress(upload_id: str, request: fastapi.Request):\n            async def sse_stream(request: fastapi.Request):\n                last_heartbeat = time.perf_counter()\n                is_done = False\n                while True:\n                    if await request.is_disconnected():\n                        file_upload_statuses.stop_tracking(upload_id)\n                        return\n                    if is_done:\n                        file_upload_statuses.stop_tracking(upload_id)\n                        return\n                    heartbeat_rate = 15\n                    check_rate = 0.05\n                    try:\n                        if file_upload_statuses.is_done(upload_id):\n                            message = {\"msg\": \"done\"}\n                            is_done = True\n                        else:\n                            update = file_upload_statuses.pop(upload_id)\n                            message = {\n                                \"msg\": \"update\",\n                                \"orig_name\": update.filename,\n                                \"chunk_size\": update.chunk_size,\n                            }\n                        yield f\"data: {json.dumps(message)}\\n\\n\"\n                    except FileUploadProgressNotTrackedError:\n                        return\n                    except FileUploadProgressNotQueuedError:\n                        await asyncio.sleep(check_rate)\n                        if time.perf_counter() - last_heartbeat > heartbeat_rate:\n                            message = {\"msg\": \"heartbeat\"}\n                            yield f\"data: {json.dumps(message)}\\n\\n\"\n                            last_heartbeat = time.perf_counter()\n            return StreamingResponse(\n                sse_stream(request),\n                media_type=\"text/event-stream\",\n            )\n        @app.post(\"/upload\", dependencies=[Depends(login_check)])\n        async def upload_file(\n            request: fastapi.Request,\n            bg_tasks: BackgroundTasks,\n            upload_id: Optional[str] = None,\n        ):\n            content_type_header = request.headers.get(\"Content-Type\")\n            content_type: bytes\n            content_type, _ = parse_options_header(content_type_header or \"\")\n            if content_type != b\"multipart/form-data\":\n                raise HTTPException(status_code=400, detail=\"Invalid content type.\")\n            try:\n                if upload_id:\n                    file_upload_statuses.track(upload_id)\n                multipart_parser = GradioMultiPartParser(\n                    request.headers,\n                    request.stream(),\n                    max_files=1000,\n                    max_fields=1000,\n                    upload_id=upload_id if upload_id else None,\n                    upload_progress=file_upload_statuses if upload_id else None,\n                )\n                form = await multipart_parser.parse()\n            except MultiPartException as exc:\n                raise HTTPException(status_code=400, detail=exc.message) from exc\n            output_files = []\n            files_to_copy = []\n            locations: list[str] = []\n            for temp_file in form.getlist(\"files\"):\n                if not isinstance(temp_file, GradioUploadFile):\n                    raise TypeError(\"File is not an instance of GradioUploadFile\")\n                if temp_file.filename:\n                    file_name = Path(temp_file.filename).name\n                    name = client_utils.strip_invalid_filename_characters(file_name)\n                else:\n                    name = f\"tmp{secrets.token_hex(5)}\"\n                directory = Path(app.uploaded_file_dir) / temp_file.sha.hexdigest()\n                directory.mkdir(exist_ok=True, parents=True)\n                dest = (directory / name).resolve()\n                temp_file.file.close()\n                try:\n                    os.rename(temp_file.file.name, dest)\n                except OSError:\n                    files_to_copy.append(temp_file.file.name)\n                    locations.append(str(dest))\n                output_files.append(dest)\n            if files_to_copy:\n                bg_tasks.add_task(\n                    move_uploaded_files_to_cache, files_to_copy, locations\n                )\n            return output_files\n        @app.on_event(\"startup\")\n        @app.get(\"/startup-events\")\n        async def startup_events():\n            if not app.startup_events_triggered:\n                app.get_blocks().startup_events()\n                app.startup_events_triggered = True\n                return True\n            return False\n        @app.get(\"/theme.css\", response_class=PlainTextResponse)\n        def theme_css():\n            return PlainTextResponse(app.get_blocks().theme_css, media_type=\"text/css\")\n        @app.get(\"/robots.txt\", response_class=PlainTextResponse)\n        def robots_txt():\n            if app.get_blocks().share:\n                return \"User-agent: *\\nDisallow: /\"\n            else:\n                return \"User-agent: *\\nDisallow: \"\n        return app",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-1727",
        "description": "[{'lang': 'en', 'value': \"A Cross-Site Request Forgery (CSRF) vulnerability in gradio-app/gradio allows attackers to upload multiple large files to a victim's system if they are running Gradio locally. By crafting a malicious HTML page that triggers an unauthorized file upload to the victim's server, an attacker can deplete the system's disk space, potentially leading to a denial of service. This issue affects the file upload functionality as implemented in gradio/routes.py.\"}]",
        "cwe_number": 352
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-128",
      "code": "    def __init__(self, **kwargs):\n        \"\"\"Create the Tree from SVG ``text``.\"\"\"\n        bytestring = kwargs.get('bytestring')\n        file_obj = kwargs.get('file_obj')\n        url = kwargs.get('url')\n        unsafe = kwargs.get('unsafe')\n        parent = kwargs.get('parent')\n        parent_children = kwargs.get('parent_children')\n        tree_cache = kwargs.get('tree_cache')\n        element_id = None\n        self.url_fetcher = kwargs.get('url_fetcher', fetch)\n        if bytestring is not None:\n            self.url = url\n        elif file_obj is not None:\n            bytestring = file_obj.read()\n            self.url = getattr(file_obj, 'name', None)\n            if self.url == '<stdin>':\n                self.url = None\n        elif url is not None:\n            parent_url = parent.url if parent else None\n            parsed_url = parse_url(url, parent_url)\n            if parsed_url.fragment:\n                self.url = urlunparse(parsed_url[:-1] + ('',))\n                element_id = parsed_url.fragment\n            else:\n                self.url = parsed_url.geturl()\n                element_id = None\n            self.url = self.url or None\n        else:\n            raise TypeError(\n                'No input. Use one of bytestring, file_obj or url.')\n        self_is_parent = (\n            (parent and self.url == parent.url) or\n            (url and url.startswith('\n        if self_is_parent:\n            root_parent = parent\n            while root_parent.parent is not None:\n                root_parent = root_parent.parent\n            tree = root_parent.xml_tree\n        else:\n            if not bytestring:\n                bytestring = self.fetch_url(\n                    parse_url(self.url), 'image/svg+xml')\n            if len(bytestring) >= 2 and bytestring[:2] == b'\\x1f\\x8b':\n                bytestring = gzip.decompress(bytestring)\n            tree = ElementTree.fromstring(\n                bytestring, forbid_entities=not unsafe,\n                forbid_external=not unsafe)\n        self.xml_tree = tree\n        root = cssselect2.ElementWrapper.from_xml_root(tree)\n        style = parent.style if parent else css.parse_stylesheets(self, url)\n        if element_id:\n            for element in root.iter_subtree():\n                if element.id == element_id:\n                    root = element\n                    self.xml_tree = element.etree_element\n                    break\n            else:\n                raise TypeError(\n                    'No tag with id=\"{}\" found.'.format(element_id))\n        super().__init__(\n            root, style, self.url_fetcher, parent, parent_children, self.url,\n            unsafe)\n        self.root = True\n        if tree_cache is not None and self.url:\n            tree_cache[(self.url, self.get('id'))] = self\nCASE_SENSITIVE_STYLE_METHODS = {\n    'id': normalize_noop_style_declaration,\n    'class': normalize_noop_style_declaration,\n    'font-family': normalize_noop_style_declaration,\n    'font': normalize_font_style_declaration,\n    'clip-path': normalize_url_style_declaration,\n    'color-profile': normalize_url_style_declaration,\n    'cursor': normalize_url_style_declaration,\n    'fill': normalize_url_style_declaration,\n    'filter': normalize_url_style_declaration,\n    'marker-start': normalize_url_style_declaration,\n    'marker-mid': normalize_url_style_declaration,\n    'marker-end': normalize_url_style_declaration,\n    'mask': normalize_url_style_declaration,\n    'stroke': normalize_url_style_declaration,\n}\ndef main(argv=None, stdout=None, stdin=None):\n    \"\"\"Entry-point of the executable.\"\"\"\n    parser = argparse.ArgumentParser(\n        description='Convert SVG files to other formats')\n    parser.add_argument('input', default='-', help='input filename or URL')\n    parser.add_argument(\n        '-v', '--version', action='version', version=VERSION)\n    parser.add_argument(\n        '-f', '--format', help='output format',\n        choices=sorted([surface.lower() for surface in SURFACES]))\n    parser.add_argument(\n        '-d', '--dpi', default=96, type=float,\n        help='ratio between 1 inch and 1 pixel')\n    parser.add_argument(\n        '-W', '--width', default=None, type=float,\n        help='width of the parent container in pixels')\n    parser.add_argument(\n        '-H', '--height', default=None, type=float,\n        help='height of the parent container in pixels')\n    parser.add_argument(\n        '-s', '--scale', default=1, type=float, help='output scaling factor')\n    parser.add_argument(\n        '-b', '--background', metavar='COLOR', help='output background color')\n    parser.add_argument(\n        '-n', '--negate-colors', action='store_true',\n        help='replace every vector color with its complement')\n    parser.add_argument(\n        '-i', '--invert-images', action='store_true',\n        help='replace every raster pixel with its complementary color')\n    parser.add_argument(\n        '-u', '--unsafe', action='store_true',\n        help='resolve XML entities and allow very large files '\n             '(WARNING: vulnerable to XXE attacks and various DoS)')\n    parser.add_argument(\n        '--output-width', default=None, type=float,\n        help='desired output width in pixels')\n    parser.add_argument(\n        '--output-height', default=None, type=float,\n        help='desired output height in pixels')\n    parser.add_argument('-o', '--output', default='-', help='output filename')\n    options = parser.parse_args(argv)\n    kwargs = {\n        'parent_width': options.width, 'parent_height': options.height,\n        'dpi': options.dpi, 'scale': options.scale, 'unsafe': options.unsafe,\n        'background_color': options.background,\n        'negate_colors': options.negate_colors,\n        'invert_images': options.invert_images,\n        'output_width': options.output_width,\n        'output_height': options.output_height}\n    stdin = stdin or sys.stdin\n    stdout = stdout or sys.stdout\n    kwargs['write_to'] = (\n        stdout.buffer if options.output == '-' else options.output)\n    if options.input == '-':\n        kwargs['file_obj'] = stdin.buffer\n    else:\n        kwargs['url'] = options.input\n    output_format = (\n        options.format or\n        os.path.splitext(options.output)[1].lstrip('.') or\n        'pdf').upper()\n    SURFACES[output_format.upper()].convert(**kwargs)\n    def convert(cls, bytestring=None, *, file_obj=None, url=None, dpi=96,\n                parent_width=None, parent_height=None, scale=1, unsafe=False,\n                background_color=None, negate_colors=False,\n                invert_images=False, write_to=None, output_width=None,\n                output_height=None, **kwargs):\n        \"\"\"Convert an SVG document to the format for this class.\n        Specify the input by passing one of these:\n        :param bytestring: The SVG source as a byte-string.\n        :param file_obj: A file-like object.\n        :param url: A filename.\n        Give some options:\n        :param dpi: The ratio between 1 inch and 1 pixel.\n        :param parent_width: The width of the parent container in pixels.\n        :param parent_height: The height of the parent container in pixels.\n        :param scale: The ouptut scaling factor.\n        :param unsafe: A boolean allowing XML entities and very large files\n                       (WARNING: vulnerable to XXE attacks and various DoS).\n        Specifiy the output with:\n        :param write_to: The filename of file-like object where to write the\n                         output. If None or not provided, return a byte string.\n        Only ``bytestring`` can be passed as a positional argument, other\n        parameters are keyword-only.\n        \"\"\"\n        tree = Tree(\n            bytestring=bytestring, file_obj=file_obj, url=url, unsafe=unsafe,\n            **kwargs)\n        output = write_to or io.BytesIO()\n        instance = cls(\n            tree, output, dpi, None, parent_width, parent_height, scale,\n            output_width, output_height, background_color,\n            map_rgba=negate_color if negate_colors else None,\n            map_image=invert_image if invert_images else None)\n        instance.finish()\n        if write_to is None:\n            return output.getvalue()",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-27586",
        "description": "[{'lang': 'en', 'value': \"CairoSVG is an SVG converter based on Cairo, a 2D graphics library. Prior to version 2.7.0, Cairo can send requests to external hosts when processing SVG files. A malicious actor could send a specially crafted SVG file that allows them to perform a server-side request forgery or denial of service. Version 2.7.0 disables CairoSVG's ability to access other files online by default.\"}]",
        "cwe_number": 918
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-129",
      "code": "  def testWrongOpBmp(self, img_format, filename):\n    base_folder = \"tensorflow/core/lib\"\n    base_path = os.path.join(base_folder, img_format.lower(), \"testdata\")\n    err_msg = \"Trying to decode \" + img_format + \" format using DecodeBmp op\"\n    with self.assertRaisesRegex(\n        (ValueError, errors.InvalidArgumentError), err_msg):\n      img_bytes = io_ops.read_file(os.path.join(base_path, filename))\n      img = image_ops.decode_bmp(img_bytes)\n      self.evaluate(img)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-41197",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. In affected versions TensorFlow allows tensor to have a large number of dimensions and each dimension can be as large as desired. However, the total number of elements in a tensor must fit within an `int64_t`. If an overflow occurs, `MultiplyWithoutOverflow` would return a negative result. In the majority of TensorFlow codebase this then results in a `CHECK`-failure. Newer constructs exist which return a `Status` instead of crashing the binary. This is similar to CVE-2021-29584. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.'}]",
        "cwe_number": 190
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-130",
      "code": "    def get_events(self, uuid):\n        \"\"\"\n        Lists occured events, may be affected to changes in future.\n        :param uuid:\n        :return: list of `Events`\n        \"\"\"\n        events = self.pyload.event_manager.get_events(uuid)\n        new_events = []\n        def conv_dest(d):\n            return (Destination.QUEUE if d == \"queue\" else Destination.COLLECTOR).value\n        for e in events:\n            event = EventInfo()\n            event.eventname = e[0]\n            if e[0] in (\"update\", \"remove\", \"insert\"):\n                event.id = e[3]\n                event.type = (\n                    ElementType.PACKAGE if e[2] == \"pack\" else ElementType.FILE\n                ).value\n                event.destination = conv_dest(e[1])\n            elif e[0] == \"order\":\n                if e[1]:\n                    event.id = e[1]\n                    event.type = (\n                        ElementType.PACKAGE if e[2] == \"pack\" else ElementType.FILE\n                    )\n                    event.destination = conv_dest(e[3])\n            elif e[0] == \"reload\":\n                event.destination = conv_dest(e[1])\n            new_events.append(event)\n        return new_events\n    def is_authorized(self, func, userdata):\n        \"\"\"\n        checks if the user is authorized for specific method.\n        :param func: function name\n        :param userdata: dictionary of user data\n        :return: boolean\n        \"\"\"\n        if userdata[\"role\"] == Role.ADMIN:\n            return True\n        elif func in perm_map and has_permission(\n            userdata[\"permission\"], perm_map[func]\n        ):\n            return True\n        else:\n            return False\n    def list_users(self):\n        self.c.execute(\"SELECT name FROM users\")\n        users = []\n        for row in self.c:\n            users.append(row[0])\n        return users\ndef is_authenticated(session=flask.session):\n    return session.get(\"name\") and session.get(\n        \"authenticated\"\n    )",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-0227",
        "description": "[{'lang': 'en', 'value': 'Insufficient Session Expiration in GitHub repository pyload/pyload prior to 0.5.0b3.dev36.'}]",
        "cwe_number": 613
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-131",
      "code": "    def _rule_dict_last_step(self, context, to_port=None, from_port=None,\n                                  ip_protocol=None, cidr_ip=None, user_id=None,\n                                  source_security_group_name=None,\n                                  source_security_group_owner_id=None):\n        values = {}\n        if source_security_group_name:\n            source_project_id = self._get_source_project_id(context,\n                source_security_group_owner_id)\n            source_security_group = db.security_group_get_by_name(\n                    context.elevated(),\n                    source_project_id,\n                    source_security_group_name)\n            notfound = exception.SecurityGroupNotFound\n            if not source_security_group:\n                raise notfound(security_group_id=source_security_group_name)\n            values['group_id'] = source_security_group['id']\n        elif cidr_ip:\n            cidr_ip = urllib.unquote(cidr_ip).decode()\n            if not utils.is_valid_cidr(cidr_ip):\n                raise exception.EC2APIError(_(\"Invalid CIDR\"))\n            values['cidr'] = cidr_ip\n        else:\n            values['cidr'] = '0.0.0.0/0'\n        if source_security_group_name:\n            ip_proto_upper = ip_protocol.upper() if ip_protocol else ''\n            if (ip_proto_upper == 'ICMP' and\n                from_port is None and to_port is None):\n                from_port = -1\n                to_port = -1\n            elif (ip_proto_upper in ['TCP', 'UDP'] and from_port is None\n                  and to_port is None):\n                from_port = 1\n                to_port = 65535\n        if ip_protocol and from_port is not None and to_port is not None:\n            ip_protocol = str(ip_protocol)\n            try:\n                from_port = int(from_port)\n                to_port = int(to_port)\n            except ValueError:\n                if ip_protocol.upper() == 'ICMP':\n                    raise exception.InvalidInput(reason=\"Type and\"\n                         \" Code must be integers for ICMP protocol type\")\n                else:\n                    raise exception.InvalidInput(reason=\"To and From ports \"\n                          \"must be integers\")\n            if ip_protocol.upper() not in ['TCP', 'UDP', 'ICMP']:\n                raise exception.InvalidIpProtocol(protocol=ip_protocol)\n            if (ip_protocol.upper() in ['TCP', 'UDP'] and\n                (from_port > to_port)):\n                raise exception.InvalidPortRange(from_port=from_port,\n                      to_port=to_port, msg=\"Former value cannot\"\n                                            \" be greater than the later\")\n            if (ip_protocol.upper() in ['TCP', 'UDP'] and\n                (from_port < 1 or to_port > 65535)):\n                raise exception.InvalidPortRange(from_port=from_port,\n                      to_port=to_port, msg=\"Valid TCP ports should\"\n                                           \" be between 1-65535\")\n            if (ip_protocol.upper() == \"ICMP\" and\n                (from_port < -1 or from_port > 255 or\n                to_port < -1 or to_port > 255)):\n                raise exception.InvalidPortRange(from_port=from_port,\n                      to_port=to_port, msg=\"For ICMP, the\"\n                                           \" type:code must be valid\")\n            values['protocol'] = ip_protocol\n            values['from_port'] = from_port\n            values['to_port'] = to_port\n        else:\n            if 'cidr' in values:\n                return None\n        return values\n    def _rule_args_to_dict(self, context, to_port=None, from_port=None,\n                                  parent_group_id=None, ip_protocol=None,\n                                  cidr=None, group_id=None):\n        values = {}\n        if group_id is not None:\n            try:\n                parent_group_id = int(parent_group_id)\n                group_id = int(group_id)\n            except ValueError:\n                msg = _(\"Parent or group id is not integer\")\n                raise exception.InvalidInput(reason=msg)\n            values['group_id'] = group_id\n            db.security_group_get(context, group_id)\n        elif cidr:\n            try:\n                cidr = urllib.unquote(cidr).decode()\n            except Exception:\n                raise exception.InvalidCidr(cidr=cidr)\n            if not utils.is_valid_cidr(cidr):\n                raise exception.InvalidCidr(cidr=cidr)\n            values['cidr'] = cidr\n        else:\n            values['cidr'] = '0.0.0.0/0'\n        if group_id:\n            ip_proto_upper = ip_protocol.upper() if ip_protocol else ''\n            if (ip_proto_upper == 'ICMP' and\n                from_port is None and to_port is None):\n                from_port = -1\n                to_port = -1\n            elif (ip_proto_upper in ['TCP', 'UDP'] and from_port is None\n                  and to_port is None):\n                from_port = 1\n                to_port = 65535\n        if ip_protocol and from_port is not None and to_port is not None:\n            ip_protocol = str(ip_protocol)\n            try:\n                from_port = int(from_port)\n                to_port = int(to_port)\n            except ValueError:\n                if ip_protocol.upper() == 'ICMP':\n                    raise exception.InvalidInput(reason=\"Type and\"\n                         \" Code must be integers for ICMP protocol type\")\n                else:\n                    raise exception.InvalidInput(reason=\"To and From ports \"\n                          \"must be integers\")\n            if ip_protocol.upper() not in ['TCP', 'UDP', 'ICMP']:\n                raise exception.InvalidIpProtocol(protocol=ip_protocol)\n            if (ip_protocol.upper() in ['TCP', 'UDP'] and\n                from_port > to_port):\n                raise exception.InvalidPortRange(from_port=from_port,\n                      to_port=to_port, msg=\"Former value cannot\"\n                                            \" be greater than the later\")\n            if (ip_protocol.upper() in ['TCP', 'UDP'] and\n                (from_port < 1 or to_port > 65535)):\n                raise exception.InvalidPortRange(from_port=from_port,\n                      to_port=to_port, msg=\"Valid TCP ports should\"\n                                           \" be between 1-65535\")\n            if (ip_protocol.upper() == \"ICMP\" and\n                (from_port < -1 or from_port > 255 or\n                to_port < -1 or to_port > 255)):\n                raise exception.InvalidPortRange(from_port=from_port,\n                      to_port=to_port, msg=\"For ICMP, the\"\n                                           \" type:code must be valid\")\n            values['protocol'] = ip_protocol\n            values['from_port'] = from_port\n            values['to_port'] = to_port\n        else:\n            if 'cidr' in values:\n                return None\n        return values\n    def instance_rules(self, instance, network_info):\n        network_info = self._handle_network_info_model(network_info)\n        ctxt = context.get_admin_context()\n        ipv4_rules = []\n        ipv6_rules = []\n        self._do_basic_rules(ipv4_rules, ipv6_rules, network_info)\n        self._do_dhcp_rules(ipv4_rules, network_info)\n        if FLAGS.allow_same_net_traffic:\n            self._do_project_network_rules(ipv4_rules, ipv6_rules,\n                                           network_info)\n        if FLAGS.use_ipv6:\n            self._do_ra_rules(ipv6_rules, network_info)\n        security_groups = db.security_group_get_by_instance(ctxt,\n                                                            instance['id'])\n        for security_group in security_groups:\n            rules = db.security_group_rule_get_by_security_group(ctxt,\n                                                          security_group['id'])\n            for rule in rules:\n                LOG.debug(_('Adding security group rule: %r'), rule,\n                          instance=instance)\n                if not rule.cidr:\n                    version = 4\n                else:\n                    version = netutils.get_ip_version(rule.cidr)\n                if version == 4:\n                    fw_rules = ipv4_rules\n                else:\n                    fw_rules = ipv6_rules\n                protocol = rule.protocol\n                if version == 6 and rule.protocol == 'icmp':\n                    protocol = 'icmpv6'\n                args = ['-j ACCEPT']\n                if protocol:\n                    args += ['-p', protocol]\n                if protocol in ['udp', 'tcp']:\n                    args += self._build_tcp_udp_rule(rule, version)\n                elif protocol == 'icmp':\n                    args += self._build_icmp_rule(rule, version)\n                if rule.cidr:\n                    LOG.debug('Using cidr %r', rule.cidr, instance=instance)\n                    args += ['-s', rule.cidr]\n                    fw_rules += [' '.join(args)]\n                else:\n                    if rule['grantee_group']:\n                        import nova.network\n                        nw_api = nova.network.API()\n                        for instance in rule['grantee_group']['instances']:\n                            nw_info = nw_api.get_instance_nw_info(ctxt,\n                                                                  instance)\n                            ips = [ip['address']\n                                for ip in nw_info.fixed_ips()\n                                    if ip['version'] == version]\n                            LOG.debug('ips: %r', ips, instance=instance)\n                            for ip in ips:\n                                subrule = args + ['-s %s' % ip]\n                                fw_rules += [' '.join(subrule)]\n                LOG.debug('Using fw_rules: %r', fw_rules, instance=instance)\n        ipv4_rules += ['-j $sg-fallback']\n        ipv6_rules += ['-j $sg-fallback']\n        return ipv4_rules, ipv6_rules",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2012-2654",
        "description": "[{'lang': 'en', 'value': 'The (1) EC2 and (2) OS APIs in OpenStack Compute (Nova) Folsom (2012.2), Essex (2012.1), and Diablo (2011.3) do not properly check the protocol when security groups are created and the network protocol is not specified entirely in lowercase, which allows remote attackers to bypass intended access restrictions.'}]",
        "cwe_number": 20
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-132",
      "code": "  def _ResizeImageWithCropOrPad(self, x, target_height, target_width,\n                                use_tensor_inputs):\n    if use_tensor_inputs:\n      target_height = ops.convert_to_tensor(target_height)\n      target_width = ops.convert_to_tensor(target_width)\n      x_tensor = ops.convert_to_tensor(x)\n    else:\n      x_tensor = x\n    @def_function.function\n    def resize_crop_or_pad(*args):\n      return image_ops.resize_image_with_crop_or_pad(*args)\n    with self.cached_session():\n      return self.evaluate(\n          resize_crop_or_pad(x_tensor, target_height, target_width))",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-41907",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. When `tf.raw_ops.ResizeNearestNeighborGrad` is given a large `size` input, it overflows. We have patched the issue in GitHub commit 00c821af032ba9e5f5fa3fe14690c8d28a657624. The fix will be included in TensorFlow 2.11. We will also cherrypick this commit on TensorFlow 2.10.1, 2.9.3, and TensorFlow 2.8.4, as these are also affected and still in supported range.'}]",
        "cwe_number": 131
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-133",
      "code": "def get_context(context):\n\tredirect_to = frappe.local.request.args.get(\"redirect-to\")\n\tif frappe.session.user != \"Guest\":\n\t\tif not redirect_to:\n\t\t\tif frappe.session.data.user_type == \"Website User\":\n\t\t\t\tredirect_to = get_home_page()\n\t\t\telse:\n\t\t\t\tredirect_to = \"/app\"\n\t\tif redirect_to != \"login\":\n\t\t\tfrappe.local.flags.redirect_location = redirect_to\n\t\t\traise frappe.Redirect\n\tcontext.no_header = True\n\tcontext.for_test = \"login.html\"\n\tcontext[\"title\"] = \"Login\"\n\tcontext[\"hide_login\"] = True\n\tcontext[\"provider_logins\"] = []\n\tcontext[\"disable_signup\"] = cint(frappe.get_website_settings(\"disable_signup\"))\n\tcontext[\"show_footer_on_login\"] = cint(frappe.get_website_settings(\"show_footer_on_login\"))\n\tcontext[\"disable_user_pass_login\"] = cint(frappe.get_system_settings(\"disable_user_pass_login\"))\n\tcontext[\"logo\"] = frappe.get_website_settings(\"app_logo\") or frappe.get_hooks(\"app_logo_url\")[-1]\n\tcontext[\"app_name\"] = (\n\t\tfrappe.get_website_settings(\"app_name\") or frappe.get_system_settings(\"app_name\") or _(\"Frappe\")\n\t)\n\tsignup_form_template = frappe.get_hooks(\"signup_form_template\")\n\tif signup_form_template and len(signup_form_template):\n\t\tpath = signup_form_template[-1]\n\t\tif not guess_is_path(path):\n\t\t\tpath = frappe.get_attr(signup_form_template[-1])()\n\telse:\n\t\tpath = \"frappe/templates/signup.html\"\n\tif path:\n\t\tcontext[\"signup_form_template\"] = frappe.get_template(path).render()\n\tproviders = frappe.get_all(\n\t\t\"Social Login Key\",\n\t\tfilters={\"enable_social_login\": 1},\n\t\tfields=[\"name\", \"client_id\", \"base_url\", \"provider_name\", \"icon\"],\n\t\torder_by=\"name\",\n\t)\n\tfor provider in providers:\n\t\tclient_secret = get_decrypted_password(\"Social Login Key\", provider.name, \"client_secret\")\n\t\tif not client_secret:\n\t\t\tcontinue\n\t\ticon = None\n\t\tif provider.icon:\n\t\t\tif provider.provider_name == \"Custom\":\n\t\t\t\ticon = get_icon_html(provider.icon, small=True)\n\t\t\telse:\n\t\t\t\ticon = f\"<img src={escape_html(provider.icon)!r} alt={escape_html(provider.provider_name)!r}>\"\n\t\tif provider.client_id and provider.base_url and get_oauth_keys(provider.name):\n\t\t\tcontext.provider_logins.append(\n\t\t\t\t{\n\t\t\t\t\t\"name\": provider.name,\n\t\t\t\t\t\"provider_name\": provider.provider_name,\n\t\t\t\t\t\"auth_url\": get_oauth2_authorize_url(provider.name, redirect_to),\n\t\t\t\t\t\"icon\": icon,\n\t\t\t\t}\n\t\t\t)\n\t\t\tcontext[\"social_login\"] = True\n\tif cint(frappe.db.get_value(\"LDAP Settings\", \"LDAP Settings\", \"enabled\")):\n\t\tfrom frappe.integrations.doctype.ldap_settings.ldap_settings import LDAPSettings\n\t\tcontext[\"ldap_settings\"] = LDAPSettings.get_ldap_client_settings()\n\tlogin_label = [_(\"Email\")]\n\tif frappe.utils.cint(frappe.get_system_settings(\"allow_login_using_mobile_number\")):\n\t\tlogin_label.append(_(\"Mobile\"))\n\tif frappe.utils.cint(frappe.get_system_settings(\"allow_login_using_user_name\")):\n\t\tlogin_label.append(_(\"Username\"))\n\tcontext[\"login_label\"] = f\" {_('or')} \".join(login_label)\n\tcontext[\"login_with_email_link\"] = frappe.get_system_settings(\"login_with_email_link\")\n\treturn context\ndef login_via_token(login_token: str):\n\tsid = frappe.cache.get_value(f\"login_token:{login_token}\", expires=True)\n\tif not sid:\n\t\tfrappe.respond_as_web_page(_(\"Invalid Request\"), _(\"Invalid Login Token\"), http_status_code=417)\n\t\treturn\n\tfrappe.local.form_dict.sid = sid\n\tfrappe.local.login_manager = LoginManager()\n\tredirect_post_login(\n\t\tdesk_user=frappe.db.get_value(\"User\", frappe.session.user, \"user_type\") == \"System User\"\n\t)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-34074",
        "description": "[{'lang': 'en', 'value': 'Frappe is a full-stack web application framework. Prior to 15.26.0 and 14.74.0, the login page accepts redirect argument and it allowed redirect to untrusted external URls. This behaviour can be used by malicious actors for phishing. This vulnerability is fixed in 15.26.0 and 14.74.0.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-134",
      "code": "    def __init__(self, **kwargs):\n        \"\"\"Initialize HTMLSerializer.\n        Keyword options (default given first unless specified) include:\n        inject_meta_charset=True|False\n          Whether it insert a meta element to define the character set of the\n          document.\n        quote_attr_values=True|False\n          Whether to quote attribute values that don't require quoting\n          per HTML5 parsing rules.\n        quote_char=u'\"'|u\"'\"\n          Use given quote character for attribute quoting. Default is to\n          use double quote unless attribute value contains a double quote,\n          in which case single quotes are used instead.\n        escape_lt_in_attrs=False|True\n          Whether to escape < in attribute values.\n        escape_rcdata=False|True\n          Whether to escape characters that need to be escaped within normal\n          elements within rcdata elements such as style.\n        resolve_entities=True|False\n          Whether to resolve named character entities that appear in the\n          source tree. The XML predefined entities &lt; &gt; &amp; &quot; &apos;\n          are unaffected by this setting.\n        strip_whitespace=False|True\n          Whether to remove semantically meaningless whitespace. (This\n          compresses all whitespace to a single space except within pre.)\n        minimize_boolean_attributes=True|False\n          Shortens boolean attributes to give just the attribute value,\n          for example <input disabled=\"disabled\"> becomes <input disabled>.\n        use_trailing_solidus=False|True\n          Includes a close-tag slash at the end of the start tag of void\n          elements (empty elements whose end tag is forbidden). E.g. <hr/>.\n        space_before_trailing_solidus=True|False\n          Places a space immediately before the closing slash in a tag\n          using a trailing solidus. E.g. <hr />. Requires use_trailing_solidus.\n        sanitize=False|True\n          Strip all unsafe or unknown constructs from output.\n          See `html5lib user documentation`_\n        omit_optional_tags=True|False\n          Omit start/end tags that are optional.\n        alphabetical_attributes=False|True\n          Reorder attributes to be in alphabetical order.\n        .. _html5lib user documentation: http://code.google.com/p/html5lib/wiki/UserDocumentation\n        \"\"\"\n        if 'quote_char' in kwargs:\n            self.use_best_quote_char = False\n        for attr in self.options:\n            setattr(self, attr, kwargs.get(attr, getattr(self, attr)))\n        self.errors = []\n        self.strict = False\n    def encode(self, string):\n        assert(isinstance(string, text_type))\n        if self.encoding:\n            return string.encode(self.encoding, unicode_encode_errors)\n        else:\n            return string\n    def encodeStrict(self, string):\n        assert(isinstance(string, text_type))\n        if self.encoding:\n            return string.encode(self.encoding, \"strict\")\n        else:\n            return string\n    def serialize(self, treewalker, encoding=None):\n        self.encoding = encoding\n        in_cdata = False\n        self.errors = []\n        if encoding and self.inject_meta_charset:\n            from ..filters.inject_meta_charset import Filter\n            treewalker = Filter(treewalker, encoding)\n        if self.strip_whitespace:\n            from ..filters.whitespace import Filter\n            treewalker = Filter(treewalker)\n        if self.sanitize:\n            from ..filters.sanitizer import Filter\n            treewalker = Filter(treewalker)\n        if self.omit_optional_tags:\n            from ..filters.optionaltags import Filter\n            treewalker = Filter(treewalker)\n        if self.alphabetical_attributes:\n            from ..filters.alphabeticalattributes import Filter\n            treewalker = Filter(treewalker)\n        for token in treewalker:\n            type = token[\"type\"]\n            if type == \"Doctype\":\n                doctype = \"<!DOCTYPE %s\" % token[\"name\"]\n                if token[\"publicId\"]:\n                    doctype += ' PUBLIC \"%s\"' % token[\"publicId\"]\n                elif token[\"systemId\"]:\n                    doctype += \" SYSTEM\"\n                if token[\"systemId\"]:\n                    if token[\"systemId\"].find('\"') >= 0:\n                        if token[\"systemId\"].find(\"'\") >= 0:\n                            self.serializeError(\"System identifer contains both single and double quote characters\")\n                        quote_char = \"'\"\n                    else:\n                        quote_char = '\"'\n                    doctype += \" %s%s%s\" % (quote_char, token[\"systemId\"], quote_char)\n                doctype += \">\"\n                yield self.encodeStrict(doctype)\n            elif type in (\"Characters\", \"SpaceCharacters\"):\n                if type == \"SpaceCharacters\" or in_cdata:\n                    if in_cdata and token[\"data\"].find(\"</\") >= 0:\n                        self.serializeError(\"Unexpected </ in CDATA\")\n                    yield self.encode(token[\"data\"])\n                else:\n                    yield self.encode(escape(token[\"data\"]))\n            elif type in (\"StartTag\", \"EmptyTag\"):\n                name = token[\"name\"]\n                yield self.encodeStrict(\"<%s\" % name)\n                if name in rcdataElements and not self.escape_rcdata:\n                    in_cdata = True\n                elif in_cdata:\n                    self.serializeError(\"Unexpected child element of a CDATA element\")\n                for (attr_namespace, attr_name), attr_value in token[\"data\"].items():\n                    k = attr_name\n                    v = attr_value\n                    yield self.encodeStrict(' ')\n                    yield self.encodeStrict(k)\n                    if not self.minimize_boolean_attributes or \\\n                        (k not in booleanAttributes.get(name, tuple()) and\n                         k not in booleanAttributes.get(\"\", tuple())):\n                        yield self.encodeStrict(\"=\")\n                        if self.quote_attr_values:\n                            quote_attr = True\n                        else:\n                            quote_attr = len(v) == 0 or quoteAttributeSpec.search(v)\n                        v = v.replace(\"&\", \"&amp;\")\n                        if self.escape_lt_in_attrs:\n                            v = v.replace(\"<\", \"&lt;\")\n                        if quote_attr:\n                            quote_char = self.quote_char\n                            if self.use_best_quote_char:\n                                if \"'\" in v and '\"' not in v:\n                                    quote_char = '\"'\n                                elif '\"' in v and \"'\" not in v:\n                                    quote_char = \"'\"\n                            if quote_char == \"'\":\n                                v = v.replace(\"'\", \"&\n                            else:\n                                v = v.replace('\"', \"&quot;\")\n                            yield self.encodeStrict(quote_char)\n                            yield self.encode(v)\n                            yield self.encodeStrict(quote_char)\n                        else:\n                            yield self.encode(v)\n                if name in voidElements and self.use_trailing_solidus:\n                    if self.space_before_trailing_solidus:\n                        yield self.encodeStrict(\" /\")\n                    else:\n                        yield self.encodeStrict(\"/\")\n                yield self.encode(\">\")\n            elif type == \"EndTag\":\n                name = token[\"name\"]\n                if name in rcdataElements:\n                    in_cdata = False\n                elif in_cdata:\n                    self.serializeError(\"Unexpected child element of a CDATA element\")\n                yield self.encodeStrict(\"</%s>\" % name)\n            elif type == \"Comment\":\n                data = token[\"data\"]\n                if data.find(\"--\") >= 0:\n                    self.serializeError(\"Comment contains --\")\n                yield self.encodeStrict(\"<!--%s-->\" % token[\"data\"])\n            elif type == \"Entity\":\n                name = token[\"name\"]\n                key = name + \";\"\n                if key not in entities:\n                    self.serializeError(\"Entity %s not recognized\" % name)\n                if self.resolve_entities and key not in xmlEntities:\n                    data = entities[key]\n                else:\n                    data = \"&%s;\" % name\n                yield self.encodeStrict(data)\n            else:\n                self.serializeError(token[\"data\"])\n    def render(self, treewalker, encoding=None):\n        if encoding:\n            return b\"\".join(list(self.serialize(treewalker, encoding)))\n        else:\n            return \"\".join(list(self.serialize(treewalker)))\n    def serializeError(self, data=\"XXX ERROR MESSAGE NEEDED\"):\n        self.errors.append(data)\n        if self.strict:\n            raise SerializeError",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2016-9910",
        "description": "[{'lang': 'en', 'value': 'The serializer in html5lib before 0.99999999 might allow remote attackers to conduct cross-site scripting (XSS) attacks by leveraging mishandling of special characters in attribute values, a different vulnerability than CVE-2016-9909.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-135",
      "code": "    def authenticate(self, context, auth=None):\n        \"\"\"Authenticate credentials and return a token.\n        Accept auth as a dict that looks like::\n            {\n                \"auth\":{\n                    \"passwordCredentials\":{\n                        \"username\":\"test_user\",\n                        \"password\":\"mypass\"\n                    },\n                    \"tenantName\":\"customer-x\"\n                }\n            }\n        In this case, tenant is optional, if not provided the token will be\n        considered \"unscoped\" and can later be used to get a scoped token.\n        Alternatively, this call accepts auth with only a token and tenant\n        that will return a token that is scoped to that tenant.\n        \"\"\"\n        token_id = uuid.uuid4().hex\n        if 'passwordCredentials' in auth:\n            username = auth['passwordCredentials'].get('username', '')\n            password = auth['passwordCredentials'].get('password', '')\n            tenant_name = auth.get('tenantName', None)\n            user_id = auth['passwordCredentials'].get('userId', None)\n            if username:\n                user_ref = self.identity_api.get_user_by_name(\n                        context=context, user_name=username)\n                if user_ref:\n                    user_id = user_ref['id']\n            tenant_id = auth.get('tenantId', None)\n            if tenant_name:\n                tenant_ref = self.identity_api.get_tenant_by_name(\n                        context=context, tenant_name=tenant_name)\n                if tenant_ref:\n                    tenant_id = tenant_ref['id']\n            try:\n                auth_info = self.identity_api.authenticate(context=context,\n                                                           user_id=user_id,\n                                                           password=password,\n                                                           tenant_id=tenant_id)\n                (user_ref, tenant_ref, metadata_ref) = auth_info\n                if not user_ref.get('enabled', True):\n                    raise exception.Forbidden(message='User has been disabled')\n            except AssertionError as e:\n                raise exception.Unauthorized(e.message)\n            token_ref = self.token_api.create_token(\n                    context, token_id, dict(id=token_id,\n                                            user=user_ref,\n                                            tenant=tenant_ref,\n                                            metadata=metadata_ref))\n            if tenant_ref:\n                catalog_ref = self.catalog_api.get_catalog(\n                        context=context,\n                        user_id=user_ref['id'],\n                        tenant_id=tenant_ref['id'],\n                        metadata=metadata_ref)\n            else:\n                catalog_ref = {}\n        elif 'token' in auth:\n            token = auth['token'].get('id', None)\n            tenant_name = auth.get('tenantName')\n            if tenant_name:\n                tenant_ref = self.identity_api.get_tenant_by_name(\n                        context=context, tenant_name=tenant_name)\n                tenant_id = tenant_ref['id']\n            else:\n                tenant_id = auth.get('tenantId', None)\n            try:\n                old_token_ref = self.token_api.get_token(context=context,\n                                                         token_id=token)\n            except exception.NotFound:\n                raise exception.Unauthorized()\n            user_ref = old_token_ref['user']\n            tenants = self.identity_api.get_tenants_for_user(context,\n                                                             user_ref['id'])\n            if tenant_id:\n                assert tenant_id in tenants\n            tenant_ref = self.identity_api.get_tenant(context=context,\n                                                      tenant_id=tenant_id)\n            if tenant_ref:\n                metadata_ref = self.identity_api.get_metadata(\n                        context=context,\n                        user_id=user_ref['id'],\n                        tenant_id=tenant_ref['id'])\n                catalog_ref = self.catalog_api.get_catalog(\n                        context=context,\n                        user_id=user_ref['id'],\n                        tenant_id=tenant_ref['id'],\n                        metadata=metadata_ref)\n            else:\n                metadata_ref = {}\n                catalog_ref = {}\n            token_ref = self.token_api.create_token(\n                    context, token_id, dict(id=token_id,\n                                            user=user_ref,\n                                            tenant=tenant_ref,\n                                            metadata=metadata_ref))\n        roles_ref = []\n        for role_id in metadata_ref.get('roles', []):\n            roles_ref.append(self.identity_api.get_role(context, role_id))\n        logging.debug('TOKEN_REF %s', token_ref)\n        return self._format_authenticate(token_ref, roles_ref, catalog_ref)\n    def _get_token_ref(self, context, token_id, belongs_to=None):\n        \"\"\"Returns a token if a valid one exists.\n        Optionally, limited to a token owned by a specific tenant.\n        \"\"\"\n        self.assert_admin(context)\n        token_ref = self.token_api.get_token(context=context,\n                                             token_id=token_id)\n        if belongs_to:\n            assert token_ref['tenant']['id'] == belongs_to\n        return token_ref\n    def update_user(self, context, user_id, user):\n        self.assert_admin(context)\n        if self.identity_api.get_user(context, user_id) is None:\n            raise exception.UserNotFound(user_id=user_id)\n        user_ref = self.identity_api.update_user(context, user_id, user)\n        return {'user': user_ref}\n    def delete_user(self, context, user_id):\n        self.assert_admin(context)\n        if self.identity_api.get_user(context, user_id) is None:\n            raise exception.UserNotFound(user_id=user_id)\n        self.identity_api.delete_user(context, user_id)\n    def set_user_enabled(self, context, user_id, user):\n        return self.update_user(context, user_id, user)\n    def update_user_tenant(self, context, user_id, user):\n        \"\"\"Update the default tenant.\"\"\"\n        tenant_id = user.get('tenantId')\n        self.identity_api.add_user_to_tenant(context, tenant_id, user_id)\n        return self.update_user(context, user_id, user)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2012-3426",
        "description": "[{'lang': 'en', 'value': 'OpenStack Keystone before 2012.1.1, as used in OpenStack Folsom before Folsom-1 and OpenStack Essex, does not properly implement token expiration, which allows remote authenticated users to bypass intended authorization restrictions by (1) creating new tokens through token chaining, (2) leveraging possession of a token for a disabled user account, or (3) leveraging possession of a token for an account with a changed password.'}]",
        "cwe_number": 264
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-136",
      "code": "    def as_const(self, eval_ctx=None):\n        eval_ctx = get_eval_context(self, eval_ctx)\n        if eval_ctx.volatile:\n            raise Impossible()\n        obj = self.node.as_const(eval_ctx)\n        args = [x.as_const(eval_ctx) for x in self.args]\n        if isinstance(obj, _context_function_types):\n            if getattr(obj, 'contextfunction', False):\n                raise Impossible()\n            elif getattr(obj, 'evalcontextfunction', False):\n                args.insert(0, eval_ctx)\n            elif getattr(obj, 'environmentfunction', False):\n                args.insert(0, self.environment)\n        kwargs = dict(x.as_const(eval_ctx) for x in self.kwargs)\n        if self.dyn_args is not None:\n            try:\n                args.extend(self.dyn_args.as_const(eval_ctx))\n            except Exception:\n                raise Impossible()\n        if self.dyn_kwargs is not None:\n            try:\n                kwargs.update(self.dyn_kwargs.as_const(eval_ctx))\n            except Exception:\n                raise Impossible()\n        try:\n            return obj(*args, **kwargs)\n        except Exception:\n            raise Impossible()",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2016-10745",
        "description": "[{'lang': 'en', 'value': 'In Pallets Jinja before 2.8.1, str.format allows a sandbox escape.'}]",
        "cwe_number": 134
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-137",
      "code": "def _ssl_wrap_socket(\n    sock, key_file, cert_file, disable_validation, ca_certs, ssl_version, hostname, key_password\n):\n    if disable_validation:\n        cert_reqs = ssl.CERT_NONE\n    else:\n        cert_reqs = ssl.CERT_REQUIRED\n    if ssl_version is None:\n        ssl_version = ssl.PROTOCOL_SSLv23\n    if hasattr(ssl, \"SSLContext\"):\n        context = ssl.SSLContext(ssl_version)\n        context.verify_mode = cert_reqs\n        context.check_hostname = cert_reqs != ssl.CERT_NONE\n        if cert_file:\n            if key_password:\n                context.load_cert_chain(cert_file, key_file, key_password)\n            else:\n                context.load_cert_chain(cert_file, key_file)\n        if ca_certs:\n            context.load_verify_locations(ca_certs)\n        return context.wrap_socket(sock, server_hostname=hostname)\n    else:\n        if key_password:\n            raise NotSupportedOnThisPlatform(\"Certificate with password is not supported.\")\n        return ssl.wrap_socket(\n            sock,\n            keyfile=key_file,\n            certfile=cert_file,\n            cert_reqs=cert_reqs,\n            ca_certs=ca_certs,\n            ssl_version=ssl_version,\n        )\nUSE_WWW_AUTH_STRICT_PARSING = 0\nWWW_AUTH_STRICT = re.compile(\n    r\"^(?:\\s*(?:,\\s*)?([^\\0-\\x1f\\x7f-\\xff()<>@,;:\\\\\\\"/[\\]?={} \\t]+)\\s*=\\s*\\\"?((?<=\\\")(?:[^\\0-\\x08\\x0A-\\x1f\\x7f-\\xff\\\\\\\"]|\\\\[\\0-\\x7f])*?(?=\\\")|(?<!\\\")[^\\0-\\x1f\\x7f-\\xff()<>@,;:\\\\\\\"/[\\]?={} \\t]+(?!\\\"))\\\"?)(.*)$\"\n)\nWWW_AUTH_RELAXED = re.compile(\n    r\"^(?:\\s*(?:,\\s*)?([^ \\t\\r\\n=]+)\\s*=\\s*\\\"?((?<=\\\")(?:[^\\\\\\\"]|\\\\.)*?(?=\\\")|(?<!\\\")[^ \\t\\r\\n,]+(?!\\\"))\\\"?)(.*)$\"\n)\nUNQUOTE_PAIRS = re.compile(r\"\\\\(.)\")\ndef _parse_www_authenticate(headers, headername=\"www-authenticate\"):\n    \"\"\"Returns a dictionary of dictionaries, one dict\n    per auth_scheme.\"\"\"\n    retval = {}\n    if headername in headers:\n        try:\n            authenticate = headers[headername].strip()\n            www_auth = (\n                USE_WWW_AUTH_STRICT_PARSING and WWW_AUTH_STRICT or WWW_AUTH_RELAXED\n            )\n            while authenticate:\n                if headername == \"authentication-info\":\n                    (auth_scheme, the_rest) = (\"digest\", authenticate)\n                else:\n                    (auth_scheme, the_rest) = authenticate.split(\" \", 1)\n                match = www_auth.search(the_rest)\n                auth_params = {}\n                while match:\n                    if match and len(match.groups()) == 3:\n                        (key, value, the_rest) = match.groups()\n                        auth_params[key.lower()] = UNQUOTE_PAIRS.sub(\n                            r\"\\1\", value\n                        )\n                    match = www_auth.search(the_rest)\n                retval[auth_scheme.lower()] = auth_params\n                authenticate = the_rest.strip()\n        except ValueError:\n            raise MalformedHeader(\"WWW-Authenticate\")\n    return retval\ndef _entry_disposition(response_headers, request_headers):\n    \"\"\"Determine freshness from the Date, Expires and Cache-Control headers.\n    We don't handle the following:\n    1. Cache-Control: max-stale\n    2. Age: headers are not used in the calculations.\n    Not that this algorithm is simpler than you might think\n    because we are operating as a private (non-shared) cache.\n    This lets us ignore 's-maxage'. We can also ignore\n    'proxy-invalidate' since we aren't a proxy.\n    We will never return a stale document as\n    fresh as a design decision, and thus the non-implementation\n    of 'max-stale'. This also lets us safely ignore 'must-revalidate'\n    since we operate as if every server has sent 'must-revalidate'.\n    Since we are private we get to ignore both 'public' and\n    'private' parameters. We also ignore 'no-transform' since\n    we don't do any transformations.\n    The 'no-store' parameter is handled at a higher level.\n    So the only Cache-Control parameters we look at are:\n    no-cache\n    only-if-cached\n    max-age\n    min-fresh\n    \"\"\"\n    retval = \"STALE\"\n    cc = _parse_cache_control(request_headers)\n    cc_response = _parse_cache_control(response_headers)\n    if (\n        \"pragma\" in request_headers\n        and request_headers[\"pragma\"].lower().find(\"no-cache\") != -1\n    ):\n        retval = \"TRANSPARENT\"\n        if \"cache-control\" not in request_headers:\n            request_headers[\"cache-control\"] = \"no-cache\"\n    elif \"no-cache\" in cc:\n        retval = \"TRANSPARENT\"\n    elif \"no-cache\" in cc_response:\n        retval = \"STALE\"\n    elif \"only-if-cached\" in cc:\n        retval = \"FRESH\"\n    elif \"date\" in response_headers:\n        date = calendar.timegm(email.Utils.parsedate_tz(response_headers[\"date\"]))\n        now = time.time()\n        current_age = max(0, now - date)\n        if \"max-age\" in cc_response:\n            try:\n                freshness_lifetime = int(cc_response[\"max-age\"])\n            except ValueError:\n                freshness_lifetime = 0\n        elif \"expires\" in response_headers:\n            expires = email.Utils.parsedate_tz(response_headers[\"expires\"])\n            if None == expires:\n                freshness_lifetime = 0\n            else:\n                freshness_lifetime = max(0, calendar.timegm(expires) - date)\n        else:\n            freshness_lifetime = 0\n        if \"max-age\" in cc:\n            try:\n                freshness_lifetime = int(cc[\"max-age\"])\n            except ValueError:\n                freshness_lifetime = 0\n        if \"min-fresh\" in cc:\n            try:\n                min_fresh = int(cc[\"min-fresh\"])\n            except ValueError:\n                min_fresh = 0\n            current_age += min_fresh\n        if freshness_lifetime > current_age:\n            retval = \"FRESH\"\n    return retval\ndef _decompressContent(response, new_content):\n    content = new_content\n    try:\n        encoding = response.get(\"content-encoding\", None)\n        if encoding in [\"gzip\", \"deflate\"]:\n            if encoding == \"gzip\":\n                content = gzip.GzipFile(fileobj=StringIO.StringIO(new_content)).read()\n            if encoding == \"deflate\":\n                content = zlib.decompress(content, -zlib.MAX_WBITS)\n            response[\"content-length\"] = str(len(content))\n            response[\"-content-encoding\"] = response[\"content-encoding\"]\n            del response[\"content-encoding\"]\n    except (IOError, zlib.error):\n        content = \"\"\n        raise FailedToDecompressContent(\n            _(\"Content purported to be compressed with %s but failed to decompress.\")\n            % response.get(\"content-encoding\"),\n            response,\n            content,\n        )\n    return content\ndef _updateCache(request_headers, response_headers, content, cache, cachekey):\n    if cachekey:\n        cc = _parse_cache_control(request_headers)\n        cc_response = _parse_cache_control(response_headers)\n        if \"no-store\" in cc or \"no-store\" in cc_response:\n            cache.delete(cachekey)\n        else:\n            info = email.Message.Message()\n            for key, value in response_headers.iteritems():\n                if key not in [\"status\", \"content-encoding\", \"transfer-encoding\"]:\n                    info[key] = value\n            vary = response_headers.get(\"vary\", None)\n            if vary:\n                vary_headers = vary.lower().replace(\" \", \"\").split(\",\")\n                for header in vary_headers:\n                    key = \"-varied-%s\" % header\n                    try:\n                        info[key] = request_headers[header]\n                    except KeyError:\n                        pass\n            status = response_headers.status\n            if status == 304:\n                status = 200\n            status_header = \"status: %d\\r\\n\" % status\n            header_str = info.as_string()\n            header_str = re.sub(\"\\r(?!\\n)|(?<!\\r)\\n\", \"\\r\\n\", header_str)\n            text = \"\".join([status_header, header_str, content])\n            cache.set(cachekey, text)\n    def request(self, method, request_uri, headers, content):\n        \"\"\"Modify the request headers\"\"\"\n        keys = _get_end2end_headers(headers)\n        keylist = \"\".join([\"%s \" % k for k in keys])\n        headers_val = \"\".join([headers[k] for k in keys])\n        created = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n        cnonce = _cnonce()\n        request_digest = \"%s:%s:%s:%s:%s\" % (\n            method,\n            request_uri,\n            cnonce,\n            self.challenge[\"snonce\"],\n            headers_val,\n        )\n        request_digest = (\n            hmac.new(self.key, request_digest, self.hashmod).hexdigest().lower()\n        )\n        headers[\"authorization\"] = (\n            'HMACDigest username=\"%s\", realm=\"%s\", snonce=\"%s\",'\n            ' cnonce=\"%s\", uri=\"%s\", created=\"%s\", '\n            'response=\"%s\", headers=\"%s\"'\n        ) % (\n            self.credentials[0],\n            self.challenge[\"realm\"],\n            self.challenge[\"snonce\"],\n            cnonce,\n            request_uri,\n            created,\n            request_digest,\n            keylist,\n        )\n    def __init__(\n        self, credentials, host, request_uri, headers, response, content, http\n    ):\n        from urllib import urlencode\n        Authentication.__init__(\n            self, credentials, host, request_uri, headers, response, content, http\n        )\n        challenge = _parse_www_authenticate(response, \"www-authenticate\")\n        service = challenge[\"googlelogin\"].get(\"service\", \"xapi\")\n        if service == \"xapi\" and request_uri.find(\"calendar\") > 0:\n            service = \"cl\"\n        auth = dict(\n            Email=credentials[0],\n            Passwd=credentials[1],\n            service=service,\n            source=headers[\"user-agent\"],\n        )\n        resp, content = self.http.request(\n            \"https://www.google.com/accounts/ClientLogin\",\n            method=\"POST\",\n            body=urlencode(auth),\n            headers={\"Content-Type\": \"application/x-www-form-urlencoded\"},\n        )\n        lines = content.split(\"\\n\")\n        d = dict([tuple(line.split(\"=\", 1)) for line in lines if line])\n        if resp.status == 403:\n            self.Auth = \"\"\n        else:\n            self.Auth = d[\"Auth\"]\n    def __init__(\n        self,\n        proxy_type,\n        proxy_host,\n        proxy_port,\n        proxy_rdns=True,\n        proxy_user=None,\n        proxy_pass=None,\n        proxy_headers=None,\n    ):\n        \"\"\"Args:\n          proxy_type: The type of proxy server.  This must be set to one of\n          socks.PROXY_TYPE_XXX constants.  For example:  p =\n          ProxyInfo(proxy_type=socks.PROXY_TYPE_HTTP, proxy_host='localhost',\n          proxy_port=8000)\n          proxy_host: The hostname or IP address of the proxy server.\n          proxy_port: The port that the proxy server is running on.\n          proxy_rdns: If True (default), DNS queries will not be performed\n          locally, and instead, handed to the proxy to resolve.  This is useful\n          if the network does not allow resolution of non-local names. In\n          httplib2 0.9 and earlier, this defaulted to False.\n          proxy_user: The username used to authenticate with the proxy server.\n          proxy_pass: The password used to authenticate with the proxy server.\n          proxy_headers: Additional or modified headers for the proxy connect\n          request.\n        \"\"\"\n        self.proxy_type = proxy_type\n        self.proxy_host = proxy_host\n        self.proxy_port = proxy_port\n        self.proxy_rdns = proxy_rdns\n        self.proxy_user = proxy_user\n        self.proxy_pass = proxy_pass\n        self.proxy_headers = proxy_headers\n    def connect(self):\n        \"Connect to a host on a given (SSL) port.\"\n        if self.proxy_info and self.proxy_info.isgood():\n            use_proxy = True\n            proxy_type, proxy_host, proxy_port, proxy_rdns, proxy_user, proxy_pass, proxy_headers = (\n                self.proxy_info.astuple()\n            )\n            host = proxy_host\n            port = proxy_port\n        else:\n            use_proxy = False\n            host = self.host\n            port = self.port\n        socket_err = None\n        address_info = socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM)\n        for family, socktype, proto, canonname, sockaddr in address_info:\n            try:\n                if use_proxy:\n                    sock = socks.socksocket(family, socktype, proto)\n                    sock.setproxy(\n                        proxy_type,\n                        proxy_host,\n                        proxy_port,\n                        proxy_rdns,\n                        proxy_user,\n                        proxy_pass,\n                        proxy_headers,\n                    )\n                else:\n                    sock = socket.socket(family, socktype, proto)\n                    sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n                if has_timeout(self.timeout):\n                    sock.settimeout(self.timeout)\n                if use_proxy:\n                    sock.connect((self.host, self.port) + sockaddr[:2])\n                else:\n                    sock.connect(sockaddr)\n                self.sock = _ssl_wrap_socket(\n                    sock,\n                    self.key_file,\n                    self.cert_file,\n                    self.disable_ssl_certificate_validation,\n                    self.ca_certs,\n                    self.ssl_version,\n                    self.host,\n                    self.key_password,\n                )\n                if self.debuglevel > 0:\n                    print(\"connect: (%s, %s)\" % (self.host, self.port))\n                    if use_proxy:\n                        print(\n                            \"proxy: %s\"\n                            % str(\n                                (\n                                    proxy_host,\n                                    proxy_port,\n                                    proxy_rdns,\n                                    proxy_user,\n                                    proxy_pass,\n                                    proxy_headers,\n                                )\n                            )\n                        )\n                if not self.disable_ssl_certificate_validation:\n                    cert = self.sock.getpeercert()\n                    hostname = self.host.split(\":\", 0)[0]\n                    if not self._ValidateCertificateHostname(cert, hostname):\n                        raise CertificateHostnameMismatch(\n                            \"Server presented certificate that does not match \"\n                            \"host %s: %s\" % (hostname, cert),\n                            hostname,\n                            cert,\n                        )\n            except (\n                ssl_SSLError,\n                ssl_CertificateError,\n                CertificateHostnameMismatch,\n            ) as e:\n                if sock:\n                    sock.close()\n                if self.sock:\n                    self.sock.close()\n                self.sock = None\n                if getattr(e, \"errno\", None) == ssl.SSL_ERROR_SSL:\n                    raise SSLHandshakeError(e)\n                else:\n                    raise\n            except (socket.timeout, socket.gaierror):\n                raise\n            except socket.error as e:\n                socket_err = e\n                if self.debuglevel > 0:\n                    print(\"connect fail: (%s, %s)\" % (self.host, self.port))\n                    if use_proxy:\n                        print(\n                            \"proxy: %s\"\n                            % str(\n                                (\n                                    proxy_host,\n                                    proxy_port,\n                                    proxy_rdns,\n                                    proxy_user,\n                                    proxy_pass,\n                                    proxy_headers,\n                                )\n                            )\n                        )\n                if self.sock:\n                    self.sock.close()\n                self.sock = None\n                continue\n            break\n        if not self.sock:\n            raise socket_err or socket.error(\"getaddrinfo returns an empty list\")\n    def __init__(\n        self,\n        cache=None,\n        timeout=None,\n        proxy_info=proxy_info_from_environment,\n        ca_certs=None,\n        disable_ssl_certificate_validation=False,\n        ssl_version=None,\n    ):\n        \"\"\"If 'cache' is a string then it is used as a directory name for\n        a disk cache. Otherwise it must be an object that supports the\n        same interface as FileCache.\n        All timeouts are in seconds. If None is passed for timeout\n        then Python's default timeout for sockets will be used. See\n        for example the docs of socket.setdefaulttimeout():\n        http://docs.python.org/library/socket.html\n        `proxy_info` may be:\n          - a callable that takes the http scheme ('http' or 'https') and\n            returns a ProxyInfo instance per request. By default, uses\n            proxy_nfo_from_environment.\n          - a ProxyInfo instance (static proxy config).\n          - None (proxy disabled).\n        ca_certs is the path of a file containing root CA certificates for SSL\n        server certificate validation.  By default, a CA cert file bundled with\n        httplib2 is used.\n        If disable_ssl_certificate_validation is true, SSL cert validation will\n        not be performed.\n        By default, ssl.PROTOCOL_SSLv23 will be used for the ssl version.\n        \"\"\"\n        self.proxy_info = proxy_info\n        self.ca_certs = ca_certs\n        self.disable_ssl_certificate_validation = disable_ssl_certificate_validation\n        self.ssl_version = ssl_version\n        self.connections = {}\n        if cache and isinstance(cache, basestring):\n            self.cache = FileCache(cache)\n        else:\n            self.cache = cache\n        self.credentials = Credentials()\n        self.certificates = KeyCerts()\n        self.authorizations = []\n        self.follow_redirects = True\n        self.redirect_codes = REDIRECT_CODES\n        self.optimistic_concurrency_methods = [\"PUT\", \"PATCH\"]\n        self.safe_methods = list(SAFE_METHODS)\n        self.follow_all_redirects = False\n        self.ignore_etag = False\n        self.force_exception_to_status_code = False\n        self.timeout = timeout\n        self.forward_authorization_headers = False\n    def _conn_request(self, conn, request_uri, method, body, headers):\n        i = 0\n        seen_bad_status_line = False\n        while i < RETRIES:\n            i += 1\n            try:\n                if hasattr(conn, \"sock\") and conn.sock is None:\n                    conn.connect()\n                conn.request(method, request_uri, body, headers)\n            except socket.timeout:\n                raise\n            except socket.gaierror:\n                conn.close()\n                raise ServerNotFoundError(\"Unable to find the server at %s\" % conn.host)\n            except ssl_SSLError:\n                conn.close()\n                raise\n            except socket.error as e:\n                err = 0\n                if hasattr(e, \"args\"):\n                    err = getattr(e, \"args\")[0]\n                else:\n                    err = e.errno\n                if err == errno.ECONNREFUSED:\n                    raise\n                if err in (errno.ENETUNREACH, errno.EADDRNOTAVAIL) and i < RETRIES:\n                    continue\n            except httplib.HTTPException:\n                if hasattr(conn, \"sock\") and conn.sock is None:\n                    if i < RETRIES - 1:\n                        conn.close()\n                        conn.connect()\n                        continue\n                    else:\n                        conn.close()\n                        raise\n                if i < RETRIES - 1:\n                    conn.close()\n                    conn.connect()\n                    continue\n            try:\n                response = conn.getresponse()\n            except httplib.BadStatusLine:\n                if not seen_bad_status_line and i == 1:\n                    i = 0\n                    seen_bad_status_line = True\n                    conn.close()\n                    conn.connect()\n                    continue\n                else:\n                    conn.close()\n                    raise\n            except (socket.error, httplib.HTTPException):\n                if i < RETRIES - 1:\n                    conn.close()\n                    conn.connect()\n                    continue\n                else:\n                    conn.close()\n                    raise\n            else:\n                content = \"\"\n                if method == \"HEAD\":\n                    conn.close()\n                else:\n                    content = response.read()\n                response = Response(response)\n                if method != \"HEAD\":\n                    content = _decompressContent(response, content)\n            break\n        return (response, content)\n    def _request(\n        self,\n        conn,\n        host,\n        absolute_uri,\n        request_uri,\n        method,\n        body,\n        headers,\n        redirections,\n        cachekey,\n    ):\n        \"\"\"Do the actual request using the connection object\n        and also follow one level of redirects if necessary\"\"\"\n        auths = [\n            (auth.depth(request_uri), auth)\n            for auth in self.authorizations\n            if auth.inscope(host, request_uri)\n        ]\n        auth = auths and sorted(auths)[0][1] or None\n        if auth:\n            auth.request(method, request_uri, headers, body)\n        (response, content) = self._conn_request(\n            conn, request_uri, method, body, headers\n        )\n        if auth:\n            if auth.response(response, body):\n                auth.request(method, request_uri, headers, body)\n                (response, content) = self._conn_request(\n                    conn, request_uri, method, body, headers\n                )\n                response._stale_digest = 1\n        if response.status == 401:\n            for authorization in self._auth_from_challenge(\n                host, request_uri, headers, response, content\n            ):\n                authorization.request(method, request_uri, headers, body)\n                (response, content) = self._conn_request(\n                    conn, request_uri, method, body, headers\n                )\n                if response.status != 401:\n                    self.authorizations.append(authorization)\n                    authorization.response(response, body)\n                    break\n        if (\n            self.follow_all_redirects\n            or method in self.safe_methods\n            or response.status in (303, 308)\n        ):\n            if self.follow_redirects and response.status in self.redirect_codes:\n                if redirections:\n                    if \"location\" not in response and response.status != 300:\n                        raise RedirectMissingLocation(\n                            _(\n                                \"Redirected but the response is missing a Location: header.\"\n                            ),\n                            response,\n                            content,\n                        )\n                    if \"location\" in response:\n                        location = response[\"location\"]\n                        (scheme, authority, path, query, fragment) = parse_uri(location)\n                        if authority == None:\n                            response[\"location\"] = urlparse.urljoin(\n                                absolute_uri, location\n                            )\n                    if response.status == 308 or (response.status == 301 and method in self.safe_methods):\n                        response[\"-x-permanent-redirect-url\"] = response[\"location\"]\n                        if \"content-location\" not in response:\n                            response[\"content-location\"] = absolute_uri\n                        _updateCache(headers, response, content, self.cache, cachekey)\n                    if \"if-none-match\" in headers:\n                        del headers[\"if-none-match\"]\n                    if \"if-modified-since\" in headers:\n                        del headers[\"if-modified-since\"]\n                    if (\n                        \"authorization\" in headers\n                        and not self.forward_authorization_headers\n                    ):\n                        del headers[\"authorization\"]\n                    if \"location\" in response:\n                        location = response[\"location\"]\n                        old_response = copy.deepcopy(response)\n                        if \"content-location\" not in old_response:\n                            old_response[\"content-location\"] = absolute_uri\n                        redirect_method = method\n                        if response.status in [302, 303]:\n                            redirect_method = \"GET\"\n                            body = None\n                        (response, content) = self.request(\n                            location,\n                            method=redirect_method,\n                            body=body,\n                            headers=headers,\n                            redirections=redirections - 1,\n                        )\n                        response.previous = old_response\n                else:\n                    raise RedirectLimit(\n                        \"Redirected more times than rediection_limit allows.\",\n                        response,\n                        content,\n                    )\n            elif response.status in [200, 203] and method in self.safe_methods:\n                if \"content-location\" not in response:\n                    response[\"content-location\"] = absolute_uri\n                _updateCache(headers, response, content, self.cache, cachekey)\n        return (response, content)\n    def request(\n        self,\n        uri,\n        method=\"GET\",\n        body=None,\n        headers=None,\n        redirections=DEFAULT_MAX_REDIRECTS,\n        connection_type=None,\n    ):\n        \"\"\" Performs a single HTTP request.\n        The 'uri' is the URI of the HTTP resource and can begin with either\n        'http' or 'https'. The value of 'uri' must be an absolute URI.\n        The 'method' is the HTTP method to perform, such as GET, POST, DELETE,\n        etc. There is no restriction on the methods allowed.\n        The 'body' is the entity body to be sent with the request. It is a\n        string object.\n        Any extra headers that are to be sent with the request should be\n        provided in the 'headers' dictionary.\n        The maximum number of redirect to follow before raising an\n        exception is 'redirections. The default is 5.\n        The return value is a tuple of (response, content), the first\n        being and instance of the 'Response' class, the second being\n        a string that contains the response entity body.\n        \"\"\"\n        conn_key = ''\n        try:\n            if headers is None:\n                headers = {}\n            else:\n                headers = self._normalize_headers(headers)\n            if \"user-agent\" not in headers:\n                headers[\"user-agent\"] = \"Python-httplib2/%s (gzip)\" % __version__\n            uri = iri2uri(uri)\n            uri = uri.replace(\" \", \"%20\").replace(\"\\r\", \"%0D\").replace(\"\\n\", \"%0A\")\n            (scheme, authority, request_uri, defrag_uri) = urlnorm(uri)\n            proxy_info = self._get_proxy_info(scheme, authority)\n            conn_key = scheme + \":\" + authority\n            conn = self.connections.get(conn_key)\n            if conn is None:\n                if not connection_type:\n                    connection_type = SCHEME_TO_CONNECTION[scheme]\n                certs = list(self.certificates.iter(authority))\n                if scheme == \"https\":\n                    if certs:\n                        conn = self.connections[conn_key] = connection_type(\n                            authority,\n                            key_file=certs[0][0],\n                            cert_file=certs[0][1],\n                            timeout=self.timeout,\n                            proxy_info=proxy_info,\n                            ca_certs=self.ca_certs,\n                            disable_ssl_certificate_validation=self.disable_ssl_certificate_validation,\n                            ssl_version=self.ssl_version,\n                            key_password=certs[0][2],\n                        )\n                    else:\n                        conn = self.connections[conn_key] = connection_type(\n                            authority,\n                            timeout=self.timeout,\n                            proxy_info=proxy_info,\n                            ca_certs=self.ca_certs,\n                            disable_ssl_certificate_validation=self.disable_ssl_certificate_validation,\n                            ssl_version=self.ssl_version,\n                        )\n                else:\n                    conn = self.connections[conn_key] = connection_type(\n                        authority, timeout=self.timeout, proxy_info=proxy_info\n                    )\n                conn.set_debuglevel(debuglevel)\n            if \"range\" not in headers and \"accept-encoding\" not in headers:\n                headers[\"accept-encoding\"] = \"gzip, deflate\"\n            info = email.Message.Message()\n            cachekey = None\n            cached_value = None\n            if self.cache:\n                cachekey = defrag_uri.encode(\"utf-8\")\n                cached_value = self.cache.get(cachekey)\n                if cached_value:\n                    try:\n                        info, content = cached_value.split(\"\\r\\n\\r\\n\", 1)\n                        feedparser = email.FeedParser.FeedParser()\n                        feedparser.feed(info)\n                        info = feedparser.close()\n                        feedparser._parse = None\n                    except (IndexError, ValueError):\n                        self.cache.delete(cachekey)\n                        cachekey = None\n                        cached_value = None\n            if (\n                method in self.optimistic_concurrency_methods\n                and self.cache\n                and \"etag\" in info\n                and not self.ignore_etag\n                and \"if-match\" not in headers\n            ):\n                headers[\"if-match\"] = info[\"etag\"]\n            if self.cache and cachekey and method not in self.safe_methods:\n                self.cache.delete(cachekey)\n            if method in self.safe_methods and \"vary\" in info:\n                vary = info[\"vary\"]\n                vary_headers = vary.lower().replace(\" \", \"\").split(\",\")\n                for header in vary_headers:\n                    key = \"-varied-%s\" % header\n                    value = info[key]\n                    if headers.get(header, None) != value:\n                        cached_value = None\n                        break\n            if (\n                self.cache\n                and cached_value\n                and (method in self.safe_methods or info[\"status\"] == \"308\")\n                and \"range\" not in headers\n            ):\n                redirect_method = method\n                if info[\"status\"] not in (\"307\", \"308\"):\n                    redirect_method = \"GET\"\n                if \"-x-permanent-redirect-url\" in info:\n                    if redirections <= 0:\n                        raise RedirectLimit(\n                            \"Redirected more times than rediection_limit allows.\",\n                            {},\n                            \"\",\n                        )\n                    (response, new_content) = self.request(\n                        info[\"-x-permanent-redirect-url\"],\n                        method=redirect_method,\n                        headers=headers,\n                        redirections=redirections - 1,\n                    )\n                    response.previous = Response(info)\n                    response.previous.fromcache = True\n                else:\n                    entry_disposition = _entry_disposition(info, headers)\n                    if entry_disposition == \"FRESH\":\n                        if not cached_value:\n                            info[\"status\"] = \"504\"\n                            content = \"\"\n                        response = Response(info)\n                        if cached_value:\n                            response.fromcache = True\n                        return (response, content)\n                    if entry_disposition == \"STALE\":\n                        if (\n                            \"etag\" in info\n                            and not self.ignore_etag\n                            and not \"if-none-match\" in headers\n                        ):\n                            headers[\"if-none-match\"] = info[\"etag\"]\n                        if \"last-modified\" in info and not \"last-modified\" in headers:\n                            headers[\"if-modified-since\"] = info[\"last-modified\"]\n                    elif entry_disposition == \"TRANSPARENT\":\n                        pass\n                    (response, new_content) = self._request(\n                        conn,\n                        authority,\n                        uri,\n                        request_uri,\n                        method,\n                        body,\n                        headers,\n                        redirections,\n                        cachekey,\n                    )\n                if response.status == 304 and method == \"GET\":\n                    for key in _get_end2end_headers(response):\n                        info[key] = response[key]\n                    merged_response = Response(info)\n                    if hasattr(response, \"_stale_digest\"):\n                        merged_response._stale_digest = response._stale_digest\n                    _updateCache(\n                        headers, merged_response, content, self.cache, cachekey\n                    )\n                    response = merged_response\n                    response.status = 200\n                    response.fromcache = True\n                elif response.status == 200:\n                    content = new_content\n                else:\n                    self.cache.delete(cachekey)\n                    content = new_content\n            else:\n                cc = _parse_cache_control(headers)\n                if \"only-if-cached\" in cc:\n                    info[\"status\"] = \"504\"\n                    response = Response(info)\n                    content = \"\"\n                else:\n                    (response, content) = self._request(\n                        conn,\n                        authority,\n                        uri,\n                        request_uri,\n                        method,\n                        body,\n                        headers,\n                        redirections,\n                        cachekey,\n                    )\n        except Exception as e:\n            is_timeout = isinstance(e, socket.timeout)\n            if is_timeout:\n                conn = self.connections.pop(conn_key, None)\n                if conn:\n                    conn.close()\n            if self.force_exception_to_status_code:\n                if isinstance(e, HttpLib2ErrorWithResponse):\n                    response = e.response\n                    content = e.content\n                    response.status = 500\n                    response.reason = str(e)\n                elif is_timeout:\n                    content = \"Request Timeout\"\n                    response = Response(\n                        {\n                            \"content-type\": \"text/plain\",\n                            \"status\": \"408\",\n                            \"content-length\": len(content),\n                        }\n                    )\n                    response.reason = \"Request Timeout\"\n                else:\n                    content = str(e)\n                    response = Response(\n                        {\n                            \"content-type\": \"text/plain\",\n                            \"status\": \"400\",\n                            \"content-length\": len(content),\n                        }\n                    )\n                    response.reason = \"Bad Request\"\n            else:\n                raise\n        return (response, content)\nDEFAULT_MAX_REDIRECTS = 5\nHOP_BY_HOP = [\n    \"connection\",\n    \"keep-alive\",\n    \"proxy-authenticate\",\n    \"proxy-authorization\",\n    \"te\",\n    \"trailers\",\n    \"transfer-encoding\",\n    \"upgrade\",\n]\nSAFE_METHODS = (\"GET\", \"HEAD\", \"OPTIONS\", \"TRACE\")\nREDIRECT_CODES = frozenset((300, 301, 302, 303, 307, 308))\ndef safename(filename):\n    \"\"\"Return a filename suitable for the cache.\n    Strips dangerous and common characters to create a filename we\n    can use to store the cache in.\n    \"\"\"\n    if isinstance(filename, bytes):\n        filename_bytes = filename\n        filename = filename.decode(\"utf-8\")\n    else:\n        filename_bytes = filename.encode(\"utf-8\")\n    filemd5 = _md5(filename_bytes).hexdigest()\n    filename = re_url_scheme.sub(\"\", filename)\n    filename = re_unsafe.sub(\"\", filename)\n    filename = filename[:90]\n    return \",\".join((filename, filemd5))\nUSE_WWW_AUTH_STRICT_PARSING = 0\nWWW_AUTH_STRICT = re.compile(\n    r\"^(?:\\s*(?:,\\s*)?([^\\0-\\x1f\\x7f-\\xff()<>@,;:\\\\\\\"/[\\]?={} \\t]+)\\s*=\\s*\\\"?((?<=\\\")(?:[^\\0-\\x08\\x0A-\\x1f\\x7f-\\xff\\\\\\\"]|\\\\[\\0-\\x7f])*?(?=\\\")|(?<!\\\")[^\\0-\\x1f\\x7f-\\xff()<>@,;:\\\\\\\"/[\\]?={} \\t]+(?!\\\"))\\\"?)(.*)$\"\n)\nWWW_AUTH_RELAXED = re.compile(\n    r\"^(?:\\s*(?:,\\s*)?([^ \\t\\r\\n=]+)\\s*=\\s*\\\"?((?<=\\\")(?:[^\\\\\\\"]|\\\\.)*?(?=\\\")|(?<!\\\")[^ \\t\\r\\n,]+(?!\\\"))\\\"?)(.*)$\"\n)\nUNQUOTE_PAIRS = re.compile(r\"\\\\(.)\")\ndef _parse_www_authenticate(headers, headername=\"www-authenticate\"):\n    \"\"\"Returns a dictionary of dictionaries, one dict\n    per auth_scheme.\"\"\"\n    retval = {}\n    if headername in headers:\n        try:\n            authenticate = headers[headername].strip()\n            www_auth = (\n                USE_WWW_AUTH_STRICT_PARSING and WWW_AUTH_STRICT or WWW_AUTH_RELAXED\n            )\n            while authenticate:\n                if headername == \"authentication-info\":\n                    (auth_scheme, the_rest) = (\"digest\", authenticate)\n                else:\n                    (auth_scheme, the_rest) = authenticate.split(\" \", 1)\n                match = www_auth.search(the_rest)\n                auth_params = {}\n                while match:\n                    if match and len(match.groups()) == 3:\n                        (key, value, the_rest) = match.groups()\n                        auth_params[key.lower()] = UNQUOTE_PAIRS.sub(\n                            r\"\\1\", value\n                        )\n                    match = www_auth.search(the_rest)\n                retval[auth_scheme.lower()] = auth_params\n                authenticate = the_rest.strip()\n        except ValueError:\n            raise MalformedHeader(\"WWW-Authenticate\")\n    return retval\ndef _entry_disposition(response_headers, request_headers):\n    \"\"\"Determine freshness from the Date, Expires and Cache-Control headers.\n    We don't handle the following:\n    1. Cache-Control: max-stale\n    2. Age: headers are not used in the calculations.\n    Not that this algorithm is simpler than you might think\n    because we are operating as a private (non-shared) cache.\n    This lets us ignore 's-maxage'. We can also ignore\n    'proxy-invalidate' since we aren't a proxy.\n    We will never return a stale document as\n    fresh as a design decision, and thus the non-implementation\n    of 'max-stale'. This also lets us safely ignore 'must-revalidate'\n    since we operate as if every server has sent 'must-revalidate'.\n    Since we are private we get to ignore both 'public' and\n    'private' parameters. We also ignore 'no-transform' since\n    we don't do any transformations.\n    The 'no-store' parameter is handled at a higher level.\n    So the only Cache-Control parameters we look at are:\n    no-cache\n    only-if-cached\n    max-age\n    min-fresh\n    \"\"\"\n    retval = \"STALE\"\n    cc = _parse_cache_control(request_headers)\n    cc_response = _parse_cache_control(response_headers)\n    if (\n        \"pragma\" in request_headers\n        and request_headers[\"pragma\"].lower().find(\"no-cache\") != -1\n    ):\n        retval = \"TRANSPARENT\"\n        if \"cache-control\" not in request_headers:\n            request_headers[\"cache-control\"] = \"no-cache\"\n    elif \"no-cache\" in cc:\n        retval = \"TRANSPARENT\"\n    elif \"no-cache\" in cc_response:\n        retval = \"STALE\"\n    elif \"only-if-cached\" in cc:\n        retval = \"FRESH\"\n    elif \"date\" in response_headers:\n        date = calendar.timegm(email.utils.parsedate_tz(response_headers[\"date\"]))\n        now = time.time()\n        current_age = max(0, now - date)\n        if \"max-age\" in cc_response:\n            try:\n                freshness_lifetime = int(cc_response[\"max-age\"])\n            except ValueError:\n                freshness_lifetime = 0\n        elif \"expires\" in response_headers:\n            expires = email.utils.parsedate_tz(response_headers[\"expires\"])\n            if None == expires:\n                freshness_lifetime = 0\n            else:\n                freshness_lifetime = max(0, calendar.timegm(expires) - date)\n        else:\n            freshness_lifetime = 0\n        if \"max-age\" in cc:\n            try:\n                freshness_lifetime = int(cc[\"max-age\"])\n            except ValueError:\n                freshness_lifetime = 0\n        if \"min-fresh\" in cc:\n            try:\n                min_fresh = int(cc[\"min-fresh\"])\n            except ValueError:\n                min_fresh = 0\n            current_age += min_fresh\n        if freshness_lifetime > current_age:\n            retval = \"FRESH\"\n    return retval\ndef _decompressContent(response, new_content):\n    content = new_content\n    try:\n        encoding = response.get(\"content-encoding\", None)\n        if encoding in [\"gzip\", \"deflate\"]:\n            if encoding == \"gzip\":\n                content = gzip.GzipFile(fileobj=io.BytesIO(new_content)).read()\n            if encoding == \"deflate\":\n                content = zlib.decompress(content, -zlib.MAX_WBITS)\n            response[\"content-length\"] = str(len(content))\n            response[\"-content-encoding\"] = response[\"content-encoding\"]\n            del response[\"content-encoding\"]\n    except (IOError, zlib.error):\n        content = \"\"\n        raise FailedToDecompressContent(\n            _(\"Content purported to be compressed with %s but failed to decompress.\")\n            % response.get(\"content-encoding\"),\n            response,\n            content,\n        )\n    return content\n    def __init__(\n        self, credentials, host, request_uri, headers, response, content, http\n    ):\n        Authentication.__init__(\n            self, credentials, host, request_uri, headers, response, content, http\n        )\n        challenge = _parse_www_authenticate(response, \"www-authenticate\")\n        self.challenge = challenge[\"digest\"]\n        qop = self.challenge.get(\"qop\", \"auth\")\n        self.challenge[\"qop\"] = (\n            (\"auth\" in [x.strip() for x in qop.split()]) and \"auth\" or None\n        )\n        if self.challenge[\"qop\"] is None:\n            raise UnimplementedDigestAuthOptionError(\n                _(\"Unsupported value for qop: %s.\" % qop)\n            )\n        self.challenge[\"algorithm\"] = self.challenge.get(\"algorithm\", \"MD5\").upper()\n        if self.challenge[\"algorithm\"] != \"MD5\":\n            raise UnimplementedDigestAuthOptionError(\n                _(\"Unsupported value for algorithm: %s.\" % self.challenge[\"algorithm\"])\n            )\n        self.A1 = \"\".join(\n            [\n                self.credentials[0],\n                \":\",\n                self.challenge[\"realm\"],\n                \":\",\n                self.credentials[1],\n            ]\n        )\n        self.challenge[\"nc\"] = 1\n    def __init__(\n        self, credentials, host, request_uri, headers, response, content, http\n    ):\n        Authentication.__init__(\n            self, credentials, host, request_uri, headers, response, content, http\n        )\n        challenge = _parse_www_authenticate(response, \"www-authenticate\")\n        self.challenge = challenge[\"hmacdigest\"]\n        self.challenge[\"reason\"] = self.challenge.get(\"reason\", \"unauthorized\")\n        if self.challenge[\"reason\"] not in [\"unauthorized\", \"integrity\"]:\n            self.challenge[\"reason\"] = \"unauthorized\"\n        self.challenge[\"salt\"] = self.challenge.get(\"salt\", \"\")\n        if not self.challenge.get(\"snonce\"):\n            raise UnimplementedHmacDigestAuthOptionError(\n                _(\"The challenge doesn't contain a server nonce, or this one is empty.\")\n            )\n        self.challenge[\"algorithm\"] = self.challenge.get(\"algorithm\", \"HMAC-SHA-1\")\n        if self.challenge[\"algorithm\"] not in [\"HMAC-SHA-1\", \"HMAC-MD5\"]:\n            raise UnimplementedHmacDigestAuthOptionError(\n                _(\"Unsupported value for algorithm: %s.\" % self.challenge[\"algorithm\"])\n            )\n        self.challenge[\"pw-algorithm\"] = self.challenge.get(\"pw-algorithm\", \"SHA-1\")\n        if self.challenge[\"pw-algorithm\"] not in [\"SHA-1\", \"MD5\"]:\n            raise UnimplementedHmacDigestAuthOptionError(\n                _(\n                    \"Unsupported value for pw-algorithm: %s.\"\n                    % self.challenge[\"pw-algorithm\"]\n                )\n            )\n        if self.challenge[\"algorithm\"] == \"HMAC-MD5\":\n            self.hashmod = _md5\n        else:\n            self.hashmod = _sha\n        if self.challenge[\"pw-algorithm\"] == \"MD5\":\n            self.pwhashmod = _md5\n        else:\n            self.pwhashmod = _sha\n        self.key = \"\".join(\n            [\n                self.credentials[0],\n                \":\",\n                self.pwhashmod.new(\n                    \"\".join([self.credentials[1], self.challenge[\"salt\"]])\n                )\n                .hexdigest()\n                .lower(),\n                \":\",\n                self.challenge[\"realm\"],\n            ]\n        )\n        self.key = self.pwhashmod.new(self.key).hexdigest().lower()\n    def request(self, method, request_uri, headers, content):\n        \"\"\"Modify the request headers\"\"\"\n        keys = _get_end2end_headers(headers)\n        keylist = \"\".join([\"%s \" % k for k in keys])\n        headers_val = \"\".join([headers[k] for k in keys])\n        created = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n        cnonce = _cnonce()\n        request_digest = \"%s:%s:%s:%s:%s\" % (\n            method,\n            request_uri,\n            cnonce,\n            self.challenge[\"snonce\"],\n            headers_val,\n        )\n        request_digest = (\n            hmac.new(self.key, request_digest, self.hashmod).hexdigest().lower()\n        )\n        headers[\"authorization\"] = (\n            'HMACDigest username=\"%s\", realm=\"%s\", snonce=\"%s\",'\n            ' cnonce=\"%s\", uri=\"%s\", created=\"%s\", '\n            'response=\"%s\", headers=\"%s\"'\n        ) % (\n            self.credentials[0],\n            self.challenge[\"realm\"],\n            self.challenge[\"snonce\"],\n            cnonce,\n            request_uri,\n            created,\n            request_digest,\n            keylist,\n        )\n    def __init__(\n        self,\n        proxy_type,\n        proxy_host,\n        proxy_port,\n        proxy_rdns=True,\n        proxy_user=None,\n        proxy_pass=None,\n        proxy_headers=None,\n    ):\n        \"\"\"Args:\n          proxy_type: The type of proxy server.  This must be set to one of\n          socks.PROXY_TYPE_XXX constants.  For example:  p =\n          ProxyInfo(proxy_type=socks.PROXY_TYPE_HTTP, proxy_host='localhost',\n          proxy_port=8000)\n          proxy_host: The hostname or IP address of the proxy server.\n          proxy_port: The port that the proxy server is running on.\n          proxy_rdns: If True (default), DNS queries will not be performed\n          locally, and instead, handed to the proxy to resolve.  This is useful\n          if the network does not allow resolution of non-local names. In\n          httplib2 0.9 and earlier, this defaulted to False.\n          proxy_user: The username used to authenticate with the proxy server.\n          proxy_pass: The password used to authenticate with the proxy server.\n          proxy_headers: Additional or modified headers for the proxy connect\n          request.\n        \"\"\"\n        if isinstance(proxy_user, bytes):\n            proxy_user = proxy_user.decode()\n        if isinstance(proxy_pass, bytes):\n            proxy_pass = proxy_pass.decode()\n        self.proxy_type, self.proxy_host, self.proxy_port, self.proxy_rdns, self.proxy_user, self.proxy_pass, self.proxy_headers = (\n            proxy_type,\n            proxy_host,\n            proxy_port,\n            proxy_rdns,\n            proxy_user,\n            proxy_pass,\n            proxy_headers,\n        )\ndef proxy_info_from_url(url, method=\"http\", noproxy=None):\n    \"\"\"Construct a ProxyInfo from a URL (such as http_proxy env var)\n    \"\"\"\n    url = urllib.parse.urlparse(url)\n    username = None\n    password = None\n    port = None\n    if \"@\" in url[1]:\n        ident, host_port = url[1].split(\"@\", 1)\n        if \":\" in ident:\n            username, password = ident.split(\":\", 1)\n        else:\n            password = ident\n    else:\n        host_port = url[1]\n    if \":\" in host_port:\n        host, port = host_port.split(\":\", 1)\n    else:\n        host = host_port\n    if port:\n        port = int(port)\n    else:\n        port = dict(https=443, http=80)[method]\n    proxy_type = 3\n    pi = ProxyInfo(\n        proxy_type=proxy_type,\n        proxy_host=host,\n        proxy_port=port,\n        proxy_user=username or None,\n        proxy_pass=password or None,\n        proxy_headers=None,\n    )\n    bypass_hosts = []\n    if noproxy is None:\n        noproxy = os.environ.get(\"no_proxy\", os.environ.get(\"NO_PROXY\", \"\"))\n    if noproxy == \"*\":\n        bypass_hosts = AllHosts\n    elif noproxy.strip():\n        bypass_hosts = noproxy.split(\",\")\n        bypass_hosts = tuple(filter(bool, bypass_hosts))\n    pi.bypass_hosts = bypass_hosts\n    return pi\n    def connect(self):\n        \"\"\"Connect to a host on a given (SSL) port.\"\"\"\n        if self.proxy_info and self.proxy_info.isgood() and self.proxy_info.applies_to(self.host):\n            use_proxy = True\n            proxy_type, proxy_host, proxy_port, proxy_rdns, proxy_user, proxy_pass, proxy_headers = (\n                self.proxy_info.astuple()\n            )\n            host = proxy_host\n            port = proxy_port\n        else:\n            use_proxy = False\n            host = self.host\n            port = self.port\n            proxy_type = None\n            proxy_headers = None\n        socket_err = None\n        address_info = socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM)\n        for family, socktype, proto, canonname, sockaddr in address_info:\n            try:\n                if use_proxy:\n                    sock = socks.socksocket(family, socktype, proto)\n                    sock.setproxy(\n                        proxy_type,\n                        proxy_host,\n                        proxy_port,\n                        proxy_rdns,\n                        proxy_user,\n                        proxy_pass,\n                    )\n                else:\n                    sock = socket.socket(family, socktype, proto)\n                    sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n                if has_timeout(self.timeout):\n                    sock.settimeout(self.timeout)\n                sock.connect((self.host, self.port))\n                self.sock = self._context.wrap_socket(sock, server_hostname=self.host)\n                if (\n                    not hasattr(self._context, \"check_hostname\")\n                    and not self.disable_ssl_certificate_validation\n                ):\n                    try:\n                        ssl.match_hostname(self.sock.getpeercert(), self.host)\n                    except Exception:\n                        self.sock.shutdown(socket.SHUT_RDWR)\n                        self.sock.close()\n                        raise\n                if self.debuglevel > 0:\n                    print(\"connect: ({0}, {1})\".format(self.host, self.port))\n                    if use_proxy:\n                        print(\n                            \"proxy: {0}\".format(\n                                str(\n                                    (\n                                        proxy_host,\n                                        proxy_port,\n                                        proxy_rdns,\n                                        proxy_user,\n                                        proxy_pass,\n                                        proxy_headers,\n                                    )\n                                )\n                            )\n                        )\n            except (ssl.SSLError, ssl.CertificateError) as e:\n                if sock:\n                    sock.close()\n                if self.sock:\n                    self.sock.close()\n                self.sock = None\n                raise\n            except (socket.timeout, socket.gaierror):\n                raise\n            except socket.error as e:\n                socket_err = e\n                if self.debuglevel > 0:\n                    print(\"connect fail: ({0}, {1})\".format(self.host, self.port))\n                    if use_proxy:\n                        print(\n                            \"proxy: {0}\".format(\n                                str(\n                                    (\n                                        proxy_host,\n                                        proxy_port,\n                                        proxy_rdns,\n                                        proxy_user,\n                                        proxy_pass,\n                                        proxy_headers,\n                                    )\n                                )\n                            )\n                        )\n                if self.sock:\n                    self.sock.close()\n                self.sock = None\n                continue\n            break\n        if not self.sock:\n            raise socket_err\n    def __init__(\n        self,\n        cache=None,\n        timeout=None,\n        proxy_info=proxy_info_from_environment,\n        ca_certs=None,\n        disable_ssl_certificate_validation=False,\n        tls_maximum_version=None,\n        tls_minimum_version=None,\n    ):\n        \"\"\"If 'cache' is a string then it is used as a directory name for\n        a disk cache. Otherwise it must be an object that supports the\n        same interface as FileCache.\n        All timeouts are in seconds. If None is passed for timeout\n        then Python's default timeout for sockets will be used. See\n        for example the docs of socket.setdefaulttimeout():\n        http://docs.python.org/library/socket.html\n        `proxy_info` may be:\n          - a callable that takes the http scheme ('http' or 'https') and\n            returns a ProxyInfo instance per request. By default, uses\n            proxy_info_from_environment.\n          - a ProxyInfo instance (static proxy config).\n          - None (proxy disabled).\n        ca_certs is the path of a file containing root CA certificates for SSL\n        server certificate validation.  By default, a CA cert file bundled with\n        httplib2 is used.\n        If disable_ssl_certificate_validation is true, SSL cert validation will\n        not be performed.\n        tls_maximum_version / tls_minimum_version require Python 3.7+ /\n        OpenSSL 1.1.0g+. A value of \"TLSv1_3\" requires OpenSSL 1.1.1+.\n\"\"\"\n        self.proxy_info = proxy_info\n        self.ca_certs = ca_certs\n        self.disable_ssl_certificate_validation = disable_ssl_certificate_validation\n        self.tls_maximum_version = tls_maximum_version\n        self.tls_minimum_version = tls_minimum_version\n        self.connections = {}\n        if cache and isinstance(cache, str):\n            self.cache = FileCache(cache)\n        else:\n            self.cache = cache\n        self.credentials = Credentials()\n        self.certificates = KeyCerts()\n        self.authorizations = []\n        self.follow_redirects = True\n        self.redirect_codes = REDIRECT_CODES\n        self.optimistic_concurrency_methods = [\"PUT\", \"PATCH\"]\n        self.safe_methods = list(SAFE_METHODS)\n        self.follow_all_redirects = False\n        self.ignore_etag = False\n        self.force_exception_to_status_code = False\n        self.timeout = timeout\n        self.forward_authorization_headers = False\n    def _conn_request(self, conn, request_uri, method, body, headers):\n        i = 0\n        seen_bad_status_line = False\n        while i < RETRIES:\n            i += 1\n            try:\n                if conn.sock is None:\n                    conn.connect()\n                conn.request(method, request_uri, body, headers)\n            except socket.timeout:\n                conn.close()\n                raise\n            except socket.gaierror:\n                conn.close()\n                raise ServerNotFoundError(\"Unable to find the server at %s\" % conn.host)\n            except socket.error as e:\n                errno_ = (\n                    e.args[0].errno if isinstance(e.args[0], socket.error) else e.errno\n                )\n                if errno_ in (errno.ENETUNREACH, errno.EADDRNOTAVAIL) and i < RETRIES:\n                    continue\n                raise\n            except http.client.HTTPException:\n                if conn.sock is None:\n                    if i < RETRIES - 1:\n                        conn.close()\n                        conn.connect()\n                        continue\n                    else:\n                        conn.close()\n                        raise\n                if i < RETRIES - 1:\n                    conn.close()\n                    conn.connect()\n                    continue\n                pass\n            try:\n                response = conn.getresponse()\n            except (http.client.BadStatusLine, http.client.ResponseNotReady):\n                if not seen_bad_status_line and i == 1:\n                    i = 0\n                    seen_bad_status_line = True\n                    conn.close()\n                    conn.connect()\n                    continue\n                else:\n                    conn.close()\n                    raise\n            except socket.timeout:\n                raise\n            except (socket.error, http.client.HTTPException):\n                conn.close()\n                if i == 0:\n                    conn.close()\n                    conn.connect()\n                    continue\n                else:\n                    raise\n            else:\n                content = b\"\"\n                if method == \"HEAD\":\n                    conn.close()\n                else:\n                    content = response.read()\n                response = Response(response)\n                if method != \"HEAD\":\n                    content = _decompressContent(response, content)\n            break\n        return (response, content)\n    def _request(\n        self,\n        conn,\n        host,\n        absolute_uri,\n        request_uri,\n        method,\n        body,\n        headers,\n        redirections,\n        cachekey,\n    ):\n        \"\"\"Do the actual request using the connection object\n        and also follow one level of redirects if necessary\"\"\"\n        auths = [\n            (auth.depth(request_uri), auth)\n            for auth in self.authorizations\n            if auth.inscope(host, request_uri)\n        ]\n        auth = auths and sorted(auths)[0][1] or None\n        if auth:\n            auth.request(method, request_uri, headers, body)\n        (response, content) = self._conn_request(\n            conn, request_uri, method, body, headers\n        )\n        if auth:\n            if auth.response(response, body):\n                auth.request(method, request_uri, headers, body)\n                (response, content) = self._conn_request(\n                    conn, request_uri, method, body, headers\n                )\n                response._stale_digest = 1\n        if response.status == 401:\n            for authorization in self._auth_from_challenge(\n                host, request_uri, headers, response, content\n            ):\n                authorization.request(method, request_uri, headers, body)\n                (response, content) = self._conn_request(\n                    conn, request_uri, method, body, headers\n                )\n                if response.status != 401:\n                    self.authorizations.append(authorization)\n                    authorization.response(response, body)\n                    break\n        if (\n            self.follow_all_redirects\n            or method in self.safe_methods\n            or response.status in (303, 308)\n        ):\n            if self.follow_redirects and response.status in self.redirect_codes:\n                if redirections:\n                    if \"location\" not in response and response.status != 300:\n                        raise RedirectMissingLocation(\n                            _(\n                                \"Redirected but the response is missing a Location: header.\"\n                            ),\n                            response,\n                            content,\n                        )\n                    if \"location\" in response:\n                        location = response[\"location\"]\n                        (scheme, authority, path, query, fragment) = parse_uri(location)\n                        if authority == None:\n                            response[\"location\"] = urllib.parse.urljoin(\n                                absolute_uri, location\n                            )\n                    if response.status == 308 or (response.status == 301 and (method in self.safe_methods)):\n                        response[\"-x-permanent-redirect-url\"] = response[\"location\"]\n                        if \"content-location\" not in response:\n                            response[\"content-location\"] = absolute_uri\n                        _updateCache(headers, response, content, self.cache, cachekey)\n                    if \"if-none-match\" in headers:\n                        del headers[\"if-none-match\"]\n                    if \"if-modified-since\" in headers:\n                        del headers[\"if-modified-since\"]\n                    if (\n                        \"authorization\" in headers\n                        and not self.forward_authorization_headers\n                    ):\n                        del headers[\"authorization\"]\n                    if \"location\" in response:\n                        location = response[\"location\"]\n                        old_response = copy.deepcopy(response)\n                        if \"content-location\" not in old_response:\n                            old_response[\"content-location\"] = absolute_uri\n                        redirect_method = method\n                        if response.status in [302, 303]:\n                            redirect_method = \"GET\"\n                            body = None\n                        (response, content) = self.request(\n                            location,\n                            method=redirect_method,\n                            body=body,\n                            headers=headers,\n                            redirections=redirections - 1,\n                        )\n                        response.previous = old_response\n                else:\n                    raise RedirectLimit(\n                        \"Redirected more times than redirection_limit allows.\",\n                        response,\n                        content,\n                    )\n            elif response.status in [200, 203] and method in self.safe_methods:\n                if \"content-location\" not in response:\n                    response[\"content-location\"] = absolute_uri\n                _updateCache(headers, response, content, self.cache, cachekey)\n        return (response, content)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-21240",
        "description": "[{'lang': 'en', 'value': 'httplib2 is a comprehensive HTTP client library for Python. In httplib2 before version 0.19.0, a malicious server which responds with long series of \"\\\\xa0\" characters in the \"www-authenticate\" header may cause Denial of Service (CPU burn while parsing header) of the httplib2 client accessing said server. This is fixed in version 0.19.0 which contains a new implementation of auth headers parsing using the pyparsing library.'}]",
        "cwe_number": 400
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-138",
      "code": "    def emit(self, s, depth, reflow=True):\n        if reflow:\n            lines = reflow_lines(s, depth)\n        else:\n            lines = [s]\n        for line in lines:\n            line = (\" \" * TABSIZE * depth) + line + \"\\n\"\n            self.file.write(line)\n    def visitField(self, field, name, sum=None, prod=None, depth=0):\n        ctype = get_c_type(field.type)\n        if field.opt:\n            check = \"exists_not_none(obj, &PyId_%s)\" % (field.name,)\n        else:\n            check = \"_PyObject_HasAttrId(obj, &PyId_%s)\" % (field.name,)\n        self.emit(\"if (%s) {\" % (check,), depth, reflow=False)\n        self.emit(\"int res;\", depth+1)\n        if field.seq:\n            self.emit(\"Py_ssize_t len;\", depth+1)\n            self.emit(\"Py_ssize_t i;\", depth+1)\n        self.emit(\"tmp = _PyObject_GetAttrId(obj, &PyId_%s);\" % field.name, depth+1)\n        self.emit(\"if (tmp == NULL) goto failed;\", depth+1)\n        if field.seq:\n            self.emit(\"if (!PyList_Check(tmp)) {\", depth+1)\n            self.emit(\"PyErr_Format(PyExc_TypeError, \\\"%s field \\\\\\\"%s\\\\\\\" must \"\n                      \"be a list, not a %%.200s\\\", tmp->ob_type->tp_name);\" %\n                      (name, field.name),\n                      depth+2, reflow=False)\n            self.emit(\"goto failed;\", depth+2)\n            self.emit(\"}\", depth+1)\n            self.emit(\"len = PyList_GET_SIZE(tmp);\", depth+1)\n            if self.isSimpleType(field):\n                self.emit(\"%s = _Ta3_asdl_int_seq_new(len, arena);\" % field.name, depth+1)\n            else:\n                self.emit(\"%s = _Ta3_asdl_seq_new(len, arena);\" % field.name, depth+1)\n            self.emit(\"if (%s == NULL) goto failed;\" % field.name, depth+1)\n            self.emit(\"for (i = 0; i < len; i++) {\", depth+1)\n            self.emit(\"%s value;\" % ctype, depth+2)\n            self.emit(\"res = obj2ast_%s(PyList_GET_ITEM(tmp, i), &value, arena);\" %\n                      field.type, depth+2, reflow=False)\n            self.emit(\"if (res != 0) goto failed;\", depth+2)\n            self.emit(\"if (len != PyList_GET_SIZE(tmp)) {\", depth+2)\n            self.emit(\"PyErr_SetString(PyExc_RuntimeError, \\\"%s field \\\\\\\"%s\\\\\\\" \"\n                      \"changed size during iteration\\\");\" %\n                      (name, field.name),\n                      depth+3, reflow=False)\n            self.emit(\"goto failed;\", depth+3)\n            self.emit(\"}\", depth+2)\n            self.emit(\"asdl_seq_SET(%s, i, value);\" % field.name, depth+2)\n            self.emit(\"}\", depth+1)\n        else:\n            self.emit(\"res = obj2ast_%s(tmp, &%s, arena);\" %\n                      (field.type, field.name), depth+1)\n            self.emit(\"if (res != 0) goto failed;\", depth+1)\n        self.emit(\"Py_CLEAR(tmp);\", depth+1)\n        self.emit(\"} else {\", depth)\n        if not field.opt:\n            message = \"required field \\\\\\\"%s\\\\\\\" missing from %s\" % (field.name, name)\n            format = \"PyErr_SetString(PyExc_TypeError, \\\"%s\\\");\"\n            self.emit(format % message, depth+1, reflow=False)\n            self.emit(\"return 1;\", depth+1)\n        else:\n            if self.isNumeric(field):\n                self.emit(\"%s = 0;\" % field.name, depth+1)\n            elif not self.isSimpleType(field):\n                self.emit(\"%s = NULL;\" % field.name, depth+1)\n            else:\n                raise TypeError(\"could not determine the default value for %s\" % field.name)\n        self.emit(\"}\", depth)\n    def visitModule(self, mod):\n        self.emit(\"\"\"\ntypedef struct {\n    PyObject_HEAD\n    PyObject *dict;\n} AST_object;\nstatic void\nast_dealloc(AST_object *self)\n{\n    Py_CLEAR(self->dict);\n    Py_TYPE(self)->tp_free(self);\n}\nstatic int\nast_traverse(AST_object *self, visitproc visit, void *arg)\n{\n    Py_VISIT(self->dict);\n    return 0;\n}\nstatic void\nast_clear(AST_object *self)\n{\n    Py_CLEAR(self->dict);\n}\nstatic int\nast_type_init(PyObject *self, PyObject *args, PyObject *kw)\n{\n    _Py_IDENTIFIER(_fields);\n    Py_ssize_t i, numfields = 0;\n    int res = -1;\n    PyObject *key, *value, *fields;\n    fields = _PyObject_GetAttrId((PyObject*)Py_TYPE(self), &PyId__fields);\n    if (!fields)\n        PyErr_Clear();\n    if (fields) {\n        numfields = PySequence_Size(fields);\n        if (numfields == -1)\n            goto cleanup;\n    }\n    res = 0; /* if no error occurs, this stays 0 to the end */\n    if (PyTuple_GET_SIZE(args) > 0) {\n        if (numfields != PyTuple_GET_SIZE(args)) {\n            PyErr_Format(PyExc_TypeError, \"%.400s constructor takes %s\"\n                         \"%zd positional argument%s\",\n                         Py_TYPE(self)->tp_name,\n                         numfields == 0 ? \"\" : \"either 0 or \",\n                         numfields, numfields == 1 ? \"\" : \"s\");\n            res = -1;\n            goto cleanup;\n        }\n        for (i = 0; i < PyTuple_GET_SIZE(args); i++) {\n            /* cannot be reached when fields is NULL */\n            PyObject *name = PySequence_GetItem(fields, i);\n            if (!name) {\n                res = -1;\n                goto cleanup;\n            }\n            res = PyObject_SetAttr(self, name, PyTuple_GET_ITEM(args, i));\n            Py_DECREF(name);\n            if (res < 0)\n                goto cleanup;\n        }\n    }\n    if (kw) {\n        i = 0;  /* needed by PyDict_Next */\n        while (PyDict_Next(kw, &i, &key, &value)) {\n            res = PyObject_SetAttr(self, key, value);\n            if (res < 0)\n                goto cleanup;\n        }\n    }\n  cleanup:\n    Py_XDECREF(fields);\n    return res;\n}\n/* Pickling support */\nstatic PyObject *\nast_type_reduce(PyObject *self, PyObject *unused)\n{\n    PyObject *res;\n    _Py_IDENTIFIER(__dict__);\n    PyObject *dict = _PyObject_GetAttrId(self, &PyId___dict__);\n    if (dict == NULL) {\n        if (PyErr_ExceptionMatches(PyExc_AttributeError))\n            PyErr_Clear();\n        else\n            return NULL;\n    }\n    if (dict) {\n        res = Py_BuildValue(\"O()O\", Py_TYPE(self), dict);\n        Py_DECREF(dict);\n        return res;\n    }\n    return Py_BuildValue(\"O()\", Py_TYPE(self));\n}\nstatic PyMethodDef ast_type_methods[] = {\n    {\"__reduce__\", ast_type_reduce, METH_NOARGS, NULL},\n    {NULL}\n};\nstatic PyGetSetDef ast_type_getsets[] = {\n    {\"__dict__\", PyObject_GenericGetDict, PyObject_GenericSetDict},\n    {NULL}\n};\nstatic PyTypeObject AST_type = {\n    PyVarObject_HEAD_INIT(NULL, 0)\n    \"_ast3.AST\",\n    sizeof(AST_object),\n    0,\n    (destructor)ast_dealloc, /* tp_dealloc */\n    0,                       /* tp_print */\n    0,                       /* tp_getattr */\n    0,                       /* tp_setattr */\n    0,                       /* tp_reserved */\n    0,                       /* tp_repr */\n    0,                       /* tp_as_number */\n    0,                       /* tp_as_sequence */\n    0,                       /* tp_as_mapping */\n    0,                       /* tp_hash */\n    0,                       /* tp_call */\n    0,                       /* tp_str */\n    PyObject_GenericGetAttr, /* tp_getattro */\n    PyObject_GenericSetAttr, /* tp_setattro */\n    0,                       /* tp_as_buffer */\n    Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE | Py_TPFLAGS_HAVE_GC, /* tp_flags */\n    0,                       /* tp_doc */\n    (traverseproc)ast_traverse, /* tp_traverse */\n    (inquiry)ast_clear,      /* tp_clear */\n    0,                       /* tp_richcompare */\n    0,                       /* tp_weaklistoffset */\n    0,                       /* tp_iter */\n    0,                       /* tp_iternext */\n    ast_type_methods,        /* tp_methods */\n    0,                       /* tp_members */\n    ast_type_getsets,        /* tp_getset */\n    0,                       /* tp_base */\n    0,                       /* tp_dict */\n    0,                       /* tp_descr_get */\n    0,                       /* tp_descr_set */\n    offsetof(AST_object, dict),/* tp_dictoffset */\n    (initproc)ast_type_init, /* tp_init */\n    PyType_GenericAlloc,     /* tp_alloc */\n    PyType_GenericNew,       /* tp_new */\n    PyObject_GC_Del,         /* tp_free */\n};\nstatic PyTypeObject* make_type(char *type, PyTypeObject* base, char**fields, int num_fields)\n{\n    PyObject *fnames, *result;\n    int i;\n    fnames = PyTuple_New(num_fields);\n    if (!fnames) return NULL;\n    for (i = 0; i < num_fields; i++) {\n        PyObject *field = PyUnicode_FromString(fields[i]);\n        if (!field) {\n            Py_DECREF(fnames);\n            return NULL;\n        }\n        PyTuple_SET_ITEM(fnames, i, field);\n    }\n    result = PyObject_CallFunction((PyObject*)&PyType_Type, \"s(O){sOss}\",\n                    type, base, \"_fields\", fnames, \"__module__\", \"_ast3\");\n    Py_DECREF(fnames);\n    return (PyTypeObject*)result;\n}\nstatic int add_attributes(PyTypeObject* type, char**attrs, int num_fields)\n{\n    int i, result;\n    _Py_IDENTIFIER(_attributes);\n    PyObject *s, *l = PyTuple_New(num_fields);\n    if (!l)\n        return 0;\n    for (i = 0; i < num_fields; i++) {\n        s = PyUnicode_FromString(attrs[i]);\n        if (!s) {\n            Py_DECREF(l);\n            return 0;\n        }\n        PyTuple_SET_ITEM(l, i, s);\n    }\n    result = _PyObject_SetAttrId((PyObject*)type, &PyId__attributes, l) >= 0;\n    Py_DECREF(l);\n    return result;\n}\n/* Conversion AST -> Python */\nstatic PyObject* ast2obj_list(asdl_seq *seq, PyObject* (*func)(void*))\n{\n    Py_ssize_t i, n = asdl_seq_LEN(seq);\n    PyObject *result = PyList_New(n);\n    PyObject *value;\n    if (!result)\n        return NULL;\n    for (i = 0; i < n; i++) {\n        value = func(asdl_seq_GET(seq, i));\n        if (!value) {\n            Py_DECREF(result);\n            return NULL;\n        }\n        PyList_SET_ITEM(result, i, value);\n    }\n    return result;\n}\nstatic PyObject* ast2obj_object(void *o)\n{\n    if (!o)\n        o = Py_None;\n    Py_INCREF((PyObject*)o);\n    return (PyObject*)o;\n}\nstatic PyObject* ast2obj_int(long b)\n{\n    return PyLong_FromLong(b);\n}\n/* Conversion Python -> AST */\nstatic int obj2ast_singleton(PyObject *obj, PyObject** out, PyArena* arena)\n{\n    if (obj != Py_None && obj != Py_True && obj != Py_False) {\n        PyErr_SetString(PyExc_ValueError,\n                        \"AST singleton must be True, False, or None\");\n        return 1;\n    }\n    *out = obj;\n    return 0;\n}\nstatic int obj2ast_object(PyObject* obj, PyObject** out, PyArena* arena)\n{\n    if (obj == Py_None)\n        obj = NULL;\n    if (obj) {\n        if (PyArena_AddPyObject(arena, obj) < 0) {\n            *out = NULL;\n            return -1;\n        }\n        Py_INCREF(obj);\n    }\n    *out = obj;\n    return 0;\n}\nstatic int obj2ast_constant(PyObject* obj, PyObject** out, PyArena* arena)\n{\n    if (obj) {\n        if (PyArena_AddPyObject(arena, obj) < 0) {\n            *out = NULL;\n            return -1;\n        }\n        Py_INCREF(obj);\n    }\n    *out = obj;\n    return 0;\n}\nstatic int obj2ast_identifier(PyObject* obj, PyObject** out, PyArena* arena)\n{\n    if (!PyUnicode_CheckExact(obj) && obj != Py_None) {\n        PyErr_SetString(PyExc_TypeError, \"AST identifier must be of type str\");\n        return 1;\n    }\n    return obj2ast_object(obj, out, arena);\n}\nstatic int obj2ast_string(PyObject* obj, PyObject** out, PyArena* arena)\n{\n    if (!PyUnicode_CheckExact(obj) && !PyBytes_CheckExact(obj)) {\n        PyErr_SetString(PyExc_TypeError, \"AST string must be of type str\");\n        return 1;\n    }\n    return obj2ast_object(obj, out, arena);\n}\nstatic int obj2ast_bytes(PyObject* obj, PyObject** out, PyArena* arena)\n{\n    if (!PyBytes_CheckExact(obj)) {\n        PyErr_SetString(PyExc_TypeError, \"AST bytes must be of type bytes\");\n        return 1;\n    }\n    return obj2ast_object(obj, out, arena);\n}\nstatic int obj2ast_int(PyObject* obj, int* out, PyArena* arena)\n{\n    int i;\n    if (!PyLong_Check(obj)) {\n        PyErr_Format(PyExc_ValueError, \"invalid integer value: %R\", obj);\n        return 1;\n    }\n    i = _PyLong_AsInt(obj);\n    if (i == -1 && PyErr_Occurred())\n        return 1;\n    *out = i;\n    return 0;\n}\nstatic int add_ast_fields(void)\n{\n    PyObject *empty_tuple, *d;\n    if (PyType_Ready(&AST_type) < 0)\n        return -1;\n    d = AST_type.tp_dict;\n    empty_tuple = PyTuple_New(0);\n    if (!empty_tuple ||\n        PyDict_SetItemString(d, \"_fields\", empty_tuple) < 0 ||\n        PyDict_SetItemString(d, \"_attributes\", empty_tuple) < 0) {\n        Py_XDECREF(empty_tuple);\n        return -1;\n    }\n    Py_DECREF(empty_tuple);\n    return 0;\n}\nstatic int exists_not_none(PyObject *obj, _Py_Identifier *id)\n{\n    int isnone;\n    PyObject *attr = _PyObject_GetAttrId(obj, id);\n    if (!attr) {\n        PyErr_Clear();\n        return 0;\n    }\n    isnone = attr == Py_None;\n    Py_DECREF(attr);\n    return !isnone;\n}\n\"\"\", 0, reflow=False)\n        self.emit(\"static int init_types(void)\",0)\n        self.emit(\"{\", 0)\n        self.emit(\"static int initialized;\", 1)\n        self.emit(\"if (initialized) return 1;\", 1)\n        self.emit(\"if (add_ast_fields() < 0) return 0;\", 1)\n        for dfn in mod.dfns:\n            self.visit(dfn)\n        self.emit(\"initialized = 1;\", 1)\n        self.emit(\"return 1;\", 1);\n        self.emit(\"}\", 0)\n    def visitProduct(self, prod, name):\n        if prod.fields:\n            fields = name+\"_fields\"\n        else:\n            fields = \"NULL\"\n        self.emit('%s_type = make_type(\"%s\", &AST_type, %s, %d);' %\n                        (name, name, fields, len(prod.fields)), 1)\n        self.emit(\"if (!%s_type) return 0;\" % name, 1)\n        if prod.attributes:\n            self.emit(\"if (!add_attributes(%s_type, %s_attributes, %d)) return 0;\" %\n                            (name, name, len(prod.attributes)), 1)\n        else:\n            self.emit(\"if (!add_attributes(%s_type, NULL, 0)) return 0;\" % name, 1)\n    def func_begin(self, name):\n        ctype = get_c_type(name)\n        self.emit(\"PyObject*\", 0)\n        self.emit(\"ast2obj_%s(void* _o)\" % (name), 0)\n        self.emit(\"{\", 0)\n        self.emit(\"%s o = (%s)_o;\" % (ctype, ctype), 1)\n        self.emit(\"PyObject *result = NULL, *value = NULL;\", 1)\n        self.emit('if (!o) {', 1)\n        self.emit(\"Py_INCREF(Py_None);\", 2)\n        self.emit('return Py_None;', 2)\n        self.emit(\"}\", 1)\n        self.emit('', 0)\n    def func_end(self):\n        self.emit(\"return result;\", 1)\n        self.emit(\"failed:\", 0)\n        self.emit(\"Py_XDECREF(value);\", 1)\n        self.emit(\"Py_XDECREF(result);\", 1)\n        self.emit(\"return NULL;\", 1)\n        self.emit(\"}\", 0)\n        self.emit(\"\", 0)\ndef main(srcfile, dump_module=False):\n    argv0 = sys.argv[0]\n    components = argv0.split(os.sep)\n    argv0 = os.sep.join(components[-2:])\n    auto_gen_msg = common_msg % argv0\n    mod = asdl.parse(srcfile)\n    if dump_module:\n        print('Parsed Module:')\n        print(mod)\n    if not asdl.check(mod):\n        sys.exit(1)\n    if INC_DIR:\n        p = \"%s/%s-ast.h\" % (INC_DIR, mod.name)\n        f = open(p, \"w\")\n        f.write(auto_gen_msg)\n        f.write('\n        c = ChainOfVisitors(TypeDefVisitor(f),\n                            StructVisitor(f),\n                            PrototypeVisitor(f),\n                            )\n        c.visit(mod)\n        f.write(\"PyObject* Ta3AST_mod2obj(mod_ty t);\\n\")\n        f.write(\"mod_ty Ta3AST_obj2mod(PyObject* ast, PyArena* arena, int mode);\\n\")\n        f.write(\"int Ta3AST_Check(PyObject* obj);\\n\")\n        f.close()\n    if SRC_DIR:\n        p = os.path.join(SRC_DIR, str(mod.name) + \"-ast.c\")\n        f = open(p, \"w\")\n        f.write(auto_gen_msg)\n        f.write('\n        f.write('\\n')\n        f.write('\n        f.write('\n        f.write('\\n')\n        f.write(\"static PyTypeObject AST_type;\\n\")\n        v = ChainOfVisitors(\n            PyTypesDeclareVisitor(f),\n            PyTypesVisitor(f),\n            Obj2ModPrototypeVisitor(f),\n            FunctionVisitor(f),\n            ObjVisitor(f),\n            Obj2ModVisitor(f),\n            ASTModuleVisitor(f),\n            PartingShots(f),\n            )\n        v.visit(mod)\n        f.close()\ndef parse(source, filename='<unknown>', mode='exec', feature_version=LATEST_MINOR_VERSION):\n    \"\"\"\n    Parse the source into an AST node including type comments.\n    Similar to compile(source, filename, mode, PyCF_ONLY_AST).\n    Set feature_version to limit the syntax parsed to that minor version of\n    Python 3.  For example, feature_version=5 will prevent new syntax features\n    from Python 3.6+ from being used, such as fstrings.  Currently only\n    fully supported for Python 3.5+ with partial support for Python 3.4.\n    So, feature_version=3 or less are all equivalent to feature_version=4.\n    When feature_version=4, the parser will forbid the use of the async/await\n    keywords and the '@' operator, but will not forbid the use of PEP 448\n    additional unpacking generalizations, which were also added in Python 3.5.\n    \"\"\"\n    return _ast3._parse(source, filename, mode, feature_version)\n_NUM_TYPES = (int, float, complex)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2019-19275",
        "description": "[{'lang': 'en', 'value': 'typed_ast 1.3.0 and 1.3.1 has an ast_for_arguments out-of-bounds read. An attacker with the ability to cause a Python interpreter to parse Python source (but not necessarily execute it) may be able to crash the interpreter process. This could be a concern, for example, in a web-based service that parses (but does not execute) Python code. (This issue also affected certain Python 3.8.0-alpha prereleases.)'}]",
        "cwe_number": 125
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-139",
      "code": "    def test_recovery_flow(self):\n        \"\"\"Test that recovery flow is linked correctly\"\"\"\n        flow = create_test_flow()\n        self.stage.recovery_flow = flow\n        self.stage.save()\n        FlowStageBinding.objects.create(\n            target=flow,\n            stage=self.stage,\n            order=0,\n        )\n        response = self.client.get(\n            reverse(\"authentik_api:flow-executor\", kwargs={\"flow_slug\": self.flow.slug}),\n        )\n        self.assertStageResponse(\n            response,\n            self.flow,\n            component=\"ak-stage-identification\",\n            user_fields=[\"email\"],\n            password_fields=False,\n            recovery_url=reverse(\n                \"authentik_core:if-flow\",\n                kwargs={\"flow_slug\": flow.slug},\n            ),\n            show_source_labels=False,\n            primary_action=\"Log in\",\n            sources=[\n                {\n                    \"challenge\": {\n                        \"component\": \"xak-flow-redirect\",\n                        \"to\": \"/source/oauth/login/test/\",\n                        \"type\": ChallengeTypes.REDIRECT.value,\n                    },\n                    \"icon_url\": \"/static/authentik/sources/default.svg\",\n                    \"name\": \"test\",\n                }\n            ],\n        )",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-39522",
        "description": "[{'lang': 'en', 'value': \"goauthentik is an open-source Identity Provider. In affected versions using a recovery flow with an identification stage an attacker is able to determine if a username exists. Only setups configured with a recovery flow are impacted by this. Anyone with a user account on a system with the recovery flow described above is susceptible to having their username/email revealed as existing. An attacker can easily enumerate and check users' existence using the recovery flow, as a clear message is shown when a user doesn't exist. Depending on configuration this can either be done by username, email, or both. This issue has been addressed in versions 2023.5.6 and 2023.6.2. Users are advised to upgrade. There are no known workarounds for this issue.\"}]",
        "cwe_number": 203
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-140",
      "code": "    def __init__(\n        self, headers: Headers, stream: typing.AsyncGenerator[bytes, None]\n    ) -> None:\n        assert (\n            multipart is not None\n        ), \"The `python-multipart` library must be installed to use form parsing.\"\n        self.headers = headers\n        self.stream = stream\n        self.messages: typing.List[typing.Tuple[MultiPartMessage, bytes]] = []\n    def on_part_begin(self) -> None:\n        message = (MultiPartMessage.PART_BEGIN, b\"\")\n        self.messages.append(message)\n    def on_part_data(self, data: bytes, start: int, end: int) -> None:\n        message = (MultiPartMessage.PART_DATA, data[start:end])\n        self.messages.append(message)\n    def on_part_end(self) -> None:\n        message = (MultiPartMessage.PART_END, b\"\")\n        self.messages.append(message)\n    def on_header_field(self, data: bytes, start: int, end: int) -> None:\n        message = (MultiPartMessage.HEADER_FIELD, data[start:end])\n        self.messages.append(message)\n    def on_header_value(self, data: bytes, start: int, end: int) -> None:\n        message = (MultiPartMessage.HEADER_VALUE, data[start:end])\n        self.messages.append(message)\n    def on_header_end(self) -> None:\n        message = (MultiPartMessage.HEADER_END, b\"\")\n        self.messages.append(message)\n    def on_headers_finished(self) -> None:\n        message = (MultiPartMessage.HEADERS_FINISHED, b\"\")\n        self.messages.append(message)\n    def on_end(self) -> None:\n        message = (MultiPartMessage.END, b\"\")\n        self.messages.append(message)\n    async def parse(self) -> FormData:\n        _, params = parse_options_header(self.headers[\"Content-Type\"])\n        charset = params.get(b\"charset\", \"utf-8\")\n        if type(charset) == bytes:\n            charset = charset.decode(\"latin-1\")\n        try:\n            boundary = params[b\"boundary\"]\n        except KeyError:\n            raise MultiPartException(\"Missing boundary in multipart.\")\n        callbacks = {\n            \"on_part_begin\": self.on_part_begin,\n            \"on_part_data\": self.on_part_data,\n            \"on_part_end\": self.on_part_end,\n            \"on_header_field\": self.on_header_field,\n            \"on_header_value\": self.on_header_value,\n            \"on_header_end\": self.on_header_end,\n            \"on_headers_finished\": self.on_headers_finished,\n            \"on_end\": self.on_end,\n        }\n        parser = multipart.MultipartParser(boundary, callbacks)\n        header_field = b\"\"\n        header_value = b\"\"\n        content_disposition = None\n        field_name = \"\"\n        data = b\"\"\n        file: typing.Optional[UploadFile] = None\n        items: typing.List[typing.Tuple[str, typing.Union[str, UploadFile]]] = []\n        item_headers: typing.List[typing.Tuple[bytes, bytes]] = []\n        async for chunk in self.stream:\n            parser.write(chunk)\n            messages = list(self.messages)\n            self.messages.clear()\n            for message_type, message_bytes in messages:\n                if message_type == MultiPartMessage.PART_BEGIN:\n                    content_disposition = None\n                    data = b\"\"\n                    item_headers = []\n                elif message_type == MultiPartMessage.HEADER_FIELD:\n                    header_field += message_bytes\n                elif message_type == MultiPartMessage.HEADER_VALUE:\n                    header_value += message_bytes\n                elif message_type == MultiPartMessage.HEADER_END:\n                    field = header_field.lower()\n                    if field == b\"content-disposition\":\n                        content_disposition = header_value\n                    item_headers.append((field, header_value))\n                    header_field = b\"\"\n                    header_value = b\"\"\n                elif message_type == MultiPartMessage.HEADERS_FINISHED:\n                    disposition, options = parse_options_header(content_disposition)\n                    try:\n                        field_name = _user_safe_decode(options[b\"name\"], charset)\n                    except KeyError:\n                        raise MultiPartException(\n                            'The Content-Disposition header field \"name\" must be '\n                            \"provided.\"\n                        )\n                    if b\"filename\" in options:\n                        filename = _user_safe_decode(options[b\"filename\"], charset)\n                        tempfile = SpooledTemporaryFile(max_size=self.max_file_size)\n                        file = UploadFile(\n                            file=tempfile,\n                            size=0,\n                            filename=filename,\n                            headers=Headers(raw=item_headers),\n                        )\n                    else:\n                        file = None\n                elif message_type == MultiPartMessage.PART_DATA:\n                    if file is None:\n                        data += message_bytes\n                    else:\n                        await file.write(message_bytes)\n                elif message_type == MultiPartMessage.PART_END:\n                    if file is None:\n                        items.append((field_name, _user_safe_decode(data, charset)))\n                    else:\n                        await file.seek(0)\n                        items.append((field_name, file))\n        parser.finalize()\n        return FormData(items)\n    async def _get_form(self) -> FormData:\n        if self._form is None:\n            assert (\n                parse_options_header is not None\n            ), \"The `python-multipart` library must be installed to use form parsing.\"\n            content_type_header = self.headers.get(\"Content-Type\")\n            content_type: bytes\n            content_type, _ = parse_options_header(content_type_header)\n            if content_type == b\"multipart/form-data\":\n                try:\n                    multipart_parser = MultiPartParser(self.headers, self.stream())\n                    self._form = await multipart_parser.parse()\n                except MultiPartException as exc:\n                    if \"app\" in self.scope:\n                        raise HTTPException(status_code=400, detail=exc.message)\n                    raise exc\n            elif content_type == b\"application/x-www-form-urlencoded\":\n                form_parser = FormParser(self.headers, self.stream())\n                self._form = await form_parser.parse()\n            else:\n                self._form = FormData()\n        return self._form\n    def form(self) -> AwaitableOrContextManager[FormData]:\n        return AwaitableOrContextManagerWrapper(self._get_form())",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-30798",
        "description": "[{'lang': 'en', 'value': \"There MultipartParser usage in Encode's Starlette python framework before versions 0.25.0 allows an unauthenticated and remote attacker to specify any number of form fields or files which can cause excessive memory usage resulting in denial of service of the HTTP service.\"}]",
        "cwe_number": 400
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-141",
      "code": "    async def _has_watch_regex_match(self, text: str) -> Tuple[Union[bool, re.Match], Optional[str]]:\n        \"\"\"\n        Return True if `text` matches any regex from `word_watchlist` or `token_watchlist` configs.\n        `word_watchlist`'s patterns are placed between word boundaries while `token_watchlist` is\n        matched as-is. Spoilers are expanded, if any, and URLs are ignored.\n        Second return value is a reason written to database about blacklist entry (can be None).\n        \"\"\"\n        if SPOILER_RE.search(text):\n            text = self._expand_spoilers(text)\n        text = self.clean_input(text)\n        if URL_RE.search(text):\n            return False, None\n        watchlist_patterns = self._get_filterlist_items('filter_token', allowed=False)\n        for pattern in watchlist_patterns:\n            match = re.search(pattern, text, flags=re.IGNORECASE)\n            if match:\n                return match, self._get_filterlist_value('filter_token', pattern, allowed=False)['comment']\n        return False, None",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-41250",
        "description": "[{'lang': 'en', 'value': 'Python discord bot is the community bot for the Python Discord community. In affected versions when a non-blacklisted URL and an otherwise triggering filter token is included in the same message the token filter does not trigger. This means that by including any non-blacklisted URL moderation filters can be bypassed. This issue has been resolved in commit 67390298852513d13e0213870e50fb3cff1424e0'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-142",
      "code": "    def visit_Call(self, node):\n        \"\"\" A couple function calls are supported: bson's ObjectId() and\n        datetime().\n        \"\"\"\n        if isinstance(node.func, ast.Name):\n            expr = None\n            if node.func.id == 'ObjectId':\n                expr = \"('\" + node.args[0].s + \"')\"\n            elif node.func.id == 'datetime':\n                values = []\n                for arg in node.args:\n                    values.append(str(arg.n))\n                expr = \"(\" + \", \".join(values) + \")\"\n            if expr:\n                self.current_value = eval(node.func.id + expr)\n    def visit_Attribute(self, node):\n        \"\"\" Attribute handler ('Contact.Id').\n        \"\"\"\n        self.visit(node.value)\n        self.current_value += \".\" + node.attr",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2018-8097",
        "description": "[{'lang': 'en', 'value': 'io/mongo/parser.py in Eve (aka pyeve) before 0.7.5 allows remote attackers to execute arbitrary code via Code Injection in the where parameter.'}]",
        "cwe_number": 94
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-143",
      "code": "def save_config_state(name):\n    current_config_state = config_states.get_config()\n    if not name:\n        name = \"Config\"\n    current_config_state[\"name\"] = name\n    timestamp = datetime.now().strftime('%Y_%m_%d-%H_%M_%S')\n    filename = os.path.join(config_states_dir, f\"{timestamp}_{name}.json\")\n    print(f\"Saving backup of webui/extension state to {filename}.\")\n    with open(filename, \"w\", encoding=\"utf-8\") as f:\n        json.dump(current_config_state, f, indent=4, ensure_ascii=False)\n    config_states.list_config_states()\n    new_value = next(iter(config_states.all_config_states.keys()), \"Current\")\n    new_choices = [\"Current\"] + list(config_states.all_config_states.keys())\n    return gr.Dropdown.update(value=new_value, choices=new_choices), f\"<span>Saved current webui/extension state to \\\"{filename}\\\"</span>\"",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-31462",
        "description": "[{'lang': 'en', 'value': 'stable-diffusion-webui is a web interface for Stable Diffusion, implemented using Gradio library. Stable-diffusion-webui 1.7.0 is vulnerable to a limited file write affecting Windows systems. The create_ui method (Backup/Restore tab) in modules/ui_extensions.py takes user input into the config_save_name variable on line 653. This user input is later used in the save_config_state method and used to create a file path on line 65, which is afterwards opened for writing on line 67, which leads to a limited file write exploitable on Windows systems. This issue may lead to limited file write. It allows for writing json files anywhere on the server where the web server has access.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-144",
      "code": "def parse_options_header(value):\n    \"\"\"\n    Parses a Content-Type header into a value in the following format:\n        (content_type, {parameters})\n    \"\"\"\n    if not value:\n        return (b'', {})\n    if isinstance(value, str):\n        value = value.encode('latin-1')\n    if b';' not in value:\n        return (value.lower().strip(), {})\n    ctype, rest = value.split(b';', 1)\n    options = {}\n    for match in OPTION_RE.finditer(rest):\n        key = match.group(1).lower()\n        value = match.group(2)\n        if value[0] == QUOTE and value[-1] == QUOTE:\n            value = value[1:-1]\n            value = value.replace(b'\\\\\\\\', b'\\\\').replace(b'\\\\\"', b'\"')\n        if key == b'filename':\n            if value[1:3] == b':\\\\' or value[:2] == b'\\\\\\\\':\n                value = value.split(b'\\\\')[-1]\n        options[key] = value\n    return ctype, options",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-24762",
        "description": "[{'lang': 'en', 'value': \"`python-multipart` is a streaming multipart parser for Python. When using form data, `python-multipart` uses a Regular Expression to parse the HTTP `Content-Type` header, including options. An attacker could send a custom-made `Content-Type` option that is very difficult for the RegEx to process, consuming CPU resources and stalling indefinitely (minutes or more) while holding the main event loop. This means that process can't handle any more requests, leading to regular expression denial of service. This vulnerability has been patched in version 0.0.7.\"}]",
        "cwe_number": 1333
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-145",
      "code": "def invoice_edit_page_get(request, invoice_id):\n    try:\n        invoice = Invoice.objects.get(id=invoice_id)\n    except Invoice.DoesNotExist:\n        return JsonResponse({\"message\": \"Invoice not found\"}, status=404)\n    data_to_populate = invoice_get_existing_data(invoice)\n    return render(request, \"pages/invoices/edit/edit.html\", data_to_populate)\ndef edit_invoice(request: HtmxHttpRequest, invoice_id):\n    try:\n        invoice = Invoice.objects.get(id=invoice_id)\n    except Invoice.DoesNotExist:\n        return JsonResponse({\"message\": \"Invoice not found\"}, status=404)\n    if request.user.logged_in_as_team and request.user.logged_in_as_team != invoice.organization:\n        return JsonResponse(\n            {\"message\": \"You do not have permission to edit this invoice\"},\n            status=403,\n        )\n    elif request.user != invoice.user:\n        return JsonResponse(\n            {\"message\": \"You do not have permission to edit this invoice\"},\n            status=403,\n        )\n    attributes_to_updates = {\n        \"date_due\": datetime.strptime(request.POST.get(\"date_due\"), \"%Y-%m-%d\").date(),\n        \"date_issued\": request.POST.get(\"date_issued\"),\n        \"self_name\": request.POST.get(\"from_name\"),\n        \"self_company\": request.POST.get(\"from_company\"),\n        \"self_address\": request.POST.get(\"from_address\"),\n        \"self_city\": request.POST.get(\"from_city\"),\n        \"self_county\": request.POST.get(\"from_county\"),\n        \"self_country\": request.POST.get(\"from_country\"),\n        \"notes\": request.POST.get(\"notes\"),\n        \"invoice_number\": request.POST.get(\"invoice_number\"),\n        \"vat_number\": request.POST.get(\"vat_number\"),\n        \"reference\": request.POST.get(\"reference\"),\n        \"sort_code\": request.POST.get(\"sort_code\"),\n        \"account_number\": request.POST.get(\"account_number\"),\n        \"account_holder_name\": request.POST.get(\"account_holder_name\"),\n    }\n    client_to_id = request.POST.get(\"selected_client\")\n    try:\n        client_to_obj = Client.objects.get(id=client_to_id, user=request.user)\n    except (Client.DoesNotExist, ValueError):\n        client_to_obj = None\n    if client_to_obj:\n        invoice.client_to = client_to_obj\n    else:\n        attributes_to_updates.update(\n            {\n                \"client_name\": request.POST.get(\"to_name\"),\n                \"client_company\": request.POST.get(\"to_company\"),\n                \"client_address\": request.POST.get(\"to_address\"),\n                \"client_city\": request.POST.get(\"to_city\"),\n                \"client_county\": request.POST.get(\"to_county\"),\n                \"client_country\": request.POST.get(\"to_country\"),\n            }\n        )\n    for column_name, new_value in attributes_to_updates.items():\n        setattr(invoice, column_name, new_value)\n    invoice_items = [\n        InvoiceItem.objects.create(name=row[0], description=row[1], hours=row[2], price_per_hour=row[3])\n        for row in zip(\n            request.POST.getlist(\"service_name[]\"),\n            request.POST.getlist(\"service_description[]\"),\n            request.POST.getlist(\"hours[]\"),\n            request.POST.getlist(\"price_per_hour[]\"),\n        )\n    ]\n    if invoice_items:\n        invoice.items.set(invoice_items)\n    invoice.save()\n    messages.success(request, \"Invoice edited\")\n    if request.htmx:\n        return render(request, \"base/toasts.html\")\n    return invoice_edit_page_get(request, invoice_id)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-37889",
        "description": "[{'lang': 'en', 'value': 'MyFinances is a web application for managing finances. MyFinances has a way to access other customer invoices while signed in as a user. This method allows an actor to access PII and financial information from another account. The vulnerability is fixed in 0.4.6.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-146",
      "code": "def event_from_pdu_json(pdu_json, outlier=False):\n    \"\"\"Construct a FrozenEvent from an event json received over federation\n    Args:\n        pdu_json (object): pdu as received over federation\n        outlier (bool): True to mark this event as an outlier\n    Returns:\n        FrozenEvent\n    Raises:\n        SynapseError: if the pdu is missing required fields\n    \"\"\"\n    assert_params_in_request(pdu_json, ('event_id', 'type'))\n    event = FrozenEvent(\n        pdu_json\n    )\n    event.internal_metadata.outlier = outlier\n    return event",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2018-10657",
        "description": "[{'lang': 'en', 'value': 'Matrix Synapse before 0.28.1 is prone to a denial of service flaw where malicious events injected with depth = 2^63 - 1 render rooms unusable, related to federation/federation_base.py and handlers/message.py, as exploited in the wild in April 2018.'}]",
        "cwe_number": 20
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-147",
      "code": "    def create_app(\n        blocks: gradio.Blocks, app_kwargs: Dict[str, Any] | None = None\n    ) -> App:\n        app_kwargs = app_kwargs or {}\n        app_kwargs.setdefault(\"default_response_class\", ORJSONResponse)\n        app = App(**app_kwargs)\n        app.configure_app(blocks)\n        if not wasm_utils.IS_WASM:\n            app.add_middleware(\n                CORSMiddleware,\n                allow_origins=[\"*\"],\n                allow_methods=[\"*\"],\n                allow_headers=[\"*\"],\n            )\n        @app.get(\"/user\")\n        @app.get(\"/user/\")\n        def get_current_user(request: fastapi.Request) -> Optional[str]:\n            token = request.cookies.get(\n                f\"access-token-{app.cookie_id}\"\n            ) or request.cookies.get(f\"access-token-unsecure-{app.cookie_id}\")\n            return app.tokens.get(token)\n        @app.get(\"/login_check\")\n        @app.get(\"/login_check/\")\n        def login_check(user: str = Depends(get_current_user)):\n            if app.auth is None or user is not None:\n                return\n            raise HTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Not authenticated\"\n            )\n        @app.get(\"/token\")\n        @app.get(\"/token/\")\n        def get_token(request: fastapi.Request) -> dict:\n            token = request.cookies.get(f\"access-token-{app.cookie_id}\")\n            return {\"token\": token, \"user\": app.tokens.get(token)}\n        @app.get(\"/app_id\")\n        @app.get(\"/app_id/\")\n        def app_id(request: fastapi.Request) -> dict:\n            return {\"app_id\": app.get_blocks().app_id}\n        @app.get(\"/dev/reload\", dependencies=[Depends(login_check)])\n        async def notify_changes(\n            request: fastapi.Request,\n        ):\n            async def reload_checker(request: fastapi.Request):\n                heartbeat_rate = 15\n                check_rate = 0.05\n                last_heartbeat = time.perf_counter()\n                while True:\n                    if await request.is_disconnected():\n                        return\n                    if app.change_event and app.change_event.is_set():\n                        app.change_event.clear()\n                        yield \"\"\"data: CHANGE\\n\\n\"\"\"\n                    await asyncio.sleep(check_rate)\n                    if time.perf_counter() - last_heartbeat > heartbeat_rate:\n                        yield \"\"\"data: HEARTBEAT\\n\\n\"\"\"\n                        last_heartbeat = time.time()\n            return StreamingResponse(\n                reload_checker(request),\n                media_type=\"text/event-stream\",\n            )\n        @app.post(\"/login\")\n        @app.post(\"/login/\")\n        def login(form_data: OAuth2PasswordRequestForm = Depends()):\n            username, password = form_data.username.strip(), form_data.password\n            if app.auth is None:\n                return RedirectResponse(url=\"/\", status_code=status.HTTP_302_FOUND)\n            if (\n                not callable(app.auth)\n                and username in app.auth\n                and app.auth[username] == password\n            ) or (callable(app.auth) and app.auth.__call__(username, password)):\n                token = secrets.token_urlsafe(16)\n                app.tokens[token] = username\n                response = JSONResponse(content={\"success\": True})\n                response.set_cookie(\n                    key=f\"access-token-{app.cookie_id}\",\n                    value=token,\n                    httponly=True,\n                    samesite=\"none\",\n                    secure=True,\n                )\n                response.set_cookie(\n                    key=f\"access-token-unsecure-{app.cookie_id}\",\n                    value=token,\n                    httponly=True,\n                )\n                return response\n            else:\n                raise HTTPException(status_code=400, detail=\"Incorrect credentials.\")\n        if app.blocks is not None and app.blocks.expects_oauth:\n            attach_oauth(app)\n        @app.head(\"/\", response_class=HTMLResponse)\n        @app.get(\"/\", response_class=HTMLResponse)\n        def main(request: fastapi.Request, user: str = Depends(get_current_user)):\n            mimetypes.add_type(\"application/javascript\", \".js\")\n            blocks = app.get_blocks()\n            root_path = (\n                request.scope.get(\"root_path\")\n                or request.headers.get(\"X-Direct-Url\")\n                or \"\"\n            )\n            if app.auth is None or user is not None:\n                config = app.get_blocks().config\n                config[\"root\"] = route_utils.strip_url(root_path)\n            else:\n                config = {\n                    \"auth_required\": True,\n                    \"auth_message\": blocks.auth_message,\n                    \"space_id\": app.get_blocks().space_id,\n                    \"root\": route_utils.strip_url(root_path),\n                }\n            try:\n                template = (\n                    \"frontend/share.html\" if blocks.share else \"frontend/index.html\"\n                )\n                return templates.TemplateResponse(\n                    template,\n                    {\"request\": request, \"config\": config},\n                )\n            except TemplateNotFound as err:\n                if blocks.share:\n                    raise ValueError(\n                        \"Did you install Gradio from source files? Share mode only \"\n                        \"works when Gradio is installed through the pip package.\"\n                    ) from err\n                else:\n                    raise ValueError(\n                        \"Did you install Gradio from source files? You need to build \"\n                        \"the frontend by running /scripts/build_frontend.sh\"\n                    ) from err\n        @app.get(\"/info/\", dependencies=[Depends(login_check)])\n        @app.get(\"/info\", dependencies=[Depends(login_check)])\n        def api_info(serialize: bool = True):\n            return app.get_blocks().get_api_info()\n        @app.get(\"/config/\", dependencies=[Depends(login_check)])\n        @app.get(\"/config\", dependencies=[Depends(login_check)])\n        def get_config(request: fastapi.Request):\n            root_path = (\n                request.scope.get(\"root_path\")\n                or request.headers.get(\"X-Direct-Url\")\n                or \"\"\n            )\n            config = app.get_blocks().config\n            config[\"root\"] = route_utils.strip_url(root_path)\n            return config\n        @app.get(\"/static/{path:path}\")\n        def static_resource(path: str):\n            static_file = safe_join(STATIC_PATH_LIB, path)\n            return FileResponse(static_file)\n        @app.get(\"/custom_component/{id}/{type}/{file_name}\")\n        def custom_component_path(id: str, type: str, file_name: str):\n            config = app.get_blocks().config\n            components = config[\"components\"]\n            location = next(\n                (item for item in components if item[\"component_class_id\"] == id), None\n            )\n            if location is None:\n                raise HTTPException(status_code=404, detail=\"Component not found.\")\n            component_instance = app.get_blocks().get_component(location[\"id\"])\n            module_name = component_instance.__class__.__module__\n            module_path = sys.modules[module_name].__file__\n            if module_path is None or component_instance is None:\n                raise HTTPException(status_code=404, detail=\"Component not found.\")\n            return FileResponse(\n                safe_join(\n                    str(Path(module_path).parent),\n                    f\"{component_instance.__class__.TEMPLATE_DIR}/{type}/{file_name}\",\n                )\n            )\n        @app.get(\"/assets/{path:path}\")\n        def build_resource(path: str):\n            build_file = safe_join(BUILD_PATH_LIB, path)\n            return FileResponse(build_file)\n        @app.get(\"/favicon.ico\")\n        async def favicon():\n            blocks = app.get_blocks()\n            if blocks.favicon_path is None:\n                return static_resource(\"img/logo.svg\")\n            else:\n                return FileResponse(blocks.favicon_path)\n        @app.head(\"/proxy={url_path:path}\", dependencies=[Depends(login_check)])\n        @app.get(\"/proxy={url_path:path}\", dependencies=[Depends(login_check)])\n        async def reverse_proxy(url_path: str):\n            try:\n                rp_req = app.build_proxy_request(url_path)\n            except PermissionError as err:\n                raise HTTPException(status_code=400, detail=str(err)) from err\n            rp_resp = await client.send(rp_req, stream=True)\n            return StreamingResponse(\n                rp_resp.aiter_raw(),\n                status_code=rp_resp.status_code,\n                headers=rp_resp.headers,\n                background=BackgroundTask(rp_resp.aclose),\n            )\n        @app.head(\"/file={path_or_url:path}\", dependencies=[Depends(login_check)])\n        @app.get(\"/file={path_or_url:path}\", dependencies=[Depends(login_check)])\n        async def file(path_or_url: str, request: fastapi.Request):\n            blocks = app.get_blocks()\n            if utils.validate_url(path_or_url):\n                return RedirectResponse(\n                    url=path_or_url, status_code=status.HTTP_302_FOUND\n                )\n            abs_path = utils.abspath(path_or_url)\n            in_blocklist = any(\n                utils.is_in_or_equal(abs_path, blocked_path)\n                for blocked_path in blocks.blocked_paths\n            )\n            is_dir = abs_path.is_dir()\n            if in_blocklist or is_dir:\n                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")\n            created_by_app = str(abs_path) in set().union(*blocks.temp_file_sets)\n            in_allowlist = any(\n                utils.is_in_or_equal(abs_path, allowed_path)\n                for allowed_path in blocks.allowed_paths\n            )\n            was_uploaded = utils.is_in_or_equal(abs_path, app.uploaded_file_dir)\n            is_cached_example = utils.is_in_or_equal(\n                abs_path, utils.abspath(CACHED_FOLDER)\n            )\n            if not (\n                created_by_app or in_allowlist or was_uploaded or is_cached_example\n            ):\n                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")\n            if not abs_path.exists():\n                raise HTTPException(404, f\"File not found: {path_or_url}.\")\n            range_val = request.headers.get(\"Range\", \"\").strip()\n            if range_val.startswith(\"bytes=\") and \"-\" in range_val:\n                range_val = range_val[6:]\n                start, end = range_val.split(\"-\")\n                if start.isnumeric() and end.isnumeric():\n                    start = int(start)\n                    end = int(end)\n                    response = ranged_response.RangedFileResponse(\n                        abs_path,\n                        ranged_response.OpenRange(start, end),\n                        dict(request.headers),\n                        stat_result=os.stat(abs_path),\n                    )\n                    return response\n            return FileResponse(abs_path, headers={\"Accept-Ranges\": \"bytes\"})\n        @app.get(\n            \"/stream/{session_hash}/{run}/{component_id}\",\n            dependencies=[Depends(login_check)],\n        )\n        async def stream(\n            session_hash: str, run: int, component_id: int, request: fastapi.Request\n        ):\n            stream: list = (\n                app.get_blocks()\n                .pending_streams[session_hash]\n                .get(run, {})\n                .get(component_id, None)\n            )\n            if stream is None:\n                raise HTTPException(404, \"Stream not found.\")\n            def stream_wrapper():\n                check_stream_rate = 0.01\n                max_wait_time = 120\n                wait_time = 0\n                while True:\n                    if len(stream) == 0:\n                        if wait_time > max_wait_time:\n                            return\n                        wait_time += check_stream_rate\n                        time.sleep(check_stream_rate)\n                        continue\n                    wait_time = 0\n                    next_stream = stream.pop(0)\n                    if next_stream is None:\n                        return\n                    yield next_stream\n            return StreamingResponse(stream_wrapper())\n        @app.get(\"/file/{path:path}\", dependencies=[Depends(login_check)])\n        async def file_deprecated(path: str, request: fastapi.Request):\n            return await file(path, request)\n        @app.post(\"/reset/\")\n        @app.post(\"/reset\")\n        async def reset_iterator(body: ResetBody):\n            if body.event_id not in app.iterators:\n                return {\"success\": False}\n            async with app.lock:\n                del app.iterators[body.event_id]\n                app.iterators_to_reset.add(body.event_id)\n                await app.get_blocks()._queue.clean_event(body.event_id)\n            return {\"success\": True}\n        @app.post(\"/run/{api_name}\", dependencies=[Depends(login_check)])\n        @app.post(\"/run/{api_name}/\", dependencies=[Depends(login_check)])\n        @app.post(\"/api/{api_name}\", dependencies=[Depends(login_check)])\n        @app.post(\"/api/{api_name}/\", dependencies=[Depends(login_check)])\n        async def predict(\n            api_name: str,\n            body: PredictBody,\n            request: fastapi.Request,\n            username: str = Depends(get_current_user),\n        ):\n            fn_index_inferred = route_utils.infer_fn_index(\n                app=app, api_name=api_name, body=body\n            )\n            if not app.get_blocks().api_open and app.get_blocks().queue_enabled_for_fn(\n                fn_index_inferred\n            ):\n                raise HTTPException(\n                    detail=\"This API endpoint does not accept direct HTTP POST requests. Please join the queue to use this API.\",\n                    status_code=status.HTTP_404_NOT_FOUND,\n                )\n            gr_request = route_utils.compile_gr_request(\n                app,\n                body,\n                fn_index_inferred=fn_index_inferred,\n                username=username,\n                request=request,\n            )\n            try:\n                output = await route_utils.call_process_api(\n                    app=app,\n                    body=body,\n                    gr_request=gr_request,\n                    fn_index_inferred=fn_index_inferred,\n                )\n            except BaseException as error:\n                show_error = app.get_blocks().show_error or isinstance(error, Error)\n                traceback.print_exc()\n                return JSONResponse(\n                    content={\"error\": str(error) if show_error else None},\n                    status_code=500,\n                )\n            return output\n        @app.get(\"/queue/join\", dependencies=[Depends(login_check)])\n        async def queue_join(\n            fn_index: int,\n            session_hash: str,\n            request: fastapi.Request,\n            username: str = Depends(get_current_user),\n            data: Optional[str] = None,\n        ):\n            blocks = app.get_blocks()\n            if blocks._queue.server_app is None:\n                blocks._queue.set_server_app(app)\n            event = Event(session_hash, fn_index, request, username)\n            if data is not None:\n                input_data = json.loads(data)\n                event.data = PredictBody(\n                    session_hash=session_hash,\n                    fn_index=fn_index,\n                    data=input_data,\n                    request=request,\n                )\n            if blocks.dependencies[event.fn_index].get(\"every\", 0):\n                await cancel_tasks({f\"{event.session_hash}_{event.fn_index}\"})\n                await blocks._queue.reset_iterators(event._id)\n                blocks._queue.continuous_tasks.append(event)\n                task = run_coro_in_background(\n                    blocks._queue.process_events, [event], False\n                )\n                set_task_name(task, event.session_hash, event.fn_index, batch=False)\n                app._asyncio_tasks.append(task)\n            else:\n                rank = blocks._queue.push(event)\n                if rank is None:\n                    event.send_message(\"queue_full\", final=True)\n                else:\n                    estimation = blocks._queue.get_estimation()\n                    await blocks._queue.send_estimation(event, estimation, rank)\n            async def sse_stream(request: fastapi.Request):\n                try:\n                    last_heartbeat = time.perf_counter()\n                    while True:\n                        if await request.is_disconnected():\n                            await blocks._queue.clean_event(event)\n                        if not event.alive:\n                            return\n                        heartbeat_rate = 15\n                        check_rate = 0.05\n                        message = None\n                        try:\n                            message = event.message_queue.get_nowait()\n                            if message is None:\n                                return\n                        except EmptyQueue:\n                            await asyncio.sleep(check_rate)\n                            if time.perf_counter() - last_heartbeat > heartbeat_rate:\n                                message = {\"msg\": \"heartbeat\"}\n                                last_heartbeat = time.perf_counter()\n                        if message:\n                            yield f\"data: {json.dumps(message)}\\n\\n\"\n                except asyncio.CancelledError as e:\n                    await blocks._queue.clean_event(event)\n                    raise e\n            return StreamingResponse(\n                sse_stream(request),\n                media_type=\"text/event-stream\",\n            )\n        @app.post(\"/queue/data\", dependencies=[Depends(login_check)])\n        async def queue_data(\n            body: PredictBody,\n            request: fastapi.Request,\n            username: str = Depends(get_current_user),\n        ):\n            blocks = app.get_blocks()\n            blocks._queue.attach_data(body)\n        @app.post(\"/component_server\", dependencies=[Depends(login_check)])\n        @app.post(\"/component_server/\", dependencies=[Depends(login_check)])\n        def component_server(body: ComponentServerBody):\n            state = app.state_holder[body.session_hash]\n            component_id = body.component_id\n            block: Block\n            if component_id in state:\n                block = state[component_id]\n            else:\n                block = app.get_blocks().blocks[component_id]\n            fn = getattr(block, body.fn_name)\n            return fn(body.data)\n        @app.get(\n            \"/queue/status\",\n            dependencies=[Depends(login_check)],\n            response_model=Estimation,\n        )\n        async def get_queue_status():\n            return app.get_blocks()._queue.get_estimation()\n        @app.get(\"/upload_progress\")\n        def get_upload_progress(upload_id: str, request: fastapi.Request):\n            async def sse_stream(request: fastapi.Request):\n                last_heartbeat = time.perf_counter()\n                is_done = False\n                while True:\n                    if await request.is_disconnected():\n                        file_upload_statuses.stop_tracking(upload_id)\n                        return\n                    if is_done:\n                        file_upload_statuses.stop_tracking(upload_id)\n                        return\n                    heartbeat_rate = 15\n                    check_rate = 0.05\n                    message = None\n                    try:\n                        if update := file_upload_statuses.status(upload_id).popleft():\n                            if update.is_done:\n                                message = {\"msg\": \"done\"}\n                                is_done = True\n                            else:\n                                message = {\n                                    \"msg\": \"update\",\n                                    \"orig_name\": update.filename,\n                                    \"chunk_size\": update.chunk_size,\n                                }\n                        else:\n                            await asyncio.sleep(check_rate)\n                            if time.perf_counter() - last_heartbeat > heartbeat_rate:\n                                message = {\"msg\": \"heartbeat\"}\n                                last_heartbeat = time.perf_counter()\n                        if message:\n                            yield f\"data: {json.dumps(message)}\\n\\n\"\n                    except IndexError:\n                        if not file_upload_statuses.is_tracked(upload_id):\n                            return\n                        continue\n            return StreamingResponse(\n                sse_stream(request),\n                media_type=\"text/event-stream\",\n            )\n        @app.post(\"/upload\", dependencies=[Depends(login_check)])\n        async def upload_file(\n            request: fastapi.Request,\n            bg_tasks: BackgroundTasks,\n            upload_id: Optional[str] = None,\n        ):\n            content_type_header = request.headers.get(\"Content-Type\")\n            content_type: bytes\n            content_type, _ = parse_options_header(content_type_header)\n            if content_type != b\"multipart/form-data\":\n                raise HTTPException(status_code=400, detail=\"Invalid content type.\")\n            try:\n                if upload_id:\n                    file_upload_statuses.track(upload_id)\n                multipart_parser = GradioMultiPartParser(\n                    request.headers,\n                    request.stream(),\n                    max_files=1000,\n                    max_fields=1000,\n                    upload_id=upload_id if upload_id else None,\n                    upload_progress=file_upload_statuses if upload_id else None,\n                )\n                form = await multipart_parser.parse()\n            except MultiPartException as exc:\n                raise HTTPException(status_code=400, detail=exc.message) from exc\n            output_files = []\n            files_to_copy = []\n            locations: list[str] = []\n            for temp_file in form.getlist(\"files\"):\n                assert isinstance(temp_file, GradioUploadFile)\n                if temp_file.filename:\n                    file_name = Path(temp_file.filename).name\n                    name = client_utils.strip_invalid_filename_characters(file_name)\n                else:\n                    name = f\"tmp{secrets.token_hex(5)}\"\n                directory = Path(app.uploaded_file_dir) / temp_file.sha.hexdigest()\n                directory.mkdir(exist_ok=True, parents=True)\n                dest = (directory / name).resolve()\n                temp_file.file.close()\n                try:\n                    os.rename(temp_file.file.name, dest)\n                except OSError:\n                    files_to_copy.append(temp_file.file.name)\n                    locations.append(str(dest))\n                output_files.append(dest)\n            if files_to_copy:\n                bg_tasks.add_task(\n                    move_uploaded_files_to_cache, files_to_copy, locations\n                )\n            return output_files\n        @app.on_event(\"startup\")\n        @app.get(\"/startup-events\")\n        async def startup_events():\n            if not app.startup_events_triggered:\n                app.get_blocks().startup_events()\n                app.startup_events_triggered = True\n                return True\n            return False\n        @app.get(\"/theme.css\", response_class=PlainTextResponse)\n        def theme_css():\n            return PlainTextResponse(app.get_blocks().theme_css, media_type=\"text/css\")\n        @app.get(\"/robots.txt\", response_class=PlainTextResponse)\n        def robots_txt():\n            if app.get_blocks().share:\n                return \"User-agent: *\\nDisallow: /\"\n            else:\n                return \"User-agent: *\\nDisallow: \"\n        return app\n    def set_event_trigger(\n        self,\n        targets: Sequence[EventListenerMethod],\n        fn: Callable | None,\n        inputs: Component | list[Component] | set[Component] | None,\n        outputs: Component | list[Component] | None,\n        preprocess: bool = True,\n        postprocess: bool = True,\n        scroll_to_output: bool = False,\n        show_progress: Literal[\"full\", \"minimal\", \"hidden\"] | None = \"full\",\n        api_name: str | None | Literal[False] = None,\n        js: str | None = None,\n        no_target: bool = False,\n        queue: bool | None = None,\n        batch: bool = False,\n        max_batch_size: int = 4,\n        cancels: list[int] | None = None,\n        every: float | None = None,\n        collects_event_data: bool | None = None,\n        trigger_after: int | None = None,\n        trigger_only_on_success: bool = False,\n        trigger_mode: Literal[\"once\", \"multiple\", \"always_last\"] | None = \"once\",\n        concurrency_limit: int | None | Literal[\"default\"] = \"default\",\n        concurrency_id: str | None = None,\n    ) -> tuple[dict[str, Any], int]:\n        \"\"\"\n        Adds an event to the component's dependencies.\n        Parameters:\n            targets: a list of EventListenerMethod objects that define the event trigger\n            fn: Callable function\n            inputs: input list\n            outputs: output list\n            preprocess: whether to run the preprocess methods of components\n            postprocess: whether to run the postprocess methods of components\n            scroll_to_output: whether to scroll to output of dependency on trigger\n            show_progress: whether to show progress animation while running.\n            api_name: defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.\n            js: Optional frontend js method to run before running 'fn'. Input arguments for js method are values of 'inputs' and 'outputs', return should be a list of values for output components\n            no_target: if True, sets \"targets\" to [], used for Blocks \"load\" event\n            queue: If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.\n            batch: whether this function takes in a batch of inputs\n            max_batch_size: the maximum batch size to send to the function\n            cancels: a list of other events to cancel when this event is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method.\n            every: Run this event 'every' number of seconds while the client connection is open. Interpreted in seconds. Queue must be enabled.\n            collects_event_data: whether to collect event data for this event\n            trigger_after: if set, this event will be triggered after 'trigger_after' function index\n            trigger_only_on_success: if True, this event will only be triggered if the previous event was successful (only applies if `trigger_after` is set)\n            trigger_mode: If \"once\" (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to \"multiple\", unlimited submissions are allowed while pending, and \"always_last\" (default for `.change()` event) would allow a second submission after the pending event is complete.\n            concurrency_limit: If set, this this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to \"default\" to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `queue()`, which itself is 1 by default).\n            concurrency_id: If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.\n        Returns: dependency information, dependency index\n        \"\"\"\n        _targets = [\n            (\n                target.block._id if target.block and not no_target else None,\n                target.event_name,\n            )\n            for target in targets\n        ]\n        if isinstance(inputs, set):\n            inputs_as_dict = True\n            inputs = sorted(inputs, key=lambda x: x._id)\n        else:\n            inputs_as_dict = False\n            if inputs is None:\n                inputs = []\n            elif not isinstance(inputs, list):\n                inputs = [inputs]\n        if isinstance(outputs, set):\n            outputs = sorted(outputs, key=lambda x: x._id)\n        else:\n            if outputs is None:\n                outputs = []\n            elif not isinstance(outputs, list):\n                outputs = [outputs]\n        if fn is not None and not cancels:\n            check_function_inputs_match(fn, inputs, inputs_as_dict)\n        if every is not None and every <= 0:\n            raise ValueError(\"Parameter every must be positive or None\")\n        if every and batch:\n            raise ValueError(\n                f\"Cannot run event in a batch and every {every} seconds. \"\n                \"Either batch is True or every is non-zero but not both.\"\n            )\n        if every and fn:\n            fn = get_continuous_fn(fn, every)\n        elif every:\n            raise ValueError(\"Cannot set a value for `every` without a `fn`.\")\n        if _targets[0][1] == \"change\" and trigger_mode is None:\n            trigger_mode = \"always_last\"\n        elif trigger_mode is None:\n            trigger_mode = \"once\"\n        elif trigger_mode not in [\"once\", \"multiple\", \"always_last\"]:\n            raise ValueError(\n                f\"Invalid value for parameter `trigger_mode`: {trigger_mode}. Please choose from: {['once', 'multiple', 'always_last']}\"\n            )\n        _, progress_index, event_data_index = (\n            special_args(fn) if fn else (None, None, None)\n        )\n        self.fns.append(\n            BlockFunction(\n                fn,\n                inputs,\n                outputs,\n                preprocess,\n                postprocess,\n                inputs_as_dict=inputs_as_dict,\n                concurrency_limit=concurrency_limit,\n                concurrency_id=concurrency_id,\n                batch=batch,\n                max_batch_size=max_batch_size,\n                tracks_progress=progress_index is not None,\n            )\n        )\n        if api_name is not None and api_name is not False:\n            api_name_ = utils.append_unique_suffix(\n                api_name, [dep[\"api_name\"] for dep in self.dependencies]\n            )\n            if api_name != api_name_:\n                api_name = api_name_\n        if collects_event_data is None:\n            collects_event_data = event_data_index is not None\n        dependency = {\n            \"targets\": _targets,\n            \"inputs\": [block._id for block in inputs],\n            \"outputs\": [block._id for block in outputs],\n            \"backend_fn\": fn is not None,\n            \"js\": js,\n            \"queue\": False if fn is None else queue,\n            \"api_name\": api_name,\n            \"scroll_to_output\": False if utils.get_space() else scroll_to_output,\n            \"show_progress\": show_progress,\n            \"every\": every,\n            \"batch\": batch,\n            \"max_batch_size\": max_batch_size,\n            \"cancels\": cancels or [],\n            \"types\": {\n                \"continuous\": bool(every),\n                \"generator\": inspect.isgeneratorfunction(fn)\n                or inspect.isasyncgenfunction(fn)\n                or bool(every),\n            },\n            \"collects_event_data\": collects_event_data,\n            \"trigger_after\": trigger_after,\n            \"trigger_only_on_success\": trigger_only_on_success,\n            \"trigger_mode\": trigger_mode,\n        }\n        self.dependencies.append(dependency)\n        return dependency, len(self.dependencies) - 1\n    def render(self):\n        if Context.root_block is not None:\n            if self._id in Context.root_block.blocks:\n                raise DuplicateBlockError(\n                    f\"A block with id: {self._id} has already been rendered in the current Blocks.\"\n                )\n            overlapping_ids = set(Context.root_block.blocks).intersection(self.blocks)\n            for id in overlapping_ids:\n                if not isinstance(self.blocks[id], components.State):\n                    raise DuplicateBlockError(\n                        \"At least one block in this Blocks has already been rendered.\"\n                    )\n            Context.root_block.blocks.update(self.blocks)\n            Context.root_block.fns.extend(self.fns)\n            dependency_offset = len(Context.root_block.dependencies)\n            for i, dependency in enumerate(self.dependencies):\n                api_name = dependency[\"api_name\"]\n                if api_name is not None and api_name is not False:\n                    api_name_ = utils.append_unique_suffix(\n                        api_name,\n                        [dep[\"api_name\"] for dep in Context.root_block.dependencies],\n                    )\n                    if api_name != api_name_:\n                        dependency[\"api_name\"] = api_name_\n                dependency[\"cancels\"] = [\n                    c + dependency_offset for c in dependency[\"cancels\"]\n                ]\n                if dependency.get(\"trigger_after\") is not None:\n                    dependency[\"trigger_after\"] += dependency_offset\n                if dependency[\"cancels\"]:\n                    updated_cancels = [\n                        Context.root_block.dependencies[i]\n                        for i in dependency[\"cancels\"]\n                    ]\n                    new_fn = BlockFunction(\n                        get_cancel_function(updated_cancels)[0],\n                        [],\n                        [],\n                        False,\n                        True,\n                        False,\n                    )\n                    Context.root_block.fns[dependency_offset + i] = new_fn\n                Context.root_block.dependencies.append(dependency)\n            Context.root_block.temp_file_sets.extend(self.temp_file_sets)\n            Context.root_block.proxy_urls.update(self.proxy_urls)\n        if Context.block is not None:\n            Context.block.children.extend(self.children)\n        return self\n    def __init__(\n        self,\n        src: str,\n        hf_token: str | None = None,\n        max_workers: int = 40,\n        serialize: bool = True,\n        output_dir: str | Path = DEFAULT_TEMP_DIR,\n        verbose: bool = True,\n        auth: tuple[str, str] | None = None,\n    ):\n        \"\"\"\n        Parameters:\n            src: Either the name of the Hugging Face Space to load, (e.g. \"abidlabs/whisper-large-v2\") or the full URL (including \"http\" or \"https\") of the hosted Gradio app to load (e.g. \"http://mydomain.com/app\" or \"https://bec81a83-5b5c-471e.gradio.live/\").\n            hf_token: The Hugging Face token to use to access private Spaces. Automatically fetched if you are logged in via the Hugging Face Hub CLI. Obtain from: https://huggingface.co/settings/token\n            max_workers: The maximum number of thread workers that can be used to make requests to the remote Gradio app simultaneously.\n            serialize: Whether the client should serialize the inputs and deserialize the outputs of the remote API. If set to False, the client will pass the inputs and outputs as-is, without serializing/deserializing them. E.g. you if you set this to False, you'd submit an image in base64 format instead of a filepath, and you'd get back an image in base64 format from the remote API instead of a filepath.\n            output_dir: The directory to save files that are downloaded from the remote API. If None, reads from the GRADIO_TEMP_DIR environment variable. Defaults to a temporary directory on your machine.\n            verbose: Whether the client should print statements to the console.\n        \"\"\"\n        self.verbose = verbose\n        self.hf_token = hf_token\n        self.serialize = serialize\n        self.headers = build_hf_headers(\n            token=hf_token,\n            library_name=\"gradio_client\",\n            library_version=utils.__version__,\n        )\n        self.space_id = None\n        self.cookies: dict[str, str] = {}\n        self.output_dir = (\n            str(output_dir) if isinstance(output_dir, Path) else output_dir\n        )\n        if src.startswith(\"http://\") or src.startswith(\"https://\"):\n            _src = src if src.endswith(\"/\") else src + \"/\"\n        else:\n            _src = self._space_name_to_src(src)\n            if _src is None:\n                raise ValueError(\n                    f\"Could not find Space: {src}. If it is a private Space, please provide an hf_token.\"\n                )\n            self.space_id = src\n        self.src = _src\n        state = self._get_space_state()\n        if state == SpaceStage.BUILDING:\n            if self.verbose:\n                print(\"Space is still building. Please wait...\")\n            while self._get_space_state() == SpaceStage.BUILDING:\n                time.sleep(2)\n                pass\n        if state in utils.INVALID_RUNTIME:\n            raise ValueError(\n                f\"The current space is in the invalid state: {state}. \"\n                \"Please contact the owner to fix this.\"\n            )\n        if self.verbose:\n            print(f\"Loaded as API: {self.src} \u2714\")\n        self.api_url = urllib.parse.urljoin(self.src, utils.API_URL)\n        self.sse_url = urllib.parse.urljoin(self.src, utils.SSE_URL)\n        self.sse_data_url = urllib.parse.urljoin(self.src, utils.SSE_DATA_URL)\n        self.ws_url = urllib.parse.urljoin(\n            self.src.replace(\"http\", \"ws\", 1), utils.WS_URL\n        )\n        self.upload_url = urllib.parse.urljoin(self.src, utils.UPLOAD_URL)\n        self.reset_url = urllib.parse.urljoin(self.src, utils.RESET_URL)\n        if auth is not None:\n            self._login(auth)\n        self.config = self._get_config()\n        self.app_version = version.parse(self.config.get(\"version\", \"2.0\"))\n        self._info = self._get_api_info()\n        self.session_hash = str(uuid.uuid4())\n        protocol = self.config.get(\"protocol\")\n        endpoint_class = Endpoint if protocol == \"sse\" else EndpointV3Compatibility\n        self.endpoints = [\n            endpoint_class(self, fn_index, dependency)\n            for fn_index, dependency in enumerate(self.config[\"dependencies\"])\n        ]\n        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)\n        threading.Thread(target=self._telemetry_thread).start()\n    def duplicate(\n        cls,\n        from_id: str,\n        to_id: str | None = None,\n        hf_token: str | None = None,\n        private: bool = True,\n        hardware: Literal[\n            \"cpu-basic\",\n            \"cpu-upgrade\",\n            \"t4-small\",\n            \"t4-medium\",\n            \"a10g-small\",\n            \"a10g-large\",\n            \"a100-large\",\n        ]\n        | SpaceHardware\n        | None = None,\n        secrets: dict[str, str] | None = None,\n        sleep_timeout: int = 5,\n        max_workers: int = 40,\n        verbose: bool = True,\n    ):\n        \"\"\"\n        Duplicates a Hugging Face Space under your account and returns a Client object\n        for the new Space. No duplication is created if the Space already exists in your\n        account (to override this, provide a new name for the new Space using `to_id`).\n        To use this method, you must provide an `hf_token` or be logged in via the Hugging\n        Face Hub CLI.\n        The new Space will be private by default and use the same hardware as the original\n        Space. This can be changed by using the `private` and `hardware` parameters. For\n        hardware upgrades (beyond the basic CPU tier), you may be required to provide\n        billing information on Hugging Face: https://huggingface.co/settings/billing\n        Parameters:\n            from_id: The name of the Hugging Face Space to duplicate in the format \"{username}/{space_id}\", e.g. \"gradio/whisper\".\n            to_id: The name of the new Hugging Face Space to create, e.g. \"abidlabs/whisper-duplicate\". If not provided, the new Space will be named \"{your_HF_username}/{space_id}\".\n            hf_token: The Hugging Face token to use to access private Spaces. Automatically fetched if you are logged in via the Hugging Face Hub CLI. Obtain from: https://huggingface.co/settings/token\n            private: Whether the new Space should be private (True) or public (False). Defaults to True.\n            hardware: The hardware tier to use for the new Space. Defaults to the same hardware tier as the original Space. Options include \"cpu-basic\", \"cpu-upgrade\", \"t4-small\", \"t4-medium\", \"a10g-small\", \"a10g-large\", \"a100-large\", subject to availability.\n            secrets: A dictionary of (secret key, secret value) to pass to the new Space. Defaults to None. Secrets are only used when the Space is duplicated for the first time, and are not updated if the duplicated Space already exists.\n            sleep_timeout: The number of minutes after which the duplicate Space will be puased if no requests are made to it (to minimize billing charges). Defaults to 5 minutes.\n            max_workers: The maximum number of thread workers that can be used to make requests to the remote Gradio app simultaneously.\n            verbose: Whether the client should print statements to the console.\n        Example:\n            import os\n            from gradio_client import Client\n            HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n            client = Client.duplicate(\"abidlabs/whisper\", hf_token=HF_TOKEN)\n            client.predict(\"audio_sample.wav\")\n            >> \"This is a test of the whisper speech recognition model.\"\n        \"\"\"\n        try:\n            original_info = huggingface_hub.get_space_runtime(from_id, token=hf_token)\n        except RepositoryNotFoundError as rnfe:\n            raise ValueError(\n                f\"Could not find Space: {from_id}. If it is a private Space, please provide an `hf_token`.\"\n            ) from rnfe\n        if to_id:\n            if \"/\" in to_id:\n                to_id = to_id.split(\"/\")[1]\n            space_id = huggingface_hub.get_full_repo_name(to_id, token=hf_token)\n        else:\n            space_id = huggingface_hub.get_full_repo_name(\n                from_id.split(\"/\")[1], token=hf_token\n            )\n        try:\n            huggingface_hub.get_space_runtime(space_id, token=hf_token)\n            if verbose:\n                print(\n                    f\"Using your existing Space: {utils.SPACE_URL.format(space_id)} \ud83e\udd17\"\n                )\n            if secrets is not None:\n                warnings.warn(\n                    \"Secrets are only used when the Space is duplicated for the first time, and are not updated if the duplicated Space already exists.\"\n                )\n        except RepositoryNotFoundError:\n            if verbose:\n                print(f\"Creating a duplicate of {from_id} for your own use... \ud83e\udd17\")\n            huggingface_hub.duplicate_space(\n                from_id=from_id,\n                to_id=space_id,\n                token=hf_token,\n                exist_ok=True,\n                private=private,\n            )\n            if secrets is not None:\n                for key, value in secrets.items():\n                    huggingface_hub.add_space_secret(\n                        space_id, key, value, token=hf_token\n                    )\n            if verbose:\n                print(f\"Created new Space: {utils.SPACE_URL.format(space_id)}\")\n        current_info = huggingface_hub.get_space_runtime(space_id, token=hf_token)\n        current_hardware = (\n            current_info.hardware or huggingface_hub.SpaceHardware.CPU_BASIC\n        )\n        hardware = hardware or original_info.hardware\n        if current_hardware != hardware:\n            huggingface_hub.request_space_hardware(space_id, hardware, token=hf_token)\n            print(\n                f\"-------\\nNOTE: this Space uses upgraded hardware: {hardware}... see billing info at https://huggingface.co/settings/billing\\n-------\"\n            )\n        if hardware != huggingface_hub.SpaceHardware.CPU_BASIC:\n            utils.set_space_timeout(\n                space_id, hf_token=hf_token, timeout_in_seconds=sleep_timeout * 60\n            )\n        if verbose:\n            print(\"\")\n        client = cls(\n            space_id, hf_token=hf_token, max_workers=max_workers, verbose=verbose\n        )\n        return client\n    def view_api(\n        self,\n        all_endpoints: bool | None = None,\n        print_info: bool = True,\n        return_format: Literal[\"dict\", \"str\"] | None = None,\n    ) -> dict | str | None:\n        \"\"\"\n        Prints the usage info for the API. If the Gradio app has multiple API endpoints, the usage info for each endpoint will be printed separately. If return_format=\"dict\" the info is returned in dictionary format, as shown in the example below.\n        Parameters:\n            all_endpoints: If True, prints information for both named and unnamed endpoints in the Gradio app. If False, will only print info about named endpoints. If None (default), will print info about named endpoints, unless there aren't any -- in which it will print info about unnamed endpoints.\n            print_info: If True, prints the usage info to the console. If False, does not print the usage info.\n            return_format: If None, nothing is returned. If \"str\", returns the same string that would be printed to the console. If \"dict\", returns the usage info as a dictionary that can be programmatically parsed, and *all endpoints are returned in the dictionary* regardless of the value of `all_endpoints`. The format of the dictionary is in the docstring of this method.\n        Example:\n            from gradio_client import Client\n            client = Client(src=\"gradio/calculator\")\n            client.view_api(return_format=\"dict\")\n            >> {\n                'named_endpoints': {\n                    '/predict': {\n                        'parameters': [\n                            {\n                                'label': 'num1',\n                                'type_python': 'int | float',\n                                'type_description': 'numeric value',\n                                'component': 'Number',\n                                'example_input': '5'\n                            },\n                            {\n                                'label': 'operation',\n                                'type_python': 'str',\n                                'type_description': 'string value',\n                                'component': 'Radio',\n                                'example_input': 'add'\n                            },\n                            {\n                                'label': 'num2',\n                                'type_python': 'int | float',\n                                'type_description': 'numeric value',\n                                'component': 'Number',\n                                'example_input': '5'\n                            },\n                        ],\n                        'returns': [\n                            {\n                                'label': 'output',\n                                'type_python': 'int | float',\n                                'type_description': 'numeric value',\n                                'component': 'Number',\n                            },\n                        ]\n                    },\n                    '/flag': {\n                        'parameters': [\n                            ...\n                            ],\n                        'returns': [\n                            ...\n                            ]\n                        }\n                    }\n                'unnamed_endpoints': {\n                    2: {\n                        'parameters': [\n                            ...\n                            ],\n                        'returns': [\n                            ...\n                            ]\n                        }\n                    }\n                }\n            }\n        \"\"\"\n        num_named_endpoints = len(self._info[\"named_endpoints\"])\n        num_unnamed_endpoints = len(self._info[\"unnamed_endpoints\"])\n        if num_named_endpoints == 0 and all_endpoints is None:\n            all_endpoints = True\n        human_info = \"Client.predict() Usage Info\\n---------------------------\\n\"\n        human_info += f\"Named API endpoints: {num_named_endpoints}\\n\"\n        for api_name, endpoint_info in self._info[\"named_endpoints\"].items():\n            human_info += self._render_endpoints_info(api_name, endpoint_info)\n        if all_endpoints:\n            human_info += f\"\\nUnnamed API endpoints: {num_unnamed_endpoints}\\n\"\n            for fn_index, endpoint_info in self._info[\"unnamed_endpoints\"].items():\n                human_info += self._render_endpoints_info(int(fn_index), endpoint_info)\n        else:\n            if num_unnamed_endpoints > 0:\n                human_info += f\"\\nUnnamed API endpoints: {num_unnamed_endpoints}, to view, run Client.view_api(all_endpoints=True)\\n\"\n        if print_info:\n            print(human_info)\n        if return_format == \"str\":\n            return human_info\n        elif return_format == \"dict\":\n            return self._info\n    async def process_events(self, events: list[Event], batch: bool) -> None:\n        awake_events: list[Event] = []\n        try:\n            for event in events:\n                if not event.data:\n                    self.awaiting_data_events[event._id] = event\n                    client_awake = await event.get_data()\n                    del self.awaiting_data_events[event._id]\n                    if not client_awake:\n                        await self.clean_event(event)\n                        continue\n                event.send_message(\"process_starts\")\n                awake_events.append(event)\n            if not awake_events:\n                return\n            begin_time = time.time()\n            try:\n                response = await self.call_prediction(awake_events, batch)\n                err = None\n            except Exception as e:\n                traceback.print_exc()\n                response = None\n                err = e\n                for event in awake_events:\n                    event.send_message(\n                        \"process_completed\",\n                        {\n                            \"output\": {\n                                \"error\": None\n                                if len(e.args) and e.args[0] is None\n                                else str(e)\n                            },\n                            \"success\": False,\n                        },\n                        final=True,\n                    )\n            if response and response.get(\"is_generating\", False):\n                old_response = response\n                old_err = err\n                while response and response.get(\"is_generating\", False):\n                    old_response = response\n                    old_err = err\n                    for event in awake_events:\n                        event.send_message(\n                            \"process_generating\",\n                            {\n                                \"output\": old_response,\n                                \"success\": old_response is not None,\n                            },\n                        )\n                    awake_events = [event for event in awake_events if event.alive]\n                    if not awake_events:\n                        return\n                    try:\n                        response = await self.call_prediction(awake_events, batch)\n                        err = None\n                    except Exception as e:\n                        response = None\n                        err = e\n                for event in awake_events:\n                    if response is None:\n                        relevant_response = err\n                    else:\n                        relevant_response = old_response or old_err\n                    event.send_message(\n                        \"process_completed\",\n                        {\n                            \"output\": {\"error\": str(relevant_response)}\n                            if isinstance(relevant_response, Exception)\n                            else relevant_response,\n                            \"success\": relevant_response\n                            and not isinstance(relevant_response, Exception),\n                        },\n                        final=True,\n                    )\n            elif response:\n                output = copy.deepcopy(response)\n                for e, event in enumerate(awake_events):\n                    if batch and \"data\" in output:\n                        output[\"data\"] = list(zip(*response.get(\"data\")))[e]\n                    event.send_message(\n                        \"process_completed\",\n                        {\n                            \"output\": output,\n                            \"success\": response is not None,\n                        },\n                        final=True,\n                    )\n            end_time = time.time()\n            if response is not None:\n                self.update_estimation(end_time - begin_time)\n        except Exception as e:\n            traceback.print_exc()\n        finally:\n            try:\n                self.active_jobs[self.active_jobs.index(events)] = None\n            except ValueError:\n                pass\n            for event in events:\n                await self.reset_iterators(event._id)\nasync def stream_sse(\n    client: httpx.AsyncClient,\n    data: dict,\n    hash_data: dict,\n    helper: Communicator,\n    sse_url: str,\n    sse_data_url: str,\n    headers: dict[str, str],\n    cookies: dict[str, str] | None = None,\n) -> dict[str, Any]:\n    try:\n        async with client.stream(\n            \"GET\",\n            sse_url,\n            params=hash_data,\n            cookies=cookies,\n            headers=headers,\n        ) as response:\n            async for line in response.aiter_text():\n                if line.startswith(\"data:\"):\n                    resp = json.loads(line[5:])\n                    with helper.lock:\n                        has_progress = \"progress_data\" in resp\n                        status_update = StatusUpdate(\n                            code=Status.msg_to_status(resp[\"msg\"]),\n                            queue_size=resp.get(\"queue_size\"),\n                            rank=resp.get(\"rank\", None),\n                            success=resp.get(\"success\"),\n                            time=datetime.now(),\n                            eta=resp.get(\"rank_eta\"),\n                            progress_data=ProgressUnit.from_msg(resp[\"progress_data\"])\n                            if has_progress\n                            else None,\n                        )\n                        output = resp.get(\"output\", {}).get(\"data\", [])\n                        if output and status_update.code != Status.FINISHED:\n                            try:\n                                result = helper.prediction_processor(*output)\n                            except Exception as e:\n                                result = [e]\n                            helper.job.outputs.append(result)\n                        helper.job.latest_status = status_update\n                    if resp[\"msg\"] == \"queue_full\":\n                        raise QueueError(\"Queue is full! Please try again.\")\n                    elif resp[\"msg\"] == \"send_data\":\n                        event_id = resp[\"event_id\"]\n                        helper.event_id = event_id\n                        req = await client.post(\n                            sse_data_url,\n                            json={\"event_id\": event_id, **data, **hash_data},\n                            cookies=cookies,\n                            headers=headers,\n                        )\n                        req.raise_for_status()\n                    elif resp[\"msg\"] == \"process_completed\":\n                        return resp[\"output\"]\n                else:\n                    raise ValueError(f\"Unexpected message: {line}\")\n        raise ValueError(\"Did not receive process_completed message.\")\n    except asyncio.CancelledError:\n        raise",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-0964",
        "description": "[{'lang': 'en', 'value': 'A local file include could be remotely triggered in Gradio due to a vulnerable user-supplied JSON value in an API request.'}]",
        "cwe_number": 22
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-148",
      "code": "def prepare_and_start_backend(\n    backend_type: BackendType,\n    product_version: int = None,\n    host: str = \"localhost\",\n    port: int = None,\n    enable_trace: bool = False,\n    log_level: int = 2,\n    api_version: ApiVersions = ApiVersions.LATEST,\n    timeout: int = 150,\n    manifest_path: str = None,\n    logs_folder: str = None,\n    hidden: bool = False,\n) -> \"Modeler\":\n    \"\"\"\n    Start the requested service locally using the ``ProductInstance`` class.\n    When calling this method, a standalone service or product session is started.\n    By default, if an endpoint is specified (by defining `host` and `port` parameters)\n    but the endpoint is not available, the startup will fail. Otherwise, it will try to\n    launch its own service.\n    Parameters\n    ----------\n    product_version: ``int``, optional\n        The product version to be started. Goes from v23.2.1 to\n        the latest. Default is ``None``.\n        If a specific product version is requested but not installed locally,\n        a SystemError will be raised.\n    host: str, optional\n        IP address at which the Geometry service will be deployed. By default,\n        its value will be ``localhost``.\n    port : int, optional\n        Port at which the Geometry service will be deployed. By default, its\n        value will be ``None``.\n    enable_trace : bool, optional\n        Boolean enabling the logs trace on the Geometry service console window.\n        By default its value is ``False``.\n    log_level : int, optional\n        Backend's log level from 0 to 3:\n            0: Chatterbox\n            1: Debug\n            2: Warning\n            3: Error\n        The default is ``2`` (Warning).\n    api_version: ``ApiVersions``, optional\n        The backend's API version to be used at runtime. Goes from API v21 to\n        the latest. Default is ``ApiVersions.LATEST``.\n    timeout : int, optional\n        Timeout for starting the backend startup process. The default is 150.\n    manifest_path : str, optional\n        Used to specify a manifest file path for the ApiServerAddin. This way,\n        it is possible to run an ApiServerAddin from a version an older product\n        version. Only applicable for Ansys Discovery and Ansys SpaceClaim.\n    logs_folder : sets the backend's logs folder path. If nothing is defined,\n        the backend will use its default path.\n    hidden : starts the product hiding its UI. Default is ``False``.\n    Raises\n    ------\n    ConnectionError\n        If the specified endpoint is already in use, a connection error will be raised.\n    SystemError\n        If there is not an Ansys product 23.2 version or later installed\n        or if a specific product's version is requested but not installed locally then\n        a SystemError will be raised.\n    Returns\n    -------\n    Modeler\n        Instance of the Geometry service.\n    \"\"\"\n    from ansys.geometry.core.modeler import Modeler\n    if os.name != \"nt\":\n        raise RuntimeError(\"Method 'prepare_and_start_backend' is only available on Windows.\")\n    port = _check_port_or_get_one(port)\n    installations = get_available_ansys_installations()\n    if product_version != None:\n        _check_version_is_available(product_version, installations)\n    else:\n        product_version = get_latest_ansys_installation()[0]\n        _check_minimal_versions(product_version)\n    args = []\n    env_copy = _get_common_env(\n        host=host,\n        port=port,\n        enable_trace=enable_trace,\n        log_level=log_level,\n        logs_folder=logs_folder,\n    )\n    if backend_type == BackendType.DISCOVERY:\n        args.append(os.path.join(installations[product_version], DISCOVERY_FOLDER, DISCOVERY_EXE))\n        if hidden is True:\n            args.append(BACKEND_DISCOVERY_HIDDEN)\n        args.append(BACKEND_SPACECLAIM_OPTIONS)\n        args.append(\n            BACKEND_ADDIN_MANIFEST_ARGUMENT\n            + _manifest_path_provider(product_version, installations, manifest_path)\n        )\n        env_copy[BACKEND_API_VERSION_VARIABLE] = str(api_version)\n    elif backend_type == BackendType.SPACECLAIM:\n        args.append(os.path.join(installations[product_version], SPACECLAIM_FOLDER, SPACECLAIM_EXE))\n        if hidden is True:\n            args.append(BACKEND_SPACECLAIM_HIDDEN)\n            args.append(BACKEND_SPLASH_OFF)\n        args.append(\n            BACKEND_ADDIN_MANIFEST_ARGUMENT\n            + _manifest_path_provider(product_version, installations, manifest_path)\n        )\n        env_copy[BACKEND_API_VERSION_VARIABLE] = str(api_version)\n        env_copy[BACKEND_SPACECLAIM_HIDDEN_ENVVAR_KEY] = BACKEND_SPACECLAIM_HIDDEN_ENVVAR_VALUE\n    elif backend_type == BackendType.WINDOWS_SERVICE:\n        latest_version = get_latest_ansys_installation()[0]\n        args.append(\n            os.path.join(\n                installations[latest_version], WINDOWS_GEOMETRY_SERVICE_FOLDER, GEOMETRY_SERVICE_EXE\n            )\n        )\n    else:\n        raise RuntimeError(\n            f\"Cannot connect to backend {backend_type.name} using ``prepare_and_start_backend()``\"\n        )\n    LOG.info(f\"Launching ProductInstance for {backend_type.name}\")\n    LOG.debug(f\"Args: {args}\")\n    LOG.debug(f\"Environment variables: {env_copy}\")\n    instance = ProductInstance(_start_program(args, env_copy).pid)\n    LOG.info(\"Waiting for backend to be ready...\")\n    _wait_for_backend(host, port, timeout)\n    return Modeler(\n        host=host, port=port, timeout=timeout, product_instance=instance, backend_type=backend_type\n    )\ndef get_available_port() -> int:\n    \"\"\"\n    Return an available port to be used.\n    Returns\n    -------\n    int\n        The available port.\n    \"\"\"\n    sock = socket.socket()\n    sock.bind((socket.gethostname(), 0))\n    port = sock.getsockname()[1]\n    sock.close()\n    return port\ndef _start_program(args: List[str], local_env: Dict[str, str]) -> subprocess.Popen:\n    \"\"\"\n    Start the program where the path is the first item of the ``args`` array argument.\n    Parameters\n    ----------\n    args : List[str]\n        List of arguments to be passed to the program. The first list's item shall\n        be the program path.\n    local_env : Dict[str,str]\n        Environment variables to be passed to the program.\n    Returns\n    -------\n    subprocess.Popen\n        The subprocess object.\n    \"\"\"\n    return subprocess.Popen(\n        args,\n        stdin=subprocess.DEVNULL,\n        stdout=subprocess.DEVNULL,\n        stderr=subprocess.DEVNULL,\n        env=local_env,\n    )\ndef _check_minimal_versions(latest_installed_version: int) -> None:\n    \"\"\"\n    Check client is compatible with Ansys Products starting from 2023.2.1 version.\n    Check that at least V232 is installed.\n    \"\"\"\n    if abs(latest_installed_version) < 232:\n        msg = (\n            \"PyAnsys Geometry is compatible with Ansys Products from version 23.2.1. \"\n            + \"Please install Ansys products 23.2.1 or later.\"\n        )\n        raise SystemError(msg)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-29189",
        "description": "[{'lang': 'en', 'value': 'PyAnsys Geometry is a Python client library for the Ansys Geometry service and other CAD Ansys products. On file src/ansys/geometry/core/connection/product_instance.py, upon calling this method _start_program directly, users could exploit its usage to perform malicious operations on the current machine where the script is ran. This vulnerability is fixed in 0.3.3 and 0.4.12.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-149",
      "code": "    def list_zones(self):\n        pipe = subprocess.Popen([self.zoneadm_cmd, 'list', '-ip'],\n                             cwd=self.runner.basedir,\n                             stdin=subprocess.PIPE,\n                             stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        zones = []\n        for l in pipe.stdout.readlines():\n          s = l.split(':')\n          if s[1] != 'global':\n            zones.append(s[1])\n        return zones\n    def _generate_cmd(self, executable, cmd):\n        if executable:\n            local_cmd = [self.zlogin_cmd, self.zone, executable, cmd]\n        else:\n            local_cmd = '%s \"%s\" %s' % (self.zlogin_cmd, self.zone, cmd)\n        return local_cmd\n    def exec_command(self, cmd, tmp_path, become_user=None, sudoable=False, executable=None, in_data=None):\n        ''' run a command on the zone '''\n        if sudoable and self.runner.become and self.runner.become_method not in self.become_methods_supported:\n            raise errors.AnsibleError(\"Internal Error: this module does not support running commands via %s\" % self.runner.become_method)\n        if in_data:\n            raise errors.AnsibleError(\"Internal Error: this module does not support optimized module pipelining\")\n        if executable == '/bin/sh':\n          executable = None\n        local_cmd = self._generate_cmd(executable, cmd)\n        vvv(\"EXEC %s\" % (local_cmd), host=self.zone)\n        p = subprocess.Popen(local_cmd, shell=isinstance(local_cmd, basestring),\n                             cwd=self.runner.basedir,\n                             stdin=subprocess.PIPE,\n                             stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        stdout, stderr = p.communicate()\n        return (p.returncode, '', stdout, stderr)\n    def _normalize_path(self, path, prefix):\n        if not path.startswith(os.path.sep):\n            path = os.path.join(os.path.sep, path)\n        normpath = os.path.normpath(path)\n        return os.path.join(prefix, normpath[1:])\n    def _copy_file(self, in_path, out_path):\n        if not os.path.exists(in_path):\n            raise errors.AnsibleFileNotFound(\"file or module does not exist: %s\" % in_path)\n        try:\n            shutil.copyfile(in_path, out_path)\n        except shutil.Error:\n            traceback.print_exc()\n            raise errors.AnsibleError(\"failed to copy: %s and %s are the same\" % (in_path, out_path))\n        except IOError:\n            traceback.print_exc()\n            raise errors.AnsibleError(\"failed to transfer file to %s\" % out_path)\n    def put_file(self, in_path, out_path):\n        ''' transfer a file from local to zone '''\n        out_path = self._normalize_path(out_path, self.get_zone_path())\n        vvv(\"PUT %s TO %s\" % (in_path, out_path), host=self.zone)\n        self._copy_file(in_path, out_path)\n    def fetch_file(self, in_path, out_path):\n        ''' fetch a file from zone to local '''\n        in_path = self._normalize_path(in_path, self.get_zone_path())\n        vvv(\"FETCH %s TO %s\" % (in_path, out_path), host=self.zone)\n        self._copy_file(in_path, out_path)\n    def close(self):\n        ''' terminate the connection; nothing to do here '''\n        pass\n    def connect(self, port=None):\n        ''' connect to the chroot; nothing to do here '''\n        vvv(\"THIS IS A LOCAL CHROOT DIR\", host=self.jail)\n        return self\n    def exec_command(self, cmd, tmp_path, become_user=None, sudoable=False, executable='/bin/sh', in_data=None):\n        ''' run a command on the chroot '''\n        if sudoable and self.runner.become and self.runner.become_method not in self.become_methods_supported:\n            raise errors.AnsibleError(\"Internal Error: this module does not support running commands via %s\" % self.runner.become_method)\n        if in_data:\n            raise errors.AnsibleError(\"Internal Error: this module does not support optimized module pipelining\")\n        local_cmd = self._generate_cmd(executable, cmd)\n        vvv(\"EXEC %s\" % (local_cmd), host=self.jail)\n        p = subprocess.Popen(local_cmd, shell=isinstance(local_cmd, basestring),\n                             cwd=self.runner.basedir,\n                             stdin=subprocess.PIPE,\n                             stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        stdout, stderr = p.communicate()\n        return (p.returncode, '', stdout, stderr)\n    def _normalize_path(self, path, prefix):\n        if not path.startswith(os.path.sep):\n            path = os.path.join(os.path.sep, path)\n        normpath = os.path.normpath(path)\n        return os.path.join(prefix, normpath[1:])\n    def _copy_file(self, in_path, out_path):\n        if not os.path.exists(in_path):\n            raise errors.AnsibleFileNotFound(\"file or module does not exist: %s\" % in_path)\n        try:\n            shutil.copyfile(in_path, out_path)\n        except shutil.Error:\n            traceback.print_exc()\n            raise errors.AnsibleError(\"failed to copy: %s and %s are the same\" % (in_path, out_path))\n        except IOError:\n            traceback.print_exc()\n            raise errors.AnsibleError(\"failed to transfer file to %s\" % out_path)\n    def put_file(self, in_path, out_path):\n        ''' transfer a file from local to chroot '''\n        out_path = self._normalize_path(out_path, self.get_jail_path())\n        vvv(\"PUT %s TO %s\" % (in_path, out_path), host=self.jail)\n        self._copy_file(in_path, out_path)\n    def fetch_file(self, in_path, out_path):\n        ''' fetch a file from chroot to local '''\n        in_path = self._normalize_path(in_path, self.get_jail_path())\n        vvv(\"FETCH %s TO %s\" % (in_path, out_path), host=self.jail)\n        self._copy_file(in_path, out_path)\n    def close(self):\n        ''' terminate the connection; nothing to do here '''\n        pass",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2015-6240",
        "description": "[{'lang': 'en', 'value': 'The chroot, jail, and zone connection plugins in ansible before 1.9.2 allow local users to escape a restricted environment via a symlink attack.'}]",
        "cwe_number": 59
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-150",
      "code": "    def get(self, request):\n        model = self.queryset.model\n        content_type = ContentType.objects.get_for_model(model)\n        display_filter_params = []\n        dynamic_filter_form = None\n        filter_form = None\n        if self.filterset:\n            filter_params = self.get_filter_params(request)\n            filterset = self.filterset(filter_params, self.queryset)\n            self.queryset = filterset.qs\n            if not filterset.is_valid():\n                messages.error(\n                    request,\n                    mark_safe(f\"Invalid filters were specified: {filterset.errors}\"),\n                )\n                self.queryset = self.queryset.none()\n            display_filter_params = [\n                check_filter_for_display(filterset.filters, field_name, values)\n                for field_name, values in filter_params.items()\n            ]\n            if request.GET:\n                factory_formset_params = convert_querydict_to_factory_formset_acceptable_querydict(\n                    request.GET, filterset\n                )\n                dynamic_filter_form = DynamicFilterFormSet(filterset=filterset, data=factory_formset_params)\n            else:\n                dynamic_filter_form = DynamicFilterFormSet(filterset=filterset)\n            if self.filterset_form:\n                filter_form = self.filterset_form(filter_params, label_suffix=\"\")\n        if request.GET.get(\"export\"):\n            et = get_object_or_404(\n                ExportTemplate,\n                content_type=content_type,\n                name=request.GET.get(\"export\"),\n            )\n            try:\n                return et.render_to_response(self.queryset)\n            except Exception as e:\n                messages.error(\n                    request,\n                    f\"There was an error rendering the selected export template ({et.name}): {e}\",\n                )\n        elif \"export\" in request.GET and hasattr(model, \"to_yaml\"):\n            response = HttpResponse(self.queryset_to_yaml(), content_type=\"text/yaml\")\n            filename = f\"{settings.BRANDING_PREPENDED_FILENAME}{self.queryset.model._meta.verbose_name_plural}.yaml\"\n            response[\"Content-Disposition\"] = f'attachment; filename=\"{filename}\"'\n            return response\n        self.queryset = self.alter_queryset(request)\n        permissions = {}\n        for action in (\"add\", \"change\", \"delete\", \"view\"):\n            perm_name = get_permission_for_model(model, action)\n            permissions[action] = request.user.has_perm(perm_name)\n        table = None\n        table_config_form = None\n        if self.table:\n            order_by = self.request.GET.getlist(\"sort\")\n            table = self.table(self.queryset, user=request.user, order_by=order_by)\n            if \"pk\" in table.base_columns and (permissions[\"change\"] or permissions[\"delete\"]):\n                table.columns.show(\"pk\")\n            paginate = {\n                \"paginator_class\": EnhancedPaginator,\n                \"per_page\": get_paginate_count(request),\n            }\n            RequestConfig(request, paginate).configure(table)\n            table_config_form = TableConfigForm(table=table)\n            max_page_size = get_settings_or_config(\"MAX_PAGE_SIZE\")\n            if max_page_size and paginate[\"per_page\"] > max_page_size:\n                messages.warning(\n                    request,\n                    f'Requested \"per_page\" is too large. No more than {max_page_size} items may be displayed at a time.',\n                )\n        q_placeholder = \"Search \" + bettertitle(model._meta.verbose_name_plural)\n        search_form = SearchForm(data=request.GET, q_placeholder=q_placeholder)\n        valid_actions = self.validate_action_buttons(request)\n        context = {\n            \"content_type\": content_type,\n            \"table\": table,\n            \"permissions\": permissions,\n            \"action_buttons\": valid_actions,\n            \"table_config_form\": table_config_form,\n            \"filter_params\": display_filter_params,\n            \"filter_form\": filter_form,\n            \"dynamic_filter_form\": dynamic_filter_form,\n            \"search_form\": search_form,\n            \"list_url\": validated_viewname(model, \"list\"),\n            \"title\": bettertitle(model._meta.verbose_name_plural),\n        }\n        setattr(self, \"request\", request)\n        context.update(self.extra_context())\n        return render(request, self.template_name, context)\n    def successful_post(self, request, obj, created, logger):\n        \"\"\"Callback after the form is successfully saved but before redirecting the user.\"\"\"\n        verb = \"Created\" if created else \"Modified\"\n        msg = f\"{verb} {self.queryset.model._meta.verbose_name}\"\n        logger.info(f\"{msg} {obj} (PK: {obj.pk})\")\n        if hasattr(obj, \"get_absolute_url\"):\n            msg = f'{msg} <a href=\"{obj.get_absolute_url()}\">{escape(obj)}</a>'\n        else:\n            msg = f\"{msg} {escape(obj)}\"\n        messages.success(request, mark_safe(msg))\n    def post(self, request):\n        logger = logging.getLogger(__name__ + \".ObjectImportView\")\n        form = ImportForm(request.POST)\n        if form.is_valid():\n            logger.debug(\"Import form validation was successful\")\n            data = form.cleaned_data[\"data\"]\n            model_form = self.model_form(data)\n            restrict_form_fields(model_form, request.user)\n            for field_name, field in model_form.fields.items():\n                if field_name not in data and hasattr(field, \"initial\"):\n                    model_form.data[field_name] = field.initial\n            if model_form.is_valid():\n                try:\n                    with transaction.atomic():\n                        obj = model_form.save()\n                        self.queryset.get(pk=obj.pk)\n                        logger.debug(f\"Created {obj} (PK: {obj.pk})\")\n                        for (\n                            field_name,\n                            related_object_form,\n                        ) in self.related_object_forms.items():\n                            logger.debug(\"Processing form for related objects: {related_object_form}\")\n                            related_obj_pks = []\n                            for i, rel_obj_data in enumerate(data.get(field_name, [])):\n                                f = related_object_form(obj, rel_obj_data)\n                                for subfield_name, field in f.fields.items():\n                                    if subfield_name not in rel_obj_data and hasattr(field, \"initial\"):\n                                        f.data[subfield_name] = field.initial\n                                if f.is_valid():\n                                    related_obj = f.save()\n                                    related_obj_pks.append(related_obj.pk)\n                                else:\n                                    for subfield_name, errors in f.errors.items():\n                                        for err in errors:\n                                            err_msg = f\"{field_name}[{i}] {subfield_name}: {err}\"\n                                            model_form.add_error(None, err_msg)\n                                    raise AbortTransaction()\n                            model = related_object_form.Meta.model\n                            if model.objects.filter(pk__in=related_obj_pks).count() != len(related_obj_pks):\n                                raise ObjectDoesNotExist\n                except AbortTransaction:\n                    pass\n                except ObjectDoesNotExist:\n                    msg = \"Object creation failed due to object-level permissions violation\"\n                    logger.debug(msg)\n                    form.add_error(None, msg)\n            if not model_form.errors:\n                logger.info(f\"Import object {obj} (PK: {obj.pk})\")\n                messages.success(\n                    request,\n                    mark_safe(f'Imported object: <a href=\"{obj.get_absolute_url()}\">{obj}</a>'),\n                )\n                if \"_addanother\" in request.POST:\n                    return redirect(request.get_full_path())\n                return_url = form.cleaned_data.get(\"return_url\")\n                if return_url is not None and is_safe_url(url=return_url, allowed_hosts=request.get_host()):\n                    return redirect(return_url)\n                else:\n                    return redirect(self.get_return_url(request, obj))\n            else:\n                logger.debug(\"Model form validation failed\")\n                for field, errors in model_form.errors.items():\n                    for err in errors:\n                        if field == \"__all__\":\n                            form.add_error(None, err)\n                        else:\n                            form.add_error(None, f\"{field}: {err}\")\n        else:\n            logger.debug(\"Import form validation failed\")\n        return render(\n            request,\n            self.template_name,\n            {\n                \"form\": form,\n                \"obj_type\": self.queryset.model._meta.verbose_name,\n                \"return_url\": self.get_return_url(request),\n            },\n        )\n    def successful_post(self, request, obj, created, _logger):\n        \"\"\"Check for data that will be invalid in a future Nautobot release and warn the user if found.\"\"\"\n        edit_url = reverse(\"ipam:prefix_edit\", kwargs={\"pk\": obj.pk})\n        warning_msg = (\n            '<p>This <a href=\"'\n            + static(\"docs/models/ipam/prefix.html\")\n            + '\n        )\n        if obj.parent and obj.parent.type != constants.PREFIX_ALLOWED_PARENT_TYPES[obj.type]:\n            parent_edit_url = reverse(\"ipam:prefix_edit\", kwargs={\"pk\": obj.parent.pk})\n            messages.warning(\n                request,\n                mark_safe(\n                    f'{obj} is a {obj.type.title()} prefix but its parent <a href=\"{obj.parent.get_absolute_url()}\">'\n                    f\"{obj.parent}</a> is a {obj.parent.type.title()}. {warning_msg} \"\n                    f'Consider <a href=\"{edit_url}\">changing the type of {obj}</a> and/or '\n                    f'<a href=\"{parent_edit_url}\">{obj.parent}</a> to resolve this issue.'\n                ),\n            )\n        invalid_children = obj.children.filter(\n            ~Q(type__in=constants.PREFIX_ALLOWED_CHILD_TYPES[obj.type]),\n        )\n        if invalid_children.exists():\n            children_link = '<a href=\"' + reverse(\"ipam:prefix_list\") + f'?parent={obj.pk}\">its children</a>'\n            if obj.type == choices.PrefixTypeChoices.TYPE_CONTAINER:\n                messages.warning(\n                    request,\n                    mark_safe(\n                        f\"{obj} is a Container prefix and should not contain child prefixes of type Pool. \"\n                        f\"{warning_msg} Consider creating an intermediary Network prefix, or changing \"\n                        f\"the type of {children_link} to Network, to resolve this issue.\"\n                    ),\n                )\n            elif obj.type == choices.PrefixTypeChoices.TYPE_NETWORK:\n                messages.warning(\n                    request,\n                    mark_safe(\n                        f\"{obj} is a Network prefix and should not contain child prefixes of types Container or \"\n                        f'Network. {warning_msg} Consider <a href=\"{edit_url}\">changing the type of {obj}</a> '\n                        f\"to Container, or changing the type of {children_link} to Pool, to resolve this issue.\"\n                    ),\n                )\n            else:\n                messages.warning(\n                    request,\n                    mark_safe(\n                        f\"{obj} is a Pool prefix and should not contain other prefixes. {warning_msg} \"\n                        f'Consider either <a href=\"{edit_url}\">changing the type of {obj}</a> '\n                        f\"to Container or Network, or deleting {children_link}, to resolve this issue.\"\n                    ),\n                )\n        if obj.ip_addresses.exists() and obj.type == choices.PrefixTypeChoices.TYPE_CONTAINER:\n            ip_warning_msg = (\n                '<p>This <a href=\"'\n                + static(\"docs/models/ipam/ipaddress.html\")\n                + '\n                \"in a future release.</p>\"\n            )\n            shortest_child_mask_length = min([ip.mask_length for ip in obj.ip_addresses.all()])\n            if shortest_child_mask_length > obj.prefix_length:\n                ip_link = '<a href=\"' + reverse(\"ipam:ipaddress_list\") + f'?parent={obj.pk}\">these IP addresses</a>'\n                create_url = reverse(\"ipam:prefix_add\") + urlencode(\n                    {\n                        \"namespace\": obj.namespace.pk,\n                        \"type\": choices.PrefixTypeChoices.TYPE_NETWORK,\n                        \"prefix\": obj.prefix,\n                    }\n                )\n                messages.warning(\n                    request,\n                    mark_safe(\n                        f\"{obj} is a Container prefix and should not directly contain IP addresses. {ip_warning_msg} \"\n                        f'Consider either <a href=\"{edit_url}\">changing the type of {obj}</a> to Network, or '\n                        f'<a href=\"{create_url}\">creating one or more child prefix(es) of type Network</a> to contain '\n                        f\"{ip_link}, to resolve this issue.\"\n                    ),\n                )\n            else:\n                messages.warning(\n                    request,\n                    mark_safe(\n                        f\"{obj} is a Container prefix and should not directly contain IP addresses. {ip_warning_msg} \"\n                        f'Consider <a href=\"{edit_url}\">changing the type of {obj}</a> to Network '\n                        \"to resolve this issue.\"\n                    ),\n                )\n        super().successful_post(request, obj, created, _logger)\n    def post(self, request):\n        collapsed_ips = IPAddress.objects.filter(pk__in=request.POST.getlist(\"pk\"))\n        merged_attributes = request.POST\n        operation_invalid = len(collapsed_ips) < 2\n        if \"_skip\" not in request.POST and not operation_invalid:\n            with cache.lock(\"ipaddress_merge\", blocking_timeout=15, timeout=settings.REDIS_LOCK_TIMEOUT):\n                with transaction.atomic():\n                    namespace = Namespace.objects.get(pk=merged_attributes.get(\"namespace\"))\n                    status = Status.objects.get(pk=merged_attributes.get(\"status\"))\n                    if merged_attributes.get(\"tenant\"):\n                        tenant = Tenant.objects.get(pk=merged_attributes.get(\"tenant\"))\n                    else:\n                        tenant = None\n                    if merged_attributes.get(\"role\"):\n                        role = Role.objects.get(pk=merged_attributes.get(\"role\"))\n                    else:\n                        role = None\n                    if merged_attributes.get(\"tags\"):\n                        tag_pk_list = merged_attributes.get(\"tags\").split(\",\")\n                        tags = Tag.objects.filter(pk__in=tag_pk_list)\n                    else:\n                        tags = []\n                    if merged_attributes.get(\"nat_inside\"):\n                        nat_inside = IPAddress.objects.get(pk=merged_attributes.get(\"nat_inside\"))\n                    else:\n                        nat_inside = None\n                    ip_in_the_same_namespace = collapsed_ips.filter(parent__namespace=namespace).first()\n                    merged_ip = IPAddress(\n                        host=merged_attributes.get(\"host\"),\n                        ip_version=ip_in_the_same_namespace.ip_version,\n                        parent=ip_in_the_same_namespace.parent,\n                        type=merged_attributes.get(\"type\"),\n                        status=status,\n                        role=role,\n                        dns_name=merged_attributes.get(\"dns_name\", \"\"),\n                        description=merged_attributes.get(\"description\"),\n                        mask_length=merged_attributes.get(\"mask_length\"),\n                        tenant=tenant,\n                        nat_inside=nat_inside,\n                        _custom_field_data=ip_in_the_same_namespace._custom_field_data,\n                    )\n                    merged_ip.tags.set(tags)\n                    for key in merged_ip._custom_field_data.keys():\n                        ip_pk = merged_attributes.get(\"cf_\" + key)\n                        merged_ip._custom_field_data[key] = IPAddress.objects.get(pk=ip_pk)._custom_field_data[key]\n                    handle_relationship_changes_when_merging_ips(merged_ip, merged_attributes, collapsed_ips)\n                    device_ip4 = list(Device.objects.filter(primary_ip4__in=collapsed_ips).values_list(\"pk\", flat=True))\n                    device_ip6 = list(Device.objects.filter(primary_ip6__in=collapsed_ips).values_list(\"pk\", flat=True))\n                    vm_ip4 = list(\n                        VirtualMachine.objects.filter(primary_ip4__in=collapsed_ips).values_list(\"pk\", flat=True)\n                    )\n                    vm_ip6 = list(\n                        VirtualMachine.objects.filter(primary_ip6__in=collapsed_ips).values_list(\"pk\", flat=True)\n                    )\n                    ip_to_interface_assignments = []\n                    for assignment in IPAddressToInterface.objects.filter(ip_address__in=collapsed_ips):\n                        updated_attributes = model_to_dict(assignment)\n                        updated_attributes[\"ip_address\"] = merged_ip\n                        updated_attributes[\"interface\"] = Interface.objects.filter(\n                            pk=updated_attributes[\"interface\"]\n                        ).first()\n                        updated_attributes[\"vm_interface\"] = VMInterface.objects.filter(\n                            pk=updated_attributes[\"vm_interface\"]\n                        ).first()\n                        ip_to_interface_assignments.append(updated_attributes)\n                    services = list(Service.objects.filter(ip_addresses__in=collapsed_ips).values_list(\"pk\", flat=True))\n                    try:\n                        _, deleted_info = collapsed_ips.delete()\n                        deleted_count = deleted_info[IPAddress._meta.label]\n                    except ProtectedError as e:\n                        logger.info(\"Caught ProtectedError while attempting to delete objects\")\n                        handle_protectederror(collapsed_ips, request, e)\n                        return redirect(self.get_return_url(request))\n                    msg = (\n                        f\"Merged {deleted_count} {self.queryset.model._meta.verbose_name} \"\n                        f'into <a href=\"{merged_ip.get_absolute_url()}\">{escape(merged_ip)}</a>'\n                    )\n                    logger_msg = f\"Merged {deleted_count} {self.queryset.model._meta.verbose_name} into {merged_ip}\"\n                    merged_ip.validated_save()\n                    for assignment in ip_to_interface_assignments:\n                        IPAddressToInterface.objects.create(**assignment)\n                    Device.objects.filter(pk__in=device_ip4).update(primary_ip4=merged_ip)\n                    Device.objects.filter(pk__in=device_ip6).update(primary_ip6=merged_ip)\n                    VirtualMachine.objects.filter(pk__in=vm_ip4).update(primary_ip4=merged_ip)\n                    VirtualMachine.objects.filter(pk__in=vm_ip6).update(primary_ip6=merged_ip)\n                    for service in services:\n                        Service.objects.get(pk=service).ip_addresses.add(merged_ip)\n                    logger.info(logger_msg)\n                    messages.success(request, mark_safe(msg))\n        return self.find_duplicate_ips(request, merged_attributes)\n    def to_form_field(\n        self, set_initial=True, enforce_required=True, for_csv_import=False, simple_json_filter=False, label=None\n    ):\n        \"\"\"\n        Return a form field suitable for setting a CustomField's value for an object.\n        Args:\n            set_initial: Set initial date for the field. This should be False when generating a field for bulk editing.\n            enforce_required: Honor the value of CustomField.required. Set to False for filtering/bulk editing.\n            for_csv_import: Return a form field suitable for bulk import of objects. Despite the parameter name,\n                this is *not* used for CSV imports since 2.0, but it *is* used for JSON/YAML import of DeviceTypes.\n            simple_json_filter: Return a TextInput widget for JSON filtering instead of the default TextArea widget.\n            label: Set the input label manually (if required); otherwise, defaults to field's __str__() implementation.\n        \"\"\"\n        initial = self.default if set_initial else None\n        required = self.required if enforce_required else False\n        if self.type == CustomFieldTypeChoices.TYPE_INTEGER:\n            field = forms.IntegerField(\n                required=required,\n                initial=initial,\n                min_value=self.validation_minimum,\n                max_value=self.validation_maximum,\n            )\n        elif self.type == CustomFieldTypeChoices.TYPE_BOOLEAN:\n            choices = (\n                (None, \"---------\"),\n                (True, \"True\"),\n                (False, \"False\"),\n            )\n            field = forms.NullBooleanField(\n                required=required,\n                initial=initial,\n                widget=StaticSelect2(choices=choices),\n            )\n        elif self.type == CustomFieldTypeChoices.TYPE_DATE:\n            field = NullableDateField(\n                required=required,\n                initial=initial,\n                widget=DatePicker(),\n            )\n        elif self.type in (CustomFieldTypeChoices.TYPE_URL, CustomFieldTypeChoices.TYPE_TEXT):\n            if self.type == CustomFieldTypeChoices.TYPE_URL:\n                field = LaxURLField(required=required, initial=initial)\n            elif self.type == CustomFieldTypeChoices.TYPE_TEXT:\n                field = forms.CharField(max_length=255, required=required, initial=initial)\n            if self.validation_regex:\n                field.validators = [\n                    RegexValidator(\n                        regex=self.validation_regex,\n                        message=mark_safe(f\"Values must match this regex: <code>{self.validation_regex}</code>\"),\n                    )\n                ]\n        elif self.type == CustomFieldTypeChoices.TYPE_MARKDOWN:\n            field = CommentField(widget=SmallTextarea, label=None)\n        elif self.type == CustomFieldTypeChoices.TYPE_JSON:\n            if simple_json_filter:\n                field = JSONField(encoder=DjangoJSONEncoder, required=required, initial=None, widget=TextInput)\n            else:\n                field = JSONField(encoder=DjangoJSONEncoder, required=required, initial=initial)\n        else:\n            choices = [(cfc.value, cfc.value) for cfc in self.custom_field_choices.all()]\n            default_choice = self.custom_field_choices.filter(value=self.default).first()\n            if self.type == CustomFieldTypeChoices.TYPE_SELECT:\n                if not required or default_choice is None:\n                    choices = add_blank_choice(choices)\n                field_class = CSVChoiceField if for_csv_import else forms.ChoiceField\n                field = field_class(\n                    choices=choices,\n                    required=required,\n                    initial=initial,\n                    widget=StaticSelect2(),\n                )\n            else:\n                field_class = CSVMultipleChoiceField if for_csv_import else forms.MultipleChoiceField\n                field = field_class(choices=choices, required=required, initial=initial, widget=StaticSelect2Multiple())\n        field.model = self\n        if label is not None:\n            field.label = label\n        else:\n            field.label = str(self)\n        if self.description:\n            field.help_text = render_markdown(self.description)\n        return field\ndef job_buttons(context, obj):\n    \"\"\"\n    Render all applicable job buttons for the given object.\n    \"\"\"\n    content_type = ContentType.objects.get_for_model(obj)\n    buttons = JobButton.objects.filter(content_types=content_type)\n    if not buttons:\n        return \"\"\n    button_context = {\n        \"obj\": obj,\n        \"debug\": context.get(\"debug\", False),\n        \"request\": context[\"request\"],\n        \"user\": context[\"user\"],\n        \"perms\": context[\"perms\"],\n    }\n    buttons_html = forms_html = \"\"\n    group_names = OrderedDict()\n    hidden_inputs = HIDDEN_INPUTS.format(\n        csrf_token=context[\"csrf_token\"],\n        object_pk=obj.pk,\n        object_model_name=f\"{content_type.app_label}.{content_type.model}\",\n        redirect_path=context[\"request\"].path,\n    )\n    for jb in buttons:\n        template_args = {\n            \"button_id\": jb.pk,\n            \"button_text\": jb.text,\n            \"button_class\": jb.button_class,\n            \"button_url\": reverse(\"extras:jobbutton_run\", kwargs={\"pk\": jb.pk}),\n            \"object\": obj,\n            \"job\": jb.job,\n            \"hidden_inputs\": hidden_inputs,\n            \"disabled\": \"\" if context[\"user\"].has_perms((\"extras.run_jobbutton\", \"extras.run_job\")) else \"disabled\",\n        }\n        if jb.group_name:\n            group_names.setdefault(jb.group_name, [])\n            group_names[jb.group_name].append(jb)\n        else:\n            try:\n                text_rendered = render_jinja2(jb.text, button_context)\n                if text_rendered:\n                    template_args[\"button_text\"] = text_rendered\n                    if jb.confirmation:\n                        buttons_html += CONFIRM_BUTTON.format(**template_args)\n                        forms_html += CONFIRM_MODAL.format(**template_args)\n                    else:\n                        buttons_html += NO_CONFIRM_BUTTON.format(**template_args)\n                        forms_html += NO_CONFIRM_FORM.format(**template_args)\n            except Exception as e:\n                buttons_html += (\n                    f'<a class=\"btn btn-sm btn-default\" disabled=\"disabled\" title=\"{e}\">'\n                    f'<i class=\"mdi mdi-alert\"></i> {jb.name}</a>\\n'\n                )\n    for group_name, buttons in group_names.items():\n        group_button_class = buttons[0].button_class\n        buttons_rendered = \"\"\n        for jb in buttons:\n            template_args = {\n                \"button_id\": jb.pk,\n                \"button_text\": jb.text,\n                \"button_class\": \"link\",\n                \"button_url\": reverse(\"extras:jobbutton_run\", kwargs={\"pk\": jb.pk}),\n                \"object\": obj,\n                \"job\": jb.job,\n                \"hidden_inputs\": hidden_inputs,\n                \"disabled\": \"\" if context[\"user\"].has_perms((\"extras.run_jobbutton\", \"extras.run_job\")) else \"disabled\",\n            }\n            try:\n                text_rendered = render_jinja2(jb.text, button_context)\n                if text_rendered:\n                    template_args[\"button_text\"] = text_rendered\n                    if jb.confirmation:\n                        buttons_rendered += \"<li>\" + CONFIRM_BUTTON.format(**template_args) + \"</li>\"\n                        forms_html += CONFIRM_MODAL.format(**template_args)\n                    else:\n                        buttons_rendered += \"<li>\" + NO_CONFIRM_BUTTON.format(**template_args) + \"</li>\"\n                        forms_html += NO_CONFIRM_FORM.format(**template_args)\n            except Exception as e:\n                buttons_rendered += (\n                    f'<li><a disabled=\"disabled\" title=\"{e}\"><span class=\"text-muted\">'\n                    f'<i class=\"mdi mdi-alert\"></i> {jb.name}</span></a></li>'\n                )\n        if buttons_rendered:\n            buttons_html += GROUP_DROPDOWN.format(\n                group_button_class=group_button_class,\n                group_name=group_name,\n                grouped_buttons=buttons_rendered,\n            )\n    return mark_safe(buttons_html + forms_html)\n    def required_related_objects_errors(\n        cls, output_for=\"ui\", initial_data=None, relationships_key_specified=False, instance=None\n    ):\n        \"\"\"\n        Args:\n            output_for (str): either \"ui\" or \"api\" depending on usage\n            initial_data (dict): submitted form/serializer data to validate against\n            relationships_key_specified (bool): if the \"relationships\" key was provided or not\n            instance (Optional[BaseModel]): an optional model instance to validate against\n        Returns:\n            (list[dict]): List of field error dicts if any are found\n        \"\"\"\n        required_relationships = Relationship.objects.get_required_for_model(cls)\n        relationships_field_errors = {}\n        for relation in required_relationships:\n            opposite_side = RelationshipSideChoices.OPPOSITE[relation.required_on]\n            if relation.skip_required(cls, opposite_side):\n                continue\n            if relation.has_many(opposite_side):\n                num_required_verbose = \"at least one\"\n            else:\n                num_required_verbose = \"a\"\n            if output_for == \"api\":\n                if (\n                    getattr(instance, \"present_in_database\", False) is True\n                    and initial_data.get(relation, {}).get(opposite_side, {}) == {}\n                    and not relationships_key_specified\n                ):\n                    filter_kwargs = {\"relationship\": relation, f\"{relation.required_on}_id\": instance.pk}\n                    if RelationshipAssociation.objects.filter(**filter_kwargs).exists():\n                        continue\n            required_model_class = getattr(relation, f\"{opposite_side}_type\").model_class()\n            required_model_meta = required_model_class._meta\n            cr_field_name = f\"cr_{relation.key}__{opposite_side}\"\n            name_plural = cls._meta.verbose_name_plural\n            field_key = relation.key if output_for == \"api\" else cr_field_name\n            field_errors = {field_key: []}\n            if not required_model_class.objects.exists():\n                hint = (\n                    f\"You need to create {num_required_verbose} {required_model_meta.verbose_name} \"\n                    f\"before instantiating a {cls._meta.verbose_name}.\"\n                )\n                if output_for == \"ui\":\n                    try:\n                        add_url = reverse(get_route_for_model(required_model_class, \"add\"))\n                        hint = (\n                            f\"<a target='_blank' href='{add_url}'>Click here</a> to create \"\n                            f\"a {required_model_meta.verbose_name}.\"\n                        )\n                    except NoReverseMatch:\n                        pass\n                elif output_for == \"api\":\n                    try:\n                        api_post_url = reverse(get_route_for_model(required_model_class, \"list\", api=True))\n                        hint = f\"Create a {required_model_meta.verbose_name} by posting to {api_post_url}\"\n                    except NoReverseMatch:\n                        pass\n                error_message = mark_safe(\n                    f\"{name_plural[0].upper()}{name_plural[1:]} require \"\n                    f\"{num_required_verbose} {required_model_meta.verbose_name}, but no \"\n                    f\"{required_model_meta.verbose_name_plural} exist yet. {hint}\"\n                )\n                field_errors[field_key].append(error_message)\n            if initial_data is not None:\n                supplied_data = []\n                if output_for == \"ui\":\n                    supplied_data = initial_data.get(field_key, [])\n                elif output_for == \"api\":\n                    supplied_data = initial_data.get(relation, {}).get(opposite_side, {})\n                if not supplied_data:\n                    if output_for == \"ui\":\n                        field_errors[field_key].append(\n                            f\"You need to select {num_required_verbose} {required_model_meta.verbose_name}.\"\n                        )\n                    elif output_for == \"api\":\n                        field_errors[field_key].append(\n                            f'You need to specify [\"relationships\"][\"{relation.key}\"][\"{opposite_side}\"][\"objects\"].'\n                        )\n            if len(field_errors[field_key]) > 0:\n                relationships_field_errors[field_key] = field_errors[field_key]\n        return relationships_field_errors\n    def post(self, request, *args, **kwargs):\n        obj = self.alter_obj(self.get_object(kwargs), request, args, kwargs)\n        form = self.model_form(data=request.POST, files=request.FILES, instance=obj)\n        restrict_form_fields(form, request.user)\n        if form.is_valid():\n            logger.debug(\"Form validation was successful\")\n            try:\n                with transaction.atomic():\n                    object_created = not form.instance.present_in_database\n                    obj = form.save()\n                    self.queryset.get(pk=obj.pk)\n                    ctx = self.get_extra_context(request, obj)\n                    choices = ctx[\"choices\"]\n                    if choices.is_valid():\n                        choices.save()\n                    else:\n                        raise RuntimeError(choices.errors)\n                verb = \"Created\" if object_created else \"Modified\"\n                msg = f\"{verb} {self.queryset.model._meta.verbose_name}\"\n                logger.info(f\"{msg} {obj} (PK: {obj.pk})\")\n                if hasattr(obj, \"get_absolute_url\"):\n                    msg = f'{msg} <a href=\"{obj.get_absolute_url()}\">{escape(obj)}</a>'\n                else:\n                    msg = f\"{msg} {escape(obj)}\"\n                messages.success(request, mark_safe(msg))\n                if \"_addanother\" in request.POST:\n                    if hasattr(obj, \"clone_fields\"):\n                        url = f\"{request.path}?{prepare_cloned_fields(obj)}\"\n                        return redirect(url)\n                    return redirect(request.get_full_path())\n                return_url = form.cleaned_data.get(\"return_url\")\n                if return_url is not None and is_safe_url(url=return_url, allowed_hosts=request.get_host()):\n                    return redirect(return_url)\n                else:\n                    return redirect(self.get_return_url(request, obj))\n            except ObjectDoesNotExist:\n                msg = \"Object save failed due to object-level permissions violation\"\n                logger.debug(msg)\n                form.add_error(None, msg)\n            except RuntimeError:\n                msg = \"Errors encountered when saving custom field choices. See below.\"\n                logger.debug(msg)\n                form.add_error(None, msg)\n            except ProtectedError as err:\n                err_msg = err.args[0]\n                protected_obj = err.protected_objects[0]\n                msg = f\"{protected_obj.value}: {err_msg} Please cancel this edit and start again.\"\n                logger.debug(msg)\n                form.add_error(None, msg)\n        else:\n            logger.debug(\"Form validation failed\")\n        return render(\n            request,\n            self.template_name,\n            {\n                \"obj\": obj,\n                \"obj_type\": self.queryset.model._meta.verbose_name,\n                \"form\": form,\n                \"return_url\": self.get_return_url(request, obj),\n                \"editing\": obj.present_in_database,\n                **self.get_extra_context(request, obj),\n            },\n        )\n    def post(self, request, pk):\n        post_data = request.POST\n        job_button = JobButton.objects.get(pk=pk)\n        job_model = job_button.job\n        result = JobResult.enqueue_job(\n            job_model=job_model,\n            user=request.user,\n            object_pk=post_data[\"object_pk\"],\n            object_model_name=post_data[\"object_model_name\"],\n        )\n        msg = f'Job enqueued. <a href=\"{result.get_absolute_url()}\">Click here for the results.</a>'\n        messages.info(request=request, message=mark_safe(msg))\n        return redirect(post_data[\"redirect_path\"])\ndef get_csv_form_fields_from_serializer_class(serializer_class):\n    \"\"\"From the given serializer class, build a list of field dicts suitable for rendering in the CSV import form.\"\"\"\n    serializer = serializer_class(context={\"request\": None, \"depth\": 0})\n    fields = []\n    for field_name, field in serializer.fields.items():\n        if field.read_only:\n            continue\n        if field_name == \"custom_fields\":\n            from nautobot.extras.choices import CustomFieldTypeChoices\n            from nautobot.extras.models import CustomField\n            cfs = CustomField.objects.get_for_model(serializer_class.Meta.model)\n            for cf in cfs:\n                cf_form_field = cf.to_form_field(set_initial=False)\n                field_info = {\n                    \"name\": cf.add_prefix_to_cf_key(),\n                    \"required\": cf_form_field.required,\n                    \"label\": cf_form_field.label,\n                    \"help_text\": cf_form_field.help_text,\n                }\n                if cf.type == CustomFieldTypeChoices.TYPE_BOOLEAN:\n                    field_info[\"format\"] = mark_safe(\"<code>true</code> or <code>false</code>\")\n                elif cf.type == CustomFieldTypeChoices.TYPE_DATE:\n                    field_info[\"format\"] = mark_safe(\"<code>YYYY-MM-DD</code>\")\n                elif cf.type == CustomFieldTypeChoices.TYPE_SELECT:\n                    field_info[\"choices\"] = {cfc.value: cfc.value for cfc in cf.custom_field_choices.all()}\n                elif cf.type == CustomFieldTypeChoices.TYPE_MULTISELECT:\n                    field_info[\"format\"] = mark_safe('<code>\"value,value\"</code>')\n                    field_info[\"choices\"] = {cfc.value: cfc.value for cfc in cf.custom_field_choices.all()}\n                fields.append(field_info)\n            continue\n        field_info = {\n            \"name\": field_name,\n            \"required\": field.required,\n            \"label\": field.label,\n            \"help_text\": field.help_text,\n        }\n        if isinstance(field, serializers.BooleanField):\n            field_info[\"format\"] = mark_safe(\"<code>true</code> or <code>false</code>\")\n        elif isinstance(field, serializers.DateField):\n            field_info[\"format\"] = mark_safe(\"<code>YYYY-MM-DD</code>\")\n        elif isinstance(field, TimeZoneSerializerField):\n            field_info[\"format\"] = mark_safe(\n                '<a href=\"https://en.wikipedia.org/wiki/List_of_tz_database_time_zones\">available options</a>'\n            )\n        elif isinstance(field, serializers.ManyRelatedField):\n            if field.field_name == \"tags\":\n                field_info[\"format\"] = mark_safe('<code>\"name,name\"</code> or <code>\"UUID,UUID\"</code>')\n            elif isinstance(field.child_relation, ContentTypeField):\n                field_info[\"format\"] = mark_safe('<code>\"app_label.model,app_label.model\"</code>')\n            else:\n                field_info[\"format\"] = mark_safe('<code>\"UUID,UUID\"</code>')\n        elif isinstance(field, serializers.RelatedField):\n            if isinstance(field, ContentTypeField):\n                field_info[\"format\"] = mark_safe(\"<code>app_label.model</code>\")\n            else:\n                field_info[\"format\"] = mark_safe(\"<code>UUID</code>\")\n        elif isinstance(field, (serializers.ListField, serializers.MultipleChoiceField)):\n            field_info[\"format\"] = mark_safe('<code>\"value,value\"</code>')\n        elif isinstance(field, (serializers.DictField, serializers.JSONField)):\n            pass\n        if isinstance(field, ChoiceField):\n            field_info[\"choices\"] = field.choices\n        fields.append(field_info)\n    fields = sorted(fields, key=lambda info: 1 if info[\"required\"] else 2)\n    return fields\ndef handle_protectederror(obj_list, request, e):\n    \"\"\"\n    Generate a user-friendly error message in response to a ProtectedError exception.\n    \"\"\"\n    protected_objects = list(e.protected_objects)\n    protected_count = len(protected_objects) if len(protected_objects) <= 50 else \"More than 50\"\n    err_message = (\n        f\"Unable to delete <strong>{', '.join(str(obj) for obj in obj_list)}</strong>. \"\n        f\"{protected_count} dependent objects were found: \"\n    )\n    dependent_objects = []\n    for dependent in protected_objects[:50]:\n        if hasattr(dependent, \"get_absolute_url\"):\n            dependent_objects.append(f'<a href=\"{dependent.get_absolute_url()}\">{escape(dependent)}</a>')\n        else:\n            dependent_objects.append(str(dependent))\n    err_message += \", \".join(dependent_objects)\n    messages.error(request, mark_safe(err_message))\ndef custom_links(context, obj):\n    \"\"\"\n    Render all applicable links for the given object.\n    \"\"\"\n    content_type = ContentType.objects.get_for_model(obj)\n    links = CustomLink.objects.filter(content_type=content_type)\n    if not links:\n        return \"\"\n    link_context = {\n        \"obj\": obj,\n        \"debug\": context.get(\"debug\", False),\n        \"request\": context[\"request\"],\n        \"user\": context[\"user\"],\n        \"perms\": context[\"perms\"],\n    }\n    template_code = \"\"\n    group_names = OrderedDict()\n    for cl in links:\n        if cl.group_name and cl.group_name in group_names:\n            group_names[cl.group_name].append(cl)\n        elif cl.group_name:\n            group_names[cl.group_name] = [cl]\n        else:\n            try:\n                text_rendered = render_jinja2(cl.text, link_context)\n                if text_rendered:\n                    link_rendered = render_jinja2(cl.target_url, link_context)\n                    link_target = ' target=\"_blank\"' if cl.new_window else \"\"\n                    template_code += LINK_BUTTON.format(link_rendered, link_target, cl.button_class, text_rendered)\n            except Exception as e:\n                template_code += (\n                    f'<a class=\"btn btn-sm btn-default\" disabled=\"disabled\" title=\"{e}\">'\n                    f'<i class=\"mdi mdi-alert\"></i> {cl.name}</a>\\n'\n                )\n    for group, links in group_names.items():\n        links_rendered = []\n        for cl in links:\n            try:\n                text_rendered = render_jinja2(cl.text, link_context)\n                if text_rendered:\n                    link_target = ' target=\"_blank\"' if cl.new_window else \"\"\n                    link_rendered = render_jinja2(cl.target_url, link_context)\n                    links_rendered.append(GROUP_LINK.format(link_rendered, link_target, text_rendered))\n            except Exception as e:\n                links_rendered.append(\n                    f'<li><a disabled=\"disabled\" title=\"{e}\"><span class=\"text-muted\">'\n                    f'<i class=\"mdi mdi-alert\"></i> {cl.name}</span></a></li>'\n                )\n        if links_rendered:\n            template_code += GROUP_BUTTON.format(links[0].button_class, group, \"\".join(links_rendered))\n    return mark_safe(template_code)\n    def clean(self):\n        super().clean()\n        if self.present_in_database and self.u_height > self._original_u_height:\n            for d in Device.objects.filter(device_type=self, position__isnull=False):\n                face_required = None if self.is_full_depth else d.face\n                u_available = d.rack.get_available_units(\n                    u_height=self.u_height, rack_face=face_required, exclude=[d.pk]\n                )\n                if d.position not in u_available:\n                    raise ValidationError(\n                        {\n                            \"u_height\": f\"Device {d} in rack {d.rack} does not have sufficient space to accommodate a height of {self.u_height}U\"\n                        }\n                    )\n        elif self.present_in_database and self._original_u_height > 0 and self.u_height == 0:\n            racked_instance_count = Device.objects.filter(device_type=self, position__isnull=False).count()\n            if racked_instance_count:\n                url = f\"{reverse('dcim:device_list')}?manufacturer={self.manufacturer_id}&device_type={self.pk}\"\n                raise ValidationError(\n                    {\n                        \"u_height\": mark_safe(\n                            f'Unable to set 0U height: Found <a href=\"{url}\">{racked_instance_count} instances</a> already '\n                            f\"mounted within racks.\"\n                        )\n                    }\n                )\n        if (self.subdevice_role != SubdeviceRoleChoices.ROLE_PARENT) and self.device_bay_templates.count():\n            raise ValidationError(\n                {\n                    \"subdevice_role\": \"Must delete all device bay templates associated with this device before \"\n                    \"declassifying it as a parent device.\"\n                }\n            )\n        if self.u_height and self.subdevice_role == SubdeviceRoleChoices.ROLE_CHILD:\n            raise ValidationError({\"u_height\": \"Child device types must be 0U.\"})\ndef _get_registered_content(obj, method, template_context, return_html=True):\n    \"\"\"\n    Given an object and a TemplateExtension method name and the template context, return all the\n    registered content for the object's model.\n    \"\"\"\n    context = {\n        \"object\": obj,\n        \"request\": template_context[\"request\"],\n        \"settings\": template_context[\"settings\"],\n        \"csrf_token\": template_context[\"csrf_token\"],\n        \"perms\": template_context[\"perms\"],\n    }\n    model_name = obj._meta.label_lower\n    template_extensions = registry[\"plugin_template_extensions\"].get(model_name, [])\n    objects = []\n    html = \"\"\n    for template_extension in template_extensions:\n        if getattr(template_extension, method) == getattr(TemplateExtension, method):\n            continue\n        plugin_name = template_extension.__module__.split(\".\")[0]\n        context[\"config\"] = settings.PLUGINS_CONFIG.get(plugin_name, {})\n        instance = template_extension(context)\n        content = getattr(instance, method)()\n        if not return_html:\n            for i, content in enumerate(content):\n                objects.append({f\"{plugin_name}:{i+1}\": content})\n        else:\n            html += content\n    if not return_html:\n        return objects\n    return mark_safe(html)\n    def filter_queryset(self, queryset):\n        \"\"\"\n        Filter a query with request querystrings.\n        \"\"\"\n        if self.filterset_class is not None:\n            self.filter_params = self.get_filter_params(self.request)\n            self.filterset = self.filterset_class(self.filter_params, queryset)\n            queryset = self.filterset.qs\n            if not self.filterset.is_valid():\n                messages.error(\n                    self.request,\n                    mark_safe(f\"Invalid filters were specified: {self.filterset.errors}\"),\n                )\n                queryset = queryset.none()\n        return queryset\n    def _process_create_or_update_form(self, form):\n        \"\"\"\n        Helper method to create or update an object after the form is validated successfully.\n        \"\"\"\n        request = self.request\n        queryset = self.get_queryset()\n        with transaction.atomic():\n            object_created = not form.instance.present_in_database\n            obj = self.form_save(form)\n            queryset.get(pk=obj.pk)\n            if hasattr(form, \"save_note\") and callable(form.save_note):\n                form.save_note(instance=obj, user=request.user)\n            msg = f'{\"Created\" if object_created else \"Modified\"} {queryset.model._meta.verbose_name}'\n            self.logger.info(f\"{msg} {obj} (PK: {obj.pk})\")\n            if hasattr(obj, \"get_absolute_url\"):\n                msg = f'{msg} <a href=\"{obj.get_absolute_url()}\">{escape(obj)}</a>'\n            else:\n                msg = f\"{msg} { escape(obj)}\"\n            messages.success(request, mark_safe(msg))\n            if \"_addanother\" in request.POST:\n                if hasattr(obj, \"clone_fields\"):\n                    url = f\"{request.path}?{prepare_cloned_fields(obj)}\"\n                    self.success_url = url\n                self.success_url = request.get_full_path()\n            else:\n                return_url = form.cleaned_data.get(\"return_url\")\n                if return_url is not None and is_safe_url(url=return_url, allowed_hosts=request.get_host()):\n                    self.success_url = return_url\n                else:\n                    self.success_url = self.get_return_url(request, obj)\n    def post(self, request, pk):\n        virtual_chassis = get_object_or_404(self.queryset, pk=pk)\n        member_select_form = forms.VCMemberSelectForm(request.POST)\n        if member_select_form.is_valid():\n            device = member_select_form.cleaned_data[\"device\"]\n            device.virtual_chassis = virtual_chassis\n            data = {k: request.POST[k] for k in [\"vc_position\", \"vc_priority\"]}\n            membership_form = forms.DeviceVCMembershipForm(data=data, validate_vc_position=True, instance=device)\n            if membership_form.is_valid():\n                membership_form.save()\n                msg = f'Added member <a href=\"{device.get_absolute_url()}\">{escape(device)}</a>'\n                messages.success(request, mark_safe(msg))\n                if \"_addanother\" in request.POST:\n                    return redirect(request.get_full_path())\n                return redirect(self.get_return_url(request, device))\n        else:\n            membership_form = forms.DeviceVCMembershipForm(data=request.POST)\n        return render(\n            request,\n            \"dcim/virtualchassis_add_member.html\",\n            {\n                \"virtual_chassis\": virtual_chassis,\n                \"member_select_form\": member_select_form,\n                \"membership_form\": membership_form,\n                \"return_url\": self.get_return_url(request, virtual_chassis),\n            },\n        )\ndef placeholder(value):\n    \"\"\"Render a muted placeholder if value is falsey, else render the value.\n    Args:\n        value (any): Input value, can be any variable.\n    Returns:\n        (str): Placeholder in HTML, or the string representation of the value.\n    Example:\n        >>> placeholder(\"\")\n        '<span class=\"text-muted\">&mdash;</span>'\n        >>> placeholder(\"hello\")\n        \"hello\"\n    \"\"\"\n    if value:\n        return value\n    return mark_safe(HTML_NONE)\ndef add_html_id(element_str, id_str):\n    \"\"\"Add an HTML `id=\"...\"` attribute to the given HTML element string.\n    Args:\n        element_str (str): String describing an HTML element.\n        id_str (str): String to add as the `id` attribute of the element_str.\n    Returns:\n        (str): HTML string with added `id`.\n    Example:\n        >>> add_html_id(\"<div></div>\", \"my-div\")\n        '<div id=\"my-div\"></div>'\n        >>> add_html_id('<a href=\"...\" title=\"...\">Hello!</a>', \"my-a\")\n        '<a id=\"my-a\" href=\"...\" title=\"...\">Hello!</a>'\n    \"\"\"\n    match = re.match(r\"^(.*?<\\w+) ?(.*)$\", element_str, flags=re.DOTALL)\n    if not match:\n        return element_str\n    return mark_safe(match.group(1) + format_html(' id=\"{}\" ', id_str) + match.group(2))\ndef render_boolean(value):\n    \"\"\"Render HTML from a computed boolean value.\n    Args:\n        value (any): Input value, can be any variable.\n            A truthy value (for example non-empty string / True / non-zero number) is considered True.\n            A falsey value other than None (for example \"\" or 0 or False) is considered False.\n            A value of None is considered neither True nor False.\n    Returns:\n        (str): HTML\n            '<span class=\"text-success\"><i class=\"mdi mdi-check-bold\" title=\"Yes\"></i></span>' if True value\n            - or -\n            '<span class=\"text-muted\">&mdash;</span>' if None value\n            - or -\n            '<span class=\"text-danger\"><i class=\"mdi mdi-close-thick\" title=\"No\"></i></span>' if False value\n    Examples:\n        >>> render_boolean(None)\n        '<span class=\"text-muted\">&mdash;</span>'\n        >>> render_boolean(True or \"arbitrary string\" or 1)\n        '<span class=\"text-success\"><i class=\"mdi mdi-check-bold\" title=\"Yes\"></i></span>'\n        >>> render_boolean(False or \"\" or 0)\n        '<span class=\"text-danger\"><i class=\"mdi mdi-close-thick\" title=\"No\"></i></span>'\n    \"\"\"\n    if value is None:\n        return mark_safe(HTML_NONE)\n    if bool(value):\n        return mark_safe(HTML_TRUE)\n    return mark_safe(HTML_FALSE)\ndef render_markdown(value):\n    \"\"\"\n    Render text as Markdown\n    Example:\n        {{ text | render_markdown }}\n    \"\"\"\n    value = strip_tags(value)\n    schemes = \"|\".join(settings.ALLOWED_URL_SCHEMES)\n    pattern = rf\"\\[(.+)\\]\\((?!({schemes})).*:(.+)\\)\"\n    value = re.sub(pattern, \"[\\\\1](\\\\3)\", value, flags=re.IGNORECASE)\n    html = markdown(value, extensions=[\"fenced_code\", \"tables\"])\n    return mark_safe(html)\ndef computed_fields(context, obj, advanced_ui=None):\n    \"\"\"\n    Render all applicable links for the given object.\n    This can also check whether the advanced_ui attribute is True or False for UI display purposes.\n    \"\"\"\n    fields = obj.get_computed_fields(label_as_key=True, advanced_ui=advanced_ui)\n    if not computed_fields:\n        return \"\"\n    template_code = \"\"\n    for label, value in fields.items():\n        escaped_label = escape(label)\n        template_code += f\"\"\"\n            <tr>\n                <td><span title=\"{escaped_label}\">{escaped_label}</span></td>\n                <td>{escape(value)}</td>\n            <tr>\n            \"\"\"\n    return mark_safe(template_code)\ndef yamllint(context):\n    \"\"\"Run yamllint to validate formatting applies to YAML standards.\"\"\"\n    command = \"yamllint nautobot/docs --format standard\"\n    run_command(context, command)\ndef tests(context, lint_only=False, keepdb=False):\n    \"\"\"Run all linters and unit tests.\"\"\"\n    black(context)\n    flake8(context)\n    prettier(context)\n    eslint(context)\n    hadolint(context)\n    markdownlint(context)\n    yamllint(context)\n    pylint(context)\n    check_migrations(context)\n    check_schema(context)\n    build_and_check_docs(context)\n    if not lint_only:\n        unittest(context, keepdb=keepdb)\ndef version(context, version=None):\n    \"\"\"\n    Show the version of Nautobot Python and NPM packages or bump them when a valid bump rule is\n    provided.\n    The version number or rules are those supported by `poetry version`.\n    \"\"\"\n    if version is None:\n        version = \"\"\n    run_command(context, f\"poetry version --short {version}\")\n    run_command(context, f\"npm --prefix nautobot/ui version {version}\")\ndef render_jinja2(template_code, context):\n    \"\"\"\n    Render a Jinja2 template with the provided context. Return the rendered content.\n    \"\"\"\n    rendering_engine = engines[\"jinja\"]\n    template = rendering_engine.from_string(template_code)\n    return template.render(context=context)\ndef shallow_compare_dict(source_dict, destination_dict, exclude=None):\n    \"\"\"\n    Return a new dictionary of the different keys. The values of `destination_dict` are returned. Only the equality of\n    the first layer of keys/values is checked. `exclude` is a list or tuple of keys to be ignored.\n    \"\"\"\n    difference = {}\n    for key in destination_dict:\n        if source_dict.get(key) != destination_dict[key]:\n            if isinstance(exclude, (list, tuple)) and key in exclude:\n                continue\n            difference[key] = destination_dict[key]\n    return difference\n    def header(self):\n        return mark_safe('<input type=\"checkbox\" class=\"toggle\" title=\"Toggle all\" />')\n    def render(self, record, bound_column, value):\n        if value:\n            name = bound_column.name\n            css_class = getattr(record, f\"get_{name}_class\")()\n            label = getattr(record, f\"get_{name}_display\")()\n            return mark_safe(f'<span class=\"label label-{css_class}\">{label}</span>')\n        return self.default\n    def render(self, value):\n        return mark_safe(f'<span class=\"label color-block\" style=\"background-color:\n    def render(self, record, value):\n        if value:\n            url = reverse(self.viewname, kwargs=self.view_kwargs)\n            if self.url_params:\n                url += \"?\" + \"&\".join([f\"{k}={getattr(record, v)}\" for k, v in self.url_params.items()])\n            return mark_safe(f'<a href=\"{url}\">{value}</a>')\n        return value\n    def render_description(self, record):\n        if record.description:\n            return mark_safe(render_markdown(record.description))\n        return self.default",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-48705",
        "description": "[{'lang': 'en', 'value': \"Nautobot is a Network Source of Truth and Network Automation Platform built as a web application All users of Nautobot versions earlier than 1.6.6 or 2.0.5 are potentially affected by a cross-site scripting vulnerability. Due to incorrect usage of Django's `mark_safe()` API when rendering certain types of user-authored content; including custom links, job buttons, and computed fields; it is possible that users with permission to create or edit these types of content could craft a malicious payload (such as JavaScript code) that would be executed when rendering pages containing this content. The maintainers have fixed the incorrect uses of `mark_safe()` (generally by replacing them with appropriate use of `format_html()` instead) to prevent such malicious data from being executed. Users on Nautobot 1.6.x LTM should upgrade to v1.6.6 and users on Nautobot 2.0.x should upgrade to v2.0.5. Appropriate object permissions can and should be applied to restrict which users are permitted to create or edit the aforementioned types of user-authored content. Other than that, there is no direct workaround available.\"}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-151",
      "code": "    def __init__(self, cfg):\n        self.cfg = cfg\n        cherrypy.config.update(\n            {\n                'ldap.uri': cfg.ldap_uri,\n                'ldap.base_dn': cfg.ldap_base_dn,\n                'ldap.bind_dn': cfg.ldap_bind_dn,\n                'ldap.bind_password': cfg.ldap_bind_password,\n                'ldap.scope': cfg.ldap_scope,\n                'ldap.tls': cfg.ldap_tls,\n                'ldap.username_attribute': cfg.ldap_username_attribute,\n                'ldap.required_group': cfg.ldap_required_group,\n                'ldap.group_attribute': cfg.ldap_group_attribute,\n                'ldap.group_attribute_is_dn': cfg.ldap_group_attribute_is_dn,\n                'ldap.version': cfg.ldap_version,\n                'ldap.network_timeout': cfg.ldap_network_timeout,\n                'ldap.timeout': cfg.ldap_timeout,\n                'ldap.encoding': cfg.ldap_encoding,\n                'smtp.server': cfg.email_host,\n                'smtp.username': cfg.email_username,\n                'smtp.password': cfg.email_password,\n                'smtp.email_from': cfg.email_sender\n                and '%s <%s>'\n                % (\n                    cfg.header_name,\n                    cfg.email_sender,\n                ),\n                'smtp.encryption': cfg.email_encryption,\n                'remove_older.execution_time': self.cfg.remove_older_time,\n                'notification.execution_time': self.cfg.email_notification_time,\n                'notification.send_changed': self.cfg.email_send_changed_notification,\n                'quota.set_quota_cmd': self.cfg.quota_set_cmd,\n                'quota.get_quota_cmd': self.cfg.quota_get_cmd,\n                'quota.get_usage_cmd': self.cfg.quota_used_cmd,\n            }\n        )\n        self.templates = rdw_templating.TemplateManager()\n        session_storage_class = cherrypy.lib.sessions.RamSession\n        rate_limit_storage_class = rdiffweb.tools.ratelimit.RamRateLimit\n        if self._session_dir:\n            session_storage_class = cherrypy.lib.sessions.FileSession\n            rate_limit_storage_class = rdiffweb.tools.ratelimit.FileRateLimit\n        config = {\n            '/': {\n                'request.uri_encoding': 'ISO-8859-1',\n                'tools.auth_basic.realm': 'rdiffweb',\n                'tools.auth_basic.checkpassword': self._checkpassword,\n                'tools.auth_form.on': True,\n                'tools.currentuser.on': True,\n                'tools.currentuser.userobj': lambda username: self.store.get_user(username),\n                'tools.csrf.on': True,\n                'tools.i18n.on': True,\n                'tools.i18n.default': 'en_US',\n                'tools.i18n.mo_dir': pkg_resources.resource_filename('rdiffweb', 'locales'),\n                'tools.i18n.domain': 'messages',\n                'tools.encode.on': True,\n                'tools.encode.encoding': 'utf-8',\n                'tools.gzip.on': True,\n                'error_page.default': self.error_page,\n                'tools.sessions.on': True,\n                'tools.sessions.debug': cfg.debug,\n                'tools.sessions.storage_class': session_storage_class,\n                'tools.sessions.storage_path': self._session_dir,\n                'tools.sessions.httponly': True,\n                'tools.ratelimit.debug': cfg.debug,\n                'tools.ratelimit.delay': 60,\n                'tools.ratelimit.anonymous_limit': cfg.rate_limit,\n                'tools.ratelimit.storage_class': rate_limit_storage_class,\n                'tools.ratelimit.storage_path': self._session_dir,\n            },\n        }\n        Application.__init__(self, root=Root(), config=config)\n        self.root.favicon_ico = static(self._favicon)\n        if self._header_logo:\n            self.root.header_logo = static(self._header_logo)\n        if self._tempdir:\n            os.environ[\"TMPDIR\"] = self._tempdir\n        self.store = Store(self)\n        self.store.create_admin_user()\n    def __init__(self):\n        HandlerTool.__init__(self, self.run, name='csrf')\n        self._priority = 71\n    def _set_headers(self):\n        response = cherrypy.serving.response\n        response.headers['X-Frame-Options'] = 'DENY'\n        cookie = response.cookie.get('session_id', None)\n        if cookie:\n            cookie['samesite'] = 'Lax'",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-3174",
        "description": "[{'lang': 'en', 'value': \"Sensitive Cookie in HTTPS Session Without 'Secure' Attribute in GitHub repository ikus060/rdiffweb prior to 2.4.2.\"}]",
        "cwe_number": 311
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-152",
      "code": "def spider_list(request, client_id, project_name):\n    \"\"\"\n    get spider list from one client\n    :param request: request Object\n    :param client_id: client id\n    :param project_name: project name\n    :return: json\n    \"\"\"\n    if request.method == 'GET':\n        client = Client.objects.get(id=client_id)\n        scrapyd = get_scrapyd(client)\n        spiders = scrapyd.list_spiders(project_name)\n        spiders = [{'name': spider, 'id': index + 1} for index, spider in enumerate(spiders)]\n        return JsonResponse(spiders)\ndef spider_start(request, client_id, project_name, spider_name):\n    \"\"\"\n    start a spider\n    :param request: request object\n    :param client_id: client id\n    :param project_name: project name\n    :param spider_name: spider name\n    :return: json\n    \"\"\"\n    if request.method == 'GET':\n        client = Client.objects.get(id=client_id)\n        scrapyd = get_scrapyd(client)\n        job = scrapyd.schedule(project_name, spider_name)\n        return JsonResponse({'job': job})\ndef project_configure(request, project_name):\n    \"\"\"\n    get configuration\n    :param request: request object\n    :param project_name: project name\n    :return: json\n    \"\"\"\n    if request.method == 'GET':\n        project = Project.objects.get(name=project_name)\n        project = model_to_dict(project)\n        project['configuration'] = json.loads(project['configuration']) if project['configuration'] else None\n        return JsonResponse(project)\n    elif request.method == 'POST':\n        project = Project.objects.filter(name=project_name)\n        data = json.loads(request.body)\n        configuration = json.dumps(data.get('configuration'), ensure_ascii=False)\n        project.update(**{'configuration': configuration})\n        project_name = re.sub('[\\!\\@\\\n        cmd = ' '.join(['gerapy', 'generate', project_name])\n        p = Popen(cmd, shell=True, stdin=PIPE, stdout=PIPE, stderr=PIPE)\n        stdout, stderr = bytes2str(p.stdout.read()), bytes2str(p.stderr.read())\n        if not stderr:\n            return JsonResponse({'status': '1'})\n        else:\n            return JsonResponse({'status': '0', 'message': stderr})\ndef project_tree(request, project_name):\n    \"\"\"\n    get file tree of project\n    :param request: request object\n    :param project_name: project name\n    :return: json of tree\n    \"\"\"\n    if request.method == 'GET':\n        path = os.path.abspath(join(os.getcwd(), PROJECTS_FOLDER))\n        tree = get_tree(join(path, project_name))\n        return JsonResponse(tree)\ndef project_create(request):\n    \"\"\"\n    create a configurable project\n    :param request: request object\n    :return: json\n    \"\"\"\n    if request.method == 'POST':\n        data = json.loads(request.body)\n        data['configurable'] = 1\n        project, result = Project.objects.update_or_create(**data)\n        path = join(os.path.abspath(join(os.getcwd(), PROJECTS_FOLDER)), data['name'])\n        os.mkdir(path)\n        return JsonResponse(model_to_dict(project))\ndef project_upload(request):\n    \"\"\"\n    upload project\n    :param request: request object\n    :return: json\n    \"\"\"\n    if request.method == 'POST':\n        file = request.FILES['file']\n        file_name = file.name\n        fs = FileSystemStorage(PROJECTS_FOLDER)\n        zip_file_name = fs.save(file_name, file)\n        logger.debug('zip file name %s', zip_file_name)\n        with zipfile.ZipFile(join(PROJECTS_FOLDER, zip_file_name), 'r') as zip_ref:\n            zip_ref.extractall(PROJECTS_FOLDER)\n        logger.debug('extracted files to %s', PROJECTS_FOLDER)\n        return JsonResponse({'status': True})\ndef project_clone(request):\n    \"\"\"\n    clone project from github\n    :param request: request object\n    :return: json\n    \"\"\"\n    if request.method == 'POST':\n        data = json.loads(request.body)\n        address = data.get('address')\n        if not address.startswith('http'):\n            return JsonResponse({'status': False})\n        address = address + '.git' if not address.endswith('.git') else address\n        cmd = 'git clone {address} {target}'.format(address=address, target=join(PROJECTS_FOLDER, Path(address).stem))\n        logger.debug('clone cmd %s', cmd)\n        p = Popen(cmd, shell=True, stdin=PIPE, stdout=PIPE, stderr=PIPE)\n        stdout, stderr = bytes2str(p.stdout.read()), bytes2str(p.stderr.read())\n        logger.debug('clone run result %s', stdout)\n        if stderr: logger.error(stderr)\n        return JsonResponse({'status': True}) if not stderr else JsonResponse({'status': False})\ndef project_remove(request, project_name):\n    \"\"\"\n    remove project from disk and db\n    :param request: request object\n    :param project_name: project name\n    :return: result of remove\n    \"\"\"\n    if request.method == 'POST':\n        project = Project.objects.get(name=project_name)\n        Deploy.objects.filter(project=project).delete()\n        result = Project.objects.filter(name=project_name).delete()\n        path = join(os.path.abspath(os.getcwd()), PROJECTS_FOLDER)\n        project_path = join(path, project_name)\n        if exists(project_path):\n            rmtree(project_path)\n        return JsonResponse({'result': result})\ndef project_version(request, client_id, project_name):\n    \"\"\"\n    get project deploy version\n    :param request: request object\n    :param client_id: client id\n    :param project_name: project name\n    :return: deploy version of project\n    \"\"\"\n    if request.method == 'GET':\n        client = Client.objects.get(id=client_id)\n        project = Project.objects.get(name=project_name)\n        scrapyd = get_scrapyd(client)\n        if Deploy.objects.filter(client=client, project=project):\n            deploy = Deploy.objects.get(client=client, project=project)\n        else:\n            try:\n                versions = scrapyd.list_versions(project_name)\n            except ConnectionError:\n                return JsonResponse({'message': 'Connect Error'}, status=500)\n            if len(versions) > 0:\n                version = versions[-1]\n                deployed_at = timezone.datetime.fromtimestamp(int(version), tz=pytz.timezone(TIME_ZONE))\n            else:\n                deployed_at = None\n            deploy, result = Deploy.objects.update_or_create(client=client, project=project, deployed_at=deployed_at)\n        return JsonResponse(model_to_dict(deploy))\ndef project_deploy(request, client_id, project_name):\n    \"\"\"\n    deploy project operation\n    :param request: request object\n    :param client_id: client id\n    :param project_name: project name\n    :return: json of deploy result\n    \"\"\"\n    if request.method == 'POST':\n        path = os.path.abspath(join(os.getcwd(), PROJECTS_FOLDER))\n        project_path = join(path, project_name)\n        egg = find_egg(project_path)\n        if not egg:\n            return JsonResponse({'message': 'egg not found'}, status=500)\n        egg_file = open(join(project_path, egg), 'rb')\n        client = Client.objects.get(id=client_id)\n        project = Project.objects.get(name=project_name)\n        scrapyd = get_scrapyd(client)\n        scrapyd.add_version(project_name, int(time.time()), egg_file.read())\n        deployed_at = timezone.now()\n        Deploy.objects.filter(client=client, project=project).delete()\n        deploy, result = Deploy.objects.update_or_create(client=client, project=project, deployed_at=deployed_at,\n                                                         description=project.description)\n        return JsonResponse(model_to_dict(deploy))\ndef project_build(request, project_name):\n    \"\"\"\n    get build info or execute build operation\n    :param request: request object\n    :param project_name: project name\n    :return: json\n    \"\"\"\n    path = os.path.abspath(join(os.getcwd(), PROJECTS_FOLDER))\n    project_path = join(path, project_name)\n    if request.method == 'GET':\n        egg = find_egg(project_path)\n        if egg:\n            built_at = timezone.datetime.fromtimestamp(os.path.getmtime(join(project_path, egg)),\n                                                       tz=pytz.timezone(TIME_ZONE))\n            if not Project.objects.filter(name=project_name):\n                Project(name=project_name, built_at=built_at, egg=egg).save()\n                model = Project.objects.get(name=project_name)\n            else:\n                model = Project.objects.get(name=project_name)\n                model.built_at = built_at\n                model.egg = egg\n                model.save()\n        else:\n            if not Project.objects.filter(name=project_name):\n                Project(name=project_name).save()\n            model = Project.objects.get(name=project_name)\n        data = model_to_dict(model)\n        return JsonResponse(data)\n    elif request.method == 'POST':\n        data = json.loads(request.body)\n        description = data['description']\n        build_project(project_name)\n        egg = find_egg(project_path)\n        if not egg:\n            return JsonResponse({'message': 'egg not found'}, status=500)\n        built_at = timezone.now()\n        if not Project.objects.filter(name=project_name):\n            Project(name=project_name, description=description, built_at=built_at, egg=egg).save()\n            model = Project.objects.get(name=project_name)\n        else:\n            model = Project.objects.get(name=project_name)\n            model.built_at = built_at\n            model.egg = egg\n            model.description = description\n            model.save()\n        data = model_to_dict(model)\n        return JsonResponse(data)\ndef project_parse(request, project_name):\n    \"\"\"\n    parse project\n    :param request: request object\n    :param project_name: project name\n    :return: requests, items, response\n    \"\"\"\n    if request.method == 'POST':\n        project_path = join(PROJECTS_FOLDER, project_name)\n        data = json.loads(request.body)\n        logger.debug('post data %s', data)\n        spider_name = data.get('spider')\n        args = {\n            'start': data.get('start', False),\n            'method': data.get('method', 'GET'),\n            'url': data.get('url'),\n            'callback': data.get('callback'),\n            'cookies': \"'\" + json.dumps(data.get('cookies', {}), ensure_ascii=False) + \"'\",\n            'headers': \"'\" + json.dumps(data.get('headers', {}), ensure_ascii=False) + \"'\",\n            'meta': \"'\" + json.dumps(data.get('meta', {}), ensure_ascii=False) + \"'\",\n            'dont_filter': data.get('dont_filter', False),\n            'priority': data.get('priority', 0),\n        }\n        body = data.get('body', '')\n        if args.get('method').lower() != 'get':\n            args['body'] = \"'\" + json.dumps(body, ensure_ascii=False) + \"'\"\n        args_cmd = ' '.join(\n            ['--{arg} {value}'.format(arg=arg, value=value) for arg, value in args.items()])\n        logger.debug('args cmd %s', args_cmd)\n        cmd = 'gerapy parse {args_cmd} {project_path} {spider_name}'.format(\n            args_cmd=args_cmd,\n            project_path=project_path,\n            spider_name=spider_name\n        )\n        logger.debug('parse cmd %s', cmd)\n        p = Popen(cmd, shell=True, stdin=PIPE, stdout=PIPE, stderr=PIPE, close_fds=True)\n        stdout, stderr = bytes2str(p.stdout.read()), bytes2str(p.stderr.read())\n        logger.debug('stdout %s, stderr %s', stdout, stderr)\n        if not stderr:\n            return JsonResponse({'status': True, 'result': json.loads(stdout)})\n        else:\n            return JsonResponse({'status': False, 'message': stderr})\ndef project_file_read(request):\n    \"\"\"\n    get content of project file\n    :param request: request object\n    :return: file content\n    \"\"\"\n    if request.method == 'POST':\n        data = json.loads(request.body)\n        path = join(data['path'], data['label'])\n        with open(path, 'rb') as f:\n            return HttpResponse(f.read().decode('utf-8'))\ndef job_log(request, client_id, project_name, spider_name, job_id):\n    \"\"\"\n    get log of jog\n    :param request: request object\n    :param client_id: client id\n    :param project_name: project name\n    :param spider_name: spider name\n    :param job_id: job id\n    :return: log of job\n    \"\"\"\n    if request.method == 'GET':\n        client = Client.objects.get(id=client_id)\n        url = log_url(client.ip, client.port, project_name, spider_name, job_id)\n        response = requests.get(url, timeout=5, headers={\n            'Range': 'bytes=-1000'\n        }, auth=(client.username, client.password) if client.auth else None)\n        encoding = response.apparent_encoding\n        if response.status_code == 404:\n            return JsonResponse({'message': 'Log Not Found'}, status=404)\n        text = response.content.decode(encoding, errors='replace')\n        return HttpResponse(text)\ndef job_cancel(request, client_id, project_name, job_id):\n    \"\"\"\n    cancel a job\n    :param request: request object\n    :param client_id: client id\n    :param project_name: project name\n    :param job_id: job id\n    :return: json of cancel\n    \"\"\"\n    if request.method == 'GET':\n        client = Client.objects.get(id=client_id)\n        scrapyd = get_scrapyd(client)\n        result = scrapyd.cancel(project_name, job_id)\n        return JsonResponse(result)\ndef task_create(request):\n    \"\"\"\n    add task\n    :param request: request object\n    :return: Bool\n    \"\"\"\n    if request.method == 'POST':\n        data = json.loads(request.body)\n        task = Task.objects.create(clients=json.dumps(data.get('clients'), ensure_ascii=False),\n                                   project=data.get('project'),\n                                   name=data.get('name'),\n                                   spider=data.get('spider'),\n                                   trigger=data.get('trigger'),\n                                   configuration=json.dumps(data.get('configuration'), ensure_ascii=False),\n                                   modified=1)\n        return JsonResponse({'result': '1', 'data': model_to_dict(task)})\ndef task_update(request, task_id):\n    \"\"\"\n    update task info\n    :param request: request object\n    :param task_id: task id\n    :return: json\n    \"\"\"\n    if request.method == 'POST':\n        task = Task.objects.filter(id=task_id)\n        data = json.loads(request.body)\n        data['clients'] = json.dumps(data.get('clients'), ensure_ascii=False)\n        data['configuration'] = json.dumps(data.get('configuration'), ensure_ascii=False)\n        data['modified'] = 1\n        task.update(**data)\n        return JsonResponse(model_to_dict(Task.objects.get(id=task_id)))\ndef task_remove(request, task_id):\n    \"\"\"\n    remove task by task_id\n    :param request:\n    :return:\n    \"\"\"\n    if request.method == 'POST':\n        task = Task.objects.get(id=task_id)\n        clients = clients_of_task(task)\n        for client in clients:\n            job_id = get_job_id(client, task)\n            DjangoJob.objects.filter(name=job_id).delete()\n        Task.objects.filter(id=task_id).delete()\n        return JsonResponse({'result': '1'})\ndef task_info(request, task_id):\n    \"\"\"\n    get task info\n    :param request: request object\n    :param task_id: task id\n    :return: json\n    \"\"\"\n    if request.method == 'GET':\n        task = Task.objects.get(id=task_id)\n        data = model_to_dict(task)\n        data['clients'] = json.loads(data.get('clients'))\n        data['configuration'] = json.loads(data.get('configuration'))\n        return JsonResponse({'data': data})\ndef task_status(request, task_id):\n    \"\"\"\n    get task status info\n    :param request: request object\n    :param task_id: task id\n    :return:\n    \"\"\"\n    if request.method == 'GET':\n        result = []\n        task = Task.objects.get(id=task_id)\n        clients = clients_of_task(task)\n        for client in clients:\n            job_id = get_job_id(client, task)\n            jobs = DjangoJob.objects.filter(name=job_id)\n            logger.debug('jobs from djangojob %s', jobs)\n            if not jobs: continue\n            job = DjangoJob.objects.get(name=job_id)\n            executions = serialize('json', DjangoJobExecution.objects.filter(job=job))\n            result.append({\n                'client': model_to_dict(client),\n                'next': job.next_run_time,\n                'executions': json.loads(executions)\n            })\n        return JsonResponse({'data': result})\ndef render_html(request):\n    \"\"\"\n    render html with url\n    :param request:\n    :return:\n    \"\"\"\n    if request.method == 'GET':\n        url = request.GET.get('url')\n        url = unquote(base64.b64decode(url).decode('utf-8'))\n        js = request.GET.get('js', 0)\n        script = request.GET.get('script')\n        response = requests.get(url, timeout=5)\n        response.encoding = response.apparent_encoding\n        html = process_html(response.text)\n        return HttpResponse(html)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-43857",
        "description": "[{'lang': 'en', 'value': 'Gerapy is a distributed crawler management framework. Gerapy prior to version 0.9.8 is vulnerable to remote code execution, and this issue is patched in version 0.9.8.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-153",
      "code": "def loginForm(request, next=''):\n    if request.method == 'POST':\n        form = Login_Form(request.POST)\n        if form.is_valid():\n            username = request.POST.get('username')\n            password = request.POST.get('password')\n            user = user = authenticate(username=username, password=password)\n            if user and user.is_active:\n                login(request, user)\n                next_url = request.POST.get('next', '')\n                if next_url and next_url.startswith('/'):\n                    return HttpResponseRedirect(next_url)\n                else:\n                    return HttpResponseRedirect(reverse('CalendarinhoApp:Dashboard'))\n            else:\n                messages.error(request, \"Invalid login details given\")\n                form = Login_Form()\n                return render(request, 'CalendarinhoApp/login.html', {'form': form})\n    else:\n        form = Login_Form()\n        return render(request, 'CalendarinhoApp/login.html', {'form': form})",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-49281",
        "description": "[{'lang': 'en', 'value': 'Calendarinho is an open source calendaring application to manage large teams of consultants. An Open Redirect issue occurs when a web application redirects users to external URLs without proper validation. This can lead to phishing attacks, where users are tricked into visiting malicious sites, potentially leading to information theft and reputational damage to the website used for redirection. The problem is has been patched in commit `15b2393`. Users are advised to update to a commit after `15b2393`. There are no known workarounds for this vulnerability. '}]",
        "cwe_number": 601
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-154",
      "code": "def store_user_session():\n    if flask_session.get('_user_id', \"\"):\n        try:\n            if not check_user_session(flask_session.get('_user_id', \"\"), flask_session.get('_id', \"\")):\n                user_session = User_Sessions(flask_session.get('_user_id', \"\"), flask_session.get('_id', \"\"))\n                session.add(user_session)\n                session.commit()\n                log.info(\"Login and store session : \" + flask_session.get('_id', \"\"))\n            else:\n                log.info(\"Found stored session : \" + flask_session.get('_id', \"\"))\n        except (exc.OperationalError, exc.InvalidRequestError) as e:\n            session.rollback()\n            log.exception(e)\n    else:\n        log.error(\"No user id in session\")\ndef delete_user_session(user_id, session_key):\n    try:\n        log.info(\"Deleted session_key : \" + session_key)\n        session.query(User_Sessions).filter(User_Sessions.user_id==user_id,\n                                            User_Sessions.session_key==session_key).delete()\n        session.commit()\n    except (exc.OperationalError, exc.InvalidRequestError):\n        session.rollback()\n        log.exception(e)\ndef edit_book_comments(comments, book):\n    modif_date = False\n    if len(book.comments):\n        if book.comments[0].text != comments:\n            book.comments[0].text = comments\n            modif_date = True\n    else:\n        if comments:\n            book.comments.append(db.Comments(text=comments, book=book.id))\n            modif_date = True\n    return modif_date",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-25964",
        "description": "[{'lang': 'en', 'value': 'In \u201cCalibre-web\u201d application, v0.6.0 to v0.6.12, are vulnerable to Stored XSS in \u201cMetadata\u201d. An attacker that has access to edit the metadata information, can inject JavaScript payload in the description field. When a victim tries to open the file, XSS will be triggered.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-155",
      "code": "    def get_project_files(self, project_name: str):\n        if not project_name:\n            return []\n        project_directory = \"-\".join(project_name.split(\" \"))\n        directory = os.path.join(os.getcwd(), 'data', 'projects', project_directory)\n        if(not os.path.exists(directory)):\n            return []\n        files = []\n        for root, _, filenames in os.walk(directory):\n            for filename in filenames:\n                file_relative_path = os.path.relpath(root, directory)\n                if file_relative_path == '.': file_relative_path = ''\n                file_path = os.path.join(file_relative_path, filename)\n                print(\"file_path\",file_path)\n                try:\n                    with open(os.path.join(root, filename), 'r') as file:\n                        print(\"File:\", filename)\n                        files.append({\n                            \"file\": file_path,\n                            \"code\": file.read()\n                        })\n                except Exception as e:\n                    print(f\"Error reading file {filename}: {e}\")\n        return files\ndef create_project():\n    data = request.json\n    project_name = data.get(\"project_name\")\n    manager.create_project(project_name)\n    return jsonify({\"message\": \"Project created\"})\ndef delete_project():\n    data = request.json\n    project_name = data.get(\"project_name\")\n    manager.delete_project(project_name)\n    AgentState().delete_state(project_name)\n    return jsonify({\"message\": \"Project deleted\"})\ndef download_project():\n    project_name = request.args.get(\"project_name\")\n    manager.project_to_zip(project_name)\n    project_path = manager.get_zip_path(project_name)\n    return send_file(project_path, as_attachment=False)\ndef download_project_pdf():\n    project_name = request.args.get(\"project_name\")\n    pdf_dir = Config().get_pdfs_dir()\n    pdf_path = os.path.join(pdf_dir, f\"{project_name}.pdf\")\n    response = make_response(send_file(pdf_path))\n    response.headers['Content-Type'] = 'project_bplication/pdf'\n    return response\ndef project_files():\n    project_name = request.args.get(\"project_name\")\n    files = AgentState.get_project_files(project_name)\n    return jsonify({\"files\": files})",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-5334",
        "description": "[{'lang': 'en', 'value': \"A local file read vulnerability exists in the stitionai/devika repository, affecting the latest version. The vulnerability is due to improper handling of the 'snapshot_path' parameter in the '/api/get-browser-snapshot' endpoint. An attacker can exploit this vulnerability by crafting a request with a malicious 'snapshot_path' parameter, leading to arbitrary file read from the system. This issue impacts the security of the application by allowing unauthorized access to sensitive files on the server.\"}]",
        "cwe_number": 73
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-156",
      "code": "def LoadSettingsFile(filename=SETTINGS_FILE):\n    \"\"\"Loads settings file in yaml format given file name.\n    :param filename: path for settings file. 'settings.yaml' by default.\n    :type filename: str.\n    :raises: SettingsError\n    \"\"\"\n    try:\n        with open(filename) as stream:\n            data = load(stream, Loader=Loader)\n    except (YAMLError, OSError) as e:\n        raise SettingsError(e)\n    return data",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-49297",
        "description": "[{'lang': 'en', 'value': 'PyDrive2 is a wrapper library of google-api-python-client that simplifies many common Google Drive API V2 tasks. Unsafe YAML deserilization will result in arbitrary code execution. A maliciously crafted YAML file can cause arbitrary code execution if PyDrive2 is run in the same directory as it, or if it is loaded in via `LoadSettingsFile`. This is a deserilization attack that will affect any user who initializes GoogleAuth from this package while a malicious yaml file is present in the same directory. This vulnerability does not require the file to be directly loaded through the code, only present. This issue has been addressed in commit `c57355dc` which is included in release version `1.16.2`. Users are advised to upgrade. There are no known workarounds for this vulnerability.'}]",
        "cwe_number": 502
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-157",
      "code": "    def __init__(self, directory=None, pattern='__jinja2_%s.cache'):\n        if directory is None:\n            directory = tempfile.gettempdir()\n        self.directory = directory\n        self.pattern = pattern",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2014-0012",
        "description": "[{'lang': 'en', 'value': \"FileSystemBytecodeCache in Jinja2 2.7.2 does not properly create temporary directories, which allows local users to gain privileges by pre-creating a temporary directory with a user's uid.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2014-1402.\"}]",
        "cwe_number": 264
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-158",
      "code": "    def perform_mutation(\n        cls,\n        _root,\n        info,\n        /,\n        shipping_address,\n        validation_rules=None,\n        checkout_id=None,\n        token=None,\n        id=None,\n    ):\n        checkout = get_checkout(\n            cls,\n            info,\n            checkout_id=checkout_id,\n            token=token,\n            id=id,\n            qs=models.Checkout.objects.prefetch_related(\n                \"lines__variant__product__product_type\"\n            ),\n        )\n        use_legacy_error_flow_for_checkout = (\n            checkout.channel.use_legacy_error_flow_for_checkout\n        )\n        lines, _ = fetch_checkout_lines(\n            checkout,\n        )\n        if use_legacy_error_flow_for_checkout and not is_shipping_required(lines):\n            raise ValidationError(\n                {\n                    \"shipping_address\": ValidationError(\n                        ERROR_DOES_NOT_SHIP,\n                        code=CheckoutErrorCode.SHIPPING_NOT_REQUIRED.value,\n                    )\n                }\n            )\n        address_validation_rules = validation_rules or {}\n        shipping_address_instance = cls.validate_address(\n            shipping_address,\n            address_type=AddressType.SHIPPING,\n            instance=checkout.shipping_address,\n            info=info,\n            format_check=address_validation_rules.get(\"check_fields_format\", True),\n            required_check=address_validation_rules.get(\"check_required_fields\", True),\n            enable_normalization=address_validation_rules.get(\n                \"enable_fields_normalization\", True\n            ),\n        )\n        manager = get_plugin_manager_promise(info.context).get()\n        shipping_channel_listings = checkout.channel.shipping_method_listings.all()\n        checkout_info = fetch_checkout_info(\n            checkout, lines, manager, shipping_channel_listings\n        )\n        country = shipping_address_instance.country.code\n        checkout.set_country(country, commit=True)\n        if lines and use_legacy_error_flow_for_checkout:\n            cls.process_checkout_lines(\n                info,\n                lines,\n                country,\n                checkout_info.channel.slug,\n                checkout_info.delivery_method_info,\n            )\n        update_checkout_shipping_method_if_invalid(checkout_info, lines)\n        shipping_address_updated_fields = []\n        with traced_atomic_transaction():\n            shipping_address_instance.save()\n            shipping_address_updated_fields = change_shipping_address_in_checkout(\n                checkout_info,\n                shipping_address_instance,\n                lines,\n                manager,\n                shipping_channel_listings,\n            )\n        invalidate_prices_updated_fields = invalidate_checkout(\n            checkout_info, lines, manager, save=False\n        )\n        checkout.save(\n            update_fields=shipping_address_updated_fields\n            + invalidate_prices_updated_fields\n        )\n        cls.call_event(manager.checkout_updated, checkout)\n        return CheckoutShippingAddressUpdate(checkout=checkout)\n    def _update_delivery_method(\n        cls,\n        manager,\n        checkout_info: \"CheckoutInfo\",\n        lines: Iterable[\"CheckoutLineInfo\"],\n        *,\n        shipping_method: Optional[ShippingMethod],\n        external_shipping_method: Optional[shipping_interface.ShippingMethodData],\n        collection_point: Optional[Warehouse],\n    ) -> None:\n        checkout = checkout_info.checkout\n        if external_shipping_method:\n            set_external_shipping_id(\n                checkout=checkout, app_shipping_id=external_shipping_method.id\n            )\n        else:\n            delete_external_shipping_id(checkout=checkout)\n        checkout.shipping_method = shipping_method\n        checkout.collection_point = collection_point\n        invalidate_prices_updated_fields = invalidate_checkout(\n            checkout_info, lines, manager, save=False\n        )\n        checkout.save(\n            update_fields=[\n                \"shipping_method\",\n                \"collection_point\",\n            ]\n            + invalidate_prices_updated_fields\n        )\n        get_or_create_checkout_metadata(checkout).save()\n        cls.call_event(manager.checkout_updated, checkout)\n    def _resolve_delivery_method_type(id_) -> Optional[str]:\n        if id_ is None:\n            return None\n        possible_types = (\"Warehouse\", \"ShippingMethod\", APP_ID_PREFIX)\n        type_, id_ = from_global_id_or_error(id_)\n        str_type = str(type_)\n        if str_type not in possible_types:\n            raise ValidationError(\n                {\n                    \"delivery_method_id\": ValidationError(\n                        \"ID does not belong to Warehouse or ShippingMethod\",\n                        code=CheckoutErrorCode.INVALID.value,\n                    )\n                }\n            )\n        return str_type",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-29888",
        "description": "[{'lang': 'en', 'value': 'Saleor is an e-commerce platform that serves high-volume companies. When using `Pickup: Local stock only` click-and-collect as a delivery method in specific conditions the customer could overwrite the warehouse address with its own, which exposes its address as click-and-collect address. This issue has been patched in versions: `3.14.61`, `3.15.37`, `3.16.34`, `3.17.32`, `3.18.28`, `3.19.15`.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-159",
      "code": "def _constant_value(ragged_factory, inner_factory, pylist, dtype, ragged_rank,\n                    inner_shape):\n  \"\"\"Constructs a constant RaggedTensor or RaggedTensorValue.\n  Args:\n    ragged_factory: A factory function with the signature:\n      `ragged_factory(values, row_splits)`\n    inner_factory: A factory function with the signature: `inner_factory(pylist,\n      dtype, shape, name)`\n    pylist: A nested `list`, `tuple` or `np.ndarray`.\n    dtype: Data type for returned value.\n    ragged_rank: Ragged rank for returned value.\n    inner_shape: Inner value shape for returned value.\n  Returns:\n    A value returned by `ragged_factory` or `inner_factory`.\n  Raises:\n    ValueError: If the scalar values in `pylist` have inconsistent nesting\n      depth; or if ragged_rank or inner_shape are incompatible with `pylist`.\n  \"\"\"\n  if ragged_tensor.is_ragged(pylist):\n    raise TypeError(\"pylist may not be a RaggedTensor or RaggedTensorValue.\")\n  if not isinstance(pylist, (list, tuple)) and np.ndim(pylist) == 0:\n    if ragged_rank is not None and ragged_rank != 0:\n      raise ValueError(\"Invalid pylist=%r: incompatible with ragged_rank=%d\" %\n                       (pylist, ragged_rank))\n    if inner_shape is not None and inner_shape:\n      raise ValueError(\n          \"Invalid pylist=%r: incompatible with dim(inner_shape)=%d\" %\n          (pylist, len(inner_shape)))\n    return inner_factory(pylist, dtype, ())\n  if ragged_rank is not None and ragged_rank < 0:\n    raise ValueError(\n        \"Invalid ragged_rank=%r: must be nonnegative\" % ragged_rank)\n  scalar_depth, max_depth = _find_scalar_and_max_depth(pylist)\n  if scalar_depth is not None:\n    if max_depth > scalar_depth:\n      raise ValueError(\"Invalid pylist=%r: empty list nesting is greater \"\n                       \"than scalar value nesting\" % pylist)\n  if inner_shape is not None and ragged_rank is not None:\n    expected_depth = ragged_rank + len(inner_shape) + 1\n    if ((scalar_depth is not None and expected_depth != scalar_depth) or\n        (scalar_depth is None and expected_depth < max_depth)):\n      raise ValueError(\n          \"Invalid pylist=%r: incompatible with ragged_rank=%d \"\n          \"and dim(inner_shape)=%d\" % (pylist, ragged_rank, len(inner_shape)))\n  if (ragged_rank == 0 or\n      (ragged_rank is None and\n       ((max_depth < 2) or\n        (inner_shape is not None and max_depth - len(inner_shape) < 2)))):\n    return inner_factory(pylist, dtype, inner_shape)\n  if inner_shape is None:\n    if ragged_rank is None:\n      inner_shape = ()\n    else:\n      inner_shape = _default_inner_shape_for_pylist(pylist, ragged_rank)\n  if ragged_rank is None:\n    if scalar_depth is None:\n      ragged_rank = max(1, max_depth - 1)\n    else:\n      ragged_rank = max(1, scalar_depth - 1 - len(inner_shape))\n  nested_splits = []\n  values = pylist\n  for dim in range(ragged_rank):\n    nested_splits.append([0])\n    concatenated_values = []\n    for row in values:\n      nested_splits[dim].append(nested_splits[dim][-1] + len(row))\n      concatenated_values.extend(row)\n    values = concatenated_values\n  values = inner_factory(\n      values, dtype=dtype, shape=(len(values),) + inner_shape, name=\"values\")\n  for row_splits in reversed(nested_splits):\n    values = ragged_factory(values, row_splits)\n  return values\ndef _find_scalar_and_max_depth(pylist):\n  \"\"\"Finds nesting depth of scalar values in pylist.\n  Args:\n    pylist: A nested python `list` or `tuple`.\n  Returns:\n    A tuple `(scalar_depth, max_depth)`.  `scalar_depth` is the nesting\n    depth of scalar values in `pylist`, or `None` if `pylist` contains no\n    scalars.  `max_depth` is the maximum depth of `pylist` (including\n    empty lists).\n  Raises:\n    ValueError: If pylist has inconsistent nesting depths for scalars.\n  \"\"\"\n  if isinstance(pylist, (list, tuple)) or np.ndim(pylist) != 0:\n    scalar_depth = None\n    max_depth = 1\n    for child in pylist:\n      child_scalar_depth, child_max_depth = _find_scalar_and_max_depth(child)\n      if child_scalar_depth is not None:\n        if scalar_depth is not None and scalar_depth != child_scalar_depth + 1:\n          raise ValueError(\"all scalar values must have the same nesting depth\")\n        scalar_depth = child_scalar_depth + 1\n      max_depth = max(max_depth, child_max_depth + 1)\n    return (scalar_depth, max_depth)\n  return (0, 0)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-29202",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. Prior to versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4, the implementation of `tf.ragged.constant` does not fully validate the input arguments. This results in a denial of service by consuming all available memory. Versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4 contain a patch for this issue.'}]",
        "cwe_number": 1284
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-160",
      "code": "def main():\n    if len(sys.argv) == 1:\n        sys.argv.append(\"spec\")\n    try:\n        with open(config.scriptPath(\"..\", \"semver.txt\")) as fh:\n            semver = fh.read().strip()\n            semverText = f\"Bikeshed v{semver}: \"\n    except FileNotFoundError:\n        semver = \"???\"\n        semverText = \"\"\n    argparser = argparse.ArgumentParser(\n        description=f\"{semverText}Processes spec source files into valid HTML.\"\n    )\n    argparser.add_argument(\"--version\", action=\"version\", version=semver)\n    argparser.add_argument(\n        \"-q\",\n        \"--quiet\",\n        dest=\"quiet\",\n        action=\"count\",\n        default=0,\n        help=\"Silences one level of message, least-important first.\",\n    )\n    argparser.add_argument(\n        \"-s\",\n        \"--silent\",\n        dest=\"silent\",\n        action=\"store_true\",\n        help=\"Shorthand for 'as many -q as you need to shut it up'\",\n    )\n    argparser.add_argument(\n        \"-f\",\n        \"--force\",\n        dest=\"errorLevel\",\n        action=\"store_const\",\n        const=\"nothing\",\n        help=\"Force the preprocessor to run to completion; fatal errors don't stop processing.\",\n    )\n    argparser.add_argument(\n        \"-d\",\n        \"--dry-run\",\n        dest=\"dryRun\",\n        action=\"store_true\",\n        help=\"Prevents the processor from actually saving anything to disk, but otherwise fully runs.\",\n    )\n    argparser.add_argument(\n        \"-a\",\n        \"--ascii-only\",\n        dest=\"asciiOnly\",\n        action=\"store_true\",\n        help=\"Force all Bikeshed messages to be ASCII-only.\",\n    )\n    argparser.add_argument(\n        \"--print\",\n        dest=\"printMode\",\n        action=\"store\",\n        default=None,\n        help=\"Print mode. Options are 'plain' (just text), 'console' (colored with console color codes), 'markup', and 'json'. Defaults to 'console'.\",\n    )\n    argparser.add_argument(\n        \"--die-on\",\n        dest=\"errorLevel\",\n        choices=[\"nothing\", \"fatal\", \"link-error\", \"warning\", \"everything\"],\n        help=\"Determines what sorts of errors cause Bikeshed to die (quit immediately with an error status code). Default is 'fatal'; the -f flag is a shorthand for 'nothing'\",\n    )\n    subparsers = argparser.add_subparsers(title=\"Subcommands\", dest=\"subparserName\")\n    specParser = subparsers.add_parser(\n        \"spec\", help=\"Process a spec source file into a valid output file.\"\n    )\n    specParser.add_argument(\n        \"infile\", nargs=\"?\", default=None, help=\"Path to the source file.\"\n    )\n    specParser.add_argument(\n        \"outfile\", nargs=\"?\", default=None, help=\"Path to the output file.\"\n    )\n    specParser.add_argument(\n        \"--debug\",\n        dest=\"debug\",\n        action=\"store_true\",\n        help=\"Switches on some debugging tools. Don't use for production!\",\n    )\n    specParser.add_argument(\n        \"--gh-token\",\n        dest=\"ghToken\",\n        nargs=\"?\",\n        help=\"GitHub access token. Useful to avoid API rate limits. Generate tokens: https://github.com/settings/tokens.\",\n    )\n    specParser.add_argument(\n        \"--byos\",\n        dest=\"byos\",\n        action=\"store_true\",\n        help=\"Bring-Your-Own-Spec: turns off all the Bikeshed auto-niceties, so you can piecemeal its features into your existing doc instead. Experimental, let me know if things get crashy or weird.\",\n    )\n    specParser.add_argument(\n        \"-l\",\n        \"--line-numbers\",\n        dest=\"lineNumbers\",\n        action=\"store_true\",\n        help=\"Hacky support for outputting line numbers on all error messages. Disables output, as this is hacky and might mess up your source.\",\n    )\n    echidnaParser = subparsers.add_parser(\n        \"echidna\",\n        help=\"Process a spec source file into a valid output file and publish it according to certain automatic protocols.\",\n    )\n    echidnaParser.add_argument(\n        \"infile\", nargs=\"?\", default=None, help=\"Path to the source file.\"\n    )\n    echidnaParser.add_argument(\n        \"--gh-token\",\n        dest=\"ghToken\",\n        nargs=\"?\",\n        help=\"GitHub access token. Useful to avoid API rate limits. Generate tokens: https://github.com/settings/tokens.\",\n    )\n    echidnaParser.add_argument(\n        \"--u\", dest=\"un\", metavar=\"USERNAME\", required=False, help=\"W3C username.\"\n    )\n    echidnaParser.add_argument(\n        \"--p\", dest=\"pw\", metavar=\"PASSWORD\", required=False, help=\"W3C password.\"\n    )\n    echidnaParser.add_argument(\n        \"--decision\",\n        dest=\"decision\",\n        metavar=\"DECISION_URL\",\n        required=False,\n        help=\"URL recording the decision to publish.\",\n    )\n    echidnaParser.add_argument(\n        \"--editorial\",\n        dest=\"editorial\",\n        action=\"store_true\",\n        required=False,\n        help=\"Flags update as editorial.\",\n    )\n    echidnaParser.add_argument(\n        \"--cc\",\n        dest=\"cc\",\n        metavar=\"EMAIL\",\n        required=False,\n        help=\"Comma-separated list of email addresses to ping with the publication status when complete.\",\n    )\n    echidnaParser.add_argument(\n        \"--additional-directories\",\n        dest=\"additionalDirectories\",\n        required=False,\n        nargs=\"*\",\n        help=\"Directories to bundle in the tar file. Defaults to examples/, diagrams/, and images/.\",\n    )\n    echidnaParser.add_argument(\n        \"--self-contained\",\n        dest=\"selfContained\",\n        action=\"store_true\",\n        help=\"The spec is self-contained, do not bundle any extra directories in the tar file.\",\n    )\n    echidnaParser.add_argument(\"--just-tar\", dest=\"justTar\", action=\"store_true\")\n    watchParser = subparsers.add_parser(\n        \"watch\",\n        help=\"Process a spec source file into a valid output file, automatically rebuilding when it changes.\",\n    )\n    watchParser.add_argument(\n        \"infile\", nargs=\"?\", default=None, help=\"Path to the source file.\"\n    )\n    watchParser.add_argument(\n        \"outfile\", nargs=\"?\", default=None, help=\"Path to the output file.\"\n    )\n    watchParser.add_argument(\n        \"--gh-token\",\n        dest=\"ghToken\",\n        nargs=\"?\",\n        help=\"GitHub access token. Useful to avoid API rate limits. Generate tokens: https://github.com/settings/tokens.\",\n    )\n    watchParser.add_argument(\n        \"--byos\",\n        dest=\"byos\",\n        action=\"store_true\",\n        help=\"Bring-Your-Own-Spec: turns off all the Bikeshed auto-niceties, so you can piecemeal its features into your existing doc instead. Experimental, let me know if things get crashy or weird.\",\n    )\n    serveParser = subparsers.add_parser(\n        \"serve\", help=\"Identical to 'watch', but also serves the folder on localhost.\"\n    )\n    serveParser.add_argument(\n        \"infile\", nargs=\"?\", default=None, help=\"Path to the source file.\"\n    )\n    serveParser.add_argument(\n        \"outfile\", nargs=\"?\", default=None, help=\"Path to the output file.\"\n    )\n    serveParser.add_argument(\n        \"--port\",\n        dest=\"port\",\n        nargs=\"?\",\n        default=\"8000\",\n        help=\"Specify the port to serve it over.\",\n    )\n    serveParser.add_argument(\n        \"--localhost\",\n        dest=\"localhost\",\n        action=\"store_true\",\n        help=\"Only allow connections from localhost.\",\n    )\n    serveParser.add_argument(\n        \"--gh-token\",\n        dest=\"ghToken\",\n        nargs=\"?\",\n        help=\"GitHub access token. Useful to avoid API rate limits. Generate tokens: https://github.com/settings/tokens.\",\n    )\n    serveParser.add_argument(\n        \"--byos\",\n        dest=\"byos\",\n        action=\"store_true\",\n        help=\"Bring-Your-Own-Spec: turns off all the Bikeshed auto-niceties, so you can piecemeal its features into your existing doc instead. Experimental, let me know if things get crashy or weird.\",\n    )\n    updateParser = subparsers.add_parser(\n        \"update\",\n        help=\"Update supporting files (those in /spec-data).\",\n        epilog=\"If no options are specified, everything is downloaded.\",\n    )\n    updateParser.add_argument(\n        \"--skip-manifest\",\n        dest=\"force\",\n        action=\"store_true\",\n        help=\"Forces Bikeshed to do a full update manually, rather than using the manifest to get the preprocessed update (which can be several minutes old).\",\n    )\n    updateParser.add_argument(\n        \"--anchors\", action=\"store_true\", help=\"Download crossref anchor data.\"\n    )\n    updateParser.add_argument(\n        \"--backrefs\", action=\"store_true\", help=\"Download link backref data.\"\n    )\n    updateParser.add_argument(\n        \"--biblio\", action=\"store_true\", help=\"Download biblio data.\"\n    )\n    updateParser.add_argument(\n        \"--caniuse\", action=\"store_true\", help=\"Download Can I Use... data.\"\n    )\n    updateParser.add_argument(\n        \"--mdn\", action=\"store_true\", help=\"Download MDN Spec Links... data.\"\n    )\n    updateParser.add_argument(\n        \"--link-defaults\",\n        dest=\"linkDefaults\",\n        action=\"store_true\",\n        help=\"Download link default data.\",\n    )\n    updateParser.add_argument(\n        \"--test-suites\",\n        dest=\"testSuites\",\n        action=\"store_true\",\n        help=\"Download test suite data.\",\n    )\n    updateParser.add_argument(\n        \"--languages\",\n        dest=\"languages\",\n        action=\"store_true\",\n        help=\"Download language/translation data.\",\n    )\n    updateParser.add_argument(\n        \"--wpt\",\n        dest=\"wpt\",\n        action=\"store_true\",\n        help=\"Download web-platform-tests data.\",\n    )\n    issueParser = subparsers.add_parser(\n        \"issues-list\",\n        help=\"Process a plain-text issues file into HTML. Call with no args to see an example input text.\",\n    )\n    issueParser.add_argument(\n        \"-t\",\n        dest=\"printTemplate\",\n        action=\"store_true\",\n        help=\"Output example Issues List template.\",\n    )\n    issueParser.add_argument(\n        \"infile\", nargs=\"?\", default=None, help=\"Path to the plain-text issue file.\"\n    )\n    issueParser.add_argument(\n        \"outfile\",\n        nargs=\"?\",\n        default=None,\n        help=\"Path to the output file. Default is file of the same name as input, with .html.\",\n    )\n    debugParser = subparsers.add_parser(\"debug\", help=\"Run various debugging commands.\")\n    debugParser.add_argument(\n        \"infile\", nargs=\"?\", default=None, help=\"Path to the source file.\"\n    )\n    debugCommands = debugParser.add_mutually_exclusive_group(required=True)\n    debugCommands.add_argument(\n        \"--print-exports\",\n        dest=\"printExports\",\n        action=\"store_true\",\n        help=\"Prints those terms that will be exported for cross-ref purposes.\",\n    )\n    debugCommands.add_argument(\n        \"--print-refs-for\",\n        dest=\"linkText\",\n        help=\"Prints the ref data for a given link text.\",\n    )\n    debugCommands.add_argument(\n        \"--print\", dest=\"code\", help=\"Runs the specified code and prints it.\"\n    )\n    debugCommands.add_argument(\n        \"--print-json\",\n        dest=\"jsonCode\",\n        help=\"Runs the specified code and prints it as formatted JSON.\",\n    )\n    debugCommands.add_argument(\n        \"--refresh-data\",\n        dest=\"refreshData\",\n        action=\"store_true\",\n        help=\"Clobbers the readonly data files with the mutable ones.\",\n    )\n    refParser = subparsers.add_parser(\"refs\", help=\"Search Bikeshed's ref database.\")\n    refParser.add_argument(\n        \"infile\", nargs=\"?\", default=None, help=\"Path to the source file.\"\n    )\n    refParser.add_argument(\"--text\", dest=\"text\", default=None)\n    refParser.add_argument(\"--type\", dest=\"linkType\", default=None)\n    refParser.add_argument(\"--for\", dest=\"linkFor\", default=None)\n    refParser.add_argument(\"--spec\", dest=\"spec\", default=None)\n    refParser.add_argument(\"--status\", dest=\"status\", default=None)\n    refParser.add_argument(\n        \"--exact\",\n        dest=\"exact\",\n        action=\"store_true\",\n        help=\"Only search for the exact text provided; don't apply Bikeshed's automatic conjugation help for plurals/etc.\",\n    )\n    refParser.add_argument(\n        \"--latest-only\",\n        dest=\"latestOnly\",\n        action=\"store_true\",\n        help=\"Apply Bikeshed's logic for only returning the latest version of a given ref when it exists in multiple levels of a spec.\",\n    )\n    sourceParser = subparsers.add_parser(\n        \"source\", help=\"Tools for formatting the *source* document.\"\n    )\n    sourceParser.add_argument(\n        \"--big-text\",\n        dest=\"bigText\",\n        action=\"store_true\",\n        help=\"Finds HTML comments containing 'Big Text: foo' and turns them into comments containing 'foo' in big text.\",\n    )\n    sourceParser.add_argument(\n        \"infile\", nargs=\"?\", default=None, help=\"Path to the source file.\"\n    )\n    sourceParser.add_argument(\n        \"outfile\", nargs=\"?\", default=None, help=\"Path to the output file.\"\n    )\n    testParser = subparsers.add_parser(\n        \"test\", help=\"Tools for running Bikeshed's testsuite.\"\n    )\n    testParser.add_argument(\n        \"--rebase\",\n        default=False,\n        action=\"store_true\",\n        help=\"Rebase the specified files.\",\n    )\n    testParser.add_argument(\n        \"--manual-only\",\n        dest=\"manualOnly\",\n        default=False,\n        action=\"store_true\",\n        help=\"Skip testing the real-world files in the repo, and only run the manually-written ones.\",\n    )\n    testParser.add_argument(\n        \"testFiles\",\n        default=[],\n        metavar=\"FILE\",\n        nargs=\"*\",\n        help=\"Run these tests. If called with no args, tests everything.\",\n    )\n    profileParser = subparsers.add_parser(\n        \"profile\",\n        help=\"Profiling Bikeshed. Needs graphviz, gprof2dot, and xdot installed.\",\n    )\n    profileParser.add_argument(\n        \"--root\",\n        dest=\"root\",\n        default=None,\n        metavar=\"ROOTFUNC\",\n        help=\"Prune the graph to start with the specified root node.\",\n    )\n    profileParser.add_argument(\n        \"--leaf\",\n        dest=\"leaf\",\n        default=None,\n        metavar=\"LEAFFUNC\",\n        help=\"Prune the graph to only show ancestors of the specified leaf node.\",\n    )\n    profileParser.add_argument(\n        \"--svg\",\n        dest=\"svgFile\",\n        default=None,\n        help=\"Save the graph to a specified SVG file, rather than outputting with xdot immediately.\",\n    )\n    subparsers.add_parser(\n        \"template\", help=\"Outputs a skeleton .bs file for you to start with.\"\n    )\n    wptParser = subparsers.add_parser(\n        \"wpt\", help=\"Tools for writing Web Platform Tests.\"\n    )\n    wptParser.add_argument(\n        \"--template\",\n        default=False,\n        action=\"store_true\",\n        help=\"Outputs a skeleton WPT file for you to start with.\",\n    )\n    options, extras = argparser.parse_known_args()\n    constants.quiet = options.quiet\n    if options.silent:\n        constants.quiet = float(\"infinity\")\n    constants.setErrorLevel(options.errorLevel)\n    constants.dryRun = options.dryRun\n    constants.asciiOnly = options.asciiOnly\n    if options.printMode is None:\n        if \"NO_COLOR\" in os.environ or os.environ.get(\"TERM\") == \"dumb\":\n            constants.printMode = \"plain\"\n        else:\n            constants.printMode = \"console\"\n    else:\n        constants.printMode = options.printMode\n    update.fixupDataFiles()\n    if options.subparserName == \"update\":\n        handleUpdate(options)\n    elif options.subparserName == \"spec\":\n        handleSpec(options, extras)\n    elif options.subparserName == \"echidna\":\n        handleEchidna(options, extras)\n    elif options.subparserName == \"watch\":\n        handleWatch(options, extras)\n    elif options.subparserName == \"serve\":\n        handleServe(options, extras)\n    elif options.subparserName == \"debug\":\n        handleDebug(options, extras)\n    elif options.subparserName == \"refs\":\n        handleRefs(options, extras)\n    elif options.subparserName == \"issues-list\":\n        handleIssuesList(options)\n    elif options.subparserName == \"source\":\n        handleSource(options)\n    elif options.subparserName == \"test\":\n        handleTest(options, extras)\n    elif options.subparserName == \"profile\":\n        handleProfile(options)\n    elif options.subparserName == \"template\":\n        handleTemplate()\n    elif options.subparserName == \"wpt\":\n        handleWpt(options)\ndef handleUpdate(options):\n    update.update(\n        anchors=options.anchors,\n        backrefs=options.backrefs,\n        biblio=options.biblio,\n        caniuse=options.caniuse,\n        mdn=options.mdn,\n        linkDefaults=options.linkDefaults,\n        testSuites=options.testSuites,\n        languages=options.languages,\n        wpt=options.wpt,\n        dryRun=constants.dryRun,\n        force=options.force,\n    )\ndef retrieveBoilerplateFile(doc, name, group=None, status=None, error=True):\n    if group is None and doc.md.group is not None:\n        group = doc.md.group.lower()\n    if status is None:\n        if doc.md.status is not None:\n            status = doc.md.status\n        elif doc.md.rawStatus is not None:\n            status = doc.md.rawStatus\n    megaGroup, status = splitStatus(status)\n    searchLocally = doc.md.localBoilerplate[name]\n    def boilerplatePath(*segs):\n        return scriptPath(\"boilerplate\", *segs)\n    statusFile = f\"{name}-{status}.include\"\n    genericFile = f\"{name}.include\"\n    sources = []\n    if searchLocally:\n        sources.append(doc.inputSource.relative(statusFile))\n        sources.append(doc.inputSource.relative(genericFile))\n    else:\n        for f in (statusFile, genericFile):\n            if doc.inputSource.cheaplyExists(f):\n                warn(\n                    (\n                        \"Found {0} next to the specification without a matching\\n\"\n                        + \"Local Boilerplate: {1} yes\\n\"\n                        + \"in the metadata. This include won't be found when building via a URL.\"\n                    ).format(f, name)\n                )\n                sources.append(doc.inputSource.relative(f))\n    if group:\n        sources.append(InputSource(boilerplatePath(group, statusFile)))\n        sources.append(InputSource(boilerplatePath(group, genericFile)))\n    if megaGroup:\n        sources.append(InputSource(boilerplatePath(megaGroup, statusFile)))\n        sources.append(InputSource(boilerplatePath(megaGroup, genericFile)))\n    sources.append(InputSource(boilerplatePath(statusFile)))\n    sources.append(InputSource(boilerplatePath(genericFile)))\n    doc.recordDependencies(*sources)\n    for source in sources:\n        if source is not None:\n            try:\n                return source.read().content\n            except OSError:\n                pass\n    else:\n        if error:\n            die(\n                \"Couldn't find an appropriate include file for the {0} inclusion, given group='{1}' and status='{2}'.\",\n                name,\n                group,\n                status,\n            )\n        return \"\"\n    def __init__(\n        self,\n        inputFilename,\n        debug=False,\n        token=None,\n        lineNumbers=False,\n        fileRequester=None,\n        testing=False,\n    ):\n        self.valid = False\n        self.lineNumbers = lineNumbers\n        if lineNumbers:\n            constants.dryRun = True\n        if inputFilename is None:\n            inputFilename = findImplicitInputFile()\n        if inputFilename is None:\n            die(\n                \"No input file specified, and no *.bs or *.src.html files found in current directory.\\nPlease specify an input file, or use - to pipe from STDIN.\"\n            )\n            return\n        self.inputSource = InputSource(inputFilename)\n        self.transitiveDependencies = set()\n        self.debug = debug\n        self.token = token\n        self.testing = testing\n        if fileRequester is None:\n            self.dataFile = config.defaultRequester\n        else:\n            self.dataFile = fileRequester\n        self.md = None\n        self.mdBaseline = None\n        self.mdDocument = None\n        self.mdCommandLine = None\n        self.mdDefaults = None\n        self.mdOverridingDefaults = None\n        self.lines = []\n        self.document = None\n        self.html = None\n        self.head = None\n        self.body = None\n        self.fillContainers = None\n        self.valid = self.initializeState()\ndef processTags(doc):\n    for el in findAll(\"[data-span-tag]\", doc):\n        tag = el.get(\"data-span-tag\")\n        if tag not in doc.md.inlineTagCommands:\n            die(\"Unknown inline tag '{0}' found:\\n  {1}\", tag, outerHTML(el), el=el)\n            continue\n        command = doc.md.inlineTagCommands[tag]\n        with Popen(command, stdin=PIPE, stdout=PIPE, stderr=PIPE, shell=True) as p:\n            out, err = p.communicate(innerHTML(el).encode(\"utf-8\"))\n            try:\n                out = out.decode(\"utf-8\")\n            except UnicodeDecodeError as e:\n                die(\n                    \"When trying to process {0}, got invalid unicode in stdout:\\n{1}\",\n                    outerHTML(el),\n                    e,\n                    el=el,\n                )\n            try:\n                err = err.decode(\"utf-8\")\n            except UnicodeDecodeError as e:\n                die(\n                    \"When trying to process {0}, got invalid unicode in stderr:\\n{1}\",\n                    outerHTML(el),\n                    e,\n                    el=el,\n                )\n            if p.returncode:\n                die(\n                    \"When trying to process {0}, got return code {1} and the following stderr:\\n{2}\",\n                    outerHTML(el),\n                    p.returncode,\n                    err,\n                    el=el,\n                )\n                continue\n            replaceContents(el, parseHTML(out))\ndef doEvery(s, action, lastTime=None):\n    import time\n    newTime = time.time()\n    if lastTime is None:\n        lastTime = newTime\n    if lastTime == 0 or newTime - lastTime > s:\n        action()\n        return newTime\n    return lastTime\n    def __new__(cls, sourceName: str):\n        \"\"\"Dispatches to the right subclass.\"\"\"\n        if cls != InputSource:\n            return super().__new__(cls)\n        if sourceName == \"-\":\n            return StdinInputSource(sourceName)\n        if sourceName.startswith(\"https:\"):\n            return UrlInputSource(sourceName)\n        return FileInputSource(sourceName)\n    def __init__(self, sourceName: str):\n        self.sourceName = sourceName\n        self.type = \"file\"\n        self.content = None\n    def __str__(self) -> str:\n        return self.sourceName\n    def read(self) -> InputContent:\n        with open(self.sourceName, encoding=\"utf-8\") as f:\n            return InputContent(\n                f.readlines(),\n                datetime.fromtimestamp(os.path.getmtime(self.sourceName)).date(),\n            )\n    def mtime(self) -> Optional[float]:\n        \"\"\"Returns the last modification time of this file, or None if it doesn't exist.\"\"\"\n        try:\n            return os.stat(self.sourceName).st_mtime\n        except FileNotFoundError:\n            return None\ndef load(doc):\n    code = config.retrieveBoilerplateFile(doc, \"bs-extensions\")\n    exec(code, globals())",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-23422",
        "description": "[{'lang': 'en', 'value': 'This affects the package bikeshed before 3.0.0. This can occur when an untrusted source file containing Inline Tag Command metadata is processed. When an arbitrary OS command is executed, the command output would be included in the HTML output.'}]",
        "cwe_number": 78
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-161",
      "code": "    def exec_command(self, cmd, tmp_path, become_user=None, sudoable=False, executable='/bin/sh', in_data=None):\n        ''' run a command on the chroot '''\n        if sudoable and self.runner.become and self.runner.become_method not in self.become_methods_supported:\n            raise errors.AnsibleError(\"Internal Error: this module does not support running commands via %s\" % self.runner.become_method)\n        if in_data:\n            raise errors.AnsibleError(\"Internal Error: this module does not support optimized module pipelining\")\n        if executable:\n            local_cmd = [self.chroot_cmd, self.chroot, executable, '-c', cmd]\n        else:\n            local_cmd = '%s \"%s\" %s' % (self.chroot_cmd, self.chroot, cmd)\n        vvv(\"EXEC %s\" % (local_cmd), host=self.chroot)\n        p = subprocess.Popen(local_cmd, shell=isinstance(local_cmd, basestring),\n                             cwd=self.runner.basedir,\n                             stdin=subprocess.PIPE,\n                             stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        stdout, stderr = p.communicate()\n        return (p.returncode, '', stdout, stderr)\n    def put_file(self, in_path, out_path):\n        ''' transfer a file from local to chroot '''\n        if not out_path.startswith(os.path.sep):\n            out_path = os.path.join(os.path.sep, out_path)\n        normpath = os.path.normpath(out_path)\n        out_path = os.path.join(self.chroot, normpath[1:])\n        vvv(\"PUT %s TO %s\" % (in_path, out_path), host=self.chroot)\n        if not os.path.exists(in_path):\n            raise errors.AnsibleFileNotFound(\"file or module does not exist: %s\" % in_path)\n        try:\n            shutil.copyfile(in_path, out_path)\n        except shutil.Error:\n            traceback.print_exc()\n            raise errors.AnsibleError(\"failed to copy: %s and %s are the same\" % (in_path, out_path))\n        except IOError:\n            traceback.print_exc()\n            raise errors.AnsibleError(\"failed to transfer file to %s\" % out_path)\n    def fetch_file(self, in_path, out_path):\n        ''' fetch a file from chroot to local '''\n        if not in_path.startswith(os.path.sep):\n            in_path = os.path.join(os.path.sep, in_path)\n        normpath = os.path.normpath(in_path)\n        in_path = os.path.join(self.chroot, normpath[1:])\n        vvv(\"FETCH %s TO %s\" % (in_path, out_path), host=self.chroot)\n        if not os.path.exists(in_path):\n            raise errors.AnsibleFileNotFound(\"file or module does not exist: %s\" % in_path)\n        try:\n            shutil.copyfile(in_path, out_path)\n        except shutil.Error:\n            traceback.print_exc()\n            raise errors.AnsibleError(\"failed to copy: %s and %s are the same\" % (in_path, out_path))\n        except IOError:\n            traceback.print_exc()\n            raise errors.AnsibleError(\"failed to transfer file to %s\" % out_path)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2015-6240",
        "description": "[{'lang': 'en', 'value': 'The chroot, jail, and zone connection plugins in ansible before 1.9.2 allow local users to escape a restricted environment via a symlink attack.'}]",
        "cwe_number": 59
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-162",
      "code": "\tdef render(self, request):\n\t\taction = \"download\"\n\t\tif \"action\" in request.args:\n\t\t\taction = request.args[\"action\"][0]\n\t\tif \"file\" in request.args:\n\t\t\tfilename = request.args[\"file\"][0].decode('utf-8', 'ignore').encode('utf-8')\n\t\t\tfilename = re.sub(\"^/+\", \"/\", os.path.realpath(filename))\n\t\t\tif not os.path.exists(filename):\n\t\t\t\treturn \"File '%s' not found\" % (filename)\n\t\t\tif action == \"stream\":\n\t\t\t\tname = \"stream\"\n\t\t\t\tif \"name\" in request.args:\n\t\t\t\t\tname = request.args[\"name\"][0]\n\t\t\t\tport = config.OpenWebif.port.value\n\t\t\t\tproto = 'http'\n\t\t\t\tif request.isSecure():\n\t\t\t\t\tport = config.OpenWebif.https_port.value\n\t\t\t\t\tproto = 'https'\n\t\t\t\tourhost = request.getHeader('host')\n\t\t\t\tm = re.match('.+\\:(\\d+)$', ourhost)\n\t\t\t\tif m is not None:\n\t\t\t\t\tport = m.group(1)\n\t\t\t\tresponse = \"\n\t\t\t\trequest.setHeader(\"Content-Disposition\", 'attachment;filename=\"%s.m3u\"' % name)\n\t\t\t\trequest.setHeader(\"Content-Type\", \"application/x-mpegurl\")\n\t\t\t\treturn response\n\t\t\telif action == \"delete\":\n\t\t\t\trequest.setResponseCode(http.OK)\n\t\t\t\treturn \"TODO: DELETE FILE: %s\" % (filename)\n\t\t\telif action == \"download\":\n\t\t\t\trequest.setHeader(\"Content-Disposition\", \"attachment;filename=\\\"%s\\\"\" % (filename.split('/')[-1]))\n\t\t\t\trfile = static.File(filename, defaultType = \"application/octet-stream\")\n\t\t\t\treturn rfile.render(request)\n\t\t\telse:\n\t\t\t\treturn \"wrong action parameter\"\n\t\tif \"dir\" in request.args:\n\t\t\tpath = request.args[\"dir\"][0]\n\t\t\tpattern = '*'\n\t\t\tdata = []\n\t\t\tif \"pattern\" in request.args:\n\t\t\t\tpattern = request.args[\"pattern\"][0]\n\t\t\tdirectories = []\n\t\t\tfiles = []\n\t\t\tif fileExists(path):\n\t\t\t\ttry:\n\t\t\t\t\tfiles = glob.glob(path+'/'+pattern)\n\t\t\t\texcept:\n\t\t\t\t\tfiles = []\n\t\t\t\tfiles.sort()\n\t\t\t\ttmpfiles = files[:]\n\t\t\t\tfor x in tmpfiles:\n\t\t\t\t\tif os.path.isdir(x):\n\t\t\t\t\t\tdirectories.append(x + '/')\n\t\t\t\t\t\tfiles.remove(x)\n\t\t\t\tdata.append({\"result\": True,\"dirs\": directories,\"files\": files})\n\t\t\telse:\n\t\t\t\tdata.append({\"result\": False,\"message\": \"path %s not exits\" % (path)})\n\t\t\trequest.setHeader(\"content-type\", \"application/json; charset=utf-8\")\n\t\t\treturn json.dumps(data, indent=2)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2018-20332",
        "description": "[{'lang': 'en', 'value': 'An issue has been discovered in the OpenWebif plugin through 1.2.4 for Enigma2 based devices. Reading of arbitrary files is possible with /file?action=download&file= followed by a full pathname, and listing of arbitrary directories is possible with /file?action=download&dir= followed by a full pathname. This is related to plugin/controllers/file.py in the e2openplugin-OpenWebif project.'}]",
        "cwe_number": 22
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-163",
      "code": "def _contains_slash(item):\n    for sep in _seps:\n        if sep in item:\n            return True\ndef _secure_path(path_tuple):\n    if _has_insecure_pathelement(path_tuple):\n        return None\n    if any([_contains_slash(item) for item in path_tuple]):\n        return None\n    encoded = '/'.join(path_tuple)\n    return encoded",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-40587",
        "description": "[{'lang': 'en', 'value': \"Pyramid is an open source Python web framework. A path traversal vulnerability in Pyramid versions 2.0.0 and 2.0.1 impacts users of Python 3.11 that are using a Pyramid static view with a full filesystem path and have a `index.html` file that is located exactly one directory above the location of the static view's file system path. No further path traversal exists, and the only file that could be disclosed accidentally is `index.html`. Pyramid version 2.0.2 rejects any path that contains a null-byte out of caution. While valid in directory/file names, we would strongly consider it a mistake to use null-bytes in naming files/directories. Secondly, Python 3.11, and 3.12 has fixed the underlying issue in `os.path.normpath` to no longer truncate on the first `0x00` found, returning the behavior to pre-3.11 Python, un an as of yet unreleased version. Fixes will be available in:Python 3.12.0rc2 and 3.11.5. Some workarounds are available. Use a version of Python 3 that is not affected, downgrade to Python 3.10 series temporarily, or wait until Python 3.11.5 is released and upgrade to the latest version of Python 3.11 series.\"}]",
        "cwe_number": 22
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-164",
      "code": "def get_jinja_env():\n    jinja_env = Environment(extensions=settings.DOCXTEMPLATE_JINJA_EXTENSIONS)\n    jinja_env.filters.update(get_jinja_filters())\n    return jinja_env",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-37301",
        "description": "[{'lang': 'en', 'value': 'Document Merge Service is a document template merge service providing an API to manage templates and merge them with given data. Versions 6.5.1 and prior are vulnerable to remote code execution via server-side template injection which, when executed as root, can result in full takeover of the affected system. As of time of publication, no patched version exists, nor have any known workarounds been disclosed.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-165",
      "code": "    def runTest(self):\n        for mode in (self.module.MODE_ECB, self.module.MODE_CBC, self.module.MODE_CFB, self.module.MODE_OFB, self.module.MODE_OPENPGP):\n            encryption_cipher = self.module.new(a2b_hex(self.key), mode, self.iv)\n            ciphertext = encryption_cipher.encrypt(self.plaintext)\n            if mode != self.module.MODE_OPENPGP:\n                decryption_cipher = self.module.new(a2b_hex(self.key), mode, self.iv)\n            else:\n                eiv = ciphertext[:self.module.block_size+2]\n                ciphertext = ciphertext[self.module.block_size+2:]\n                decryption_cipher = self.module.new(a2b_hex(self.key), mode, eiv)\n            decrypted_plaintext = decryption_cipher.decrypt(ciphertext)\n            self.assertEqual(self.plaintext, decrypted_plaintext)\nclass PGPTest(unittest.TestCase):\n    def __init__(self, module, params):\n        unittest.TestCase.__init__(self)\n        self.module = module\n        self.key = b(params['key'])\n    def shortDescription(self):\n        return \"MODE_PGP was implemented incorrectly and insecurely. It's completely banished now.\"\n    def runTest(self):\n        self.assertRaises(ValueError, self.module.new, a2b_hex(self.key),\n                self.module.MODE_PGP)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2013-7459",
        "description": "[{'lang': 'en', 'value': 'Heap-based buffer overflow in the ALGnew function in block_templace.c in Python Cryptography Toolkit (aka pycrypto) allows remote attackers to execute arbitrary code as demonstrated by a crafted iv parameter to cryptmsg.py.'}]",
        "cwe_number": 119
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-166",
      "code": "    async def login(cls, username: str, password: str) -> t.Optional[int]:\n        \"\"\"\n        Make sure the user exists and the password is valid. If so, the\n        ``last_login`` value is updated in the database.\n        :returns:\n            The id of the user if a match is found, otherwise ``None``.\n        \"\"\"\n        if len(username) > cls.username.length:\n            logger.warning(\"Excessively long username provided.\")\n            return None\n        if len(password) > cls._max_password_length:\n            logger.warning(\"Excessively long password provided.\")\n            return None\n        response = (\n            await cls.select(cls._meta.primary_key, cls.password)\n            .where(cls.username == username)\n            .first()\n            .run()\n        )\n        if not response:\n            return None\n        stored_password = response[\"password\"]\n        algorithm, iterations_, salt, hashed = cls.split_stored_password(\n            stored_password\n        )\n        iterations = int(iterations_)\n        if cls.hash_password(password, salt, iterations) == stored_password:\n            if iterations != cls._pbkdf2_iteration_count:\n                await cls.update_password(username, password)\n            await cls.update({cls.last_login: datetime.datetime.now()}).where(\n                cls.username == username\n            )\n            return response[\"id\"]\n        else:\n            return None",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-41885",
        "description": "[{'lang': 'en', 'value': 'Piccolo is an ORM and query builder which supports asyncio. In versions 0.120.0 and prior, the implementation of `BaseUser.login` leaks enough information to a malicious user such that they would be able to successfully generate a list of valid users on the platform. As Piccolo on its own does not also enforce strong passwords, these lists of valid accounts are likely to be used in a password spray attack with the outcome being attempted takeover of user accounts on the platform. The impact of this vulnerability is minor as it requires chaining with other attack vectors in order to gain more then simply a list of valid users on the underlying platform. The likelihood of this vulnerability is possible as it requires minimal skills to pull off, especially given the underlying login functionality for Piccolo based sites is open source. This issue has been patched in version 0.121.0.'}]",
        "cwe_number": 203
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-167",
      "code": "    def clean(self):\n        super().clean()\n        if self.errors:\n            return self.cleaned_data\n        oldpassword = self.cleaned_data.get(\"oldpassword\")\n        newpassword = self.cleaned_data.get(\"newpassword\")\n        confirmation = self.cleaned_data.get(\"confirmation\")\n        if newpassword and confirmation:\n            if oldpassword:\n                if newpassword != confirmation:\n                    self.add_error(\"confirmation\", _(\"Passwords mismatch\"))\n                else:\n                    password_validation.validate_password(\n                        confirmation, self.instance)\n            else:\n                self.add_error(\"oldpassword\", _(\"This field is required.\"))\n        return self.cleaned_data\n    def save(self, commit=True):\n        user = super().save(commit=False)\n        if commit:\n            if self.cleaned_data.get(\"confirmation\", \"\") != \"\":\n                user.set_password(\n                    self.cleaned_data[\"confirmation\"],\n                    self.cleaned_data[\"oldpassword\"]\n                )\n            user.save()\n        return user",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-2160",
        "description": "[{'lang': 'en', 'value': 'Weak Password Requirements in GitHub repository modoboa/modoboa prior to 2.1.0.\\n\\n'}]",
        "cwe_number": 521
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-168",
      "code": "    def aendern(self, name, amount, item):\n        keks = self.DBcursor.execute(u\"SELECT nickname, count from kekse WHERE nickname like '%s' AND item =='%s' LIMIT 1\" % (name, item)).fetchall()\n        if(len(keks)>0 and len(keks[0]) > 1):\n            keks = keks[0][1]\n            if amount == 0:\n                return keks\n            keks2 = keks+amount\n            if (keks2 >= 1):\n                self.DBcursor.execute(u\"UPDATE kekse set count=? WHERE item==? AND nickname like ?\", (keks2, item, name))\n                self.DBconn.commit()\n                return keks2\n            else:\n                self.DBcursor.execute(u\"UPDATE kekse set count=? WHERE nickname like ? AND item==?\", (0, name, item))\n                self.DBconn.commit()\n                if keks>0:\n                    return 0\n                return -999\n        else:\n            if (amount >= 0):\n                self.DBcursor.execute(u\"INSERT INTO kekse (nickname, count, item) VALUES (?, ?, ?)\", (name, amount, item))\n                self.DBconn.commit()\n                return amount\n            else:\n                return -999;\n    def lesen(self, params, name, words):\n        if len(params.args) == 1:\n            keks = 0\n            anrede = u\"Du hast\"\n            if params.args[0]==params.target:\n                keks = self.aendern(params.target, 0, name)\n            else:\n                keks = self.aendern(params.args[0],0, name)\n                anrede=u\"%s hat\" % params.args[0]\n            self.parent.privmsg(u\"%s %i %s.\" % (anrede, keks, words['plural']), params.channel)\n        elif len(params.args) == 0:\n            keks = self.DBcursor.execute(u\"SELECT `nickname`, `count` from kekse WHERE item=='%s' AND `count`>=1 ORDER BY `count` DESC LIMIT 10\" % (name)).fetchall()\n            if len(keks) <= 0:\n                self.parent.privmsg(u\"Es gibt noch keine %s?\" % words['plural'], params.channel)\n                return\n            kekst = u\"[Top-%s-Sammler] \" % words['singular']\n            first = True\n            for keksi in keks:\n                if len(keksi) > 1:\n                    if first:\n                        kekst += u\"%s hat %i\" % keksi\n                        first = False\n                    else:\n                        kekst += u\", %s hat %i\" %keksi\n            self.parent.privmsg(kekst, params.channel)\n        else:\n            self.parent.privmsg(u\"Ich hab keine Ahnung was du mir damit sagen willst...\", params.channel)\n            return",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2013-10009",
        "description": "[{'lang': 'en', 'value': 'A vulnerability was found in DrAzraelTod pyChao and classified as critical. Affected by this issue is the function klauen/lesen of the file mod_fun/__init__.py. The manipulation leads to sql injection. The patch is identified as 9d8adbc07c384ba51c2583ce0819c9abb77dc648. It is recommended to apply a patch to fix this issue. VDB-217634 is the identifier assigned to this vulnerability.'}]",
        "cwe_number": 89
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-169",
      "code": "    def sign_in(self, timeout=60, safe=True, tries=1, channel=None):\n        '''\n        Send a sign in request to the master, sets the key information and\n        returns a dict containing the master publish interface to bind to\n        and the decrypted aes key for transport decryption.\n        :param int timeout: Number of seconds to wait before timing out the sign-in request\n        :param bool safe: If True, do not raise an exception on timeout. Retry instead.\n        :param int tries: The number of times to try to authenticate before giving up.\n        :raises SaltReqTimeoutError: If the sign-in request has timed out and :param safe: is not set\n        :return: Return a string on failure indicating the reason for failure. On success, return a dictionary\n        with the publication port and the shared AES key.\n        '''\n        auth = {}\n        auth_timeout = self.opts.get('auth_timeout', None)\n        if auth_timeout is not None:\n            timeout = auth_timeout\n        auth_safemode = self.opts.get('auth_safemode', None)\n        if auth_safemode is not None:\n            safe = auth_safemode\n        auth_tries = self.opts.get('auth_tries', None)\n        if auth_tries is not None:\n            tries = auth_tries\n        m_pub_fn = os.path.join(self.opts['pki_dir'], self.mpub)\n        auth['master_uri'] = self.opts['master_uri']\n        if not channel:\n            channel = salt.transport.client.AsyncReqChannel.factory(self.opts,\n                                                                crypt='clear',\n                                                                io_loop=self.io_loop)\n        sign_in_payload = self.minion_sign_in_payload()\n        try:\n            payload = yield channel.send(\n                sign_in_payload,\n                tries=tries,\n                timeout=timeout\n            )\n        except SaltReqTimeoutError as e:\n            if safe:\n                log.warning('SaltReqTimeoutError: {0}'.format(e))\n                raise tornado.gen.Return('retry')\n            if self.opts.get('detect_mode') is True:\n                raise tornado.gen.Return('retry')\n            else:\n                raise SaltClientError('Attempt to authenticate with the salt master failed with timeout error')\n        if 'load' in payload:\n            if 'ret' in payload['load']:\n                if not payload['load']['ret']:\n                    if self.opts['rejected_retry']:\n                        log.error(\n                            'The Salt Master has rejected this minion\\'s public '\n                            'key.\\nTo repair this issue, delete the public key '\n                            'for this minion on the Salt Master.\\nThe Salt '\n                            'Minion will attempt to to re-authenicate.'\n                        )\n                        raise tornado.gen.Return('retry')\n                    else:\n                        log.critical(\n                            'The Salt Master has rejected this minion\\'s public '\n                            'key!\\nTo repair this issue, delete the public key '\n                            'for this minion on the Salt Master and restart this '\n                            'minion.\\nOr restart the Salt Master in open mode to '\n                            'clean out the keys. The Salt Minion will now exit.'\n                        )\n                        sys.exit(salt.defaults.exitcodes.EX_OK)\n                elif payload['load']['ret'] == 'full':\n                    raise tornado.gen.Return('full')\n                else:\n                    log.error(\n                        'The Salt Master has cached the public key for this '\n                        'node, this salt minion will wait for {0} seconds '\n                        'before attempting to re-authenticate'.format(\n                            self.opts['acceptance_wait_time']\n                        )\n                    )\n                    raise tornado.gen.Return('retry')\n        auth['aes'] = self.verify_master(payload, master_pub='token' in sign_in_payload)\n        if not auth['aes']:\n            log.critical(\n                'The Salt Master server\\'s public key did not authenticate!\\n'\n                'The master may need to be updated if it is a version of Salt '\n                'lower than {0}, or\\n'\n                'If you are confident that you are connecting to a valid Salt '\n                'Master, then remove the master public key and restart the '\n                'Salt Minion.\\nThe master public key can be found '\n                'at:\\n{1}'.format(salt.version.__version__, m_pub_fn)\n            )\n            raise SaltClientError('Invalid master key')\n        if self.opts.get('syndic_master', False):\n            syndic_finger = self.opts.get('syndic_finger', self.opts.get('master_finger', False))\n            if syndic_finger:\n                if salt.utils.pem_finger(m_pub_fn, sum_type=self.opts['hash_type']) != syndic_finger:\n                    self._finger_fail(syndic_finger, m_pub_fn)\n        else:\n            if self.opts.get('master_finger', False):\n                if salt.utils.pem_finger(m_pub_fn, sum_type=self.opts['hash_type']) != self.opts['master_finger']:\n                    self._finger_fail(self.opts['master_finger'], m_pub_fn)\n        auth['publish_port'] = payload['publish_port']\n        raise tornado.gen.Return(auth)\n    def get_keys(self):\n        '''\n        Return keypair object for the minion.\n        :rtype: Crypto.PublicKey.RSA._RSAobj\n        :return: The RSA keypair\n        '''\n        user = self.opts.get('user', 'root')\n        salt.utils.verify.check_path_traversal(self.opts['pki_dir'], user)\n        if os.path.exists(self.rsa_path):\n            with salt.utils.fopen(self.rsa_path) as f:\n                key = RSA.importKey(f.read())\n            log.debug('Loaded minion key: {0}'.format(self.rsa_path))\n        else:\n            log.info('Generating keys: {0}'.format(self.opts['pki_dir']))\n            gen_keys(self.opts['pki_dir'],\n                     'minion',\n                     self.opts['keysize'],\n                     self.opts.get('user'))\n            with salt.utils.fopen(self.rsa_path) as f:\n                key = RSA.importKey(f.read())\n        return key\n    def handle_message(self, stream, payload):\n        '''\n        Handle incoming messages from underylying TCP streams\n        :stream ZMQStream stream: A ZeroMQ stream.\n        See http://zeromq.github.io/pyzmq/api/generated/zmq.eventloop.zmqstream.html\n        :param dict payload: A payload to process\n        '''\n        try:\n            payload = self.serial.loads(payload[0])\n            payload = self._decode_payload(payload)\n        except Exception as exc:\n            exc_type = type(exc).__name__\n            if exc_type == 'AuthenticationError':\n                log.debug(\n                    'Minion failed to auth to master. Since the payload is '\n                    'encrypted, it is not known which minion failed to '\n                    'authenticate. It is likely that this is a transient '\n                    'failure due to the master rotating its public key.'\n                )\n            else:\n                log.error('Bad load from minion: %s: %s', exc_type, exc)\n            stream.send(self.serial.dumps('bad load'))\n            raise tornado.gen.Return()\n        if not isinstance(payload, dict) or not isinstance(payload.get('load'), dict):\n            log.error('payload and load must be a dict. Payload was: {0} and load was {1}'.format(payload, payload.get('load')))\n            stream.send(self.serial.dumps('payload and load must be a dict'))\n            raise tornado.gen.Return()\n        if payload['enc'] == 'clear' and payload.get('load', {}).get('cmd') == '_auth':\n            stream.send(self.serial.dumps(self._auth(payload['load'])))\n            raise tornado.gen.Return()\n        try:\n            ret, req_opts = yield self.payload_handler(payload)\n        except Exception as e:\n            stream.send('Some exception handling minion payload')\n            log.error('Some exception handling a payload from minion', exc_info=True)\n            raise tornado.gen.Return()\n        req_fun = req_opts.get('fun', 'send')\n        if req_fun == 'send_clear':\n            stream.send(self.serial.dumps(ret))\n        elif req_fun == 'send':\n            stream.send(self.serial.dumps(self.crypticle.dumps(ret)))\n        elif req_fun == 'send_private':\n            stream.send(self.serial.dumps(self._encrypt_private(ret,\n                                                                req_opts['key'],\n                                                                req_opts['tgt'],\n                                                                )))\n        else:\n            log.error('Unknown req_fun {0}'.format(req_fun))\n            stream.send('Server-side exception handling payload')\n        raise tornado.gen.Return()\n    def __setup_signals(self):\n        signal.signal(signal.SIGINT, self._handle_signals)\n        signal.signal(signal.SIGTERM, self._handle_signals)\n    def _handle_signals(self, signum, sigframe):\n        msg = '{0} received a '.format(self.__class__.__name__)\n        if signum == signal.SIGINT:\n            msg += 'SIGINT'\n        elif signum == signal.SIGTERM:\n            msg += 'SIGTERM'\n        msg += '. Exiting'\n        log.debug(msg)\n        self.close()\n        sys.exit(salt.defaults.exitcodes.EX_OK)\n    def handle_message(self, stream, header, payload):\n        '''\n        Handle incoming messages from underylying tcp streams\n        '''\n        try:\n            try:\n                payload = self._decode_payload(payload)\n            except Exception:\n                stream.write(salt.transport.frame.frame_msg('bad load', header=header))\n                raise tornado.gen.Return()\n            if not isinstance(payload, dict) or not isinstance(payload.get('load'), dict):\n                yield stream.write(salt.transport.frame.frame_msg(\n                    'payload and load must be a dict', header=header))\n                raise tornado.gen.Return()\n            if payload['enc'] == 'clear' and payload.get('load', {}).get('cmd') == '_auth':\n                yield stream.write(salt.transport.frame.frame_msg(\n                    self._auth(payload['load']), header=header))\n                raise tornado.gen.Return()\n            try:\n                ret, req_opts = yield self.payload_handler(payload)\n            except Exception as e:\n                stream.write('Some exception handling minion payload')\n                log.error('Some exception handling a payload from minion', exc_info=True)\n                stream.close()\n                raise tornado.gen.Return()\n            req_fun = req_opts.get('fun', 'send')\n            if req_fun == 'send_clear':\n                stream.write(salt.transport.frame.frame_msg(ret, header=header))\n            elif req_fun == 'send':\n                stream.write(salt.transport.frame.frame_msg(self.crypticle.dumps(ret), header=header))\n            elif req_fun == 'send_private':\n                stream.write(salt.transport.frame.frame_msg(self._encrypt_private(ret,\n                                                             req_opts['key'],\n                                                             req_opts['tgt'],\n                                                             ), header=header))\n            else:\n                log.error('Unknown req_fun {0}'.format(req_fun))\n                stream.write('Server-side exception handling payload')\n                stream.close()\n        except tornado.gen.Return:\n            raise\n        except tornado.iostream.StreamClosedError:\n            log.error('Connection was unexpectedly closed', exc_info=True)\n        except Exception as exc:\n            log.error('Unexpected exception occurred: {0}'.format(exc), exc_info=True)\n        raise tornado.gen.Return()\nclass SaltMessageServer(tornado.tcpserver.TCPServer, object):\n    def __init__(self, message_handler, *args, **kwargs):\n        super(SaltMessageServer, self).__init__(*args, **kwargs)\n        self.clients = []\n        self.message_handler = message_handler",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2017-14696",
        "description": "[{'lang': 'en', 'value': 'SaltStack Salt before 2016.3.8, 2016.11.x before 2016.11.8, and 2017.7.x before 2017.7.2 allows remote attackers to cause a denial of service via a crafted authentication request.'}]",
        "cwe_number": 20
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-170",
      "code": "    def writeDataFile(self, filename, text, content_type, subdir=None):\n        \"\"\" See IExportContext.\n        \"\"\"\n        if subdir is not None:\n            filename = '/'.join((subdir, filename))\n        sep = filename.rfind('/')\n        if sep != -1:\n            subdir = filename[:sep]\n            filename = filename[sep+1:]\n        if six.PY2 and isinstance(text, six.text_type):\n            encoding = self.getEncoding() or 'utf-8'\n            text = text.encode(encoding)\n        folder = self._ensureSnapshotsFolder(subdir)\n        ob = self._createObjectByType(filename, text, content_type)\n        folder._setObject(str(filename), ob)\n    def getSnapshotURL(self):\n        \"\"\" See IExportContext.\n        \"\"\"\n        return '%s/%s' % (self._tool.absolute_url(), self._snapshot_id)\n    def _ensureSnapshotsFolder(self, subdir=None):\n        \"\"\" Ensure that the appropriate snapshot folder exists.\n        \"\"\"\n        path = ['snapshots', self._snapshot_id]\n        if subdir is not None:\n            path.extend(subdir.split('/'))\n        current = self._tool\n        for element in path:\n            if element not in current.objectIds():\n                current._setObject(str(element), Folder(element))\n            current = current._getOb(element)\n        return current\nInitializeClass(SnapshotExportContext)\nclass SnapshotImportContext(BaseContext):",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-21360",
        "description": "[{'lang': 'en', 'value': 'Products.GenericSetup is a mini-framework for expressing the configured state of a Zope Site as a set of filesystem artifacts. In Products.GenericSetup before version 2.1.1 there is an information disclosure vulnerability - anonymous visitors may view log and snapshot files generated by the Generic Setup Tool. The problem has been fixed in version 2.1.1. Depending on how you have installed Products.GenericSetup, you should change the buildout version pin to 2.1.1 and re-run the buildout, or if you used pip simply do pip install `\"Products.GenericSetup>=2.1.1\"`.'}]",
        "cwe_number": 200
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-171",
      "code": "    def publish(self, id_, identity, uow=None):\n        \"\"\"Publish a draft.\n        Idea:\n            - Get the draft from the data layer (draft is not passed in)\n            - Validate it more strictly than when it was originally saved\n              (drafts can be incomplete but only complete drafts can be turned\n              into records)\n            - Create or update associated (published) record with data\n        \"\"\"\n        self.require_permission(identity, \"publish\")\n        draft = self.draft_cls.pid.resolve(id_, registered_only=False)\n        self._validate_draft(identity, draft)\n        latest_id = draft.versions.latest_id\n        record = self.record_cls.publish(draft)\n        self.run_components(\n            'publish', identity, draft=draft, record=record, uow=uow)\n        uow.register(RecordCommitOp(record, indexer=self.indexer))\n        uow.register(RecordDeleteOp(draft, force=False, indexer=self.indexer))\n        if latest_id:\n            self._reindex_latest(latest_id, uow=uow)\n        return self.result_item(\n            self, identity, record, links_tpl=self.links_item_tpl)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-43781",
        "description": "[{'lang': 'en', 'value': 'Invenio-Drafts-Resources is a submission/deposit module for Invenio, a software framework for research data management. Invenio-Drafts-Resources prior to versions 0.13.7 and 0.14.6 does not properly check permissions when a record is published. The vulnerability is exploitable in a default installation of InvenioRDM. An authenticated a user is able via REST API calls to publish draft records of other users if they know the record identifier and the draft validates (e.g. all require fields filled out). An attacker is not able to modify the data in the record, and thus e.g. *cannot* change a record from restricted to public. The problem is patched in Invenio-Drafts-Resources v0.13.7 and 0.14.6, which is part of InvenioRDM v6.0.1 and InvenioRDM v7.0 respectively.'}]",
        "cwe_number": 863
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-172",
      "code": "def index():\n    \"\"\" Index handler \"\"\"\n    send = request.vars.send\n    if DEMO_MODE:\n        session.authorized = True\n        session.last_time = t0\n    if not send:\n        send = URL('site')\n    if session.authorized:\n        redirect(send)\n    elif failed_login_count() >= allowed_number_of_attempts:\n        time.sleep(2 ** allowed_number_of_attempts)\n        raise HTTP(403)\n    elif request.vars.password:\n        if verify_password(request.vars.password[:1024]):\n            session.authorized = True\n            login_record(True)\n            if CHECK_VERSION:\n                session.check_version = True\n            else:\n                session.check_version = False\n            session.last_time = t0\n            if isinstance(send, list):\n                send = str(send[0])\n            redirect(send)\n        else:\n            times_denied = login_record(False)\n            if times_denied >= allowed_number_of_attempts:\n                response.flash = \\\n                    T('admin disabled because too many invalid login attempts')\n            elif times_denied == allowed_number_of_attempts - 1:\n                response.flash = \\\n                    T('You have one more login attempt before you are locked out')\n            else:\n                response.flash = T('invalid password.')\n    return dict(send=send)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-33146",
        "description": "[{'lang': 'en', 'value': 'Open redirect vulnerability in web2py versions prior to 2.22.5 allows a remote attacker to redirect a user to an arbitrary web site and conduct a phishing attack by having a user to access a specially crafted URL.'}]",
        "cwe_number": 601
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-173",
      "code": "    def _has_sneaky_javascript(self, style):\n        \"\"\"\n        Depending on the browser, stuff like ``e x p r e s s i o n(...)``\n        can get interpreted, or ``expre/* stuff */ssion(...)``.  This\n        checks for attempt to do stuff like this.\n        Typically the response will be to kill the entire style; if you\n        have just a bit of Javascript in the style another rule will catch\n        that and remove only the Javascript from the style; this catches\n        more sneaky attempts.\n        \"\"\"\n        style = self._substitute_comments('', style)\n        style = style.replace('\\\\', '')\n        style = _substitute_whitespace('', style)\n        style = style.lower()\n        if 'javascript:' in style:\n            return True\n        if 'expression(' in style:\n            return True\n        if '</noscript' in style:\n            return True\n        if _looks_like_tag_content(style):\n            return True\n        return False\n    def clean_html(self, html):\n        result_type = type(html)\n        if isinstance(html, basestring):\n            doc = fromstring(html)\n        else:\n            doc = copy.deepcopy(html)\n        self(doc)\n        return _transform_result(result_type, doc)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-43818",
        "description": "[{'lang': 'en', 'value': 'lxml is a library for processing XML and HTML in the Python language. Prior to version 4.6.5, the HTML Cleaner in lxml.html lets certain crafted script content pass through, as well as script content in SVG files embedded using data URIs. Users that employ the HTML cleaner in a security relevant context should upgrade to lxml 4.6.5 to receive a patch. There are no known workarounds available.'}]",
        "cwe_number": 74
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-174",
      "code": "  def _VerifyOneTest(self, pool_func, input_sizes, window, strides, padding,\n                     data_format, expected, use_gpu):\n    \"\"\"Verifies the output values of the pooling function.\n    Args:\n      pool_func: Function to be called: co.MaxPool, co.AvgPool.\n      input_sizes: Input tensor dimensions.\n      window: Tuple of kernel dims: planes, rows, cols.\n      strides: Tuple of strides for dims: planes, rows, cols.\n      padding: Padding type.\n      data_format: The data format we use to run the pooling operation.\n      expected: An array containing the expected operation outputs.\n      use_gpu: Whether to run ops on GPU.\n    \"\"\"\n    total_size = 1\n    for s in input_sizes:\n      total_size *= s\n    x = [f * 1.0 for f in range(1, total_size + 1)]\n    with self.cached_session(use_gpu=use_gpu) as sess:\n      t = constant_op.constant(x, shape=input_sizes)\n      window = [1] + list(window) + [1]\n      strides = [1] + list(strides) + [1]\n      if data_format == \"NCDHW\":\n        t = test_util.NHWCToNCHW(t)\n        window = test_util.NHWCToNCHW(window)\n        strides = test_util.NHWCToNCHW(strides)\n      t = pool_func(\n          t,\n          ksize=window,\n          strides=strides,\n          padding=padding,\n          data_format=data_format)\n      if data_format == \"NCDHW\":\n        t = test_util.NCHWToNHWC(t)\n      vals = self.evaluate(t)\n    actual = vals.flatten()\n    self.assertAllClose(expected, actual)\n  def testMaxPool3dValidPadding(self):\n    expected_output = [40.0, 41.0, 42.0]\n    self._VerifyValues(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 3, 3, 3, 3],\n        window=(2, 2, 2),\n        strides=(2, 2, 2),\n        padding=\"VALID\",\n        expected=expected_output)\n  def testMaxPool3dSamePadding(self):\n    expected_output = [31., 32., 33., 34., 35., 36.]\n    self._VerifyValues(\n        nn_ops.max_pool3d,\n        input_sizes=[1, 2, 2, 3, 3],\n        window=(2, 2, 2),\n        strides=(2, 2, 2),\n        padding=\"SAME\",\n        expected=expected_output)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-35959",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. The implementation of `AvgPool3DGradOp` does not fully validate the input `orig_input_shape`. This results in an overflow that results in a `CHECK` failure which can be used to trigger a denial of service attack. We have patched the issue in GitHub commit 9178ac9d6389bdc54638ab913ea0e419234d14eb. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue.'}]",
        "cwe_number": 617
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-175",
      "code": "    def download_compressed(self, url, zippath, unzippedpath):\n        \"\"\"Downloads a compressed file and extracts it\n        Args:\n            url (str): download link\n            zippath (str): path to download compressed file\n            unzippedpath (str): path to unzip compressed file\n        \"\"\"\n        response = requests.get(url, stream=True)\n        with open(zippath, \"wb\") as f:\n            f.write(response.raw.read())\n        shutil.unpack_archive(zippath, unzippedpath)\n        os.remove(zippath)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-23530",
        "description": "[{'lang': 'en', 'value': 'GuardDog is a CLI tool to identify malicious PyPI packages. Versions prior to v0.1.8 are vulnerable to arbitrary file write when scanning a specially-crafted remote PyPI package. Extracting files using shutil.unpack_archive() from a potentially malicious tarball without validating that the destination file path is within the intended destination directory can cause files outside the destination directory to be overwritten.  This issue is patched in version 0.1.8. Potential workarounds include using a safer module, like zipfile, and validating the location of the extracted files and discarding those with malicious paths.'}]",
        "cwe_number": 22
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-176",
      "code": "    def get_field_options(self, field):\n        options = {}\n        options['label'] = field.label\n        options['help_text'] = field.help_text\n        options['required'] = field.required\n        options['initial'] = field.default_value\n        return options",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2020-15118",
        "description": "[{'lang': 'en', 'value': \"In Wagtail before versions 2.7.4 and 2.9.3, when a form page type is made available to Wagtail editors through the `wagtail.contrib.forms` app, and the page template is built using Django's standard form rendering helpers such as form.as_p, any HTML tags used within a form field's help text will be rendered unescaped in the page. Allowing HTML within help text is an intentional design decision by Django; however, as a matter of policy Wagtail does not allow editors to insert arbitrary HTML by default, as this could potentially be used to carry out cross-site scripting attacks, including privilege escalation. This functionality should therefore not have been made available to editor-level users. The vulnerability is not exploitable by an ordinary site visitor without access to the Wagtail admin. Patched versions have been released as Wagtail 2.7.4 (for the LTS 2.7 branch) and Wagtail 2.9.3 (for the current 2.9 branch). In these versions, help text will be escaped to prevent the inclusion of HTML tags. Site owners who wish to re-enable the use of HTML within help text (and are willing to accept the risk of this being exploited by editors) may set WAGTAILFORMS_HELP_TEXT_ALLOW_HTML = True in their configuration settings. Site owners who are unable to upgrade to the new versions can secure their form page templates by rendering forms field-by-field as per Django's documentation, but omitting the |safe filter when outputting the help text.\"}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-177",
      "code": "def _process_shipping_data_for_order(\n    checkout_info: \"CheckoutInfo\",\n    base_shipping_price: Money,\n    shipping_price: TaxedMoney,\n    manager: \"PluginsManager\",\n    lines: Iterable[\"CheckoutLineInfo\"],\n) -> dict[str, Any]:\n    \"\"\"Fetch, process and return shipping data from checkout.\"\"\"\n    delivery_method_info = checkout_info.delivery_method_info\n    shipping_address = delivery_method_info.shipping_address\n    if (\n        delivery_method_info.store_as_customer_address\n        and checkout_info.user\n        and shipping_address\n    ):\n        store_user_address(\n            checkout_info.user, shipping_address, AddressType.SHIPPING, manager=manager\n        )\n        if checkout_info.user.addresses.filter(pk=shipping_address.pk).exists():\n            shipping_address = shipping_address.get_copy()\n    shipping_method = delivery_method_info.delivery_method\n    tax_class = getattr(shipping_method, \"tax_class\", None)\n    result: dict[str, Any] = {\n        \"shipping_address\": shipping_address,\n        \"base_shipping_price\": base_shipping_price,\n        \"shipping_price\": shipping_price,\n        \"weight\": checkout_info.checkout.get_total_weight(lines),\n        **get_shipping_tax_class_kwargs_for_order(tax_class),\n    }\n    result.update(delivery_method_info.delivery_method_order_field)\n    result.update(delivery_method_info.delivery_method_name)\n    return result\ndef _process_user_data_for_order(checkout_info: \"CheckoutInfo\", manager):\n    \"\"\"Fetch, process and return shipping data from checkout.\"\"\"\n    billing_address = checkout_info.billing_address\n    if checkout_info.user and billing_address:\n        store_user_address(\n            checkout_info.user, billing_address, AddressType.BILLING, manager=manager\n        )\n        if checkout_info.user.addresses.filter(pk=billing_address.pk).exists():\n            billing_address = billing_address.get_copy()\n    return {\n        \"user\": checkout_info.user,\n        \"user_email\": checkout_info.get_customer_email(),\n        \"billing_address\": billing_address,\n        \"customer_note\": checkout_info.checkout.note,\n    }",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-29888",
        "description": "[{'lang': 'en', 'value': 'Saleor is an e-commerce platform that serves high-volume companies. When using `Pickup: Local stock only` click-and-collect as a delivery method in specific conditions the customer could overwrite the warehouse address with its own, which exposes its address as click-and-collect address. This issue has been patched in versions: `3.14.61`, `3.15.37`, `3.16.34`, `3.17.32`, `3.18.28`, `3.19.15`.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-178",
      "code": "    def __init__(self, path):\n        self.path = path\n    def write(self, contents: dict):\n        try:\n            secrets_to_write = self.read()\n            secrets_to_write.update(contents)\n        except (FileNotFoundError, KeyError, json.decoder.JSONDecodeError):\n            secrets_to_write = contents\n        self._write(secrets_to_write)\n    def read(self) -> dict:\n        LOGGER.debug(f'Reading from {self.path}')\n        with open(self.path, 'r') as fp:\n            contents = json.loads(fp.read())\n        return contents",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-32303",
        "description": "[{'lang': 'en', 'value': \"Planet is software that provides satellite data. The secret file stores the user's Planet API authentication information. It should only be accessible by the user, but before version 2.0.1, its permissions allowed the user's group and non-group to read the file as well. This issue was patched in version 2.0.1. As a workaround, set the secret file permissions to only user read/write by hand.\\n\"}]",
        "cwe_number": 732
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-179",
      "code": "def collect_messages(\n    env: Environment,\n    args: argparse.Namespace,\n    request_body_read_callback: Callable[[bytes], None] = None,\n) -> Iterable[RequestsMessage]:\n    httpie_session = None\n    httpie_session_headers = None\n    if args.session or args.session_read_only:\n        httpie_session = get_httpie_session(\n            config_dir=env.config.directory,\n            session_name=args.session or args.session_read_only,\n            host=args.headers.get('Host'),\n            url=args.url,\n        )\n        httpie_session_headers = httpie_session.headers\n    request_kwargs = make_request_kwargs(\n        env,\n        args=args,\n        base_headers=httpie_session_headers,\n        request_body_read_callback=request_body_read_callback\n    )\n    send_kwargs = make_send_kwargs(args)\n    send_kwargs_mergeable_from_env = make_send_kwargs_mergeable_from_env(args)\n    requests_session = build_requests_session(\n        ssl_version=args.ssl_version,\n        ciphers=args.ciphers,\n        verify=bool(send_kwargs_mergeable_from_env['verify'])\n    )\n    if httpie_session:\n        httpie_session.update_headers(request_kwargs['headers'])\n        requests_session.cookies = httpie_session.cookies\n        if args.auth_plugin:\n            httpie_session.auth = {\n                'type': args.auth_plugin.auth_type,\n                'raw_auth': args.auth_plugin.raw_auth,\n            }\n        elif httpie_session.auth:\n            request_kwargs['auth'] = httpie_session.auth\n    if args.debug:\n        dump_request(request_kwargs)\n    request = requests.Request(**request_kwargs)\n    prepared_request = requests_session.prepare_request(request)\n    apply_missing_repeated_headers(prepared_request, request.headers)\n    if args.path_as_is:\n        prepared_request.url = ensure_path_as_is(\n            orig_url=args.url,\n            prepped_url=prepared_request.url,\n        )\n    if args.compress and prepared_request.body:\n        compress_request(\n            request=prepared_request,\n            always=args.compress > 1,\n        )\n    response_count = 0\n    expired_cookies = []\n    while prepared_request:\n        yield prepared_request\n        if not args.offline:\n            send_kwargs_merged = requests_session.merge_environment_settings(\n                url=prepared_request.url,\n                **send_kwargs_mergeable_from_env,\n            )\n            with max_headers(args.max_headers):\n                response = requests_session.send(\n                    request=prepared_request,\n                    **send_kwargs_merged,\n                    **send_kwargs,\n                )\n            response._httpie_headers_parsed_at = monotonic()\n            expired_cookies += get_expired_cookies(\n                response.headers.get('Set-Cookie', '')\n            )\n            response_count += 1\n            if response.next:\n                if args.max_redirects and response_count == args.max_redirects:\n                    raise requests.TooManyRedirects\n                if args.follow:\n                    prepared_request = response.next\n                    if args.all:\n                        yield response\n                    continue\n            yield response\n        break\n    if httpie_session:\n        if httpie_session.is_new() or not args.session_read_only:\n            httpie_session.cookies = requests_session.cookies\n            httpie_session.remove_cookies(\n                cookie['name'] for cookie in expired_cookies\n            )\n            httpie_session.save()\nparser = HTTPieManagerArgumentParser(\n    prog='httpie',\n    description=dedent(\n        '''\n        Managing interface for the HTTPie itself. <https://httpie.io/docs\n        Be aware that you might be looking for http/https commands for sending\n        HTTP requests. This command is only available for managing the HTTTPie\n        plugins and the configuration around it.\n        '''\n    ),\n)\nparser.add_argument(\n    '--debug',\n    action='store_true',\n    default=False,\n    help='''\n    Prints the exception traceback should one occur, as well as other\n    information useful for debugging HTTPie itself and for reporting bugs.\n    '''\n)\nparser.add_argument(\n    '--traceback',\n    action='store_true',\n    default=False,\n    help='''\n    Prints the exception traceback should one occur.\n    '''\n)\nparser.add_argument(\n    '--version',\n    action='version',\n    version=__version__,\n    help='''\n    Show version and exit.\n    '''\n)\ngenerate_subparsers(parser, parser, COMMANDS)\n    def __init__(self, path: Union[str, Path]):\n        super().__init__(path=Path(path))\n        self['headers'] = {}\n        self['cookies'] = {}\n        self['auth'] = {\n            'type': None,\n            'username': None,\n            'password': None\n        }\n    def update_headers(self, request_headers: HTTPHeadersDict):\n        \"\"\"\n        Update the session headers with the request ones while ignoring\n        certain name prefixes.\n        \"\"\"\n        headers = self.headers\n        for name, value in request_headers.copy().items():\n            if value is None:\n                continue\n            if type(value) is not str:\n                value = value.decode()\n            if name.lower() == 'user-agent' and value.startswith('HTTPie/'):\n                continue\n            if name.lower() == 'cookie':\n                for cookie_name, morsel in SimpleCookie(value).items():\n                    self['cookies'][cookie_name] = {'value': morsel.value}\n                del request_headers[name]\n                continue\n            for prefix in SESSION_IGNORED_HEADER_PREFIXES:\n                if name.lower().startswith(prefix.lower()):\n                    break\n            else:\n                headers[name] = value\n        self['headers'] = dict(headers)\n    def headers(self) -> HTTPHeadersDict:\n        return HTTPHeadersDict(self['headers'])\n    def cookies(self) -> RequestsCookieJar:\n        jar = RequestsCookieJar()\n        for name, cookie_dict in self['cookies'].items():\n            jar.set_cookie(create_cookie(\n                name, cookie_dict.pop('value'), **cookie_dict))\n        jar.clear_expired_cookies()\n        return jar\n    def cookies(self, jar: RequestsCookieJar):\n        stored_attrs = ['value', 'path', 'secure', 'expires']\n        self['cookies'] = {}\n        for cookie in jar:\n            self['cookies'][cookie.name] = {\n                attname: getattr(cookie, attname)\n                for attname in stored_attrs\n            }\n    def auth(self) -> Optional[AuthBase]:\n        auth = self.get('auth', None)\n        if not auth or not auth['type']:\n            return\n        plugin = plugin_manager.get_auth_plugin(auth['type'])()\n        credentials = {'username': None, 'password': None}\n        try:\n            plugin.raw_auth = auth['raw_auth']\n        except KeyError:\n            credentials = {\n                'username': auth['username'],\n                'password': auth['password'],\n            }\n        else:\n            if plugin.auth_parse:\n                from .cli.argtypes import parse_auth\n                parsed = parse_auth(plugin.raw_auth)\n                credentials = {\n                    'username': parsed.key,\n                    'password': parsed.value,\n                }\n        return plugin.get_auth(**credentials)\n    def auth(self, auth: dict):\n        assert {'type', 'raw_auth'} == auth.keys()\n        self['auth'] = auth\n    def remove_cookies(self, names: Iterable[str]):\n        for name in names:\n            if name in self['cookies']:\n                del self['cookies'][name]\nclass BaseConfigDict(dict):\n    name = None\n    helpurl = None\n    about = None\n    def __init__(self, path: Path):\n        super().__init__()\n        self.path = path\n    def ensure_directory(self):\n        self.path.parent.mkdir(mode=0o700, parents=True, exist_ok=True)\n    def is_new(self) -> bool:\n        return not self.path.exists()\n    def save(self):\n        self['__meta__'] = {\n            'httpie': __version__\n        }\n        if self.helpurl:\n            self['__meta__']['help'] = self.helpurl\n        if self.about:\n            self['__meta__']['about'] = self.about\n        self.ensure_directory()\n        json_string = json.dumps(\n            obj=self,\n            indent=4,\n            sort_keys=True,\n            ensure_ascii=True,\n        )\n        self.path.write_text(json_string + '\\n', encoding=UTF8)\nclass Config(BaseConfigDict):\n    FILENAME = 'config.json'\n    DEFAULTS = {\n    def __init__(self, directory: Union[str, Path] = DEFAULT_CONFIG_DIR):\n        self.directory = Path(directory)\n        super().__init__(path=self.directory / self.FILENAME)\n        self.update(self.DEFAULTS)\n    def plugins_dir(self) -> Path:\n        return Path(self.get('plugins_dir', self.directory / 'plugins')).resolve()\ndef program(args: argparse.Namespace, env: Environment) -> ExitStatus:\n    if args.action is None:\n        parser.error(MSG_NAKED_INVOCATION)\n    if args.action == 'plugins':\n        plugins = PluginInstaller(env, debug=args.debug)\n        return plugins.run(args.plugins_action, args)\n    return ExitStatus.SUCCESS",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-0430",
        "description": "[{'lang': 'en', 'value': 'Exposure of Sensitive Information to an Unauthorized Actor in GitHub repository httpie/httpie prior to 3.1.0.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-180",
      "code": "def create_env(nzo: Optional[NzbObject] = None, extra_env_fields: Dict[str, Any] = {}) -> Optional[Dict[str, Any]]:\n    \"\"\"Modify the environment for pp-scripts with extra information\n    macOS: Return copy of environment without PYTHONPATH and PYTHONHOME\n    other: return None\n    \"\"\"\n    env = os.environ.copy()\n    if nzo:\n        for field in ENV_NZO_FIELDS:\n            try:\n                field_value = getattr(nzo, field)\n                if field_value is None:\n                    env[\"SAB_\" + field.upper()] = \"\"\n                elif isinstance(field_value, bool):\n                    env[\"SAB_\" + field.upper()] = str(field_value * 1)\n                else:\n                    env[\"SAB_\" + field.upper()] = str(field_value)\n            except:\n                pass\n    extra_env_fields.update(\n        {\n            \"program_dir\": sabnzbd.DIR_PROG,\n            \"par2_command\": sabnzbd.newsunpack.PAR2_COMMAND,\n            \"multipar_command\": sabnzbd.newsunpack.MULTIPAR_COMMAND,\n            \"rar_command\": sabnzbd.newsunpack.RAR_COMMAND,\n            \"zip_command\": sabnzbd.newsunpack.ZIP_COMMAND,\n            \"7zip_command\": sabnzbd.newsunpack.SEVENZIP_COMMAND,\n            \"version\": sabnzbd.__version__,\n        }\n    )\n    for field in extra_env_fields:\n        try:\n            if extra_env_fields[field] is not None:\n                env[\"SAB_\" + field.upper()] = str(extra_env_fields[field])\n            else:\n                env[\"SAB_\" + field.upper()] = \"\"\n        except:\n            pass\n    if sabnzbd.MACOS:\n        if \"PYTHONPATH\" in env:\n            del env[\"PYTHONPATH\"]\n        if \"PYTHONHOME\" in env:\n            del env[\"PYTHONHOME\"]\n    elif not nzo:\n        return None\n    return env\ndef send_nscript(title, msg, gtype, force=False, test=None):\n    \"\"\"Run user's notification script\"\"\"\n    if test:\n        script = test.get(\"nscript_script\")\n        env = {\"notification_parameters\": test.get(\"nscript_parameters\")}\n    else:\n        script = sabnzbd.cfg.nscript_script()\n        env = {\"notification_parameters\": sabnzbd.cfg.nscript_parameters()}\n    if not script:\n        return T(\"Cannot send, missing required data\")\n    title = \"SABnzbd: \" + T(NOTIFICATION.get(gtype, \"other\"))\n    if force or check_classes(gtype, \"nscript\"):\n        script_path = make_script_path(script)\n        if script_path:\n            ret = -1\n            output = None\n            try:\n                p = build_and_run_command([script_path, gtype, title, msg], env=create_env(extra_env_fields=env))\n                output = p.stdout.read()\n                ret = p.wait()\n            except:\n                logging.info(\"Failed script %s, Traceback: \", script, exc_info=True)\n            if ret:\n                logging.error(T('Script returned exit code %s and output \"%s\"'), ret, output)\n                return T('Script returned exit code %s and output \"%s\"') % (ret, output)\n            else:\n                logging.info(\"Successfully executed notification script %s\", script_path)\n        else:\n            return T('Notification script \"%s\" does not exist') % script_path\n    return \"\"",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-34237",
        "description": "[{'lang': 'en', 'value': 'SABnzbd is an open source automated Usenet download tool. A design flaw was discovered in SABnzbd that could allow remote code execution. Manipulating the Parameters setting in the Notification Script functionality allows code execution with the privileges of the SABnzbd process. Exploiting the vulnerabilities requires access to the web interface. Remote exploitation is possible if users[exposed their setup to the internet or other untrusted networks without setting a username/password. By default SABnzbd is only accessible from `localhost`, with no authentication required for the web interface. This issue has been patched in commits `e3a722` and `422b4f` which have been included in the 4.0.2 release. Users are advised to upgrade. Users unable to upgrade should ensure that a username and password have been set if their instance is web accessible.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-181",
      "code": "def Body(\n    *,\n    media_type: Union[str, \"RequestEncodingType\"] = RequestEncodingType.JSON,\n    examples: Optional[List[\"Example\"]] = None,\n    external_docs: Optional[\"ExternalDocumentation\"] = None,\n    content_encoding: Optional[str] = None,\n    default: Any = Empty,\n    title: Optional[str] = None,\n    description: Optional[str] = None,\n    const: Optional[bool] = None,\n    gt: Optional[float] = None,\n    ge: Optional[float] = None,\n    lt: Optional[float] = None,\n    le: Optional[float] = None,\n    multiple_of: Optional[float] = None,\n    min_items: Optional[int] = None,\n    max_items: Optional[int] = None,\n    min_length: Optional[int] = None,\n    max_length: Optional[int] = None,\n    regex: Optional[str] = None\n) -> Any:\n    \"\"\"Create an extended request body kwarg definition.\n    Args:\n        media_type: Defaults to RequestEncodingType.JSON.\n        examples: A list of Example models.\n        external_docs: A url pointing at external documentation for the given\n            parameter.\n        content_encoding: The content encoding of the value. Applicable on to string values. See\n            OpenAPI 3.1 for details.\n        default: A default value. If const is true, this value is required.\n        title: String value used in the title section of the OpenAPI schema for the given\n            parameter.\n        description: String value used in the description section of the OpenAPI schema for the\n            given parameter.\n        const: A boolean flag dictating whether this parameter is a constant. If True, the value passed\n            to the parameter must equal its default value. This also causes the OpenAPI const field to be populated with\n            the default value.\n        gt: Constrict value to be greater than a given float or int. Equivalent to\n            exclusiveMinimum in the OpenAPI specification.\n        ge: Constrict value to be greater or equal to a given float or int. Equivalent to\n            minimum in the OpenAPI specification.\n        lt: Constrict value to be less than a given float or int. Equivalent to\n            exclusiveMaximum in the OpenAPI specification.\n        le: Constrict value to be less or equal to a given float or int. Equivalent to maximum\n            in the OpenAPI specification.\n        multiple_of: Constrict value to a multiple of a given float or int. Equivalent to\n            multipleOf in the OpenAPI specification.\n        min_items: Constrict a set or a list to have a minimum number of items. Equivalent to\n            minItems in the OpenAPI specification.\n        max_items: Constrict a set or a list to have a maximum number of items. Equivalent to\n            maxItems in the OpenAPI specification.\n        min_length: Constrict a string or bytes value to have a minimum length. Equivalent to\n            minLength in the OpenAPI specification.\n        max_length: Constrict a string or bytes value to have a maximum length. Equivalent to\n            maxLength in the OpenAPI specification.\n        regex: A string representing a regex against which the given string will be matched.\n            Equivalent to pattern in the OpenAPI specification.\n    \"\"\"\n    return BodyKwarg(\n        media_type=media_type,\n        examples=examples,\n        external_docs=external_docs,\n        content_encoding=content_encoding,\n        default=default,\n        title=title,\n        description=description,\n        const=const,\n        gt=gt,\n        ge=ge,\n        lt=lt,\n        le=le,\n        multiple_of=multiple_of,\n        min_items=min_items,\n        max_items=max_items,\n        min_length=min_length,\n        max_length=max_length,\n        regex=regex,\n    )\ndef parse_multipart_form(body: bytes, boundary: bytes) -> Dict[str, Any]:\n    \"\"\"Parse multipart form data.\n    Args:\n        body: Body of the request.\n        boundary: Boundary of the multipart message.\n    Returns:\n        A dictionary of parsed results.\n    \"\"\"\n    fields: DefaultDict[str, List[Any]] = defaultdict(list)\n    if body and boundary:\n        form_parts = body.split(boundary)\n        for form_part in form_parts[1:-1]:\n            file_name = None\n            content_type = \"text/plain\"\n            content_charset = \"utf-8\"\n            field_name = None\n            line_index = 2\n            line_end_index = 0\n            headers: List[Tuple[str, str]] = []\n            while line_end_index != -1:\n                line_end_index = form_part.find(b\"\\r\\n\", line_index)\n                form_line = form_part[line_index:line_end_index].decode(\"utf-8\")\n                if not form_line:\n                    break\n                line_index = line_end_index + 2\n                colon_index = form_line.index(\":\")\n                current_idx = colon_index + 2\n                form_header_field = form_line[0:colon_index].lower()\n                form_header_value, form_parameters = parse_content_header(form_line[current_idx:])\n                if form_header_field == \"content-disposition\":\n                    field_name = form_parameters.get(\"name\")\n                    file_name = form_parameters.get(\"filename\")\n                    if file_name is None and (filename_with_asterisk := form_parameters.get(\"filename*\")):\n                        encoding, _, value = decode_rfc2231(filename_with_asterisk)\n                        file_name = unquote(value, encoding=encoding or content_charset)\n                elif form_header_field == \"content-type\":\n                    content_type = form_header_value\n                    content_charset = form_parameters.get(\"charset\", \"utf-8\")\n                headers.append((form_header_field, form_header_value))\n            if field_name:\n                post_data = form_part[line_index:-4].lstrip(b\"\\r\\n\")\n                if file_name:\n                    form_file = UploadFile(\n                        content_type=content_type, filename=file_name, file_data=post_data, headers=dict(headers)\n                    )\n                    fields[field_name].append(form_file)\n                else:\n                    try:\n                        fields[field_name].append(decode_json(post_data))\n                    except SerializationException:\n                        fields[field_name].append(post_data.decode(content_charset))\n    return {k: v if len(v) > 1 else v[0] for k, v in fields.items()}\ndef create_multipart_extractor(\n    signature_field: \"SignatureField\", is_data_optional: bool\n) -> Callable[[\"ASGIConnection[Any, Any, Any]\"], Coroutine[Any, Any, Any]]:\n    \"\"\"Create a multipart form-data extractor.\n    Args:\n        signature_field: A SignatureField instance.\n        is_data_optional: Boolean dictating whether the field is optional.\n    Returns:\n        An extractor function.\n    \"\"\"\n    async def extract_multipart(\n        connection: \"Request[Any, Any]\",\n    ) -> Any:\n        connection.scope[\"_form\"] = form_values = (\n            connection.scope[\"_form\"]\n            if \"_form\" in connection.scope\n            else parse_multipart_form(\n                body=await connection.body(), boundary=connection.content_type[-1].get(\"boundary\", \"\").encode()\n            )\n        )\n        if signature_field.is_non_string_sequence:\n            return list(form_values.values())\n        if signature_field.is_simple_type and signature_field.field_type is UploadFile and form_values:\n            return [v for v in form_values.values() if isinstance(v, UploadFile)][0]\n        return form_values if form_values or not is_data_optional else None\n    return cast(\"Callable[[ASGIConnection[Any, Any, Any]], Coroutine[Any, Any, Any]]\", extract_multipart)\n    async def form(self) -> FormMultiDict:\n        \"\"\"Retrieve form data from the request. If the request is either a 'multipart/form-data' or an\n        'application/x-www-form- urlencoded', return a FormMultiDict instance populated with the values sent in the\n        request, otherwise, an empty instance.\n        Returns:\n            A FormMultiDict instance\n        \"\"\"\n        if self._form is Empty:\n            content_type, options = self.content_type\n            if content_type == RequestEncodingType.MULTI_PART:\n                self._form = self.scope[\"_form\"] = form_values = parse_multipart_form(\n                    body=await self.body(), boundary=options.get(\"boundary\", \"\").encode()\n                )\n                return FormMultiDict(form_values)\n            if content_type == RequestEncodingType.URL_ENCODED:\n                self._form = self.scope[\"_form\"] = form_values = parse_url_encoded_form_data(\n                    await self.body(),\n                )\n                return FormMultiDict(form_values)\n            return FormMultiDict()\n        return FormMultiDict(self._form)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-25578",
        "description": "[{'lang': 'en', 'value': 'Starlite is an Asynchronous Server Gateway Interface (ASGI) framework. Prior to version 1.5.2, the request body parsing in `starlite` allows a potentially unauthenticated attacker to consume a large amount of CPU time and RAM. The multipart body parser processes an unlimited number of file parts and an unlimited number of field parts. This is a remote, potentially unauthenticated Denial of Service vulnerability. This vulnerability affects applications with a request handler that accepts a `Body(media_type=RequestEncodingType.MULTI_PART)`. The large amount of CPU time required for processing requests can block all available worker processes and significantly delay or slow down the processing of legitimate user requests. The large amount of RAM accumulated while processing requests can lead to Out-Of-Memory kills. Complete DoS is achievable by sending many concurrent multipart requests in a loop. Version 1.51.2 contains a patch for this issue.\\n'}]",
        "cwe_number": 770
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-182",
      "code": "  def testParallelConcatShapeZero(self):\n    if not tf2.enabled():\n      self.skipTest(\"only fails in TF2\")\n    @def_function.function\n    def f():\n      y = gen_array_ops.parallel_concat(values=[[\"tf\"]], shape=0)\n      return y\n    with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                r\"0th dimension of value .* is less than\"):\n      f()\nif __name__ == \"__main__\":\n  test.main()",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-25676",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source machine learning platform. When running versions prior to 2.12.0 and 2.11.1 with XLA, `tf.raw_ops.ParallelConcat` segfaults with a nullptr dereference when given a parameter `shape` with rank that is not greater than zero. A fix is available in TensorFlow 2.12.0 and 2.11.1.'}]",
        "cwe_number": 476
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-183",
      "code": "def break_long_headers(header):\n    \"\"\"\n    Breaks headers longer than 160 characters (~page length)\n    when possible (are comma separated)\n    \"\"\"\n    if len(header) > 160 and ',' in header:\n        header = mark_safe('<br> ' + ', <br>'.join(header.split(',')))\n    return header",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-21520",
        "description": "[{'lang': 'en', 'value': 'Versions of the package djangorestframework before 3.15.2 are vulnerable to Cross-site Scripting (XSS) via the break_long_headers template filter due to improper input sanitization before splitting and joining with <br> tags.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-184",
      "code": "    def parse(self, content):\n        raw = {}\n        root = etree.fromstring(content)\n        for child in root:\n            raw[child.tag] = child.text\n        formatted = self.format(raw)\n        msg_type = formatted['type']\n        msg_parser = getattr(self, 'parse_{0}'.format(msg_type), None)\n        if callable(msg_parser):\n            parsed = msg_parser(raw)\n        else:\n            parsed = self.parse_invalid_type(raw)\n        formatted.update(parsed)\n        return formatted\n    def to_dict(self, content):\n        raw = {}\n        root = etree.fromstring(content.encode(\"utf-8\"))\n        for child in root:\n            raw[child.tag] = child.text\n        return raw",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2018-25082",
        "description": "[{'lang': 'en', 'value': 'A vulnerability was found in zwczou WeChat SDK Python 0.3.0 and classified as critical. This issue affects the function validate/to_xml. The manipulation leads to xml external entity reference. The attack may be initiated remotely. Upgrading to version 0.5.5 is able to address this issue. The patch is named e54abadc777715b6dcb545c13214d1dea63df6c9. It is recommended to upgrade the affected component. The associated identifier of this vulnerability is VDB-223403.'}]",
        "cwe_number": 611
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-185",
      "code": "async def open_file(file_path: FilePath):\n    \"\"\"\n    Opens code in vs code.\n    :param file_path: The file path object.\n    :return: A JSON response with the status of the operation.\n    \"\"\"\n    try:\n        path = file_path.path\n        if not validate_file_path(path):\n            return {\"status\":False,\"error\":\"Invalid file path\"}\n        path = os.path.realpath(path)\n        subprocess.Popen([\"start\", path], shell=True)\n        return {\"status\": True, \"execution_time\": 0}\n    except Exception as ex:\n        trace_exception(ex)\n        lollmsElfServer.error(ex)\n        return {\"status\":False,\"error\":str(ex)}\nasync def open_code_in_vs_code(vs_code_data: VSCodeData):\n    \"\"\"\n    Opens code in vs code.\n    :param vs_code_data: The data object.\n    :return: A JSON response with the status of the operation.\n    \"\"\"\n    try:\n        discussion_id = vs_code_data.discussion_id\n        message_id = vs_code_data.message_id\n        code = vs_code_data.code\n        ASCIIColors.info(\"Opening folder:\")\n        root_folder = Path(os.path.realpath(lollmsElfServer.lollms_paths.personal_outputs_path/\"discussions\"/f\"d_{discussion_id}\"/f\"{message_id}.py\"))\n        root_folder.mkdir(parents=True,exist_ok=True)\n        tmp_file = root_folder/f\"ai_code_{message_id}.py\"\n        with open(tmp_file,\"w\") as f:\n            f.write(code)\n        subprocess.Popen([\"code\", str(root_folder)], shell=True)\n        return {\"status\": True, \"execution_time\": 0}\n    except Exception as ex:\n        trace_exception(ex)\n        lollmsElfServer.error(ex)\n        return {\"status\":False,\"error\":str(ex)}\nasync def open_code_folder(request: FolderRequest):\n    \"\"\"\n    Opens code folder.\n    :param request: The HTTP request object.\n    :return: A JSON response with the status of the operation.\n    \"\"\"\n    try:\n        if request.discussion_id:\n            discussion_id = request.discussion_id\n            ASCIIColors.info(\"Opening folder:\")\n            root_folder = lollmsElfServer.lollms_paths.personal_outputs_path / \"discussions\" / f\"d_{discussion_id}\"\n            root_folder.mkdir(parents=True, exist_ok=True)\n            if platform.system() == 'Windows':\n                subprocess.run(['start', str(root_folder)], check=True, shell=True)\n            elif platform.system() == 'Linux':\n                subprocess.run(['xdg-open', str(root_folder)], check=True)\n            elif platform.system() == 'Darwin':\n                subprocess.run(['open', str(root_folder)], check=True)\n            return {\"status\": True, \"execution_time\": 0}\n        elif request.folder_path:\n            folder_path = os.path.realpath(request.folder_path)\n            root_folder = Path(folder_path)\n            is_valid_folder_path = root_folder.is_dir()\n            if not is_valid_folder_path:\n                return {\"status\":False, \"error\":\"Invalid folder path\"}\n            ASCIIColors.info(\"Opening folder:\")\n            root_folder.mkdir(parents=True, exist_ok=True)\n            if platform.system() == 'Windows':\n                subprocess.run(['start', str(root_folder)], check=True, shell=True)\n            elif platform.system() == 'Linux':\n                subprocess.run(['xdg-open', str(root_folder)], check=True)\n            elif platform.system() == 'Darwin':\n                subprocess.run(['open', str(root_folder)], check=True)\n            return {\"status\": True, \"execution_time\": 0}\n    except Exception as ex:\n        trace_exception(ex)\n        lollmsElfServer.error(ex)\n        return {\"status\": False, \"error\": \"An error occurred while processing the request\"}",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-1520",
        "description": "[{'lang': 'en', 'value': \"An OS Command Injection vulnerability exists in the '/open_code_folder' endpoint of the parisneo/lollms-webui application, due to improper validation of user-supplied input in the 'discussion_id' parameter. Attackers can exploit this vulnerability by injecting malicious OS commands, leading to unauthorized command execution on the underlying operating system. This could result in unauthorized access, data leakage, or complete system compromise.\"}]",
        "cwe_number": 78
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-186",
      "code": "async def largest_content_len(urls: list[str]) -> tuple[str, int]:\n    largest_url = \"\"\n    largest_len = 0\n    async def do(client: AsyncClient, url: str) -> Response:\n        return await client.head(url, headers={\"User-Agent\": _FIREFOX_UA})\n    async with AsyncClient() as client:\n        tasks = [do(client, url) for url in urls]\n        responses: list[Response] = await gather_with_concurrency(10, *tasks, ignore_exceptions=True)\n        for response in responses:\n            len_int = int(response.headers.get(\"Content-Length\", 0))\n            if len_int > largest_len:\n                largest_url = str(response.url)\n                largest_len = len_int\n    return largest_url, largest_len\n    def _validate_image_url(url: str) -> bool:\n        \"\"\"\n        Validates that the URL is of an allowed source and restricts certain sources to prevent\n        malicious images from being downloaded.\n        \"\"\"\n        invalid_domains = {\"127.0.0.1\", \"localhost\"}\n        for domain in invalid_domains:\n            if domain in url:\n                return False\n        return True\n    async def scrape_image(self, image_url) -> None:\n        self.logger.info(f\"Image URL: {image_url}\")\n        if not self._validate_image_url(image_url):\n            self.logger.error(f\"Invalid image URL: {image_url}\")\n            raise InvalidDomainError(f\"Invalid domain: {image_url}\")\n        if isinstance(image_url, str):\n            pass\n        elif isinstance(image_url, list):\n            image_url, _ = await largest_content_len(image_url)\n        elif isinstance(image_url, dict):\n            for key in image_url:\n                if key == \"url\":\n                    image_url = image_url.get(\"url\")\n        ext = image_url.split(\".\")[-1]\n        if ext not in img.IMAGE_EXTENSIONS:\n            ext = \"jpg\"\n        file_name = f\"{str(self.recipe_id)}.{ext}\"\n        file_path = Recipe.directory_from_id(self.recipe_id).joinpath(\"images\", file_name)\n        async with AsyncClient() as client:\n            try:\n                r = await client.get(image_url, headers={\"User-Agent\": _FIREFOX_UA})\n            except Exception:\n                self.logger.exception(\"Fatal Image Request Exception\")\n                return None\n            if r.status_code != 200:\n                return None\n            content_type = r.headers.get(\"content-type\", \"\")\n            if \"image\" not in content_type:\n                self.logger.error(f\"Content-Type: {content_type} is not an image\")\n                raise NotAnImageError(f\"Content-Type {content_type} is not an image\")\n            self.logger.debug(f\"File Name Suffix {file_path.suffix}\")\n            self.write_image(r.read(), file_path.suffix)\n            file_path.unlink(missing_ok=True)\nasync def safe_scrape_html(url: str) -> str:\n    \"\"\"\n    Scrapes the html from a url but will cancel the request\n    if the request takes longer than 15 seconds. This is used to mitigate\n    DDOS attacks from users providing a url with arbitrary large content.\n    \"\"\"\n    async with AsyncClient() as client:\n        html_bytes = b\"\"\n        async with client.stream(\"GET\", url, timeout=SCRAPER_TIMEOUT, headers={\"User-Agent\": _FIREFOX_UA}) as resp:\n            start_time = time.time()\n            async for chunk in resp.aiter_bytes(chunk_size=1024):\n                html_bytes += chunk\n                if time.time() - start_time > SCRAPER_TIMEOUT:\n                    raise ForceTimeoutException()\n        content = None\n        encoding = resp.encoding\n        if not html_bytes:\n            return \"\"\n        if encoding is None:\n            encoding = resp.apparent_encoding\n        try:\n            content = str(html_bytes, encoding, errors=\"replace\")\n        except (LookupError, TypeError):\n            content = str(html_bytes, errors=\"replace\")\n        return content\nasync def create_from_url(url: str, translator: Translator) -> tuple[Recipe, ScrapedExtras | None]:\n    \"\"\"Main entry point for generating a recipe from a URL. Pass in a URL and\n    a Recipe object will be returned if successful.\n    Args:\n        url (str): a valid string representing a URL\n    Returns:\n        Recipe: Recipe Object\n    \"\"\"\n    scraper = RecipeScraper(translator)\n    new_recipe, extras = await scraper.scrape(url)\n    if not new_recipe:\n        raise HTTPException(status.HTTP_400_BAD_REQUEST, {\"details\": ParserErrors.BAD_RECIPE_DATA.value})\n    new_recipe.id = uuid4()\n    logger = get_logger()\n    logger.debug(f\"Image {new_recipe.image}\")\n    recipe_data_service = RecipeDataService(new_recipe.id)\n    try:\n        await recipe_data_service.scrape_image(new_recipe.image)\n        if new_recipe.name is None:\n            new_recipe.name = \"Untitled\"\n        new_recipe.slug = slugify(new_recipe.name)\n        new_recipe.image = cache.new_key(4)\n    except Exception as e:\n        recipe_data_service.logger.exception(f\"Error Scraping Image: {e}\")\n        new_recipe.image = \"no image\"\n    if new_recipe.name is None or new_recipe.name == \"\":\n        new_recipe.name = f\"No Recipe Name Found - {str(uuid4())}\"\n        new_recipe.slug = slugify(new_recipe.name)\n    return new_recipe, extras",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-31993",
        "description": "[{'lang': 'en', 'value': 'Mealie is a self hosted recipe manager and meal planner. Prior to 1.4.0, the scrape_image function will retrieve an image based on a user-provided URL, however the provided URL is not validated to point to an external location and does not have any enforced rate limiting. The response from the Mealie server will also vary depending on whether or not the target file is an image, is not an image, or does not exist. Additionally, when a file is retrieved the file may remain stored on Mealie\u2019s file system as original.jpg under the UUID of the recipe it was requested for. If the attacker has access to an admin account (e.g. the default changeme@example.com), this file can then be retrieved. Note that if Mealie is running in a development setting this could be leveraged by an attacker to retrieve any file that the Mealie server had downloaded in this fashion without the need for administrator access. This vulnerability is fixed in 1.4.0.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-187",
      "code": "def filter_materialized_view(\n        engine,\n        view_name,\n        schema=\"web\",\n        state_code=None,\n        area=None,\n        distance_grid=None,\n        building=None,\n        buildingfp=None,\n        limit=None,\n        keys=None,\n):\n    if schema is not None:\n        view_name = \"{}.{}\".format(schema, view_name)\n    if limit is None:\n        limit = \"\"\n    else:\n        limit = \" LIMIT {}\".format(limit)\n    filter_cond = \"\"\n    if state_code is not None:\n        key = \"adm1_pcode\"\n        filter_cond = f\" WHERE {view_name}.{key}='{state_code}'\"\n    if area is not None:\n        key = \"area_km2\"\n        if \"WHERE\" in filter_cond:\n            filter_cond = filter_cond + f\" AND {view_name}.{key} >= {area[0]} AND\" \\\n                                        f\" {view_name}.{key} <= {area[1]}\"\n        else:\n            filter_cond = f\" WHERE {view_name}.{key} >= {area[0]} AND {view_name}.{key} <= {area[1]}\"\n    if distance_grid is not None:\n        key = \"grid_dist_km\"\n        if \"WHERE\" in filter_cond:\n            filter_cond = filter_cond + f\" AND {view_name}.{key} >= {distance_grid[0]} AND\" \\\n                                        f\" {view_name}.{key} <= {distance_grid[1]}\"\n        else:\n            filter_cond = f\" WHERE {view_name}.{key} >= {distance_grid[0]} AND\" \\\n                          f\" {view_name}.{key} <= {distance_grid[1]}\"\n    if building is not None:\n        key = \"building_count\"\n        if \"WHERE\" in filter_cond:\n            filter_cond = filter_cond + f\" AND {view_name}.{key} >= {building[0]} AND\" \\\n                                        f\" {view_name}.{key} <= {building[1]}\"\n        else:\n            filter_cond = f\" WHERE {view_name}.{key} >= {building[0]} AND\" \\\n                          f\" {view_name}.{key} <= {building[1]}\"\n    if buildingfp is not None:\n        key = \"percentage_building_area\"\n        if \"WHERE\" in filter_cond:\n            filter_cond = filter_cond + f\" AND {view_name}.{key} >= {buildingfp[0]} AND\" \\\n                                        f\" {view_name}.{key} <= {buildingfp[1]}\"\n        else:\n            filter_cond = f\" WHERE {view_name}.{key} >= {buildingfp[0]} AND\" \\\n                          f\" {view_name}.{key} <= {buildingfp[1]}\"\n    if keys is None:\n        columns = \"*\"\n    else:\n        if not isinstance(keys, str):\n            columns = \", \".join(keys)\n        else:\n            columns = \"COUNT({})\".format(keys)\n    with engine.connect() as con:\n        query = 'SELECT {} FROM {}{}{};'.format(columns, view_name, filter_cond, limit)\n        rs = con.execute(query)\n        data = rs.fetchall()\n    return data",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2020-36768",
        "description": "[{'lang': 'en', 'value': 'A vulnerability was found in rl-institut NESP2 Initial Release/1.0. It has been classified as critical. Affected is an unknown function of the file app/database.py. The manipulation leads to sql injection. It is possible to launch the attack remotely. The exploit has been disclosed to the public and may be used. The patch is identified as 07c0cdf36cf6a4345086d07b54423723a496af5e. It is recommended to apply a patch to fix this issue. VDB-246642 is the identifier assigned to this vulnerability.'}]",
        "cwe_number": 89
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-188",
      "code": "def remote_static():\n    \"\"\"Stream *large* static files with special requirements.\"\"\"\n    file_path = request.args.get(\"file\") or \".\"\n    if current_user.is_authenticated is False or file_path not in session.get(\"igv_tracks\", []):\n        LOG.warning(f\"{file_path} not in {session.get('igv_tracks', [])}\")\n        return abort(403)\n    range_header = request.headers.get(\"Range\", None)\n    if not range_header and (file_path.endswith(\".bam\") or file_path.endswith(\".cram\")):\n        return abort(500)\n    new_resp = send_file_partial(file_path)\n    return new_resp\ndef set_session_tracks(display_obj):\n    \"\"\"Save igv tracks as a session object. This way it's easy to verify that a user is requesting one of these files from remote_static view endpoint\n    Args:\n        display_obj(dict): A display object containing case name, list of genes, lucus and tracks\n    \"\"\"\n    session_tracks = list(display_obj.get(\"reference_track\", {}).values())\n    for key, track_items in display_obj.items():\n        if key not in [\"tracks\", \"custom_tracks\", \"sample_tracks\"]:\n            continue\n        for track_item in track_items:\n            session_tracks += list(track_item.values())\n    session[\"igv_tracks\"] = session_tracks",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-1592",
        "description": "[{'lang': 'en', 'value': 'Server-Side Request Forgery in scout in GitHub repository clinical-genomics/scout prior to v4.42. An attacker could make the application perform arbitrary requests to fishing steal cookie, request to private area, or lead to xss...'}]",
        "cwe_number": 918
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-189",
      "code": "    def urlopen(self, method, url, redirect=True, **kw):\n        \"\"\"\n        Same as :meth:`urllib3.connectionpool.HTTPConnectionPool.urlopen`\n        with custom cross-host redirect logic and only sends the request-uri\n        portion of the ``url``.\n        The given ``url`` parameter must be absolute, such that an appropriate\n        :class:`urllib3.connectionpool.ConnectionPool` can be chosen for it.\n        \"\"\"\n        u = parse_url(url)\n        conn = self.connection_from_host(u.host, port=u.port, scheme=u.scheme)\n        kw['assert_same_host'] = False\n        kw['redirect'] = False\n        if 'headers' not in kw:\n            kw['headers'] = self.headers.copy()\n        if self.proxy is not None and u.scheme == \"http\":\n            response = conn.urlopen(method, url, **kw)\n        else:\n            response = conn.urlopen(method, u.request_uri, **kw)\n        redirect_location = redirect and response.get_redirect_location()\n        if not redirect_location:\n            return response\n        redirect_location = urljoin(url, redirect_location)\n        if response.status == 303:\n            method = 'GET'\n        retries = kw.get('retries')\n        if not isinstance(retries, Retry):\n            retries = Retry.from_int(retries, redirect=redirect)\n        if (retries.remove_headers_on_redirect\n                and not conn.is_same_host(redirect_location)):\n            for header in retries.remove_headers_on_redirect:\n                kw['headers'].pop(header, None)\n        try:\n            retries = retries.increment(method, url, response=response, _pool=conn)\n        except MaxRetryError:\n            if retries.raise_on_redirect:\n                raise\n            return response\n        kw['retries'] = retries\n        kw['redirect'] = redirect\n        log.info(\"Redirecting %s -> %s\", url, redirect_location)\n        return self.urlopen(method, redirect_location, **kw)\nclass ProxyManager(PoolManager):\n    def __init__(self, total=10, connect=None, read=None, redirect=None, status=None,\n                 method_whitelist=DEFAULT_METHOD_WHITELIST, status_forcelist=None,\n                 backoff_factor=0, raise_on_redirect=True, raise_on_status=True,\n                 history=None, respect_retry_after_header=True,\n                 remove_headers_on_redirect=DEFAULT_REDIRECT_HEADERS_BLACKLIST):\n        self.total = total\n        self.connect = connect\n        self.read = read\n        self.status = status\n        if redirect is False or total is False:\n            redirect = 0\n            raise_on_redirect = False\n        self.redirect = redirect\n        self.status_forcelist = status_forcelist or set()\n        self.method_whitelist = method_whitelist\n        self.backoff_factor = backoff_factor\n        self.raise_on_redirect = raise_on_redirect\n        self.raise_on_status = raise_on_status\n        self.history = history or tuple()\n        self.respect_retry_after_header = respect_retry_after_header\n        self.remove_headers_on_redirect = remove_headers_on_redirect",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2018-25091",
        "description": "[{'lang': 'en', 'value': 'urllib3 before 1.24.2 does not remove the authorization HTTP header when following a cross-origin redirect (i.e., a redirect that differs in host, port, or scheme). This can allow for credentials in the authorization header to be exposed to unintended hosts or transmitted in cleartext. NOTE: this issue exists because of an incomplete fix for CVE-2018-20060 (which was case-sensitive).'}]",
        "cwe_number": 601
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-190",
      "code": "def is_local_uri(uri):\n    \"\"\"Returns true if this is a local file path (/foo or file:/foo).\"\"\"\n    if uri == \"databricks\":\n        return False\n    if is_windows() and uri.startswith(\"\\\\\\\\\"):\n        return False\n    parsed_uri = urllib.parse.urlparse(uri)\n    if parsed_uri.hostname:\n        return False\n    scheme = parsed_uri.scheme\n    if scheme == \"\" or scheme == \"file\":\n        return True\n    if is_windows() and len(scheme) == 1 and scheme.lower() == pathlib.Path(uri).drive.lower()[0]:\n        return True\n    return False\ndef _validate_source(source: str, run_id: str) -> None:\n    if not is_local_uri(source):\n        return\n    if run_id:\n        store = _get_tracking_store()\n        run = store.get_run(run_id)\n        source = pathlib.Path(local_file_uri_to_path(source)).resolve()\n        run_artifact_dir = pathlib.Path(local_file_uri_to_path(run.info.artifact_uri)).resolve()\n        if run_artifact_dir in [source, *source.parents]:\n            return\n    raise MlflowException(\n        f\"Invalid source: '{source}'. To use a local path as source, the run_id request parameter \"\n        \"has to be specified and the local path has to be contained within the artifact directory \"\n        \"of the run specified by the run_id.\",\n        INVALID_PARAMETER_VALUE,\n    )",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-2780",
        "description": "[{'lang': 'en', 'value': \"Path Traversal: '\\\\..\\\\filename' in GitHub repository mlflow/mlflow prior to 2.3.1.\"}]",
        "cwe_number": 29
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-191",
      "code": "def url(tgpath, tgparams=None, **kwargs):\n    '''Computes URLs.\n    This is a replacement for :func:`turbogears.controllers.url` (aka\n    :func:`tg.url` in the template).  In addition to the functionality that\n    :func:`tg.url` provides, it adds a token to prevent :term:`CSRF` attacks.\n    :arg tgpath:  a list or a string. If the path is absolute (starts\n        with a \"/\"), the :attr:`server.webpath`, :envvar:`SCRIPT_NAME` and\n        the approot of the application are prepended to the path. In order for\n        the approot to be detected properly, the root object should extend\n        :class:`turbogears.controllers.RootController`.\n    :kwarg tgparams: See param: ``kwargs``\n    :kwarg kwargs: Query parameters for the URL can be passed in as a\n        dictionary in the second argument *or* as keyword parameters.\n        Values which are a list or a tuple are used to create multiple\n        key-value pairs.\n    :returns: The changed path\n    .. versionadded:: 0.3.10\n       Modified from turbogears.controllers.url for :ref:`CSRF-Protection`\n    '''\n    if not isinstance(tgpath, six.string_types):\n        tgpath = '/'.join(list(tgpath))\n    if tgpath.startswith('/'):\n        webpath = (config.get('server.webpath') or '').rstrip('/')\n        if tg_util.request_available():\n            check_app_root()\n            tgpath = request.app_root + tgpath\n            try:\n                webpath += request.wsgi_environ['SCRIPT_NAME'].rstrip('/')\n            except (AttributeError, KeyError):\n                pass\n        tgpath = webpath + tgpath\n    if tgparams is None:\n        tgparams = kwargs\n    else:\n        try:\n            tgparams = tgparams.copy()\n            tgparams.update(kwargs)\n        except AttributeError:\n            raise TypeError(\n                'url() expects a dictionary for query parameters')\n    args = []\n    try:\n        if identity.current.csrf_token:\n            tgparams.update({'_csrf_token': identity.current.csrf_token})\n    except RequestRequiredException:\n        pass\n    query_params = six.iteritems(tgparams)\n    scheme, netloc, path, params, query_s, fragment = urlparse(tgpath)\n    if query_s:\n        query_params = chain((p for p in cgi.parse_qsl(query_s) if p[0] !=\n                              '_csrf_token'), query_params)\n    for key, value in query_params:\n        if value is None:\n            continue\n        if isinstance(value, (list, tuple)):\n            pairs = [(key, v) for v in value]\n        else:\n            pairs = [(key, value)]\n        for key, value in pairs:\n            if value is None:\n                continue\n            if isinstance(value, unicode):\n                value = value.encode('utf8')\n            args.append((key, str(value)))\n    query_string = urlencode(args, True)\n    tgpath = urlunparse((scheme, netloc, path, params, query_string, fragment))\n    return tgpath\ndef _get_server_name():\n    \"\"\"Return name of the server this application runs on.\n    Respects 'Host' and 'X-Forwarded-Host' header.\n    See the docstring of the 'absolute_url' function for more information.\n    .. note:: This comes from turbogears 1.1 branch.  It is only needed for\n        _tg_absolute_url().  If we find that turbogears.get_server_name()\n        exists, we replace this function with that one.\n    \"\"\"\n    get = config.get\n    h = request.headers\n    host = get('tg.url_domain') or h.get('X-Forwarded-Host', h.get('Host'))\n    if not host:\n        host = '%s:%s' % (get('server.socket_host', 'localhost'),\n                          get('server.socket_port', 8080))\n    return host",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2017-1002150",
        "description": "[{'lang': 'en', 'value': 'python-fedora 0.8.0 and lower is vulnerable to an open redirect resulting in loss of CSRF protection'}]",
        "cwe_number": 601
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-192",
      "code": "  def _ValidateFractionalMaxPoolResult(self, input_tensor, pooling_ratio,\n                                       pseudo_random, overlapping):\n    \"\"\"Validate FractionalMaxPool's result against expected.\n    Expected result is computed given input_tensor, and pooling region defined\n    by row_seq and col_seq.\n    Args:\n      input_tensor: A tensor or numpy ndarray.\n      pooling_ratio: A list or tuple of length 4, first and last element be 1.\n      pseudo_random: Use pseudo random method to generate pooling sequence.\n      overlapping: Use overlapping when pooling.\n    Returns:\n      None\n    \"\"\"\n    with self.cached_session() as sess:\n      p, r, c = nn_ops.fractional_max_pool_v2(\n          input_tensor,\n          pooling_ratio,\n          pseudo_random,\n          overlapping,\n          seed=self._SEED)\n      actual, row_seq, col_seq = self.evaluate([p, r, c])\n      expected = self._GetExpectedFractionalMaxPoolResult(input_tensor, row_seq,\n                                                          col_seq, overlapping)\n      self.assertShapeEqual(expected, p)\n      self.assertAllClose(expected, actual)\n  def _testVisually(self):\n    \"\"\"Manual test by printing out intermediate result of a small random tensor.\n    Since _GetExpectedFractionalMaxPoolResult is 'automated', it feel safer to\n    have a test case that you can see what's happening.\n    This test will generate a small, random, int 2D matrix, and feed it to\n    FractionalMaxPool and _GetExpectedFractionalMaxPoolResult.\n    \"\"\"\n    num_rows = 6\n    num_cols = 6\n    tensor_shape = (1, num_rows, num_cols, 1)\n    pseudo_random = False\n    for overlapping in True, False:\n      print(\"-\" * 70)\n      print(\"Testing FractionalMaxPool with overlapping = {}\".format(\n          overlapping))\n      rand_mat = self._PRNG.randint(10, size=tensor_shape)\n      pooling_ratio = [1, math.sqrt(2), math.sqrt(2), 1]\n      with self.cached_session() as sess:\n        p, r, c = nn_ops.fractional_max_pool_v2(\n            rand_mat,\n            pooling_ratio,\n            pseudo_random,\n            overlapping,\n            seed=self._SEED)\n        tensor_output, row_seq, col_seq = self.evaluate([p, r, c])\n        expected_result = self._GetExpectedFractionalMaxPoolResult(rand_mat,\n                                                                   row_seq,\n                                                                   col_seq,\n                                                                   overlapping)\n        print(\"row sequence:\")\n        print(row_seq)\n        print(\"column sequence:\")\n        print(col_seq)\n        print(\"Input:\")\n        for i in range(num_rows):\n          row_to_print = []\n          for j in range(num_cols):\n            if j in col_seq:\n              row_to_print.append(\"|\")\n            row_to_print.append(str(rand_mat[0, i, j, 0]))\n          row_to_print.append(\"|\")\n          if i in row_seq:\n            print(\"-\" * 2 * len(row_to_print))\n          print(\" \".join(row_to_print))\n        print(\"-\" * 2 * len(row_to_print))\n        print(\"Output from FractionalMaxPool:\")\n        print(tensor_output[0, :, :, 0])\n        print(\"Expected result:\")\n        print(expected_result[0, :, :, 0])\nif __name__ == \"__main__\":\n  test.main()",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-35981",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. `FractionalMaxPoolGrad` validates its inputs with `CHECK` failures instead of with returning errors. If it gets incorrectly sized inputs, the `CHECK` failure can be used to trigger a denial of service attack. We have patched the issue in GitHub commit 8741e57d163a079db05a7107a7609af70931def4. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue.'}]",
        "cwe_number": 617
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-193",
      "code": "def upload_user_data(user_stats_dict):\n    \"\"\"\n    Takes the User Stats dict that is populated by the other functions and\n    then populates the user_info and user_system_summary_stats tables\n    in the metrics MySQL DB.\n    \"\"\"\n    total_users = len(user_stats_dict.keys())\n    rows_info_inserted = 0\n    rows_info_updated = 0\n    rows_stats_inserted = 0\n    db_connection = mysql.connect(\n        host=sql_host, user=\"metrics\", passwd=metrics_mysql_password, database=\"metrics\"\n    )\n    cursor = db_connection.cursor()\n    query = \"use \" + query_on\n    cursor.execute(query)\n    counter_user_id = -1\n    get_max_user_id_q = (\n\t\"select max(user_id) from metrics.user_info \"\n    )\n    cursor.execute(get_max_user_id_q)\n    for row in cursor:\n        counter_user_id = row[0]\n    existing_user_info = dict()\n    query = (\n        \"select username, display_name, email, orcid, globus_login, google_login, \"\n        \"kb_internal_user, institution, country, \"\n        \"signup_date, last_signin_date, department, job_title, job_title_other, \"\n        \"city, state, postal_code, funding_source, research_statement, \"\n        \"research_interests, avatar_option, gravatar_default , \"\n        \"how_u_hear_selected, how_u_hear_other from metrics.user_info\"\n    )\n    cursor.execute(query)\n    for (\n            username,\n            display_name,\n            email,\n            orcid,\n            globus_login,\n            google_login,\n            kb_internal_user,\n            institution,\n            country,\n            signup_date,\n            last_signin_date,\n            department,\n            job_title,\n            job_title_other,\n            city,\n            state,\n            postal_code,\n            funding_source,\n            research_statement,\n            research_interests,\n            avatar_option,\n            gravatar_default,\n            how_u_hear_selected,\n            how_u_hear_other\n    ) in cursor:\n        existing_user_info[username] = {\n            \"name\": display_name,\n            \"email\": email,\n            \"orcid\": orcid,\n            \"globus_login\": globus_login,\n            \"google_login\": google_login,\n            \"kb_internal_user\": kb_internal_user,\n            \"institution\": institution,\n            \"country\": country,\n            \"signup_date\": signup_date,\n            \"last_signin_date\": last_signin_date,\n            \"department\": department,\n            \"job_title\": job_title,\n            \"job_title_other\": job_title_other,\n            \"city\" : city,\n            \"state\" : state,\n            \"postal_code\" : postal_code,\n            \"funding_source\" : funding_source,\n            \"research_statement\" : research_statement,\n            \"research_interests\" : research_interests,\n            \"avatar_option\" : avatar_option,\n            \"gravatar_default\" : gravatar_default,\n            \"how_u_hear_selected\" : how_u_hear_selected,\n            \"how_u_hear_other\" : how_u_hear_other\n        }\n    print(\"Number of existing users:\" + str(len(existing_user_info)))\n    prep_cursor = db_connection.cursor(prepared=True)\n    user_info_insert_statement = (\n        \"insert into user_info \"\n        \"(username, display_name, email, orcid, \"\n        \"globus_login, google_login, \"\n        \"user_id, kb_internal_user, institution, \"\n        \"country, signup_date, last_signin_date, \"\n        \"department, job_title, job_title_other, \"\n        \"city, state, postal_code, funding_source, \"\n        \"research_statement, research_interests, \"\n        \"avatar_option, gravatar_default, \"\n        \"how_u_hear_selected, how_u_hear_other)\"\n        \"values(%s, %s, %s, %s, \"\n        \"%s, %s, \"\n        \"%s, %s, %s, \"\n        \"%s, %s, %s, \"\n        \"%s, %s, %s, \"\n        \"%s, %s, %s, %s, \"\n        \"%s, %s, \"\n        \"%s, %s, \"\n        \"%s, %s);\")\n    update_prep_cursor = db_connection.cursor(prepared=True)\n    user_info_update_statement = (\n        \"update user_info \"\n        \"set display_name = %s, email = %s, \"\n        \"orcid = %s, globus_login = %s, \"\n        \"google_login = %s, kb_internal_user = %s, \"\n        \"institution = %s, country = %s, \"\n        \"signup_date = %s, last_signin_date = %s, \"\n        \"department = %s, job_title = %s, \"\n        \"job_title_other = %s, \"\n        \"city = %s, state = %s, \"\n        \"postal_code = %s, funding_source = %s, \"\n        \"research_statement = %s, \"\n        \"research_interests = %s, \"\n        \"avatar_option = %s, \"\n        \"gravatar_default = %s, \"\n        \"how_u_hear_selected = %s, \"\n        \"how_u_hear_other = %s \"\n        \"where username = %s;\"\n    )\n    new_user_info_count = 0\n    users_info_updated_count = 0\n    for username in user_stats_dict:\n        if username not in existing_user_info:\n            counter_user_id += 1\n            input = (\n                username,\n                user_stats_dict[username][\"name\"],\n                user_stats_dict[username][\"email\"],\n                user_stats_dict[username][\"orcid\"],\n                user_stats_dict[username][\"globus_login\"],\n                user_stats_dict[username][\"google_login\"],\n                counter_user_id,\n                user_stats_dict[username][\"kbase_internal_user\"],\n                user_stats_dict[username][\"institution\"],\n                user_stats_dict[username][\"country\"],\n                user_stats_dict[username][\"signup_date\"],\n                user_stats_dict[username][\"last_signin_date\"],\n                user_stats_dict[username][\"department\"],\n                user_stats_dict[username][\"job_title\"],\n                user_stats_dict[username][\"job_title_other\"],\n                user_stats_dict[username][\"city\"],\n                user_stats_dict[username][\"state\"],\n                user_stats_dict[username][\"postal_code\"],\n                user_stats_dict[username][\"funding_source\"],\n                user_stats_dict[username][\"research_statement\"],\n                user_stats_dict[username][\"research_interests\"],\n                user_stats_dict[username][\"avatar_option\"],\n                user_stats_dict[username][\"gravatar_default\"],\n                user_stats_dict[username][\"how_u_hear_selected\"],\n                user_stats_dict[username][\"how_u_hear_other\"],\n            )\n            prep_cursor.execute(user_info_insert_statement, input)\n            new_user_info_count += 1\n        else:\n            if not (\n                (\n                    user_stats_dict[username][\"last_signin_date\"] is None\n                    or user_stats_dict[username][\"last_signin_date\"].strftime(\n                        \"%Y-%m-%d %H:%M:%S\"\n                    )\n                    == str(existing_user_info[username][\"last_signin_date\"])\n                )\n                and (\n                    user_stats_dict[username][\"signup_date\"].strftime(\n                        \"%Y-%m-%d %H:%M:%S\"\n                    )\n                    == str(existing_user_info[username][\"signup_date\"])\n                )\n                and user_stats_dict[username][\"country\"]\n                    == existing_user_info[username][\"country\"]\n                and user_stats_dict[username][\"institution\"]\n                    == existing_user_info[username][\"institution\"]\n                and user_stats_dict[username][\"kbase_internal_user\"]\n                    == existing_user_info[username][\"kb_internal_user\"]\n                and user_stats_dict[username][\"orcid\"]\n                    == existing_user_info[username][\"orcid\"]\n                and user_stats_dict[username][\"globus_login\"]\n                    == existing_user_info[username][\"globus_login\"]\n                and user_stats_dict[username][\"google_login\"]\n                    == existing_user_info[username][\"google_login\"]\n                and user_stats_dict[username][\"email\"]\n                    == existing_user_info[username][\"email\"]\n                and user_stats_dict[username][\"name\"]\n                    == existing_user_info[username][\"name\"]\n                and user_stats_dict[username][\"department\"]\n                    == existing_user_info[username][\"department\"]\n                and user_stats_dict[username][\"job_title\"]\n                    == existing_user_info[username][\"job_title\"]\n                and user_stats_dict[username][\"job_title_other\"]\n                    == existing_user_info[username][\"job_title_other\"]\n                and user_stats_dict[username][\"city\"]\n                    == existing_user_info[username][\"city\"]\n                and user_stats_dict[username][\"state\"]\n                    == existing_user_info[username][\"state\"]\n                and user_stats_dict[username][\"postal_code\"]\n                    == existing_user_info[username][\"postal_code\"]\n                and user_stats_dict[username][\"funding_source\"]\n                    == existing_user_info[username][\"funding_source\"]\n                and user_stats_dict[username][\"research_statement\"]\n                    == existing_user_info[username][\"research_statement\"]\n                and user_stats_dict[username][\"research_interests\"]\n                    == existing_user_info[username][\"research_interests\"]\n                and user_stats_dict[username][\"avatar_option\"]\n                    == existing_user_info[username][\"avatar_option\"]\n                and user_stats_dict[username][\"gravatar_default\"]\n                    == existing_user_info[username][\"gravatar_default\"]\n                and user_stats_dict[username][\"how_u_hear_selected\"]\n                    == existing_user_info[username][\"how_u_hear_selected\"]\n                and user_stats_dict[username][\"how_u_hear_other\"]\n                    == existing_user_info[username][\"how_u_hear_other\"]\n            ):\n                input = (\n                    user_stats_dict[username][\"name\"],\n                    user_stats_dict[username][\"email\"],\n                    user_stats_dict[username][\"orcid\"],\n                    user_stats_dict[username][\"globus_login\"],\n                    user_stats_dict[username][\"google_login\"],\n                    user_stats_dict[username][\"kbase_internal_user\"],\n                    user_stats_dict[username][\"institution\"],\n                    user_stats_dict[username][\"country\"],\n                    user_stats_dict[username][\"signup_date\"],\n                    user_stats_dict[username][\"last_signin_date\"],\n                    user_stats_dict[username][\"department\"],\n                    user_stats_dict[username][\"job_title\"],\n                    user_stats_dict[username][\"job_title_other\"],\n                    user_stats_dict[username][\"city\"],\n                    user_stats_dict[username][\"state\"],\n                    user_stats_dict[username][\"postal_code\"],\n                    user_stats_dict[username][\"funding_source\"],\n                    user_stats_dict[username][\"research_statement\"],\n                    user_stats_dict[username][\"research_interests\"],\n                    user_stats_dict[username][\"avatar_option\"],\n                    user_stats_dict[username][\"gravatar_default\"],\n                    user_stats_dict[username][\"how_u_hear_selected\"],\n                    user_stats_dict[username][\"how_u_hear_other\"],\n                    username,\n                )\n                update_prep_cursor.execute(user_info_update_statement, input)\n                users_info_updated_count += 1\n    db_connection.commit()\n    print(\"Number of new users info inserted:\" + str(new_user_info_count))\n    print(\"Number of users updated:\" + str(users_info_updated_count))\n    dev_tokens_users = get_dev_token_users_from_mongo()\n    dev_tokens_string = \"', '\".join(dev_tokens_users)\n    update_new_dev_tokens_statement = (\n        \"update user_info set dev_token_first_seen = now() \"\n        \"where dev_token_first_seen is null and \"\n        \"username in ('\" + dev_tokens_string + \"')\"\n        )\n    cursor.execute(update_new_dev_tokens_statement)\n    db_connection.commit()\n    user_summary_stats_insert_statement = (\n        \"insert into user_system_summary_stats \"\n        \"(username,num_orgs, narrative_count, \"\n        \"shared_count, narratives_shared) \"\n        \"values(%s,%s,%s,%s,%s);\"\n    )\n    existing_user_summary_stats = dict()\n    query = (\n        \"select username, num_orgs, narrative_count, shared_count, narratives_shared \"\n        \"from user_system_summary_stats_current\"\n    )\n    cursor.execute(query)\n    for (\n        username,\n        num_orgs,\n        narrative_count,\n        shared_count,\n        narratives_shared,\n    ) in cursor:\n        existing_user_summary_stats[username] = {\n            \"num_orgs\": num_orgs,\n            \"narrative_count\": narrative_count,\n            \"shared_count\": shared_count,\n            \"narratives_shared\": narratives_shared,\n        }\n    print(\"Number of existing user summaries:\" + str(len(existing_user_summary_stats)))\n    new_user_summary_count = 0\n    existing_user_summary_count = 0\n    for username in user_stats_dict:\n        if username not in existing_user_summary_stats:\n            input = (\n                username,\n                user_stats_dict[username][\"num_orgs\"],\n                user_stats_dict[username][\"narrative_count\"],\n                user_stats_dict[username][\"shared_count\"],\n                user_stats_dict[username][\"narratives_shared\"],\n            )\n            prep_cursor.execute(user_summary_stats_insert_statement, input)\n            new_user_summary_count += 1\n        else:\n            if not (\n                user_stats_dict[username][\"num_orgs\"]\n                == existing_user_summary_stats[username][\"num_orgs\"]\n                and user_stats_dict[username][\"narrative_count\"]\n                == existing_user_summary_stats[username][\"narrative_count\"]\n                and user_stats_dict[username][\"shared_count\"]\n                == existing_user_summary_stats[username][\"shared_count\"]\n                and user_stats_dict[username][\"narratives_shared\"]\n                == existing_user_summary_stats[username][\"narratives_shared\"]\n            ):\n                input = (\n                    username,\n                    user_stats_dict[username][\"num_orgs\"],\n                    user_stats_dict[username][\"narrative_count\"],\n                    user_stats_dict[username][\"shared_count\"],\n                    user_stats_dict[username][\"narratives_shared\"],\n                )\n                prep_cursor.execute(user_summary_stats_insert_statement, input)\n                existing_user_summary_count += 1\n    db_connection.commit()\n    query = \"UPDATE metrics.user_info set exclude = False where last_signin_date is not NULL\"\n    cursor.execute(query)\n    db_connection.commit()\n    print(\"Number of new users summary inserted:\" + str(new_user_summary_count))\n    print(\n        \"Number of existing users summary inserted:\" + str(existing_user_summary_count)\n    )\n    return 1",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-4860",
        "description": "[{'lang': 'en', 'value': 'A vulnerability was found in KBase Metrics. It has been classified as critical. This affects the function upload_user_data of the file source/daily_cron_jobs/methods_upload_user_stats.py. The manipulation leads to sql injection. The patch is named 959dfb6b05991e30b0fa972a1ecdcaae8e1dae6d. It is recommended to apply a patch to fix this issue. The associated identifier of this vulnerability is VDB-217059.'}]",
        "cwe_number": 89
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-194",
      "code": "    def _make_cmd(self, tmpfilename, info_dict):\n        cmd = [self.exe, '-O', tmpfilename, '-nv', '--no-cookies', '--compression=auto']\n        if info_dict.get('http_headers') is not None:\n            for key, val in info_dict['http_headers'].items():\n                cmd += ['--header', f'{key}: {val}']\n        cmd += self._option('--limit-rate', 'ratelimit')\n        retry = self._option('--tries', 'retries')\n        if len(retry) == 2:\n            if retry[1] in ('inf', 'infinite'):\n                retry[1] = '0'\n            cmd += retry\n        cmd += self._option('--bind-address', 'source_address')\n        proxy = self.params.get('proxy')\n        if proxy:\n            for var in ('http_proxy', 'https_proxy'):\n                cmd += ['--execute', f'{var}={proxy}']\n        cmd += self._valueless_option('--no-check-certificate', 'nocheckcertificate')\n        cmd += self._configuration_args()\n        cmd += ['--', info_dict['url']]\n        return cmd",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-35934",
        "description": "[{'lang': 'en', 'value': \"yt-dlp is a command-line program to download videos from video sites. During file downloads, yt-dlp or the external downloaders that yt-dlp employs may leak cookies on HTTP redirects to a different host, or leak them when the host for download fragments differs from their parent manifest's host. This vulnerable behavior is present in yt-dlp prior to 2023.07.06 and nightly 2023.07.06.185519. All native and external downloaders are affected, except for `curl` and `httpie` (version 3.1.0 or later).\\n\\nAt the file download stage, all cookies are passed by yt-dlp to the file downloader as a `Cookie` header, thereby losing their scope. This also occurs in yt-dlp's info JSON output, which may be used by external tools. As a result, the downloader or external tool may indiscriminately send cookies with requests to domains or paths for which the cookies are not scoped.\\n\\nyt-dlp version 2023.07.06 and nightly 2023.07.06.185519 fix this issue by removing the `Cookie` header upon HTTP redirects; having native downloaders calculate the `Cookie` header from the cookiejar, utilizing external downloaders' built-in support for cookies instead of passing them as header arguments, disabling HTTP redirectiong if the external downloader does not have proper cookie support, processing cookies passed as HTTP headers to limit their scope, and having a separate field for cookies in the info dict storing more information about scoping\\n\\nSome workarounds are available for those who are unable to upgrade. Avoid using cookies and user authentication methods. While extractors may set custom cookies, these usually do not contain sensitive information. Alternatively, avoid using `--load-info-json`. Or, if authentication is a must: verify the integrity of download links from unknown sources in browser (including redirects) before passing them to yt-dlp; use `curl` as external downloader, since it is not impacted; and/or avoid fragmented formats such as HLS/m3u8, DASH/mpd and ISM.\"}]",
        "cwe_number": 200
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-195",
      "code": "    def flatten(self):\n        \"\"\"Generator yielding ungrouped tokens.\n        This method is recursively called for all child tokens.\n        \"\"\"\n        for token in self.tokens:\n            if token.is_group:\n                yield from token.flatten()\n            else:\n                yield token",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-4340",
        "description": "[{'lang': 'en', 'value': 'Passing a heavily nested list to sqlparse.parse() leads to a Denial of Service due to RecursionError.\\n\\n'}]",
        "cwe_number": 674
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-196",
      "code": "def main() -> None:\n    try:\n        run(sys.argv)\n    except KeyboardInterrupt:\n        pass\n    except (OSError, TypeError, ValueError) as e:\n        print(f\"Error: {e}\", file=sys.stderr)\n        sys.exit(1)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-47163",
        "description": "[{'lang': 'en', 'value': 'Remarshal prior to v0.17.1 expands YAML alias nodes unlimitedly, hence Remarshal is vulnerable to Billion Laughs Attack. Processing untrusted YAML files may cause a denial-of-service (DoS) condition.'}]",
        "cwe_number": 674
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-197",
      "code": "def commentcounts(context, filediff, interfilediff=None):\n    \"\"\"\n    Returns a JSON array of current comments for a filediff, sorted by\n    line number.\n    Each entry in the array has a dictionary containing the following keys:\n      =========== ==================================================\n      Key         Description\n      =========== ==================================================\n      comment_id  The ID of the comment\n      text        The text of the comment\n      line        The first line number\n      num_lines   The number of lines this comment spans\n      user        A dictionary containing \"username\" and \"name\" keys\n                  for the user\n      url         The URL to the comment\n      localdraft  True if this is the current user's draft comment\n      =========== ==================================================\n    \"\"\"\n    comment_dict = {}\n    user = context.get('user', None)\n    if interfilediff:\n        query = Comment.objects.filter(filediff=filediff,\n                                       interfilediff=interfilediff)\n    else:\n        query = Comment.objects.filter(filediff=filediff,\n                                       interfilediff__isnull=True)\n    for comment in query:\n        review = get_object_or_none(comment.review)\n        if review and (review.public or review.user == user):\n            key = (comment.first_line, comment.num_lines)\n            comment_dict.setdefault(key, []).append({\n                'comment_id': comment.id,\n                'text': comment.text,\n                'line': comment.first_line,\n                'num_lines': comment.num_lines,\n                'user': {\n                    'username': review.user.username,\n                    'name': review.user.get_full_name() or review.user.username,\n                },\n                'url': comment.get_review_url(),\n                'localdraft': review.user == user and \\\n                              not review.public,\n            })\n    comments_array = []\n    for key, value in comment_dict.iteritems():\n        comments_array.append({\n            'linenum': key[0],\n            'num_lines': key[1],\n            'comments': value,\n        })\n    comments_array.sort(cmp=lambda x, y: cmp(x['linenum'], y['linenum'] or\n                                         cmp(x['num_lines'], y['num_lines'])))\n    return simplejson.dumps(comments_array)\ndef screenshotcommentcounts(context, screenshot):\n    \"\"\"\n    Returns a JSON array of current comments for a screenshot.\n    Each entry in the array has a dictionary containing the following keys:\n      =========== ==================================================\n      Key         Description\n      =========== ==================================================\n      text        The text of the comment\n      localdraft  True if this is the current user's draft comment\n      x           The X location of the comment's region\n      y           The Y location of the comment's region\n      w           The width of the comment's region\n      h           The height of the comment's region\n      =========== ==================================================\n    \"\"\"\n    comments = {}\n    user = context.get('user', None)\n    for comment in screenshot.comments.all():\n        review = get_object_or_none(comment.review)\n        if review and (review.public or review.user == user):\n            position = '%dx%d+%d+%d' % (comment.w, comment.h, \\\n                                        comment.x, comment.y)\n            comments.setdefault(position, []).append({\n                'id': comment.id,\n                'text': comment.text,\n                'user': {\n                    'username': review.user.username,\n                    'name': review.user.get_full_name() or review.user.username,\n                },\n                'url': comment.get_review_url(),\n                'localdraft' : review.user == user and \\\n                               not review.public,\n                'x' : comment.x,\n                'y' : comment.y,\n                'w' : comment.w,\n                'h' : comment.h,\n            })\n    return simplejson.dumps(comments)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2011-4312",
        "description": "[{'lang': 'en', 'value': 'Multiple cross-site scripting (XSS) vulnerabilities in the commenting system in Review Board before 1.5.7 and 1.6.x before 1.6.3 allow remote attackers to inject arbitrary web script or HTML via vectors involving the (1) diff viewer or (2) screenshot component.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-198",
      "code": "def save_cover_from_url(url, book_path):\n    try:\n        if not cli.allow_localhost:\n            ip = socket.getaddrinfo(urlparse(url).hostname, 0)[0][4][0]\n            if ip.startswith(\"127.\") or ip.startswith('::ffff:7f') or ip == \"::1\":\n                log.error(\"Localhost was accessed for cover upload\")\n                return False, _(\"You are not allowed to access localhost for cover uploads\")\n        img = requests.get(url, timeout=(10, 200))\n        img.raise_for_status()\n        return save_cover(img, book_path)\n    except (socket.gaierror,\n            requests.exceptions.HTTPError,\n            requests.exceptions.ConnectionError,\n            requests.exceptions.Timeout) as ex:\n        log.info(u'Cover Download Error %s', ex)\n        return False, _(\"Error Downloading Cover\")\n    except MissingDelegateError as ex:\n        log.info(u'File Format Error %s', ex)\n        return False, _(\"Cover Format Error\")",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-0767",
        "description": "[{'lang': 'en', 'value': 'Server-Side Request Forgery (SSRF) in GitHub repository janeczku/calibre-web prior to 0.6.17.'}]",
        "cwe_number": 918
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-199",
      "code": "    def __init__(\n        self,\n        method: str,\n        url: URL,\n        *,\n        params: Optional[Mapping[str, str]] = None,\n        headers: Optional[LooseHeaders] = None,\n        skip_auto_headers: Iterable[str] = frozenset(),\n        data: Any = None,\n        cookies: Optional[LooseCookies] = None,\n        auth: Optional[BasicAuth] = None,\n        version: http.HttpVersion = http.HttpVersion11,\n        compress: Optional[str] = None,\n        chunked: Optional[bool] = None,\n        expect100: bool = False,\n        loop: Optional[asyncio.AbstractEventLoop] = None,\n        response_class: Optional[Type[\"ClientResponse\"]] = None,\n        proxy: Optional[URL] = None,\n        proxy_auth: Optional[BasicAuth] = None,\n        timer: Optional[BaseTimerContext] = None,\n        session: Optional[\"ClientSession\"] = None,\n        ssl: Union[SSLContext, Literal[False], Fingerprint, None] = None,\n        proxy_headers: Optional[LooseHeaders] = None,\n        traces: Optional[List[\"Trace\"]] = None,\n        trust_env: bool = False,\n        server_hostname: Optional[str] = None,\n    ):\n        if loop is None:\n            loop = asyncio.get_event_loop()\n        assert isinstance(url, URL), url\n        assert isinstance(proxy, (URL, type(None))), proxy\n        self._session = cast(\"ClientSession\", session)\n        if params:\n            q = MultiDict(url.query)\n            url2 = url.with_query(params)\n            q.extend(url2.query)\n            url = url.with_query(q)\n        self.original_url = url\n        self.url = url.with_fragment(None)\n        self.method = method.upper()\n        self.chunked = chunked\n        self.compress = compress\n        self.loop = loop\n        self.length = None\n        if response_class is None:\n            real_response_class = ClientResponse\n        else:\n            real_response_class = response_class\n        self.response_class: Type[ClientResponse] = real_response_class\n        self._timer = timer if timer is not None else TimerNoop()\n        self._ssl = ssl\n        self.server_hostname = server_hostname\n        if loop.get_debug():\n            self._source_traceback = traceback.extract_stack(sys._getframe(1))\n        self.update_version(version)\n        self.update_host(url)\n        self.update_headers(headers)\n        self.update_auto_headers(skip_auto_headers)\n        self.update_cookies(cookies)\n        self.update_content_encoding(data)\n        self.update_auth(auth, trust_env)\n        self.update_proxy(proxy, proxy_auth, proxy_headers)\n        self.update_body_from_data(data)\n        if data is not None or self.method not in self.GET_METHODS:\n            self.update_transfer_encoding()\n        self.update_expect_continue(expect100)\n        if traces is None:\n            traces = []\n        self._traces = traces",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-49082",
        "description": "[{'lang': 'en', 'value': 'aiohttp is an asynchronous HTTP client/server framework for asyncio and Python. Improper validation makes it possible for an attacker to modify the HTTP request (e.g. insert a new header) or even create a new HTTP request if the attacker controls the HTTP method. The vulnerability occurs only if the attacker can control the HTTP method (GET, POST etc.) of the request. If the attacker can control the HTTP version of the request it will be able to modify the request (request smuggling). This issue has been patched in version 3.9.0.'}]",
        "cwe_number": 20
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-200",
      "code": "  def _RunAndVerifyGradients(self, dtype):\n    with self.cached_session():\n      shape = np.random.randint(1, 5, size=4)\n      shape[3] += 1\n      lrn_depth_radius = np.random.randint(1, min(8, shape[3]))\n      bias = 1.0 + np.random.rand()\n      alpha = 1.0 * np.random.rand()\n      beta = 0.01 + 1.0 * np.random.rand()\n      if dtype == dtypes.float32:\n        inp_array = np.random.rand(*shape).astype(np.float32)\n      else:\n        inp_array = np.random.rand(*shape).astype(np.float16)\n      inp = constant_op.constant(\n          list(inp_array.ravel(order=\"C\")), shape=shape, dtype=dtype)\n      lrn_op = nn.local_response_normalization(\n          inp,\n          name=\"lrn\",\n          depth_radius=lrn_depth_radius,\n          bias=bias,\n          alpha=alpha,\n          beta=beta)\n      err = gradient_checker.compute_gradient_error(inp, shape, lrn_op, shape)\n    print(\"LRN Gradient error for bias \", bias, \"alpha \", alpha, \" beta \", beta,\n          \" is \", err)\n    if dtype == dtypes.float32:\n      self.assertLess(err, 1e-4)\n    else:\n      self.assertLess(err, 1.0)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-35985",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. If `LRNGrad` is given an `output_image` input tensor that is not 4-D, it results in a `CHECK` fail that can be used to trigger a denial of service attack. We have patched the issue in GitHub commit bd90b3efab4ec958b228cd7cfe9125be1c0cf255. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue.'}]",
        "cwe_number": 617
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-201",
      "code": "    def authenticate(self, username, password):\n        child = None\n        try:\n            child = pexpect.spawn('/bin/sh', ['-c', '/bin/su -c \"/bin/echo SUCCESS\" - %s' % username], timeout=5)\n            child.expect('.*:')\n            child.sendline(password)\n            result = child.expect(['su: .*', 'SUCCESS'])\n        except Exception as err:\n            if child and child.isalive():\n                child.close()\n            logging.error('Error checking password: %s', err)\n            return False\n        if result == 0:\n            return False\n        else:\n            return True\n    def authorize(self, username, permission):\n        return True\n    def get_isolation_uid(self, username):\n        return pwd.getpwnam(username).pw_uid",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2019-25066",
        "description": "[{'lang': 'en', 'value': 'A vulnerability has been found in ajenti 2.1.31 and classified as critical. This vulnerability affects unknown code of the component API. The manipulation leads to privilege escalation. The attack can be initiated remotely. The exploit has been disclosed to the public and may be used. Upgrading to version 2.1.32 is able to address this issue. The name of the patch is 7aa146b724e0e20cfee2c71ca78fafbf53a8767c. It is recommended to upgrade the affected component.'}]",
        "cwe_number": 78
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-202",
      "code": "  def testInt64(self):\n    @def_function.function\n    def g():\n      x = random_ops.random_normal(shape=[int(1e10)])\n      y = array_ops.ones(shape=[int(1e10)])\n      return array_ops.searchsorted(x, y, out_type=dtypes.int64)\n    _ = g.get_concrete_function()",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-35965",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. If `LowerBound` or `UpperBound` is given an empty`sorted_inputs` input, it results in a `nullptr` dereference, leading to a segfault that can be used to trigger a denial of service attack. We have patched the issue in GitHub commit bce3717eaef4f769019fd18e990464ca4a2efeea. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue.'}]",
        "cwe_number": 476
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-203",
      "code": "    def value(self):\n        \"\"\"Returns a formatted version of the data for final output.\n        This takes into consideration the\n        :attr:`~horizon.tables.Column.link`` and\n        :attr:`~horizon.tables.Column.empty_value`\n        attributes.\n        \"\"\"\n        try:\n            data = self.column.get_data(self.datum)\n            if data is None:\n                if callable(self.column.empty_value):\n                    data = self.column.empty_value(self.datum)\n                else:\n                    data = self.column.empty_value\n        except Exception:\n            data = None\n            exc_info = sys.exc_info()\n            raise template.TemplateSyntaxError, exc_info[1], exc_info[2]\n        if self.url:\n            link_classes = ' '.join(self.column.link_classes)\n            data = mark_safe('<a href=\"%s\" class=\"%s\">%s</a>' %\n                             (self.url, link_classes, escape(unicode(data))))\n        return data\n    def get_context_data(self, request):\n        stack = self.tab_group.kwargs['stack']\n        try:\n            stack_identifier = '%s/%s' % (stack.stack_name, stack.id)\n            events = api.heat.events_list(self.request, stack_identifier)\n            LOG.debug('got events %s' % events)\n        except Exception:\n            events = []\n            messages.error(request, _(\n                'Unable to get events for stack \"%s\".') % stack.stack_name)\n        return {\"stack\": stack,\n                \"table\": project_tables.EventsTable(request, data=events), }\nclass StackResourcesTab(tabs.Tab):\nclass EventsTable(tables.DataTable):\n    logical_resource = tables.Column('resource_name',",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2014-125078",
        "description": "[{'lang': 'en', 'value': 'A vulnerability was found in yanheven console and classified as problematic. Affected by this issue is some unknown functionality of the file horizon/static/horizon/js/horizon.instances.js. The manipulation leads to cross site scripting. The attack may be launched remotely. The patch is identified as 32a7b713468161282f2ea01d5e2faff980d924cd. It is recommended to apply a patch to fix this issue. VDB-218354 is the identifier assigned to this vulnerability.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-204",
      "code": "    def _get_default_expire_time(self):\n        \"\"\"Determine when a token should expire based on the config.\n        :returns: a naive utc datetime.datetime object\n        \"\"\"\n        expire_delta = datetime.timedelta(seconds=CONF.token.expiration)\n        return datetime.datetime.utcnow() + expire_delta\n    def update_user_tenant(self, context, user_id, user):\n        \"\"\"Update the default tenant.\"\"\"\n        tenant_id = user.get('tenantId')\n        self.identity_api.add_user_to_tenant(context, tenant_id, user_id)\n        return self.update_user(context, user_id, user)\nclass RoleController(wsgi.Application):\n    def __init__(self):\n        self.identity_api = Manager()\n        self.token_api = token.Manager()\n        self.policy_api = policy.Manager()\n        super(RoleController, self).__init__()",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2012-3426",
        "description": "[{'lang': 'en', 'value': 'OpenStack Keystone before 2012.1.1, as used in OpenStack Folsom before Folsom-1 and OpenStack Essex, does not properly implement token expiration, which allows remote authenticated users to bypass intended authorization restrictions by (1) creating new tokens through token chaining, (2) leveraging possession of a token for a disabled user account, or (3) leveraging possession of a token for an account with a changed password.'}]",
        "cwe_number": 264
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-205",
      "code": "    class Meta:\n        fields = ['engine', 'min_level', 'file']\n    engine = forms.CharField(widget=forms.Select(",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-43829",
        "description": "[{'lang': 'en', 'value': 'PatrOwl is a free and open-source solution for orchestrating Security Operations. In versions prior to 1.7.7 PatrowlManager unrestrictly handle upload files in the findings import feature. This vulnerability is capable of uploading dangerous type of file to server leading to XSS attacks and potentially other forms of code injection. Users are advised to update to 1.7.7 as soon as possible. There are no known workarounds for this issue.'}]",
        "cwe_number": 434
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-206",
      "code": "    def real_download(self, filename, info_dict):\n        self.report_destination(filename)\n        tmpfilename = self.temp_name(filename)\n        try:\n            started = time.time()\n            retval = self._call_downloader(tmpfilename, info_dict)\n        except KeyboardInterrupt:\n            if not info_dict.get('is_live'):\n                raise\n            retval = 0\n            self.to_screen('[%s] Interrupted by user' % self.get_basename())\n        if retval == 0:\n            status = {\n                'filename': filename,\n                'status': 'finished',\n                'elapsed': time.time() - started,\n            }\n            if filename != '-':\n                fsize = os.path.getsize(encodeFilename(tmpfilename))\n                self.try_rename(tmpfilename, filename)\n                status.update({\n                    'downloaded_bytes': fsize,\n                    'total_bytes': fsize,\n                })\n            self._hook_progress(status, info_dict)\n            return True\n        else:\n            self.to_stderr('\\n')\n            self.report_error('%s exited with code %d' % (\n                self.get_basename(), retval))\n            return False\n    def get_basename(cls):\n        return cls.__name__[:-2].lower()\n    def _call_downloader(self, tmpfilename, info_dict):\n        \"\"\" Either overwrite this or implement _make_cmd \"\"\"\n        cmd = [encodeArgument(a) for a in self._make_cmd(tmpfilename, info_dict)]\n        self._debug_cmd(cmd)\n        if 'fragments' not in info_dict:\n            _, stderr, returncode = self._call_process(cmd, info_dict)\n            if returncode and stderr:\n                self.to_stderr(stderr)\n            return returncode\n        skip_unavailable_fragments = self.params.get('skip_unavailable_fragments', True)\n        retry_manager = RetryManager(self.params.get('fragment_retries'), self.report_retry,\n                                     frag_index=None, fatal=not skip_unavailable_fragments)\n        for retry in retry_manager:\n            _, stderr, returncode = self._call_process(cmd, info_dict)\n            if not returncode:\n                break\n            if stderr:\n                self.to_stderr(stderr)\n            retry.error = Exception()\n            continue\n        if not skip_unavailable_fragments and retry_manager.error:\n            return -1\n        decrypt_fragment = self.decrypter(info_dict)\n        dest, _ = self.sanitize_open(tmpfilename, 'wb')\n        for frag_index, fragment in enumerate(info_dict['fragments']):\n            fragment_filename = '%s-Frag%d' % (tmpfilename, frag_index)\n            try:\n                src, _ = self.sanitize_open(fragment_filename, 'rb')\n            except OSError as err:\n                if skip_unavailable_fragments and frag_index > 1:\n                    self.report_skip_fragment(frag_index, err)\n                    continue\n                self.report_error(f'Unable to open fragment {frag_index}; {err}')\n                return -1\n            dest.write(decrypt_fragment(fragment, src.read()))\n            src.close()\n            if not self.params.get('keep_fragments', False):\n                self.try_remove(encodeFilename(fragment_filename))\n        dest.close()\n        self.try_remove(encodeFilename('%s.frag.urls' % tmpfilename))\n        return 0\n    def _make_cmd(self, tmpfilename, info_dict):\n        cmd = [self.exe, '--location', '-o', tmpfilename, '--compressed']\n        if info_dict.get('http_headers') is not None:\n            for key, val in info_dict['http_headers'].items():\n                cmd += ['--header', f'{key}: {val}']\n        cmd += self._bool_option('--continue-at', 'continuedl', '-', '0')\n        cmd += self._valueless_option('--silent', 'noprogress')\n        cmd += self._valueless_option('--verbose', 'verbose')\n        cmd += self._option('--limit-rate', 'ratelimit')\n        retry = self._option('--retry', 'retries')\n        if len(retry) == 2:\n            if retry[1] in ('inf', 'infinite'):\n                retry[1] = '2147483647'\n            cmd += retry\n        cmd += self._option('--max-filesize', 'max_filesize')\n        cmd += self._option('--interface', 'source_address')\n        cmd += self._option('--proxy', 'proxy')\n        cmd += self._valueless_option('--insecure', 'nocheckcertificate')\n        cmd += self._configuration_args()\n        cmd += ['--', info_dict['url']]\n        return cmd\nclass AxelFD(ExternalFD):\n    def _make_cmd(self, tmpfilename, info_dict):\n        cmd = [self.exe, '-o', tmpfilename]\n        if info_dict.get('http_headers') is not None:\n            for key, val in info_dict['http_headers'].items():\n                cmd += ['-H', f'{key}: {val}']\n        cmd += self._configuration_args()\n        cmd += ['--', info_dict['url']]\n        return cmd\nclass WgetFD(ExternalFD):\n    def _call_downloader(self, tmpfilename, info_dict):\n        ffpp = FFmpegPostProcessor(downloader=self)\n        if not ffpp.available:\n            self.report_error('m3u8 download detected but ffmpeg could not be found. Please install')\n            return False\n        ffpp.check_version()\n        args = [ffpp.executable, '-y']\n        for log_level in ('quiet', 'verbose'):\n            if self.params.get(log_level, False):\n                args += ['-loglevel', log_level]\n                break\n        if not self.params.get('verbose'):\n            args += ['-hide_banner']\n        args += traverse_obj(info_dict, ('downloader_options', 'ffmpeg_args'), default=[])\n        args += info_dict.get('_ffmpeg_args') or []\n        seekable = info_dict.get('_seekable')\n        if seekable is not None:\n            args += ['-seekable', '1' if seekable else '0']\n        env = None\n        proxy = self.params.get('proxy')\n        if proxy:\n            if not re.match(r'^[\\da-zA-Z]+://', proxy):\n                proxy = 'http://%s' % proxy\n            if proxy.startswith('socks'):\n                self.report_warning(\n                    '%s does not support SOCKS proxies. Downloading is likely to fail. '\n                    'Consider adding --hls-prefer-native to your command.' % self.get_basename())\n            env = os.environ.copy()\n            env['HTTP_PROXY'] = proxy\n            env['http_proxy'] = proxy\n        protocol = info_dict.get('protocol')\n        if protocol == 'rtmp':\n            player_url = info_dict.get('player_url')\n            page_url = info_dict.get('page_url')\n            app = info_dict.get('app')\n            play_path = info_dict.get('play_path')\n            tc_url = info_dict.get('tc_url')\n            flash_version = info_dict.get('flash_version')\n            live = info_dict.get('rtmp_live', False)\n            conn = info_dict.get('rtmp_conn')\n            if player_url is not None:\n                args += ['-rtmp_swfverify', player_url]\n            if page_url is not None:\n                args += ['-rtmp_pageurl', page_url]\n            if app is not None:\n                args += ['-rtmp_app', app]\n            if play_path is not None:\n                args += ['-rtmp_playpath', play_path]\n            if tc_url is not None:\n                args += ['-rtmp_tcurl', tc_url]\n            if flash_version is not None:\n                args += ['-rtmp_flashver', flash_version]\n            if live:\n                args += ['-rtmp_live', 'live']\n            if isinstance(conn, list):\n                for entry in conn:\n                    args += ['-rtmp_conn', entry]\n            elif isinstance(conn, str):\n                args += ['-rtmp_conn', conn]\n        start_time, end_time = info_dict.get('section_start') or 0, info_dict.get('section_end')\n        selected_formats = info_dict.get('requested_formats') or [info_dict]\n        for i, fmt in enumerate(selected_formats):\n            if fmt.get('http_headers') and re.match(r'^https?://', fmt['url']):\n                args.extend(['-headers', ''.join(f'{key}: {val}\\r\\n' for key, val in fmt['http_headers'].items())])\n            if start_time:\n                args += ['-ss', str(start_time)]\n            if end_time:\n                args += ['-t', str(end_time - start_time)]\n            args += self._configuration_args((f'_i{i + 1}', '_i')) + ['-i', fmt['url']]\n        if not (start_time or end_time) or not self.params.get('force_keyframes_at_cuts'):\n            args += ['-c', 'copy']\n        if info_dict.get('requested_formats') or protocol == 'http_dash_segments':\n            for i, fmt in enumerate(selected_formats):\n                stream_number = fmt.get('manifest_stream_number', 0)\n                args.extend(['-map', f'{i}:{stream_number}'])\n        if self.params.get('test', False):\n            args += ['-fs', str(self._TEST_FILE_SIZE)]\n        ext = info_dict['ext']\n        if protocol in ('m3u8', 'm3u8_native'):\n            use_mpegts = (tmpfilename == '-') or self.params.get('hls_use_mpegts')\n            if use_mpegts is None:\n                use_mpegts = info_dict.get('is_live')\n            if use_mpegts:\n                args += ['-f', 'mpegts']\n            else:\n                args += ['-f', 'mp4']\n                if (ffpp.basename == 'ffmpeg' and ffpp._features.get('needs_adtstoasc')) and (not info_dict.get('acodec') or info_dict['acodec'].split('.')[0] in ('aac', 'mp4a')):\n                    args += ['-bsf:a', 'aac_adtstoasc']\n        elif protocol == 'rtmp':\n            args += ['-f', 'flv']\n        elif ext == 'mp4' and tmpfilename == '-':\n            args += ['-f', 'mpegts']\n        elif ext == 'unknown_video':\n            ext = determine_ext(remove_end(tmpfilename, '.part'))\n            if ext == 'unknown_video':\n                self.report_warning(\n                    'The video format is unknown and cannot be downloaded by ffmpeg. '\n                    'Explicitly set the extension in the filename to attempt download in that format')\n            else:\n                self.report_warning(f'The video format is unknown. Trying to download as {ext} according to the filename')\n                args += ['-f', EXT_TO_OUT_FORMATS.get(ext, ext)]\n        else:\n            args += ['-f', EXT_TO_OUT_FORMATS.get(ext, ext)]\n        args += self._configuration_args(('_o1', '_o', ''))\n        args = [encodeArgument(opt) for opt in args]\n        args.append(encodeFilename(ffpp._ffmpeg_filename_argument(tmpfilename), True))\n        self._debug_cmd(args)\n        piped = any(fmt['url'] in ('-', 'pipe:') for fmt in selected_formats)\n        with Popen(args, stdin=subprocess.PIPE, env=env) as proc:\n            if piped:\n                self.on_process_started(proc, proc.stdin)\n            try:\n                retval = proc.wait()\n            except BaseException as e:\n                if isinstance(e, KeyboardInterrupt) and sys.platform != 'win32' and not piped:\n                    proc.communicate_or_kill(b'q')\n                else:\n                    proc.kill(timeout=None)\n                raise\n            return retval\nclass AVconvFD(FFmpegFD):\n_BY_NAME = {\n    klass.get_basename(): klass\n    for name, klass in globals().items()\n    if name.endswith('FD') and name not in ('ExternalFD', 'FragmentFD')\n}\ndef list_external_downloaders():\n    return sorted(_BY_NAME.keys())\ndef get_external_downloader(external_downloader):\n    \"\"\" Given the name of the executable, see whether we support the given downloader \"\"\"\n    bn = os.path.splitext(os.path.basename(external_downloader))[0]\n    return _BY_NAME.get(bn) or next((\n        klass for klass in _BY_NAME.values() if klass.EXE_NAME in bn\n    ), None)\n    def clear(self, *args, **kwargs):\n        with contextlib.suppress(KeyError):\n            return super().clear(*args, **kwargs)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-35934",
        "description": "[{'lang': 'en', 'value': \"yt-dlp is a command-line program to download videos from video sites. During file downloads, yt-dlp or the external downloaders that yt-dlp employs may leak cookies on HTTP redirects to a different host, or leak them when the host for download fragments differs from their parent manifest's host. This vulnerable behavior is present in yt-dlp prior to 2023.07.06 and nightly 2023.07.06.185519. All native and external downloaders are affected, except for `curl` and `httpie` (version 3.1.0 or later).\\n\\nAt the file download stage, all cookies are passed by yt-dlp to the file downloader as a `Cookie` header, thereby losing their scope. This also occurs in yt-dlp's info JSON output, which may be used by external tools. As a result, the downloader or external tool may indiscriminately send cookies with requests to domains or paths for which the cookies are not scoped.\\n\\nyt-dlp version 2023.07.06 and nightly 2023.07.06.185519 fix this issue by removing the `Cookie` header upon HTTP redirects; having native downloaders calculate the `Cookie` header from the cookiejar, utilizing external downloaders' built-in support for cookies instead of passing them as header arguments, disabling HTTP redirectiong if the external downloader does not have proper cookie support, processing cookies passed as HTTP headers to limit their scope, and having a separate field for cookies in the info dict storing more information about scoping\\n\\nSome workarounds are available for those who are unable to upgrade. Avoid using cookies and user authentication methods. While extractors may set custom cookies, these usually do not contain sensitive information. Alternatively, avoid using `--load-info-json`. Or, if authentication is a must: verify the integrity of download links from unknown sources in browser (including redirects) before passing them to yt-dlp; use `curl` as external downloader, since it is not impacted; and/or avoid fragmented formats such as HLS/m3u8, DASH/mpd and ISM.\"}]",
        "cwe_number": 200
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-207",
      "code": "def async_run(prog, args):\n    pid = os.fork()\n    if pid:\n        os.waitpid(pid, 0)\n    else:\n        dpid = os.fork()\n        if not dpid:\n            os.system(\" \".join([prog] + args))\n        os._exit(0)\n    def process_statistics(self, metadata, _):\n        args = [metadata.hostname, '-p', metadata.profile, '-g',\n                ':'.join([g for g in metadata.groups])]\n        for notifier in os.listdir(self.data):\n            if ((notifier[-1] == '~') or\n                (notifier[:2] == '.\n                (notifier[-4:] == '.swp') or\n                (notifier in ['SCCS', '.svn', '4913'])):\n                continue\n            npath = self.data + '/' + notifier\n            self.logger.debug(\"Running %s %s\" % (npath, \" \".join(args)))\n            async_run(npath, args)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2012-3366",
        "description": "[{'lang': 'en', 'value': 'The Trigger plugin in bcfg2 1.2.x before 1.2.3 allows remote attackers with root access to the client to execute arbitrary commands via shell metacharacters in the UUID field to the server process (bcfg2-server).'}]",
        "cwe_number": 78
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-208",
      "code": "    def sql_execute(self, sentence):\n        self.cursor.execute(sentence)\n        return self.cursor.fetchall()\n    def sql_one_row(self, sentence, column):\n        self.cursor.execute(sentence)\n        return self.cursor.fetchone()[column]\n    def sql_insert(self, sentence):\n        self.cursor.execute(sentence)\n        self.conn.commit()\n        return True\n    def prop_sentences_stats(self, type, vId = None):\n        return {\n            'get_data' : \"SELECT victims.*, geo.*, victims.ip AS ip_local, COUNT(clicks.id) FROM victims INNER JOIN geo ON victims.id = geo.id LEFT JOIN clicks ON clicks.id = victims.id GROUP BY victims.id ORDER BY victims.time DESC\",\n            'all_networks' : \"SELECT networks.* FROM networks ORDER BY id\",\n            'get_preview' : \"SELECT victims.*, geo.*, victims.ip AS ip_local FROM victims INNER JOIN geo ON victims.id = geo.id WHERE victims.id = '%s'\" % (vId),\n            'id_networks' : \"SELECT networks.* FROM networks WHERE id = '%s'\" % (vId),\n            'get_requests' : \"SELECT requests.*, geo.ip FROM requests INNER JOIN geo on geo.id = requests.user_id ORDER BY requests.date DESC, requests.id \",\n            'get_sessions' : \"SELECT COUNT(*) AS Total FROM networks\",\n            'get_clicks' : \"SELECT COUNT(*) AS Total FROM clicks\",\n            'get_online' : \"SELECT COUNT(*) AS Total FROM victims WHERE status = '%s'\" % ('online')\n        }.get(type, False)\n    def sentences_stats(self, type, vId = None):\n        return self.sql_execute(self.prop_sentences_stats(type, vId))\n    def prop_sentences_victim(self, type, data = None):\n        if type == 'count_victim':\n            return \"SELECT COUNT(*) AS C FROM victims WHERE id = '%s'\" % (data)\n        elif type == 'count_times':\n            return \"SELECT COUNT(*) AS C FROM clicks WHERE id = '%s'\" % (data)\n        elif type == 'update_victim':\n            return \"UPDATE victims SET ip = '%s', date = '%s', bVersion = '%s', browser = '%s', device = '%s', ports = '%s', time = '%s', cpu = '%s', status = '%s' WHERE id = '%s'\" % (data[0].ip, data[0].date, data[0].version, data[0].browser, data[0].device, data[0].ports, data[2], data[0].cpu, 'online', data[1])\n        elif type == 'update_victim_geo':\n            return \"UPDATE geo SET city = '%s', country_code = '%s', country_name = '%s', ip = '%s', latitude = '%s', longitude = '%s', metro_code = '%s', region_code = '%s', region_name = '%s', time_zone = '%s', zip_code = '%s', isp = '%s', ua='%s' WHERE id = '%s'\" % (data[0].city, data[0].country_code, data[0].country_name, data[0].ip, data[0].latitude, data[0].longitude, data[0].metro_code, data[0].region_code, data[0].region_name, data[0].time_zone, data[0].zip_code, data[0].isp, data[0].ua, data[1])\n        elif type == 'insert_victim':\n            return \"INSERT INTO victims(id, ip, date, bVersion, browser, device, ports, time, cpu, status) VALUES('%s','%s', '%s','%s', '%s','%s', '%s', '%s', '%s', '%s')\" % (data[1], data[0].ip, data[0].date, data[0].version, data[0].browser, data[0].device, data[0].ports, data[2], data[0].cpu, 'online')\n        elif type == 'insert_victim_geo':\n            return \"INSERT INTO geo(id, city, country_code, country_name, ip, latitude, longitude, metro_code, region_code, region_name, time_zone, zip_code, isp, ua) VALUES('%s', '%s', '%s', '%s', '%s', '%s', '%s', '%s', '%s', '%s', '%s', '%s', '%s', '%s')\"  % (data[1], data[0].city, data[0].country_code, data[0].country_name, data[0].ip, data[0].latitude, data[0].longitude, data[0].metro_code, data[0].region_code, data[0].region_name, data[0].time_zone, data[0].zip_code, data[0].isp, data[0].ua)\n        elif type == 'count_victim_network':\n            return \"SELECT COUNT(*) AS C FROM networks WHERE id = '%s' AND network = '%s'\" % (data[0], data[1])\n        elif type == 'delete_networks':\n            return \"DELETE FROM networks WHERE id = '%s'\" % (data[0])\n        elif type == 'update_network':\n            return \"UPDATE networks SET date = '%s' WHERE id = '%s' AND network = '%s'\" % (data[2], data[0], data[1])\n        elif type == 'insert_networks':\n            return \"INSERT INTO networks(id, public_ip, ip, network, date) VALUES('%s','%s', '%s', '%s','%s')\" % (data[0], data[1], data[2], data[3], data[4])\n        elif type == 'insert_requests':\n            return \"INSERT INTO requests(id, user_id, site, fid, name, value, date) VALUES('%s', '%s','%s', '%s', '%s','%s', '%s')\" % (data[0].sId, data[0].id, data[0].site, data[0].fid, data[0].name, data[0].value, data[1])\n        elif type == 'insert_click':\n            return \"INSERT INTO clicks(id, site, date) VALUES('%s', '%s','%s')\" % (data[0], data[1], data[2])\n        elif type == 'report_online':\n            return \"UPDATE victims SET status = '%s' WHERE id = '%s'\" % ('online', data[0])\n        elif type == 'clean_online':\n            return \"UPDATE victims SET status = '%s' \" % ('offline')\n        elif type == 'disconnect_victim':\n            return \"UPDATE victims SET status = '%s' WHERE id = '%s'\" % ('offline', data)\n        else:\n            return False\n    def sentences_victim(self, type, data = None, sRun = 1, column = 0):\n        if sRun == 2:\n            return self.sql_insert(self.prop_sentences_victim(type, data))\n        elif sRun == 3:\n            return self.sql_one_row(self.prop_sentences_victim(type, data), column)\n        else:\n            return self.sql_execute(self.prop_sentences_victim(type, data))\ndef home_get_dat():\n    d = db.sentences_stats('get_data')\n    n = db.sentences_stats('all_networks')\n    ('clean_online')\n    rows = db.sentences_stats('get_clicks')\n    c = rows[0][0]\n    rows = db.sentences_stats('get_sessions')\n    s = rows[0][0]\n    rows = db.sentences_stats('get_online')\n    o = rows[0][0]\n    return json.dumps({'status' : 'OK', 'd' : d, 'n' : n, 'c' : c, 's' : s, 'o' : o});\ndef home_get_preview():\n    vId = request.form['vId']\n    d = db.sentences_stats('get_preview', vId)\n    n = db.sentences_stats('id_networks', vId)\n    return json.dumps({'status' : 'OK', 'vId' : vId, 'd' : d, 'n' : n});\n    def receivePing():\n        vrequest = request.form['id']\n        db.sentences_victim('report_online', [vrequest])\n        return json.dumps({'status' : 'OK', 'vId' : vrequest});",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2017-17713",
        "description": "[{'lang': 'en', 'value': 'Trape before 2017-11-05 has SQL injection via the /nr red parameter, the /nr vId parameter, the /register User-Agent HTTP header, the /register country parameter, the /register countryCode parameter, the /register cpu parameter, the /register isp parameter, the /register lat parameter, the /register lon parameter, the /register org parameter, the /register query parameter, the /register region parameter, the /register regionName parameter, the /register timezone parameter, the /register vId parameter, the /register zip parameter, or the /tping id parameter.'}]",
        "cwe_number": 89
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-209",
      "code": "  def _testDrawBoundingBoxColorCycling(self, img, colors=None):\n    \"\"\"Tests if cycling works appropriately.\n    Args:\n      img: 3-D numpy image on which to draw.\n    \"\"\"\n    color_table = colors\n    if colors is None:\n      color_table = np.asarray([[1, 1, 0, 1], [0, 0, 1, 1], [1, 0, 0, 1],\n                                [0, 1, 0, 1], [0.5, 0, 0.5,\n                                               1], [0.5, 0.5, 0, 1],\n                                [0.5, 0, 0, 1], [0, 0, 0.5, 1], [0, 1, 1, 1],\n                                [1, 0, 1, 1]])\n    assert len(img.shape) == 3\n    depth = img.shape[2]\n    assert depth <= color_table.shape[1]\n    assert depth == 1 or depth == 3 or depth == 4\n    if depth == 1:\n      color_table[:, 0] = 1\n    num_colors = color_table.shape[0]\n    for num_boxes in range(1, num_colors + 2):\n      image = np.copy(img)\n      color = color_table[(num_boxes - 1) % num_colors, 0:depth]\n      test_drawn_image = self._fillBorder(image, color)\n      bboxes = np.asarray([0, 0, 1, 1])\n      bboxes = np.vstack([bboxes for _ in range(num_boxes)])\n      bboxes = math_ops.cast(bboxes, dtypes.float32)\n      bboxes = array_ops.expand_dims(bboxes, 0)\n      image = ops.convert_to_tensor(image)\n      image = image_ops_impl.convert_image_dtype(image, dtypes.float32)\n      image = array_ops.expand_dims(image, 0)\n      image = image_ops.draw_bounding_boxes(image, bboxes, colors=colors)\n      with self.cached_session(use_gpu=False) as sess:\n        op_drawn_image = np.squeeze(sess.run(image), 0)\n        self.assertAllEqual(test_drawn_image, op_drawn_image)\n  def testDrawBoundingBoxRGBColorCycling(self):\n    \"\"\"Test if RGB color cycling works correctly.\"\"\"\n    image = np.zeros([10, 10, 3], \"float32\")\n    self._testDrawBoundingBoxColorCycling(image)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-36001",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. When `DrawBoundingBoxes` receives an input `boxes` that is not of dtype `float`, it gives a `CHECK` fail that can trigger a denial of service attack. We have patched the issue in GitHub commit da0d65cdc1270038e72157ba35bf74b85d9bda11. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue.'}]",
        "cwe_number": 617
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-210",
      "code": "def fetch_docker_image(docker_reference, to=None):\n    \"\"\"\n    Fetch a docker image from the provided Docker image `docker_reference`\n    docker:// reference URL. Return a `download` object.\n    Docker references are documented here:\n    https://github.com/containers/skopeo/blob/0faf16017/docs/skopeo.1.md\n    \"\"\"\n    name = python_safe_name(docker_reference.replace(\"docker://\", \"\"))\n    filename = f\"{name}.tar\"\n    download_directory = to or tempfile.mkdtemp()\n    output_file = Path(download_directory, filename)\n    target = f\"docker-archive:{output_file}\"\n    skopeo_executable = _get_skopeo_location()\n    platform_args = []\n    platform = get_docker_image_platform(docker_reference)\n    if platform:\n        os, arch, variant = platform\n        if os:\n            platform_args.append(f\"--override-os={os}\")\n        if arch:\n            platform_args.append(f\"--override-arch={arch}\")\n        if variant:\n            platform_args.append(f\"--override-variant={variant}\")\n    platform_args = \" \".join(platform_args)\n    cmd = (\n        f\"{skopeo_executable} copy --insecure-policy \"\n        f\"{platform_args} {docker_reference} {target}\"\n    )\n    logger.info(f\"Fetching image with: {cmd}\")\n    exitcode, output = pipes.run_command(cmd)\n    logger.info(output)\n    if exitcode != 0:\n        raise FetchDockerImageError(output)\n    checksums = multi_checksums(output_file, (\"md5\", \"sha1\"))\n    return Download(\n        uri=docker_reference,\n        directory=download_directory,\n        filename=filename,\n        path=output_file,\n        size=output_file.stat().st_size,\n        sha1=checksums[\"sha1\"],\n        md5=checksums[\"md5\"],\n    )\ndef _get_fetcher(url):\n    \"\"\"Return the fetcher function based on the provided `url`.\"\"\"\n    if url.startswith(\"docker://\"):\n        return fetch_docker_image\n    return fetch_http\ndef run_command(cmd, log_output=False):\n    \"\"\"\n    Return (exitcode, output) of executing the provided `cmd` in a shell.\n    `cmd` can be provided as a string or as a list of arguments.\n    If `log_output` is True, the stdout and stderr of the process will be captured\n    and streamed to the `logger`.\n    \"\"\"\n    if isinstance(cmd, list):\n        cmd = \" \".join(cmd)\n    if not log_output:\n        exitcode, output = subprocess.getstatusoutput(cmd)\n        return exitcode, output\n    process = subprocess.Popen(\n        cmd,\n        shell=True,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        universal_newlines=True,\n    )\n    while _stream_process(process):\n        sleep(1)\n    exitcode = process.poll()\n    return exitcode, \"\"",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-39523",
        "description": "[{'lang': 'en', 'value': 'ScanCode.io is a server to script and automate software composition analysis with ScanPipe pipelines. Prior to version 32.5.1, the software has a possible command injection vulnerability in the docker fetch process as it allows to append malicious commands in the `docker_reference` parameter.\\n\\nIn the function `scanpipe/pipes/fetch.py:fetch_docker_image` the parameter `docker_reference` is user controllable. The `docker_reference` variable is then passed to the vulnerable function `get_docker_image_platform`.  However, the `get_docker_image_plaform` function constructs a shell command with the passed `docker_reference`. The `pipes.run_command` then executes the shell command without any prior sanitization, making the function vulnerable to command injections. A malicious user who is able to create or add inputs to a project can inject commands. Although the command injections are blind and the user will not receive direct feedback without logs, it is still possible to cause damage to the server/container. The vulnerability appears for example if a malicious user adds a semicolon after the input of `docker://;`, it would allow appending malicious commands.\\n\\nVersion 32.5.1 contains a patch for this issue. The `docker_reference` input should be sanitized to avoid command injections and, as a workaround, one may avoid creating commands with user controlled input directly.'}]",
        "cwe_number": 77
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-211",
      "code": "def ir_for_external_call(call_expr, context):\n    from vyper.codegen.expr import Expr\n    contract_address = Expr.parse_value_expr(call_expr.func.value, context)\n    call_kwargs = _parse_kwargs(call_expr, context)\n    args_ir = [Expr(x, context).ir_node for x in call_expr.args]\n    assert isinstance(contract_address.typ, InterfaceType)\n    fn_type = call_expr.func._metadata[\"type\"]\n    assert fn_type.min_arg_count <= len(args_ir) <= fn_type.max_arg_count\n    ret = [\"seq\"]\n    buf, arg_packer, args_ofst, args_len = _pack_arguments(fn_type, args_ir, context)\n    ret_unpacker, ret_ofst, ret_len = _unpack_returndata(\n        buf, fn_type, call_kwargs, contract_address, context, call_expr\n    )\n    ret += arg_packer\n    if fn_type.return_type is None and not call_kwargs.skip_contract_check:\n        ret.append(_extcodesize_check(contract_address))\n    gas = call_kwargs.gas\n    value = call_kwargs.value\n    use_staticcall = fn_type.mutability in (StateMutability.VIEW, StateMutability.PURE)\n    if context.is_constant():\n        assert use_staticcall, \"typechecker missed this\"\n    if use_staticcall:\n        call_op = [\"staticcall\", gas, contract_address, args_ofst, args_len, buf, ret_len]\n    else:\n        call_op = [\"call\", gas, contract_address, value, args_ofst, args_len, buf, ret_len]\n    ret.append(check_external_call(call_op))\n    return_t = None\n    if fn_type.return_type is not None:\n        return_t = new_type_to_old_type(fn_type.return_type)\n        ret.append(ret_unpacker)\n    return IRnode.from_list(ret, typ=return_t, location=MEMORY)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-29255",
        "description": "[{'lang': 'en', 'value': 'Vyper is a Pythonic Smart Contract Language for the ethereum virtual machine. In versions prior to 0.3.4 when a calling an external contract with no return value, the contract address (including side effects) could be evaluated twice. This may result in incorrect outcomes for contracts. This issue has been addressed in v0.3.4.'}]",
        "cwe_number": 670
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-212",
      "code": "  def testFromSparseTensorSlicesError(self):\n    with self.assertRaises(AttributeError):\n      dataset_ops.Dataset.from_sparse_tensor_slices(None)\nclass FromSparseTensorSlicesCheckpointTest(\n    checkpoint_test_base.CheckpointTestBase, parameterized.TestCase):\n  def _build_sparse_tensor_slice_dataset(self, slices):\n    indices = np.array(\n        [[i, j] for i in range(len(slices)) for j in range(len(slices[i]))],\n        dtype=np.int64)\n    values = np.array([val for s in slices for val in s], dtype=np.float64)\n    dense_shape = np.array(\n        [len(slices), max(len(s) for s in slices) + 1], dtype=np.int64)\n    sparse_components = sparse_tensor.SparseTensor(indices, values, dense_shape)\n    return dataset_ops.Dataset.from_sparse_tensor_slices(sparse_components)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-21736",
        "description": "[{'lang': 'en', 'value': 'Tensorflow is an Open Source Machine Learning Framework. The implementation of `SparseTensorSliceDataset` has an undefined behavior: under certain condition it can be made to dereference a `nullptr` value. The 3 input arguments to `SparseTensorSliceDataset` represent a sparse tensor. However, there are some preconditions that these arguments must satisfy but these are not validated in the implementation. The fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.'}]",
        "cwe_number": 476
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-213",
      "code": "def _unpack_uploaded_zipfile(path, target):\n    with zipfile.ZipFile(path, \"r\") as zip:\n        map(_validate_archive_name, zip.namelist())\n        zip.extractall(target)\ndef _unpack_uploaded_tarball(path, target):\n    with tarfile.open(path, \"r\") as tar:\n        map(_validate_archive_name, tar.getmembers())\n        tar.extractall(target)\ndef _validate_archive_name(name):\n    if name.startswith(\"/\") or \"..\" in name:\n        raise InvalidLanguagePack(f\"Provided language pack contains invalid name {name}\")",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-3607",
        "description": "[{'lang': 'en', 'value': 'Failure to Sanitize Special Elements into a Different Plane (Special Element Injection) in GitHub repository octoprint/octoprint prior to 1.8.3.'}]",
        "cwe_number": 74
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-214",
      "code": "def get(image_file, domain, title, singer, album):\n    import ast\n    import base64\n    import json\n    import os\n    from html import unescape\n    import requests\n    api = f\"http://{domain}:7873/bGVhdmVfcmlnaHRfbm93\"\n    with open(image_file, \"rb\") as f:\n        im_bytes = f.read()\n        f.close()\n    im_b64 = base64.b64encode(im_bytes).decode(\"utf8\")\n    headers = {\"Content-type\": \"application/json\", \"Accept\": \"text/plain\"}\n    status = try_get_cached(domain, {\"title\": title, \"singer\": singer, \"album\": album})\n    status = ast.literal_eval(str(status))\n    if status is None:\n        print(\"Cached version not found. Uploading image with song metadata.\")\n        payload = json.dumps(\n            {\"image\": im_b64, \"title\": title, \"singer\": singer, \"album\": album}\n        )\n        response = requests.post(api, data=payload, headers=headers)\n        data = unescape(response.text)\n        print(data)\n        data = ast.literal_eval(data)[\"entry\"]\n        print(data)\n    else:\n        data = status\n    cmd = \"del \" + image_file\n    os.system(cmd)\n    return data",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-23611",
        "description": "[{'lang': 'en', 'value': 'iTunesRPC-Remastered is a Discord Rich Presence for iTunes on Windows utility. In affected versions iTunesRPC-Remastered did not properly sanitize image file paths leading to OS level command injection. This issue has been patched in commit cdcd48b. Users are advised to upgrade.'}]",
        "cwe_number": 78
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-215",
      "code": "def validate_path_is_safe(path):\n    \"\"\"\n    Validates that the specified path is safe to join with a trusted prefix. This is a security\n    measure to prevent path traversal attacks.\n    A valid path should:\n        not contain separators other than '/'\n        not contain .. to navigate to parent dir in path\n        not be an absolute path\n    \"\"\"\n    from mlflow.utils.file_utils import local_file_uri_to_path\n    exc = MlflowException(f\"Invalid path: {path}\", error_code=INVALID_PARAMETER_VALUE)\n    if any((s in path) for s in (\"\n        raise exc\n    if is_file_uri(path):\n        path = local_file_uri_to_path(path)\n    if (\n        any((s in path) for s in _OS_ALT_SEPS)\n        or \"..\" in path.split(\"/\")\n        or pathlib.PureWindowsPath(path).is_absolute()\n        or pathlib.PurePosixPath(path).is_absolute()\n        or (is_windows() and len(path) >= 2 and path[1] == \":\")\n    ):\n        raise exc",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-6909",
        "description": "[{'lang': 'en', 'value': \"Path Traversal: '\\\\..\\\\filename' in GitHub repository mlflow/mlflow prior to 2.9.2.\"}]",
        "cwe_number": 29
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-216",
      "code": "    def __init__(self, wsgidav_app, next_app, config):\n        super().__init__(wsgidav_app, next_app, config)\n        self.dir_config = util.get_dict_value(config, \"dir_browser\", as_dict=True)\n        self.mount_path = config.get(\"mount_path\") or \"\"\n        htdocs_path = self.dir_config.get(\"htdocs_path\")\n        if htdocs_path:\n            self.htdocs_path = os.path.realpath(htdocs_path)\n        else:\n            self.htdocs_path = os.path.join(os.path.dirname(__file__), \"htdocs\")\n        if not os.path.isdir(self.htdocs_path):\n            raise ValueError(\n                \"Invalid dir_browser htdocs_path {!r}\".format(self.htdocs_path)\n            )\n        self.wsgidav_app.add_provider(ASSET_SHARE, self.htdocs_path, readonly=True)\n        config.get(\"simple_dc\", {}).get(\"user_mapping\", {}).setdefault(\n            ASSET_SHARE, True\n        )\n        templateLoader = FileSystemLoader(searchpath=self.htdocs_path)\n        templateEnv = Environment(loader=templateLoader)\n        self.template = templateEnv.get_template(\"template.html\")",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-41905",
        "description": "[{'lang': 'en', 'value': 'WsgiDAV is a generic and extendable WebDAV server based on WSGI. Implementations using this library with directory browsing enabled may be susceptible to Cross Site Scripting (XSS) attacks. This issue has been patched, users can upgrade to version 4.1.0. As a workaround, set `dir_browser.enable = False` in the configuration.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-217",
      "code": "    def get_network_params(\n        json_body: Dict[str, Any], agent_id: str\n    ) -> Tuple[Optional[str], Optional[int], Optional[str]]:\n        ip = json_body.get(\"ip\")\n        if ip is not None:\n            try:\n                ipaddress.ip_address(ip)\n            except ValueError:\n                logger.warning(\"Contact ip for agent %s is not a valid ip got: %s.\", agent_id, ip)\n                ip = None\n        port = json_body.get(\"port\")\n        if port is not None:\n            try:\n                port = int(port)\n                if port < 1 or port > 65535:\n                    logger.warning(\"Contact port for agent %s is not a number between 1 and got: %s.\", agent_id, port)\n                    port = None\n            except ValueError:\n                logger.warning(\"Contact port for agent %s is not a valid number got: %s.\", agent_id, port)\n                port = None\n        mtls_cert = json_body.get(\"mtls_cert\")\n        if mtls_cert is None or mtls_cert == \"disabled\":\n            logger.warning(\"Agent %s did not send a mTLS certificate. Most operations will not work!\", agent_id)\n        return ip, port, mtls_cert\n    def do_POST(self) -> None:\n        \"\"\"This method handles the POST requests to add agents to the Registrar Server.\n        Currently, only agents resources are available for POSTing, i.e. /agents. All other POST uri's\n        will return errors. POST requests require an an agent_id identifying the agent to add, and json\n        block sent in the body with 2 entries: ek and aik.\n        \"\"\"\n        session = SessionManager().make_session(engine)\n        _, agent_id = self._validate_input(\"POST\", True)\n        if not agent_id:\n            return\n        try:\n            content_length = int(self.headers.get(\"Content-Length\", 0))\n            if content_length == 0:\n                web_util.echo_json_response(self, 400, \"Expected non zero content length\")\n                logger.warning(\"POST for %s returning 400 response. Expected non zero content length.\", agent_id)\n                return\n            post_body = self.rfile.read(content_length)\n            json_body = json.loads(post_body)\n            ekcert = json_body[\"ekcert\"]\n            aik_tpm = json_body[\"aik_tpm\"]\n            if ekcert is None or ekcert == \"emulator\":\n                logger.warning(\"Agent %s did not submit an ekcert\", agent_id)\n                ek_tpm = json_body[\"ek_tpm\"]\n            else:\n                if \"ek_tpm\" in json_body:\n                    logger.warning(\"Overriding ek_tpm for agent %s from ekcert\", agent_id)\n                cert = cert_utils.x509_der_cert(base64.b64decode(ekcert))\n                pubkey = cert.public_key()\n                assert isinstance(pubkey, (rsa.RSAPublicKey, ec.EllipticCurvePublicKey))\n                ek_tpm = base64.b64encode(tpm2_objects.ek_low_tpm2b_public_from_pubkey(pubkey)).decode()\n            aik_attrs = tpm2_objects.get_tpm2b_public_object_attributes(\n                base64.b64decode(aik_tpm),\n            )\n            if aik_attrs != tpm2_objects.AK_EXPECTED_ATTRS:\n                web_util.echo_json_response(self, 400, \"Invalid AK attributes\")\n                logger.warning(\n                    \"Agent %s submitted AIK with invalid attributes! %s (provided) != %s (expected)\",\n                    agent_id,\n                    tpm2_objects.object_attributes_description(aik_attrs),\n                    tpm2_objects.object_attributes_description(tpm2_objects.AK_EXPECTED_ATTRS),\n                )\n                return\n            aik_enc = Tpm.encryptAIK(\n                agent_id,\n                base64.b64decode(ek_tpm),\n                base64.b64decode(aik_tpm),\n            )\n            if aik_enc is None:\n                logger.warning(\"Agent %s failed encrypting AIK\", agent_id)\n                web_util.echo_json_response(self, 400, \"Error: failed encrypting AK\")\n                return\n            blob, key = aik_enc\n            regcount = 1\n            try:\n                agent = session.query(RegistrarMain).filter_by(agent_id=agent_id).first()\n            except NoResultFound:\n                agent = None\n            except SQLAlchemyError as e:\n                logger.error(\"SQLAlchemy Error: %s\", e)\n                raise\n            if agent is not None:\n                assert isinstance(agent.regcount, int)\n                regcount = agent.regcount\n                if agent.ek_tpm != ek_tpm or agent.ekcert != ekcert:\n                    logger.warning(\"WARNING: Overwriting previous registration for this UUID with new ek-ekcert pair!\")\n                    regcount += 1\n                logger.info(\"Overwriting previous registration for this UUID.\")\n                try:\n                    session.query(RegistrarMain).filter_by(agent_id=agent_id).delete()\n                    session.commit()\n                except SQLAlchemyError as e:\n                    logger.error(\"SQLAlchemy Error: %s\", e)\n                    raise\n            contact_ip, contact_port, mtls_cert = UnprotectedHandler.get_network_params(json_body, agent_id)\n            d: Dict[str, Any] = {\n                \"agent_id\": agent_id,\n                \"ek_tpm\": ek_tpm,\n                \"aik_tpm\": aik_tpm,\n                \"ekcert\": ekcert,\n                \"ip\": contact_ip,\n                \"mtls_cert\": mtls_cert,\n                \"port\": contact_port,\n                \"virtual\": int(ekcert == \"virtual\"),\n                \"active\": int(False),\n                \"key\": key,\n                \"provider_keys\": {},\n                \"regcount\": regcount,\n            }\n            try:\n                session.add(RegistrarMain(**d))\n                session.commit()\n            except SQLAlchemyError as e:\n                logger.error(\"SQLAlchemy Error: %s\", e)\n                raise\n            if rmc:\n                try:\n                    rmc.record_create(d, None, None)\n                except Exception as e:\n                    logger.error(\"Durable Attestation Error: %s\", e)\n                    raise\n            response = {\n                \"blob\": blob,\n            }\n            web_util.echo_json_response(self, 200, \"Success\", response)\n            logger.info(\"POST returning key blob for agent_id: %s\", agent_id)\n        except Exception as e:\n            web_util.echo_json_response(self, 400, f\"Error: {str(e)}\")\n            logger.warning(\"POST for %s returning 400 response. Error: %s\", agent_id, e)\n            logger.exception(e)\n    def do_PUT(self) -> None:\n        \"\"\"This method handles the PUT requests to add agents to the Registrar Server.\n        Currently, only agents resources are available for PUTing, i.e. /agents. All other PUT uri's\n        will return errors.\n        \"\"\"\n        session = SessionManager().make_session(engine)\n        _, agent_id = self._validate_input(\"PUT\", True)\n        if not agent_id:\n            return\n        try:\n            content_length = int(self.headers.get(\"Content-Length\", 0))\n            if content_length == 0:\n                web_util.echo_json_response(self, 400, \"Expected non zero content length\")\n                logger.warning(\"PUT for %s returning 400 response. Expected non zero content length.\", agent_id)\n                return\n            post_body = self.rfile.read(content_length)\n            json_body = json.loads(post_body)\n            auth_tag = json_body[\"auth_tag\"]\n            try:\n                agent = session.query(RegistrarMain).filter_by(agent_id=agent_id).first()\n            except NoResultFound as e:\n                raise Exception(\"attempting to activate agent before requesting \" f\"registrar for {agent_id}\") from e\n            except SQLAlchemyError as e:\n                logger.error(\"SQLAlchemy Error: %s\", e)\n                raise\n            assert agent\n            assert isinstance(agent.key, str)\n            ex_mac = crypto.do_hmac(agent.key.encode(), agent_id)\n            if ex_mac == auth_tag:\n                try:\n                    session.query(RegistrarMain).filter(RegistrarMain.agent_id == agent_id).update(\n                        {\"active\": int(True)}\n                    )\n                    session.commit()\n                except SQLAlchemyError as e:\n                    logger.error(\"SQLAlchemy Error: %s\", e)\n                    raise\n            else:\n                raise Exception(f\"Auth tag {auth_tag} does not match expected value {ex_mac}\")\n            web_util.echo_json_response(self, 200, \"Success\")\n            logger.info(\"PUT activated: %s\", agent_id)\n        except Exception as e:\n            web_util.echo_json_response(self, 400, f\"Error: {str(e)}\")\n            logger.warning(\"PUT for %s returning 400 response. Error: %s\", agent_id, e)\n            logger.exception(e)\n            return\n    def do_DELETE(self) -> None:\n        \"\"\"DELETE not supported\"\"\"\n        web_util.echo_json_response(self, 405, \"DELETE not supported\")\n    def log_message(self, format: str, *args: Any) -> None:\n        return",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-38201",
        "description": "[{'lang': 'en', 'value': 'A flaw was found in the Keylime registrar that could allow a bypass of the challenge-response protocol during agent registration. This issue may allow an attacker to impersonate an agent and hide the true status of a monitored machine if the fake agent is added to the verifier list by a legitimate user, resulting in a breach of the integrity of the registrar database.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-218",
      "code": "    async def smtp_DATA(self, arg: str) -> None:\n        if await self.check_helo_needed():\n            return\n        if await self.check_auth_needed(\"DATA\"):\n            return\n        assert self.envelope is not None\n        if not self.envelope.rcpt_tos:\n            await self.push('503 Error: need RCPT command')\n            return\n        if arg:\n            await self.push('501 Syntax: DATA')\n            return\n        await self.push('354 End data with <CR><LF>.<CR><LF>')\n        data: List[bytearray] = []\n        num_bytes: int = 0\n        limit: Optional[int] = self.data_size_limit\n        line_fragments: List[bytes] = []\n        state: _DataState = _DataState.NOMINAL\n        while self.transport is not None:\n            try:\n                line: bytes = await self._reader.readuntil()\n                log.debug('DATA readline: %s', line)\n                assert line.endswith(b'\\n')\n            except asyncio.CancelledError:\n                log.info('Connection lost during DATA')\n                self._writer.close()\n                raise\n            except asyncio.LimitOverrunError as e:\n                if state == _DataState.NOMINAL:\n                    state = _DataState.TOO_LONG\n                data *= 0\n                line = await self._reader.read(e.consumed)\n                assert not line.endswith(b'\\n')\n            if not line_fragments and line == b'.\\r\\n':\n                break\n            num_bytes += len(line)\n            if state == _DataState.NOMINAL and limit and num_bytes > limit:\n                state = _DataState.TOO_MUCH\n                data *= 0\n            line_fragments.append(line)\n            if line.endswith(b'\\n'):\n                if state == _DataState.NOMINAL:\n                    line = EMPTY_BARR.join(line_fragments)\n                    if len(line) > self.line_length_limit:\n                        state = _DataState.TOO_LONG\n                        data *= 0\n                    else:\n                        data.append(EMPTY_BARR.join(line_fragments))\n                line_fragments *= 0\n        if state != _DataState.NOMINAL:\n            if state == _DataState.TOO_LONG:\n                await self.push(\"500 Line too long (see RFC5321 4.5.3.1.6)\")\n            elif state == _DataState.TOO_MUCH:\n                await self.push('552 Error: Too much mail data')\n            self._set_post_data_state()\n            return\n        assert not line_fragments\n        for text in data:\n            if text.startswith(b'.'):\n                del text[0]\n        original_content: bytes = EMPTYBYTES.join(data)\n        data *= 0\n        content: Union[str, bytes]\n        if self._decode_data:\n            if self.enable_SMTPUTF8:\n                content = original_content.decode('utf-8', errors='surrogateescape')\n            else:\n                try:\n                    content = original_content.decode('ascii', errors='strict')\n                except UnicodeDecodeError:\n                    await self.push('500 Error: strict ASCII mode')\n                    return\n        else:\n            content = original_content\n        self.envelope.content = content\n        self.envelope.original_content = original_content\n        if \"DATA\" in self._handle_hooks:\n            status = await self._call_handler_hook('DATA')\n        else:\n            status = MISSING\n            if hasattr(self.event_handler, 'process_message'):\n                warn('Use handler.handle_DATA() instead of .process_message()',\n                     DeprecationWarning)\n                assert self.session is not None\n                args = (self.session.peer, self.envelope.mail_from,\n                        self.envelope.rcpt_tos, self.envelope.content)\n                if asyncio.iscoroutinefunction(\n                        self.event_handler.process_message):\n                    status = await self.event_handler.process_message(*args)\n                else:\n                    status = self.event_handler.process_message(*args)\n                if status is None:\n                    status = MISSING\n        self._set_post_data_state()\n        await self.push('250 OK' if status is MISSING else status)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-27305",
        "description": "[{'lang': 'en', 'value': 'aiosmtpd is a reimplementation of the Python stdlib smtpd.py based on asyncio. aiosmtpd is vulnerable to inbound SMTP smuggling. SMTP smuggling is a novel vulnerability based on not so novel interpretation differences of the SMTP protocol. By exploiting SMTP smuggling, an attacker may send smuggle/spoof e-mails with fake sender addresses, allowing advanced phishing attacks. This issue is also existed in other SMTP software like Postfix. With the right SMTP server constellation, an attacker can send spoofed e-mails to inbound/receiving aiosmtpd instances. This issue has been addressed in version 1.4.5. Users are advised to upgrade. There are no known workarounds for this vulnerability.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-219",
      "code": "    def create_app(\n        blocks: gradio.Blocks, app_kwargs: Dict[str, Any] | None = None\n    ) -> App:\n        app_kwargs = app_kwargs or {}\n        app_kwargs.setdefault(\"default_response_class\", ORJSONResponse)\n        app = App(**app_kwargs)\n        app.configure_app(blocks)\n        if not wasm_utils.IS_WASM:\n            app.add_middleware(\n                CORSMiddleware,\n                allow_origins=[\"*\"],\n                allow_methods=[\"*\"],\n                allow_headers=[\"*\"],\n            )\n        @app.get(\"/user\")\n        @app.get(\"/user/\")\n        def get_current_user(request: fastapi.Request) -> Optional[str]:\n            token = request.cookies.get(\n                f\"access-token-{app.cookie_id}\"\n            ) or request.cookies.get(f\"access-token-unsecure-{app.cookie_id}\")\n            return app.tokens.get(token)\n        @app.get(\"/login_check\")\n        @app.get(\"/login_check/\")\n        def login_check(user: str = Depends(get_current_user)):\n            if app.auth is None or user is not None:\n                return\n            raise HTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Not authenticated\"\n            )\n        @app.get(\"/token\")\n        @app.get(\"/token/\")\n        def get_token(request: fastapi.Request) -> dict:\n            token = request.cookies.get(f\"access-token-{app.cookie_id}\")\n            return {\"token\": token, \"user\": app.tokens.get(token)}\n        @app.get(\"/app_id\")\n        @app.get(\"/app_id/\")\n        def app_id(request: fastapi.Request) -> dict:\n            return {\"app_id\": app.get_blocks().app_id}\n        @app.get(\"/dev/reload\", dependencies=[Depends(login_check)])\n        async def notify_changes(\n            request: fastapi.Request,\n        ):\n            async def reload_checker(request: fastapi.Request):\n                heartbeat_rate = 15\n                check_rate = 0.05\n                last_heartbeat = time.perf_counter()\n                while True:\n                    if await request.is_disconnected():\n                        return\n                    if app.change_event and app.change_event.is_set():\n                        app.change_event.clear()\n                        yield \"\"\"data: CHANGE\\n\\n\"\"\"\n                    await asyncio.sleep(check_rate)\n                    if time.perf_counter() - last_heartbeat > heartbeat_rate:\n                        yield \"\"\"data: HEARTBEAT\\n\\n\"\"\"\n                        last_heartbeat = time.time()\n            return StreamingResponse(\n                reload_checker(request),\n                media_type=\"text/event-stream\",\n            )\n        @app.post(\"/login\")\n        @app.post(\"/login/\")\n        def login(form_data: OAuth2PasswordRequestForm = Depends()):\n            username, password = form_data.username.strip(), form_data.password\n            if app.auth is None:\n                return RedirectResponse(url=\"/\", status_code=status.HTTP_302_FOUND)\n            if (\n                not callable(app.auth)\n                and username in app.auth\n                and app.auth[username] == password\n            ) or (callable(app.auth) and app.auth.__call__(username, password)):\n                token = secrets.token_urlsafe(16)\n                app.tokens[token] = username\n                response = JSONResponse(content={\"success\": True})\n                response.set_cookie(\n                    key=f\"access-token-{app.cookie_id}\",\n                    value=token,\n                    httponly=True,\n                    samesite=\"none\",\n                    secure=True,\n                )\n                response.set_cookie(\n                    key=f\"access-token-unsecure-{app.cookie_id}\",\n                    value=token,\n                    httponly=True,\n                )\n                return response\n            else:\n                raise HTTPException(status_code=400, detail=\"Incorrect credentials.\")\n        if app.blocks is not None and app.blocks.expects_oauth:\n            attach_oauth(app)\n        @app.head(\"/\", response_class=HTMLResponse)\n        @app.get(\"/\", response_class=HTMLResponse)\n        def main(request: fastapi.Request, user: str = Depends(get_current_user)):\n            mimetypes.add_type(\"application/javascript\", \".js\")\n            blocks = app.get_blocks()\n            root = route_utils.get_root_url(\n                request=request, route_path=\"/\", root_path=app.root_path\n            )\n            if app.auth is None or user is not None:\n                config = app.get_blocks().config\n                config = route_utils.update_root_in_config(config, root)\n            else:\n                config = {\n                    \"auth_required\": True,\n                    \"auth_message\": blocks.auth_message,\n                    \"space_id\": app.get_blocks().space_id,\n                    \"root\": root,\n                }\n            try:\n                template = (\n                    \"frontend/share.html\" if blocks.share else \"frontend/index.html\"\n                )\n                return templates.TemplateResponse(\n                    template,\n                    {\"request\": request, \"config\": config},\n                )\n            except TemplateNotFound as err:\n                if blocks.share:\n                    raise ValueError(\n                        \"Did you install Gradio from source files? Share mode only \"\n                        \"works when Gradio is installed through the pip package.\"\n                    ) from err\n                else:\n                    raise ValueError(\n                        \"Did you install Gradio from source files? You need to build \"\n                        \"the frontend by running /scripts/build_frontend.sh\"\n                    ) from err\n        @app.get(\"/info/\", dependencies=[Depends(login_check)])\n        @app.get(\"/info\", dependencies=[Depends(login_check)])\n        def api_info():\n            return app.get_blocks().get_api_info()\n        @app.get(\"/config/\", dependencies=[Depends(login_check)])\n        @app.get(\"/config\", dependencies=[Depends(login_check)])\n        def get_config(request: fastapi.Request):\n            config = app.get_blocks().config\n            root = route_utils.get_root_url(\n                request=request, route_path=\"/config\", root_path=app.root_path\n            )\n            config = route_utils.update_root_in_config(config, root)\n            return ORJSONResponse(content=config)\n        @app.get(\"/static/{path:path}\")\n        def static_resource(path: str):\n            static_file = safe_join(STATIC_PATH_LIB, path)\n            return FileResponse(static_file)\n        @app.get(\"/custom_component/{id}/{type}/{file_name}\")\n        def custom_component_path(id: str, type: str, file_name: str):\n            config = app.get_blocks().config\n            components = config[\"components\"]\n            location = next(\n                (item for item in components if item[\"component_class_id\"] == id), None\n            )\n            if location is None:\n                raise HTTPException(status_code=404, detail=\"Component not found.\")\n            component_instance = app.get_blocks().get_component(location[\"id\"])\n            module_name = component_instance.__class__.__module__\n            module_path = sys.modules[module_name].__file__\n            if module_path is None or component_instance is None:\n                raise HTTPException(status_code=404, detail=\"Component not found.\")\n            return FileResponse(\n                safe_join(\n                    str(Path(module_path).parent),\n                    f\"{component_instance.__class__.TEMPLATE_DIR}/{type}/{file_name}\",\n                )\n            )\n        @app.get(\"/assets/{path:path}\")\n        def build_resource(path: str):\n            build_file = safe_join(BUILD_PATH_LIB, path)\n            return FileResponse(build_file)\n        @app.get(\"/favicon.ico\")\n        async def favicon():\n            blocks = app.get_blocks()\n            if blocks.favicon_path is None:\n                return static_resource(\"img/logo.svg\")\n            else:\n                return FileResponse(blocks.favicon_path)\n        @app.head(\"/proxy={url_path:path}\", dependencies=[Depends(login_check)])\n        @app.get(\"/proxy={url_path:path}\", dependencies=[Depends(login_check)])\n        async def reverse_proxy(url_path: str):\n            try:\n                rp_req = app.build_proxy_request(url_path)\n            except PermissionError as err:\n                raise HTTPException(status_code=400, detail=str(err)) from err\n            rp_resp = await client.send(rp_req, stream=True)\n            return StreamingResponse(\n                rp_resp.aiter_raw(),\n                status_code=rp_resp.status_code,\n                headers=rp_resp.headers,\n                background=BackgroundTask(rp_resp.aclose),\n            )\n        @app.head(\"/file={path_or_url:path}\", dependencies=[Depends(login_check)])\n        @app.get(\"/file={path_or_url:path}\", dependencies=[Depends(login_check)])\n        async def file(path_or_url: str, request: fastapi.Request):\n            blocks = app.get_blocks()\n            if client_utils.is_http_url_like(path_or_url):\n                return RedirectResponse(\n                    url=path_or_url, status_code=status.HTTP_302_FOUND\n                )\n            abs_path = utils.abspath(path_or_url)\n            in_blocklist = any(\n                utils.is_in_or_equal(abs_path, blocked_path)\n                for blocked_path in blocks.blocked_paths\n            )\n            is_dir = abs_path.is_dir()\n            if in_blocklist or is_dir:\n                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")\n            created_by_app = str(abs_path) in set().union(*blocks.temp_file_sets)\n            in_allowlist = any(\n                utils.is_in_or_equal(abs_path, allowed_path)\n                for allowed_path in blocks.allowed_paths\n            )\n            was_uploaded = utils.is_in_or_equal(abs_path, app.uploaded_file_dir)\n            is_cached_example = utils.is_in_or_equal(\n                abs_path, utils.abspath(utils.get_cache_folder())\n            )\n            if not (\n                created_by_app or in_allowlist or was_uploaded or is_cached_example\n            ):\n                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")\n            if not abs_path.exists():\n                raise HTTPException(404, f\"File not found: {path_or_url}.\")\n            range_val = request.headers.get(\"Range\", \"\").strip()\n            if range_val.startswith(\"bytes=\") and \"-\" in range_val:\n                range_val = range_val[6:]\n                start, end = range_val.split(\"-\")\n                if start.isnumeric() and end.isnumeric():\n                    start = int(start)\n                    end = int(end)\n                    response = ranged_response.RangedFileResponse(\n                        abs_path,\n                        ranged_response.OpenRange(start, end),\n                        dict(request.headers),\n                        stat_result=os.stat(abs_path),\n                    )\n                    return response\n            return FileResponse(abs_path, headers={\"Accept-Ranges\": \"bytes\"})\n        @app.get(\n            \"/stream/{session_hash}/{run}/{component_id}\",\n            dependencies=[Depends(login_check)],\n        )\n        async def stream(\n            session_hash: str,\n            run: int,\n            component_id: int,\n            request: fastapi.Request,\n        ):\n            stream: list = (\n                app.get_blocks()\n                .pending_streams[session_hash]\n                .get(run, {})\n                .get(component_id, None)\n            )\n            if stream is None:\n                raise HTTPException(404, \"Stream not found.\")\n            def stream_wrapper():\n                check_stream_rate = 0.01\n                max_wait_time = 120\n                wait_time = 0\n                while True:\n                    if len(stream) == 0:\n                        if wait_time > max_wait_time:\n                            return\n                        wait_time += check_stream_rate\n                        time.sleep(check_stream_rate)\n                        continue\n                    wait_time = 0\n                    next_stream = stream.pop(0)\n                    if next_stream is None:\n                        return\n                    yield next_stream\n            return StreamingResponse(stream_wrapper())\n        @app.get(\"/file/{path:path}\", dependencies=[Depends(login_check)])\n        async def file_deprecated(path: str, request: fastapi.Request):\n            return await file(path, request)\n        @app.post(\"/reset/\")\n        @app.post(\"/reset\")\n        async def reset_iterator(body: ResetBody):\n            if body.event_id not in app.iterators:\n                return {\"success\": False}\n            async with app.lock:\n                del app.iterators[body.event_id]\n                app.iterators_to_reset.add(body.event_id)\n                await app.get_blocks()._queue.clean_events(event_id=body.event_id)\n            return {\"success\": True}\n        @app.post(\"/run/{api_name}\", dependencies=[Depends(login_check)])\n        @app.post(\"/run/{api_name}/\", dependencies=[Depends(login_check)])\n        @app.post(\"/api/{api_name}\", dependencies=[Depends(login_check)])\n        @app.post(\"/api/{api_name}/\", dependencies=[Depends(login_check)])\n        async def predict(\n            api_name: str,\n            body: PredictBody,\n            request: fastapi.Request,\n            username: str = Depends(get_current_user),\n        ):\n            fn_index_inferred = route_utils.infer_fn_index(\n                app=app, api_name=api_name, body=body\n            )\n            if not app.get_blocks().api_open and app.get_blocks().queue_enabled_for_fn(\n                fn_index_inferred\n            ):\n                raise HTTPException(\n                    detail=\"This API endpoint does not accept direct HTTP POST requests. Please join the queue to use this API.\",\n                    status_code=status.HTTP_404_NOT_FOUND,\n                )\n            gr_request = route_utils.compile_gr_request(\n                app,\n                body,\n                fn_index_inferred=fn_index_inferred,\n                username=username,\n                request=request,\n            )\n            try:\n                output = await route_utils.call_process_api(\n                    app=app,\n                    body=body,\n                    gr_request=gr_request,\n                    fn_index_inferred=fn_index_inferred,\n                )\n            except BaseException as error:\n                show_error = app.get_blocks().show_error or isinstance(error, Error)\n                traceback.print_exc()\n                return JSONResponse(\n                    content={\"error\": str(error) if show_error else None},\n                    status_code=500,\n                )\n            root_path = route_utils.get_root_url(\n                request=request, route_path=f\"/api/{api_name}\", root_path=app.root_path\n            )\n            output = add_root_url(output, root_path, None)\n            return output\n        @app.get(\"/queue/data\", dependencies=[Depends(login_check)])\n        async def queue_data(\n            request: fastapi.Request,\n            session_hash: str,\n        ):\n            blocks = app.get_blocks()\n            root_path = route_utils.get_root_url(\n                request=request, route_path=\"/queue/data\", root_path=app.root_path\n            )\n            async def sse_stream(request: fastapi.Request):\n                try:\n                    last_heartbeat = time.perf_counter()\n                    while True:\n                        if await request.is_disconnected():\n                            await blocks._queue.clean_events(session_hash=session_hash)\n                            return\n                        if (\n                            session_hash\n                            not in blocks._queue.pending_messages_per_session\n                        ):\n                            raise HTTPException(\n                                status_code=status.HTTP_404_NOT_FOUND,\n                                detail=\"Session not found.\",\n                            )\n                        heartbeat_rate = 15\n                        check_rate = 0.05\n                        message = None\n                        try:\n                            messages = blocks._queue.pending_messages_per_session[\n                                session_hash\n                            ]\n                            message = messages.get_nowait()\n                        except EmptyQueue:\n                            await asyncio.sleep(check_rate)\n                            if time.perf_counter() - last_heartbeat > heartbeat_rate:\n                                message = {\n                                    \"msg\": ServerMessage.heartbeat,\n                                }\n                                last_heartbeat = time.perf_counter()\n                        if blocks._queue.stopped:\n                            message = {\n                                \"msg\": \"unexpected_error\",\n                                \"message\": \"Server stopped unexpectedly.\",\n                                \"success\": False,\n                            }\n                        if message:\n                            add_root_url(message, root_path, None)\n                            yield f\"data: {json.dumps(message)}\\n\\n\"\n                            if message[\"msg\"] == ServerMessage.process_completed:\n                                blocks._queue.pending_event_ids_session[\n                                    session_hash\n                                ].remove(message[\"event_id\"])\n                                if message[\"msg\"] == ServerMessage.server_stopped or (\n                                    message[\"msg\"] == ServerMessage.process_completed\n                                    and (\n                                        len(\n                                            blocks._queue.pending_event_ids_session[\n                                                session_hash\n                                            ]\n                                        )\n                                        == 0\n                                    )\n                                ):\n                                    return\n                except BaseException as e:\n                    message = {\n                        \"msg\": \"unexpected_error\",\n                        \"success\": False,\n                        \"message\": str(e),\n                    }\n                    yield f\"data: {json.dumps(message)}\\n\\n\"\n                    if isinstance(e, asyncio.CancelledError):\n                        del blocks._queue.pending_messages_per_session[session_hash]\n                        await blocks._queue.clean_events(session_hash=session_hash)\n                    raise e\n            return StreamingResponse(\n                sse_stream(request),\n                media_type=\"text/event-stream\",\n            )\n        @app.post(\"/queue/join\", dependencies=[Depends(login_check)])\n        async def queue_join(\n            body: PredictBody,\n            request: fastapi.Request,\n            username: str = Depends(get_current_user),\n        ):\n            blocks = app.get_blocks()\n            if blocks._queue.server_app is None:\n                blocks._queue.set_server_app(app)\n            if blocks._queue.stopped:\n                raise HTTPException(\n                    status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n                    detail=\"Queue is stopped.\",\n                )\n            success, event_id = await blocks._queue.push(body, request, username)\n            if not success:\n                status_code = (\n                    status.HTTP_503_SERVICE_UNAVAILABLE\n                    if \"Queue is full.\" in event_id\n                    else status.HTTP_400_BAD_REQUEST\n                )\n                raise HTTPException(status_code=status_code, detail=event_id)\n            return {\"event_id\": event_id}\n        @app.post(\"/component_server\", dependencies=[Depends(login_check)])\n        @app.post(\"/component_server/\", dependencies=[Depends(login_check)])\n        def component_server(body: ComponentServerBody):\n            state = app.state_holder[body.session_hash]\n            component_id = body.component_id\n            block: Block\n            if component_id in state:\n                block = state[component_id]\n            else:\n                block = app.get_blocks().blocks[component_id]\n            fn = getattr(block, body.fn_name, None)\n            if fn is None or not getattr(fn, \"_is_server_fn\", False):\n                raise HTTPException(\n                    status_code=status.HTTP_404_NOT_FOUND,\n                    detail=\"Function not found.\",\n                )\n            return fn(body.data)\n        @app.get(\n            \"/queue/status\",\n            dependencies=[Depends(login_check)],\n            response_model=Estimation,\n        )\n        async def get_queue_status():\n            return app.get_blocks()._queue.get_status()\n        @app.get(\"/upload_progress\")\n        def get_upload_progress(upload_id: str, request: fastapi.Request):\n            async def sse_stream(request: fastapi.Request):\n                last_heartbeat = time.perf_counter()\n                is_done = False\n                while True:\n                    if await request.is_disconnected():\n                        file_upload_statuses.stop_tracking(upload_id)\n                        return\n                    if is_done:\n                        file_upload_statuses.stop_tracking(upload_id)\n                        return\n                    heartbeat_rate = 15\n                    check_rate = 0.05\n                    try:\n                        if file_upload_statuses.is_done(upload_id):\n                            message = {\"msg\": \"done\"}\n                            is_done = True\n                        else:\n                            update = file_upload_statuses.pop(upload_id)\n                            message = {\n                                \"msg\": \"update\",\n                                \"orig_name\": update.filename,\n                                \"chunk_size\": update.chunk_size,\n                            }\n                        yield f\"data: {json.dumps(message)}\\n\\n\"\n                    except FileUploadProgressNotTrackedError:\n                        return\n                    except FileUploadProgressNotQueuedError:\n                        await asyncio.sleep(check_rate)\n                        if time.perf_counter() - last_heartbeat > heartbeat_rate:\n                            message = {\"msg\": \"heartbeat\"}\n                            yield f\"data: {json.dumps(message)}\\n\\n\"\n                            last_heartbeat = time.perf_counter()\n            return StreamingResponse(\n                sse_stream(request),\n                media_type=\"text/event-stream\",\n            )\n        @app.post(\"/upload\", dependencies=[Depends(login_check)])\n        async def upload_file(\n            request: fastapi.Request,\n            bg_tasks: BackgroundTasks,\n            upload_id: Optional[str] = None,\n        ):\n            content_type_header = request.headers.get(\"Content-Type\")\n            content_type: bytes\n            content_type, _ = parse_options_header(content_type_header)\n            if content_type != b\"multipart/form-data\":\n                raise HTTPException(status_code=400, detail=\"Invalid content type.\")\n            try:\n                if upload_id:\n                    file_upload_statuses.track(upload_id)\n                multipart_parser = GradioMultiPartParser(\n                    request.headers,\n                    request.stream(),\n                    max_files=1000,\n                    max_fields=1000,\n                    upload_id=upload_id if upload_id else None,\n                    upload_progress=file_upload_statuses if upload_id else None,\n                )\n                form = await multipart_parser.parse()\n            except MultiPartException as exc:\n                raise HTTPException(status_code=400, detail=exc.message) from exc\n            output_files = []\n            files_to_copy = []\n            locations: list[str] = []\n            for temp_file in form.getlist(\"files\"):\n                assert isinstance(temp_file, GradioUploadFile)\n                if temp_file.filename:\n                    file_name = Path(temp_file.filename).name\n                    name = client_utils.strip_invalid_filename_characters(file_name)\n                else:\n                    name = f\"tmp{secrets.token_hex(5)}\"\n                directory = Path(app.uploaded_file_dir) / temp_file.sha.hexdigest()\n                directory.mkdir(exist_ok=True, parents=True)\n                dest = (directory / name).resolve()\n                temp_file.file.close()\n                try:\n                    os.rename(temp_file.file.name, dest)\n                except OSError:\n                    files_to_copy.append(temp_file.file.name)\n                    locations.append(str(dest))\n                output_files.append(dest)\n            if files_to_copy:\n                bg_tasks.add_task(\n                    move_uploaded_files_to_cache, files_to_copy, locations\n                )\n            return output_files\n        @app.on_event(\"startup\")\n        @app.get(\"/startup-events\")\n        async def startup_events():\n            if not app.startup_events_triggered:\n                app.get_blocks().startup_events()\n                app.startup_events_triggered = True\n                return True\n            return False\n        @app.get(\"/theme.css\", response_class=PlainTextResponse)\n        def theme_css():\n            return PlainTextResponse(app.get_blocks().theme_css, media_type=\"text/css\")\n        @app.get(\"/robots.txt\", response_class=PlainTextResponse)\n        def robots_txt():\n            if app.get_blocks().share:\n                return \"User-agent: *\\nDisallow: /\"\n            else:\n                return \"User-agent: *\\nDisallow: \"\n        return app\ndef update_root_in_config(config: dict, root: str) -> dict:\n    \"\"\"\n    Updates the root \"key\" in the config dictionary to the new root url. If the\n    root url has changed, all of the urls in the config that correspond to component\n    file urls are updated to use the new root url.\n    \"\"\"\n    previous_root = config.get(\"root\", None)\n    if previous_root is None or previous_root != root:\n        config[\"root\"] = root\n        config = processing_utils.add_root_url(config, root, previous_root)\n    return config",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-1729",
        "description": "[{'lang': 'en', 'value': 'A timing attack vulnerability exists in the gradio-app/gradio repository, specifically within the login function in routes.py. The vulnerability arises from the use of a direct comparison operation (`app.auth[username] == password`) to validate user credentials, which can be exploited to guess passwords based on response times. Successful exploitation of this vulnerability could allow an attacker to bypass authentication mechanisms and gain unauthorized access.'}]",
        "cwe_number": 367
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-220",
      "code": "def sign_in():\n  if session.get('signed_in') != None: return redirect('/')\n  authorize_url = 'https://stage-id.valtech.com/oauth2/authorize?response_type=%s&client_id=%s&scope=%s' % ('code', CLIENT_ID, 'email openid')\n  return redirect(authorize_url)\ndef sign_in_callback():\n  code = request.args.get('code')\n  tokens = exchange_code_for_tokens(code)\n  user_info = jwt.decode(tokens[\"id_token\"], verify=False)\n  session['signed_in'] = True\n  session['email'] = user_info['email']\n  return redirect('/')\ndef sign_out():\n  session.clear()\n  return redirect('https://stage-id.valtech.com/oidc/end-session?client_id=%s' % CLIENT_ID)\ndef exchange_code_for_tokens(code):\n  data = {\n    'grant_type': 'authorization_code',\n    'code': code,\n    'client_id': CLIENT_ID,\n    'client_secret': CLIENT_SECRET\n  }\n  res = requests.post('https://stage-id.valtech.com/oauth2/token', data=data)\n  return res.json()\ndef fetch_user_info(access_token):\n  res = requests.get('https://stage-id.valtech.com/api/users/me', headers={ 'Authorization': 'Bearer %s' % access_token })\n  return res.json()",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2014-125028",
        "description": "[{'lang': 'en', 'value': 'A vulnerability was found in valtech IDP Test Client and classified as problematic. Affected by this issue is some unknown functionality of the file python-flask/main.py. The manipulation leads to cross-site request forgery. The attack may be launched remotely. The name of the patch is f1e7b3d431c8681ec46445557125890c14fa295f. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-217148.'}]",
        "cwe_number": 352
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-221",
      "code": "def get_parser():\n    parser = configargparse.ArgumentParser(\n        prog='rdiffweb',\n        description='Web interface to browse and restore rdiff-backup repositories.',\n        default_config_files=['/etc/rdiffweb/rdw.conf', '/etc/rdiffweb/rdw.conf.d/*.conf'],\n        add_env_var_help=True,\n        auto_env_var_prefix='RDIFFWEB_',\n        config_file_parser_class=ConfigFileParser,\n        conflict_handler='resolve',\n    )\n    parser.add_argument(\n        '-f', '--config', is_config_file=True, metavar='FILE', help='location of Rdiffweb configuration file'\n    )\n    parser.add(\n        '--database-uri',\n        '--sqlitedb-file',\n        '--sqlitedbfile',\n        metavar='URI',\n        help=\"\"\"Location of the database used for persistence. SQLite and PostgreSQL\n            database are supported officially. To use a SQLite database you may\n            define the location using a file path or a URI.\n            e.g.: /srv/rdiffweb/file.db or sqlite:///srv/rdiffweb/file.db`.\n            To use PostgreSQL server you must provide\n            a URI similar to postgresql://user:pass@10.255.1.34/dbname and you\n            must install required dependencies.\n            By default, Rdiffweb uses a SQLite embedded database located at\n            /etc/rdiffweb/rdw.db.\"\"\",\n        default='/etc/rdiffweb/rdw.db',\n    )\n    parser.add_argument(\n        '-d',\n        '--debug',\n        action='store_true',\n        help='enable rdiffweb debug mode - change the log level to DEBUG, print exception stack trace to the web interface and show SQL query in logs',\n    )\n    parser.add_argument(\n        '--admin-user',\n        '--adminuser',\n        metavar='USERNAME',\n        help='administrator username. The administrator user get created on startup if the database is empty.',\n        default='admin',\n    )\n    parser.add_argument(\n        '--admin-password',\n        metavar='USERNAME',\n        help=\"\"\"administrator encrypted password as SSHA. Read online\n            documentation to know more about how to encrypt your password\n            into SSHA or use http://projects.marsching.org/weave4j/util/genpassword.php\n            When defined, administrator password cannot be updated using the web interface.\n            When undefined, default administrator password is `admin123` and\n            it can be updated using the web interface.\"\"\",\n    )\n    parser.add_argument(\n        '--default-theme',\n        '--defaulttheme',\n        help='define the default theme. Either: default, blue, orange or custom. Define a default set of colors and font for the web interface. Also read more about link-color, navbar-cloor and font-family.',\n        choices=['default', 'blue', 'orange', 'custom'],\n        default='default',\n    )\n    parser.add_argument(\n        '--environment',\n        choices=['development', 'production'],\n        help='define the type of environment: development, production. This is used to limit the information shown to the user when an error occur.',\n        default='production',\n    )\n    parser.add_argument(\n        '--email-encryption',\n        '--emailencryption',\n        choices=['none', 'ssl', 'starttls'],\n        help='type of encryption to be used when establishing communication with SMTP server. Default: none',\n        default='none',\n    )\n    parser.add_argument(\n        '--email-host',\n        '--emailhost',\n        metavar='HOST',\n        help='SMTP server used to send email in the form <host>:<port>. If the port is not provided, default to standard port 25 or 465 is used. e.g.: smtp.gmail.com:587',\n    )\n    parser.add_argument(\n        '--email-sender',\n        '--emailsender',\n        metavar='EMAIL',\n        help='email addres used for the `from:` field when sending email.',\n    )\n    parser.add_argument(\n        '--email-notification-time',\n        '--emailnotificationtime',\n        metavar='TIME',\n        help='time when the email notifcation should be sent for inactive backups. e.g.: 22:00 Default value: 23:00',\n        default='23:00',\n    )\n    parser.add_argument(\n        '--email-username',\n        '--emailusername',\n        metavar='USERNAME',\n        help='username used for authentication with the SMTP server.',\n    )\n    parser.add_argument(\n        '--email-password',\n        '--emailpassword',\n        metavar='PASSWORD',\n        help='password used for authentication with the SMTP server.',\n    )\n    parser.add_argument(\n        '--email-send-changed-notification',\n        '--emailsendchangednotification',\n        help='True to send notification when sensitive information get change in user profile.',\n        action='store_true',\n        default=True,\n    )\n    parser.add_argument(\n        '--brand-favicon',\n        '--favicon',\n        dest='favicon',\n        help='location of an icon to be used as a favicon displayed in web browser.',\n    )\n    parser.add_argument(\n        '--footer-name',\n        '--footername',\n        help=argparse.SUPPRESS,\n        default='rdiffweb',\n    )\n    parser.add_argument(\n        '--footer-url',\n        '--footerurl',\n        help=argparse.SUPPRESS,\n        default='https://rdiffweb.org/',\n    )\n    parser.add_argument(\n        '--brand-logo',\n        '--logo',\n        dest='logo',\n        help='location of an image (preferably a .svg) to be used as a replacement for the rdiffweb logo displayed in Login page.',\n    )\n    parser.add_argument(\n        '--brand-header-logo',\n        '--header-logo',\n        '--headerlogo',\n        dest='header_logo',\n        help='location of an image (preferably a .svg) to be used as a replacement for the rdiffweb header logo displayed in navigation bar.',\n    )\n    parser.add_argument(\n        '--brand-header-name',\n        '--header-name',\n        '--headername',\n        dest='header_name',\n        help='application name displayed in the title bar and header menu.',\n        default='Rdiffweb',\n    )\n    parser.add_argument(\n        '--brand-link-color',\n        '--link-color',\n        type=css_color,\n        dest='link_color',\n        help='define a CSS color to be used for link. e.g.: ff0000',\n    )\n    parser.add_argument(\n        '--brand-btn-bg-color',\n        '--btn-bg-color',\n        type=css_color,\n        dest='btn_bg_color',\n        help=\"define a CSS color to be used for button's background. Default to `link-color` if undefined\",\n    )\n    parser.add_argument(\n        '--brand-btn-fg-color',\n        '--btn-fg-color',\n        type=css_color,\n        dest='btn_fg_color',\n        help=\"define a CSS color to be used for button's text. Default to white if undefined\",\n    )\n    parser.add_argument(\n        '--brand-btn-rounded',\n        '--btn-rounded',\n        type=bool,\n        dest='btn_rounded',\n        help='define if the button should be rounded',\n    )\n    parser.add_argument(\n        '--brand-navbar-color',\n        '--navbar-color',\n        type=css_color,\n        dest='navbar_color',\n        help='define a CSS color to be used for navigation bar background e.g.: 00ff00',\n    )\n    parser.add_argument(\n        '--brand-font-family',\n        '--font-family',\n        type=css_font,\n        dest='font_family',\n        help='define a CSS font to be used as main font. e.g.: Roboto',\n    )\n    parser.add_argument(\n        '--ldap-add-missing-user',\n        '--addmissinguser',\n        action='store_true',\n        help='enable creation of users from LDAP when the credential are valid.',\n        default=False,\n    )\n    parser.add_argument(\n        '--ldap-add-user-default-role',\n        help='default role used when creating users from LDAP. This parameter is only useful when `--ldap-add-missing-user` is enabled.',\n        default='user',\n        choices=['admin', 'maintainer', 'user'],\n    )\n    parser.add_argument(\n        '--ldap-add-user-default-userroot',\n        help='default user root directory used when creating users from LDAP. LDAP attributes may be used to define the default location. e.g.: `/backups/{uid[0]}/`. This parameter is only useful when `--ldap-add-missing-user` is enabled.',\n        default='',\n    )\n    parser.add_argument(\n        '--ldap-uri',\n        '--ldapuri',\n        help='URL to the LDAP server used to validate user credentials. e.g.: ldap://localhost:389',\n    )\n    parser.add_argument(\n        '--ldap-base-dn',\n        '--ldapbasedn',\n        metavar='DN',\n        help='DN of the branch of the directory where all searches should start from. e.g.: dc=my,dc=domain',\n        default=\"\",\n    )\n    parser.add_argument(\n        '--ldap-scope',\n        '--ldapscope',\n        help='scope of the search. Can be either base, onelevel or subtree',\n        choices=['base', 'onelevel', 'subtree'],\n        default=\"subtree\",\n    )\n    parser.add_argument('--ldap-tls', '--ldaptls', action='store_true', help='enable TLS')\n    parser.add_argument(\n        '--ldap-username-attribute',\n        '--ldapattribute',\n        metavar='ATTRIBUTE',\n        help=\"The attribute to search username. If no attributes are provided, the default is to use `uid`. It's a good idea to choose an attribute that will be unique across all entries in the subtree you will be using.\",\n        default='uid',\n    )\n    parser.add_argument(\n        '--ldap-filter',\n        '--ldapfilter',\n        help=\"search filter to limit LDAP lookup. If not provided, defaults to (objectClass=*), which searches for all objects in the tree.\",\n        default='(objectClass=*)',\n    )\n    parser.add_argument(\n        '--ldap-required-group',\n        '--ldaprequiredgroup',\n        metavar='GROUPNAME',\n        help=\"name of the group of which the user must be a member to access rdiffweb. Should be used with ldap-group-attribute and ldap-group-attribute-is-dn.\",\n    )\n    parser.add_argument(\n        '--ldap-group-attribute',\n        '--ldapgroupattribute',\n        metavar='ATTRIBUTE',\n        help=\"name of the attribute defining the groups of which the user is a member. Should be used with ldap-required-group and ldap-group-attribute-is-dn.\",\n        default='member',\n    )\n    parser.add_argument(\n        '--ldap-group-attribute-is-dn',\n        '--ldapgroupattributeisdn',\n        help=\"True if the content of the attribute `ldap-group-attribute` is a DN.\",\n        action='store_true',\n    )\n    parser.add_argument(\n        '--ldap-bind-dn',\n        '--ldapbinddn',\n        metavar='DN',\n        help=\"optional DN used to bind to the server when searching for entries. If not provided, will use an anonymous bind.\",\n        default=\"\",\n    )\n    parser.add_argument(\n        '--ldap-bind-password',\n        '--ldapbindpassword',\n        metavar='PASSWORD',\n        help=\"password to use in conjunction with LdapBindDn. Note that the bind password is probably sensitive data, and should be properly protected. You should only use the LdapBindDn and LdapBindPassword if you absolutely need them to search the directory.\",\n        default=\"\",\n    )\n    parser.add_argument(\n        '--ldap-version',\n        '--ldapversion',\n        '--ldapprotocolversion',\n        help=\"version of LDAP in use either 2 or 3. Default to 3.\",\n        default=3,\n        type=int,\n        choices=[2, 3],\n    )\n    parser.add_argument(\n        '--ldap-network-timeout',\n        '--ldapnetworktimeout',\n        metavar='SECONDS',\n        help=\"timeout in seconds value used for LDAP connection\",\n        default=100,\n        type=int,\n    )\n    parser.add_argument(\n        '--ldap-timeout',\n        '--ldaptimeout',\n        metavar='SECONDS',\n        help=\"timeout in seconds value used for LDAP request\",\n        default=300,\n        type=int,\n    )\n    parser.add_argument(\n        '--ldap-encoding',\n        '--ldapencoding',\n        metavar='ENCODING',\n        help=\"encoding used by your LDAP server.\",\n        default=\"utf-8\",\n    )\n    parser.add_argument(\n        '--log-access-file', '--logaccessfile', metavar='FILE', help='location of Rdiffweb log access file.'\n    )\n    parser.add_argument(\n        '--log-file',\n        '--logfile',\n        metavar='FILE',\n        help='location of Rdiffweb log file. Print log to the console if not define in config file.',\n    )\n    parser.add_argument(\n        '--log-level',\n        '--loglevel',\n        help='Define the log level.',\n        choices=['ERROR', 'WARN', 'INFO', 'DEBUG'],\n        default='INFO',\n    )\n    parser.add_argument(\n        '--max-depth',\n        '--maxdepth',\n        metavar='DEPTH',\n        help=\"define the maximum folder depthness to search into the user's root directory to find repositories. This is commonly used if you repositories are organised with multiple sub-folder.\",\n        type=int,\n        default=3,\n    )\n    parser.add('--quota-set-cmd', '--quotasetcmd', metavar='COMMAND', help=\"command line to set the user's quota.\")\n    parser.add('--quota-get-cmd', '--quotagetcmd', metavar='COMMAND', help=\"command line to get the user's quota.\")\n    parser.add(\n        '--quota-used-cmd', '--quotausedcmd', metavar='COMMAND', help=\"Command line to get user's quota disk usage.\"\n    )\n    parser.add(\n        '--remove-older-time',\n        '--removeoldertime',\n        metavar='TIME',\n        help=\"Time when to execute the remove older scheduled job. e.g.: 22:30\",\n        default='23:00',\n    )\n    parser.add('--server-host', '--serverhost', metavar='IP', default='127.0.0.1', help='IP address to listen to')\n    parser.add(\n        '--server-port',\n        '--serverport',\n        metavar='PORT',\n        help='port to listen to for HTTP request',\n        default='8080',\n        type=int,\n    )\n    parser.add(\n        '--rate-limit-dir',\n        '--session-dir',\n        '--sessiondir',\n        metavar='FOLDER',\n        help='location where to store rate-limit information. When undefined, the data is kept in memory. `--session-dir` are deprecated and kept for backward compatibility.',\n    )\n    parser.add(\n        '--rate-limit',\n        metavar='LIMIT',\n        type=int,\n        default=20,\n        help='maximum number of requests per hour that can be made on sensitive endpoints. When this limit is reached, an HTTP 429 message is returned to the user or the user is logged out. This security measure is used to limit brute force attacks on the login page and the RESTful API.',\n    )\n    parser.add(\n        '--session-idle-timeout',\n        metavar='MINUTES',\n        help='This timeout defines the amount of time a session will remain active in case there is no activity in the session. User Session will be revoke after this period of inactivity, unless the user selected \"remember me\". Default 5 minutes.',\n        default=5,\n    )\n    parser.add(\n        '--session-absolute-timeout',\n        metavar='MINUTES',\n        help='This timeout defines the maximum amount of time a session can be active. After this period, user is forced to (re)authenticate, unless the user selected \"remember me\". Default 20 minutes.',\n        default=20,\n    )\n    parser.add(\n        '--session-persistent-timeout',\n        metavar='MINUTES',\n        help='This timeout defines the maximum amount of time to remember and trust a user device. This timeout is used when user select \"remember me\". Default 30 days.',\n        default=43200,\n    )\n    parser.add(\n        '--ssl-certificate',\n        '--sslcertificate',\n        metavar='CERT',\n        help='location of the SSL Certification to enable HTTPS (not recommended)',\n    )\n    parser.add(\n        '--ssl-private-key',\n        '--sslprivatekey',\n        metavar='KEY',\n        help='location of the SSL Private Key to enable HTTPS (not recommended)',\n    )\n    parser.add(\n        '--tempdir',\n        metavar='FOLDER',\n        help='alternate temporary folder to be used when restoring files. Might be useful if the default location has limited disk space. Default to TEMPDIR environment or `/tmp`.',\n    )\n    parser.add(\n        '--disable-ssh-keys',\n        action='store_true',\n        help='used to hide SSH Key management to avoid users to add or remove SSH Key using the web application',\n        default=False,\n    )\n    parser.add(\n        '--password-min-length',\n        type=int,\n        help=\"Minimum length of the user's password\",\n        default=8,\n    )\n    parser.add(\n        '--password-max-length',\n        type=int,\n        help=\"Maximum length of the user's password\",\n        default=128,\n    )\n    parser.add(\n        '--password-score',\n        type=lambda x: max(1, min(int(x), 4)),\n        help=\"Minimum zxcvbn's score for password. Value from 1 to 4. Default value 2. Read more about it here: https://github.com/dropbox/zxcvbn\",\n        default=2,\n    )\n    parser.add_argument('--version', action='version', version='%(prog)s ' + VERSION)\n    flags = ['--welcome-msg'] + ['--welcome-msg-' + i for i in ['ca', 'en', 'es', 'fr', 'ru']] + ['--welcomemsg']\n    parser.add_argument(\n        *flags,\n        metavar='HTML',\n        help='replace the welcome message displayed in the login page for default locale or for a specific locale',\n        action=LocaleAction\n    )\n    return parser\ndef check_ratelimit(\n    delay=3600, limit=25, return_status=429, logout=False, scope=None, methods=None, debug=False, hit=1, **conf\n):\n    \"\"\"\n    Verify the ratelimit. By default return a 429 HTTP error code (Too Many Request). After 25 request within the same hour.\n    Arguments:\n        delay:         Time window for analysis in seconds. Default per hour (3600 seconds)\n        limit:         Number of request allowed for an entry point. Default 25\n        return_status: HTTP Error code to return.\n        logout:        True to logout user when limit is reached\n        scope:         if specify, define the scope of rate limit. Default to path_info.\n        methods:       if specify, only the methods in the list will be rate limited.\n    \"\"\"\n    assert delay > 0, 'invalid delay'\n    if limit <= 0:\n        return\n    request = cherrypy.request\n    if methods is not None and request.method not in methods:\n        if debug:\n            cherrypy.log(\n                'skip rate limit for HTTP method %s' % (request.method,),\n                'TOOLS.RATELIMIT',\n            )\n        return\n    datastore = getattr(cherrypy.request.app, '_ratelimit_datastore', None)\n    if datastore is None:\n        storage_class = conf.get('storage_class', RamRateLimit)\n        datastore = storage_class(**conf)\n        cherrypy.request.app._ratelimit_datastore = datastore\n    token = (request.login or request.remote.ip) + '.' + (scope or request.path_info)\n    hits = datastore.get_and_increment(token, delay, hit)\n    if debug:\n        cherrypy.log(\n            'check and increase rate limit for scope %s, limit %s, hits %s' % (token, limit, hits), 'TOOLS.RATELIMIT'\n        )\n    if limit <= hits:\n        if logout:\n            if hasattr(cherrypy.serving, 'session'):\n                cherrypy.serving.session.clear()\n            raise cherrypy.HTTPRedirect(\"/\")\n        raise cherrypy.HTTPError(return_status)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-4723",
        "description": "[{'lang': 'en', 'value': 'Allocation of Resources Without Limits or Throttling in GitHub repository ikus060/rdiffweb prior to 2.5.5.'}]",
        "cwe_number": 770
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-222",
      "code": "    def rebuild_proxies(self, prepared_request, proxies):\n        \"\"\"This method re-evaluates the proxy configuration by considering the\n        environment variables. If we are redirected to a URL covered by\n        NO_PROXY, we strip the proxy configuration. Otherwise, we set missing\n        proxy keys for this URL (in case they were stripped by a previous\n        redirect).\n        This method also replaces the Proxy-Authorization header where\n        necessary.\n        :rtype: dict\n        \"\"\"\n        headers = prepared_request.headers\n        scheme = urlparse(prepared_request.url).scheme\n        new_proxies = resolve_proxies(prepared_request, proxies, self.trust_env)\n        if \"Proxy-Authorization\" in headers:\n            del headers[\"Proxy-Authorization\"]\n        try:\n            username, password = get_auth_from_url(new_proxies[scheme])\n        except KeyError:\n            username, password = None, None\n        if username and password:\n            headers[\"Proxy-Authorization\"] = _basic_auth_str(username, password)\n        return new_proxies",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-32681",
        "description": "[{'lang': 'en', 'value': 'Requests is a HTTP library. Since Requests 2.3.0, Requests has been leaking Proxy-Authorization headers to destination servers when redirected to an HTTPS endpoint. This is a product of how we use `rebuild_proxies` to reattach the `Proxy-Authorization` header to requests. For HTTP connections sent through the tunnel, the proxy will identify the header in the request itself and remove it prior to forwarding to the destination server. However when sent over HTTPS, the `Proxy-Authorization` header must be sent in the CONNECT request as the proxy has no visibility into the tunneled request. This results in Requests forwarding proxy credentials to the destination server unintentionally, allowing a malicious actor to potentially exfiltrate sensitive information. This issue has been patched in version 2.31.0.\\n\\n'}]",
        "cwe_number": 200
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-223",
      "code": "class FractionalAvgPoolGradTest(test.TestCase):",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-21730",
        "description": "[{'lang': 'en', 'value': 'Tensorflow is an Open Source Machine Learning Framework. The implementation of `FractionalAvgPoolGrad` does not consider cases where the input tensors are invalid allowing an attacker to read from outside of bounds of heap. The fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.'}]",
        "cwe_number": 125
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-224",
      "code": "def _start_program(args: List[str], local_env: Dict[str, str]) -> subprocess.Popen:\n    \"\"\"\n    Start the program where the path is the first item of the ``args`` array argument.\n    Parameters\n    ----------\n    args : List[str]\n        List of arguments to be passed to the program. The first list's item shall\n        be the program path.\n    local_env : Dict[str,str]\n        Environment variables to be passed to the program.\n    Returns\n    -------\n    subprocess.Popen\n        The subprocess object.\n    \"\"\"\n    return subprocess.Popen(\n        args,\n        shell=os.name != \"nt\",\n        stdin=subprocess.DEVNULL,\n        stdout=subprocess.DEVNULL,\n        stderr=subprocess.DEVNULL,\n        env=local_env,\n    )",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-29189",
        "description": "[{'lang': 'en', 'value': 'PyAnsys Geometry is a Python client library for the Ansys Geometry service and other CAD Ansys products. On file src/ansys/geometry/core/connection/product_instance.py, upon calling this method _start_program directly, users could exploit its usage to perform malicious operations on the current machine where the script is ran. This vulnerability is fixed in 0.3.3 and 0.4.12.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-225",
      "code": "    def __call__(self, request):\n        file_fields = request.POST.getlist(\"s3file\")\n        for field_name in file_fields:\n            paths = request.POST.getlist(field_name)\n            request.FILES.setlist(field_name, list(self.get_files_from_storage(paths)))\n        if local_dev and request.path == \"/__s3_mock__/\":\n            return views.S3MockView.as_view()(request)\n        return self.get_response(request)\n    def get_files_from_storage(paths):\n        \"\"\"Return S3 file where the name does not include the path.\"\"\"\n        for path in paths:\n            path = pathlib.PurePosixPath(path)\n            try:\n                location = storage.aws_location\n            except AttributeError:\n                location = storage.location\n            try:\n                f = storage.open(str(path.relative_to(location)))\n                f.name = path.name\n                yield f\n            except (OSError, ValueError):\n                logger.exception(\"File not found: %s\", path)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-24840",
        "description": "[{'lang': 'en', 'value': 'django-s3file is a lightweight file upload input for Django and Amazon S3 . In versions prior to 5.5.1 it was possible to traverse the entire AWS S3 bucket and in most cases to access or delete files. If the `AWS_LOCATION` setting was set, traversal was limited to that location only. The issue was discovered by the maintainer. There were no reports of the vulnerability being known to or exploited by a third party, prior to the release of the patch. The vulnerability has been fixed in version 5.5.1 and above. There is no feasible workaround. We must urge all users to immediately updated to a patched version.'}]",
        "cwe_number": 22
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-226",
      "code": "    def test_proxy_url_forgery(self):\n        \"\"\"The GeoNode Proxy should preserve the original request headers.\"\"\"\n        import geonode.proxy.views\n        from urllib.parse import urlsplit\n        class Response:\n            status_code = 200\n            content = \"Hello World\"\n            headers = {\n                \"Content-Type\": \"text/plain\",\n                \"Vary\": \"Authorization, Accept-Language, Cookie, origin\",\n                \"X-Content-Type-Options\": \"nosniff\",\n                \"X-XSS-Protection\": \"1; mode=block\",\n                \"Referrer-Policy\": \"same-origin\",\n                \"X-Frame-Options\": \"SAMEORIGIN\",\n                \"Content-Language\": \"en-us\",\n                \"Content-Length\": \"119\",\n                \"Content-Disposition\": 'attachment; filename=\"filename.tif\"',\n            }\n        request_mock = MagicMock()\n        request_mock.return_value = (Response(), None)\n        geonode.proxy.views.http_client.request = request_mock\n        url = f\"http://example.org\\\\@%23{urlsplit(settings.SITEURL).hostname}\"\n        response = self.client.get(f\"{self.proxy_url}?url={url}\")\n        self.assertEqual(response.status_code, 403)\n        url = f\"http://125.126.127.128\\\\@%23{urlsplit(settings.SITEURL).hostname}\"\n        response = self.client.get(f\"{self.proxy_url}?url={url}\")\n        self.assertEqual(response.status_code, 403)\n        url = f\"/\\\\@%23{urlsplit(settings.SITEURL).hostname}\"\n        response = self.client.get(f\"{self.proxy_url}?url={url}\")\n        self.assertEqual(response.status_code, 200)\n        url = f\"{settings.SITEURL}\\\\@%23{urlsplit(settings.SITEURL).hostname}\"\n        response = self.client.get(f\"{self.proxy_url}?url={url}\")\n        self.assertEqual(response.status_code, 200)\nclass DownloadResourceTestCase(GeoNodeBaseTestSupport):\n    def setUp(self):\n        super().setUp()\n        self.maxDiff = None\n        create_models(type=\"dataset\")\n    def test_download_url_with_not_existing_file(self):\n        dataset = Dataset.objects.all().first()\n        self.client.login(username=\"admin\", password=\"admin\")\n        response = self.client.get(reverse(\"download\", args=(dataset.id,)))\n        self.assertEqual(response.status_code, 404)\n        content = response.content\n        if isinstance(content, bytes):\n            content = content.decode(\"UTF-8\")\n        data = content\n        self.assertTrue(\"No files have been found for this resource. Please, contact a system administrator.\" in data)\ndef extract_ip_or_domain(url):\n    ip_regex = re.compile(\"^(?:http://|https://)(\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3})\")\n    domain_regex = re.compile(\"^(?:http://|https://)([a-zA-Z0-9.-]+)\")\n    match = ip_regex.findall(url)\n    if len(match):\n        ip_address = match[0]\n        try:\n            ipaddress.ip_address(ip_address)\n            return ip_address\n        except ValueError:\n            pass\n    match = domain_regex.findall(url)\n    if len(match):\n        return match[0]\n    return None\ndef get_xpath_value(\n    element: etree.Element, xpath_expression: str, nsmap: typing.Optional[dict] = None\n) -> typing.Optional[str]:\n    if not nsmap:\n        nsmap = element.nsmap\n    values = element.xpath(f\"{xpath_expression}//text()\", namespaces=nsmap)\n    return \"\".join(values).strip() or None\ndef get_geonode_app_types():\n    from geonode.geoapps.models import GeoApp\n    return list(set(GeoApp.objects.values_list(\"resource_type\", flat=True)))",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-42439",
        "description": "[{'lang': 'en', 'value': 'GeoNode is an open source platform that facilitates the creation, sharing, and collaborative use of geospatial data. A SSRF vulnerability exists starting in version 3.2.0, bypassing existing controls on the software. This can allow a user to request internal services for a full read SSRF, returning any data from the internal network. The application is using a whitelist, but the whitelist can be bypassed. The bypass will trick the application that the first host is a whitelisted address, but the browser will use `@` or `%40` as a credential to the host geoserver on port 8080, this will return the data to that host on the response. Version 4.1.3.post1 is the first available version that contains a patch.'}]",
        "cwe_number": 918
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-227",
      "code": "    async def on_exchange_third_party_invite_request(\n        self, room_id: str, event_dict: JsonDict\n    ) -> None:\n        \"\"\"Handle an exchange_third_party_invite request from a remote server\n        The remote server will call this when it wants to turn a 3pid invite\n        into a normal m.room.member invite.\n        Args:\n            room_id: The ID of the room.\n            event_dict (dict[str, Any]): Dictionary containing the event body.\n        \"\"\"\n        room_version = await self.store.get_room_version_id(room_id)\n        builder = self.event_builder_factory.new(room_version, event_dict)\n        event, context = await self.event_creation_handler.create_new_client_event(\n            builder=builder\n        )\n        event, context = await self.add_display_name_to_third_party_invite(\n            room_version, event_dict, event, context\n        )\n        try:\n            await self.auth.check_from_context(room_version, event, context)\n        except AuthError as e:\n            logger.warning(\"Denying third party invite %r because %s\", event, e)\n            raise e\n        await self._check_signature(event, context)\n        event.internal_metadata.send_on_behalf_of = get_domain_from_id(event.sender)\n        member_handler = self.hs.get_room_member_handler()\n        await member_handler.send_membership_event(None, event, context)\n    async def on_context_state_request(\n        self, origin: str, room_id: str, event_id: str\n    ) -> Tuple[int, Dict[str, Any]]:\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, room_id)\n        in_room = await self.auth.check_host_in_room(room_id, origin)\n        if not in_room:\n            raise AuthError(403, \"Host not in room.\")\n        with (await self._server_linearizer.queue((origin, room_id))):\n            resp = dict(\n                await self._state_resp_cache.wrap(\n                    (room_id, event_id),\n                    self._on_context_state_request_compute,\n                    room_id,\n                    event_id,\n                )\n            )\n        room_version = await self.store.get_room_version_id(room_id)\n        resp[\"room_version\"] = room_version\n        return 200, resp\n    async def on_send_join_request(\n        self, origin: str, content: JsonDict, room_id: str\n    ) -> Dict[str, Any]:\n        logger.debug(\"on_send_join_request: content: %s\", content)\n        room_version = await self.store.get_room_version(room_id)\n        pdu = event_from_pdu_json(content, room_version)\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, pdu.room_id)\n        logger.debug(\"on_send_join_request: pdu sigs: %s\", pdu.signatures)\n        pdu = await self._check_sigs_and_hash(room_version, pdu)\n        res_pdus = await self.handler.on_send_join_request(origin, pdu)\n        time_now = self._clock.time_msec()\n        return {\n            \"state\": [p.get_pdu_json(time_now) for p in res_pdus[\"state\"]],\n            \"auth_chain\": [p.get_pdu_json(time_now) for p in res_pdus[\"auth_chain\"]],\n        }\n    async def on_send_leave_request(\n        self, origin: str, content: JsonDict, room_id: str\n    ) -> dict:\n        logger.debug(\"on_send_leave_request: content: %s\", content)\n        room_version = await self.store.get_room_version(room_id)\n        pdu = event_from_pdu_json(content, room_version)\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, pdu.room_id)\n        logger.debug(\"on_send_leave_request: pdu sigs: %s\", pdu.signatures)\n        pdu = await self._check_sigs_and_hash(room_version, pdu)\n        await self.handler.on_send_leave_request(origin, pdu)\n        return {}\n    async def on_exchange_third_party_invite_request(\n        self, room_id: str, event_dict: Dict\n    ):\n        ret = await self.handler.on_exchange_third_party_invite_request(\n            room_id, event_dict\n        )\n        return ret\n    async def on_GET(self, origin, content, query, context):\n        return await self.handler.on_context_state_request(\n            origin,\n            context,\n            parse_string_from_args(query, \"event_id\", None, required=False),\n        )\n    async def on_GET(self, origin, _content, query, context, user_id):\n        \"\"\"\n        Args:\n            origin (unicode): The authenticated server_name of the calling server\n            _content (None): (GETs don't have bodies)\n            query (dict[bytes, list[bytes]]): Query params from the request.\n            **kwargs (dict[unicode, unicode]): the dict mapping keys to path\n                components as specified in the path match regexp.\n        Returns:\n            Tuple[int, object]: (response code, response object)\n        \"\"\"\n        versions = query.get(b\"ver\")\n        if versions is not None:\n            supported_versions = [v.decode(\"utf-8\") for v in versions]\n        else:\n            supported_versions = [\"1\"]\n        content = await self.handler.on_make_join_request(\n            origin, context, user_id, supported_versions=supported_versions\n        )\n        return 200, content\n    async def on_GET(self, origin, content, query, context, user_id):\n        content = await self.handler.on_make_leave_request(origin, context, user_id)\n        return 200, content\n    async def on_PUT(self, origin, content, query, room_id, event_id):\n        content = await self.handler.on_send_leave_request(origin, content, room_id)\n        return 200, (200, content)\n    async def on_GET(self, origin, content, query, context, event_id):\n        return await self.handler.on_event_auth(origin, context, event_id)\n    async def on_PUT(self, origin, content, query, context, event_id):\n        content = await self.handler.on_send_join_request(origin, content, context)\n        return 200, (200, content)\n    async def on_PUT(self, origin, content, query, room_id):\n        content = await self.handler.on_exchange_third_party_invite_request(\n            room_id, content\n        )\n        return 200, content",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2020-26257",
        "description": "[{'lang': 'en', 'value': 'Matrix is an ecosystem for open federated Instant Messaging and VoIP. Synapse is a reference \"homeserver\" implementation of Matrix. A malicious or poorly-implemented homeserver can inject malformed events into a room by specifying a different room id in the path of a `/send_join`, `/send_leave`, `/invite` or `/exchange_third_party_invite` request. This can lead to a denial of service in which future events will not be correctly sent to other servers over federation. This affects any server which accepts federation requests from untrusted servers. The Matrix Synapse reference implementation before version 1.23.1 the implementation is vulnerable to this injection attack. Issue is fixed in version 1.23.1. As a workaround homeserver administrators could limit access to the federation API to trusted servers (for example via `federation_domain_whitelist`).'}]",
        "cwe_number": 400
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-228",
      "code": "    def _rule_dict_last_step(self, context, to_port=None, from_port=None,\n                                  ip_protocol=None, cidr_ip=None, user_id=None,\n                                  source_security_group_name=None,\n                                  source_security_group_owner_id=None):\n        values = {}\n        if source_security_group_name:\n            source_project_id = self._get_source_project_id(context,\n                source_security_group_owner_id)\n            source_security_group = db.security_group_get_by_name(\n                    context.elevated(),\n                    source_project_id,\n                    source_security_group_name)\n            notfound = exception.SecurityGroupNotFound\n            if not source_security_group:\n                raise notfound(security_group_id=source_security_group_name)\n            values['group_id'] = source_security_group['id']\n        elif cidr_ip:\n            cidr_ip = urllib.unquote(cidr_ip).decode()\n            if not utils.is_valid_cidr(cidr_ip):\n                raise exception.EC2APIError(_(\"Invalid CIDR\"))\n            values['cidr'] = cidr_ip\n        else:\n            values['cidr'] = '0.0.0.0/0'\n        if source_security_group_name:\n            ip_proto_upper = ip_protocol.upper() if ip_protocol else ''\n            if (ip_proto_upper == 'ICMP' and\n                from_port is None and to_port is None):\n                from_port = -1\n                to_port = -1\n            elif (ip_proto_upper in ['TCP', 'UDP'] and from_port is None\n                  and to_port is None):\n                from_port = 1\n                to_port = 65535\n        if ip_protocol and from_port is not None and to_port is not None:\n            ip_protocol = str(ip_protocol)\n            try:\n                from_port = int(from_port)\n                to_port = int(to_port)\n            except ValueError:\n                if ip_protocol.upper() == 'ICMP':\n                    raise exception.InvalidInput(reason=\"Type and\"\n                         \" Code must be integers for ICMP protocol type\")\n                else:\n                    raise exception.InvalidInput(reason=\"To and From ports \"\n                          \"must be integers\")\n            if ip_protocol.upper() not in ['TCP', 'UDP', 'ICMP']:\n                raise exception.InvalidIpProtocol(protocol=ip_protocol)\n            if (ip_protocol.upper() in ['TCP', 'UDP'] and\n                (from_port > to_port)):\n                raise exception.InvalidPortRange(from_port=from_port,\n                      to_port=to_port, msg=\"Former value cannot\"\n                                            \" be greater than the later\")\n            if (ip_protocol.upper() in ['TCP', 'UDP'] and\n                (from_port < 1 or to_port > 65535)):\n                raise exception.InvalidPortRange(from_port=from_port,\n                      to_port=to_port, msg=\"Valid TCP ports should\"\n                                           \" be between 1-65535\")\n            if (ip_protocol.upper() == \"ICMP\" and\n                (from_port < -1 or from_port > 255 or\n                to_port < -1 or to_port > 255)):\n                raise exception.InvalidPortRange(from_port=from_port,\n                      to_port=to_port, msg=\"For ICMP, the\"\n                                           \" type:code must be valid\")\n            values['protocol'] = ip_protocol\n            values['from_port'] = from_port\n            values['to_port'] = to_port\n        else:\n            if 'cidr' in values:\n                return None\n        return values\n    def _rule_args_to_dict(self, context, to_port=None, from_port=None,\n                                  parent_group_id=None, ip_protocol=None,\n                                  cidr=None, group_id=None):\n        values = {}\n        if group_id is not None:\n            try:\n                parent_group_id = int(parent_group_id)\n                group_id = int(group_id)\n            except ValueError:\n                msg = _(\"Parent or group id is not integer\")\n                raise exception.InvalidInput(reason=msg)\n            values['group_id'] = group_id\n            db.security_group_get(context, group_id)\n        elif cidr:\n            try:\n                cidr = urllib.unquote(cidr).decode()\n            except Exception:\n                raise exception.InvalidCidr(cidr=cidr)\n            if not utils.is_valid_cidr(cidr):\n                raise exception.InvalidCidr(cidr=cidr)\n            values['cidr'] = cidr\n        else:\n            values['cidr'] = '0.0.0.0/0'\n        if group_id:\n            ip_proto_upper = ip_protocol.upper() if ip_protocol else ''\n            if (ip_proto_upper == 'ICMP' and\n                from_port is None and to_port is None):\n                from_port = -1\n                to_port = -1\n            elif (ip_proto_upper in ['TCP', 'UDP'] and from_port is None\n                  and to_port is None):\n                from_port = 1\n                to_port = 65535\n        if ip_protocol and from_port is not None and to_port is not None:\n            ip_protocol = str(ip_protocol)\n            try:\n                from_port = int(from_port)\n                to_port = int(to_port)\n            except ValueError:\n                if ip_protocol.upper() == 'ICMP':\n                    raise exception.InvalidInput(reason=\"Type and\"\n                         \" Code must be integers for ICMP protocol type\")\n                else:\n                    raise exception.InvalidInput(reason=\"To and From ports \"\n                          \"must be integers\")\n            if ip_protocol.upper() not in ['TCP', 'UDP', 'ICMP']:\n                raise exception.InvalidIpProtocol(protocol=ip_protocol)\n            if (ip_protocol.upper() in ['TCP', 'UDP'] and\n                from_port > to_port):\n                raise exception.InvalidPortRange(from_port=from_port,\n                      to_port=to_port, msg=\"Former value cannot\"\n                                            \" be greater than the later\")\n            if (ip_protocol.upper() in ['TCP', 'UDP'] and\n                (from_port < 1 or to_port > 65535)):\n                raise exception.InvalidPortRange(from_port=from_port,\n                      to_port=to_port, msg=\"Valid TCP ports should\"\n                                           \" be between 1-65535\")\n            if (ip_protocol.upper() == \"ICMP\" and\n                (from_port < -1 or from_port > 255 or\n                to_port < -1 or to_port > 255)):\n                raise exception.InvalidPortRange(from_port=from_port,\n                      to_port=to_port, msg=\"For ICMP, the\"\n                                           \" type:code must be valid\")\n            values['protocol'] = ip_protocol\n            values['from_port'] = from_port\n            values['to_port'] = to_port\n        else:\n            if 'cidr' in values:\n                return None\n        return values\n    def instance_rules(self, instance, network_info):\n        network_info = self._handle_network_info_model(network_info)\n        ctxt = context.get_admin_context()\n        ipv4_rules = []\n        ipv6_rules = []\n        self._do_basic_rules(ipv4_rules, ipv6_rules, network_info)\n        self._do_dhcp_rules(ipv4_rules, network_info)\n        if FLAGS.allow_same_net_traffic:\n            self._do_project_network_rules(ipv4_rules, ipv6_rules,\n                                           network_info)\n        if FLAGS.use_ipv6:\n            self._do_ra_rules(ipv6_rules, network_info)\n        security_groups = db.security_group_get_by_instance(ctxt,\n                                                            instance['id'])\n        for security_group in security_groups:\n            rules = db.security_group_rule_get_by_security_group(ctxt,\n                                                          security_group['id'])\n            for rule in rules:\n                LOG.debug(_('Adding security group rule: %r'), rule,\n                          instance=instance)\n                if not rule.cidr:\n                    version = 4\n                else:\n                    version = netutils.get_ip_version(rule.cidr)\n                if version == 4:\n                    fw_rules = ipv4_rules\n                else:\n                    fw_rules = ipv6_rules\n                protocol = rule.protocol\n                if version == 6 and rule.protocol == 'icmp':\n                    protocol = 'icmpv6'\n                args = ['-j ACCEPT']\n                if protocol:\n                    args += ['-p', protocol]\n                if protocol in ['udp', 'tcp']:\n                    args += self._build_tcp_udp_rule(rule, version)\n                elif protocol == 'icmp':\n                    args += self._build_icmp_rule(rule, version)\n                if rule.cidr:\n                    LOG.debug('Using cidr %r', rule.cidr, instance=instance)\n                    args += ['-s', rule.cidr]\n                    fw_rules += [' '.join(args)]\n                else:\n                    if rule['grantee_group']:\n                        import nova.network\n                        nw_api = nova.network.API()\n                        for instance in rule['grantee_group']['instances']:\n                            nw_info = nw_api.get_instance_nw_info(ctxt,\n                                                                  instance)\n                            ips = [ip['address']\n                                for ip in nw_info.fixed_ips()\n                                    if ip['version'] == version]\n                            LOG.debug('ips: %r', ips, instance=instance)\n                            for ip in ips:\n                                subrule = args + ['-s %s' % ip]\n                                fw_rules += [' '.join(subrule)]\n                LOG.debug('Using fw_rules: %r', fw_rules, instance=instance)\n        ipv4_rules += ['-j $sg-fallback']\n        ipv6_rules += ['-j $sg-fallback']\n        return ipv4_rules, ipv6_rules",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2012-2654",
        "description": "[{'lang': 'en', 'value': 'The (1) EC2 and (2) OS APIs in OpenStack Compute (Nova) Folsom (2012.2), Essex (2012.1), and Diablo (2011.3) do not properly check the protocol when security groups are created and the network protocol is not specified entirely in lowercase, which allows remote attackers to bypass intended access restrictions.'}]",
        "cwe_number": 20
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-229",
      "code": "def boboAwareZopeTraverse(object, path_items, econtext):\n    \"\"\"Traverses a sequence of names, first trying attributes then items.\n    This uses zope.traversing path traversal where possible and interacts\n    correctly with objects providing OFS.interface.ITraversable when\n    necessary (bobo-awareness).\n    \"\"\"\n    request = getattr(econtext, 'request', None)\n    path_items = list(path_items)\n    path_items.reverse()\n    while path_items:\n        name = path_items.pop()\n        if name == '_':\n            warnings.warn('Traversing to the name `_` is deprecated '\n                          'and will be removed in Zope 6.',\n                          DeprecationWarning)\n        elif name.startswith('_'):\n            raise NotFound(name)\n        if OFS.interfaces.ITraversable.providedBy(object):\n            object = object.restrictedTraverse(name)\n        else:\n            object = traversePathElement(object, name, path_items,\n                                         request=request)\n    return object\n    def traverse(cls, base, request, path_items):\n        \"\"\"See ``zope.app.pagetemplate.engine``.\"\"\"\n        path_items = list(path_items)\n        path_items.reverse()\n        while path_items:\n            name = path_items.pop()\n            if name == '_':\n                warnings.warn('Traversing to the name `_` is deprecated '\n                              'and will be removed in Zope 6.',\n                              DeprecationWarning)\n            elif name.startswith('_'):\n                raise NotFound(name)\n            if ITraversable.providedBy(base):\n                base = getattr(base, cls.traverse_method)(name)\n            else:\n                base = traversePathElement(base, name, path_items,\n                                           request=request)\n        return base",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-32674",
        "description": "[{'lang': 'en', 'value': \"Zope is an open-source web application server. This advisory extends the previous advisory at https://github.com/zopefoundation/Zope/security/advisories/GHSA-5pr9-v234-jw36 with additional cases of TAL expression traversal vulnerabilities. Most Python modules are not available for using in TAL expressions that you can add through-the-web, for example in Zope Page Templates. This restriction avoids file system access, for example via the 'os' module. But some of the untrusted modules are available indirectly through Python modules that are available for direct use. By default, you need to have the Manager role to add or edit Zope Page Templates through the web. Only sites that allow untrusted users to add/edit Zope Page Templates through the web are at risk. The problem has been fixed in Zope 5.2.1 and 4.6.1. The workaround is the same as for https://github.com/zopefoundation/Zope/security/advisories/GHSA-5pr9-v234-jw36: A site administrator can restrict adding/editing Zope Page Templates through the web using the standard Zope user/role permission mechanisms. Untrusted users should not be assigned the Zope Manager role and adding/editing Zope Page Templates through the web should be restricted to trusted users only.\"}]",
        "cwe_number": 22
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-230",
      "code": "class HTTPDatasetSource(DatasetSource):\n    def load(self, dst_path=None) -> str:\n        \"\"\"\n        Downloads the dataset source to the local filesystem.\n        :param dst_path: Path of the local filesystem destination directory to which to download the\n                         dataset source. If the directory does not exist, it is created. If\n                         unspecified, the dataset source is downloaded to a new uniquely-named\n                         directory on the local filesystem.\n        :return: The path to the downloaded dataset source on the local filesystem.\n        \"\"\"\n        resp = cloud_storage_http_request(\n            method=\"GET\",\n            url=self.url,\n            stream=True,\n        )\n        augmented_raise_for_status(resp)\n        path = urlparse(self.url).path\n        content_disposition = resp.headers.get(\"Content-Disposition\")\n        if content_disposition is not None and (\n            file_name := next(re.finditer(r\"filename=(.+)\", content_disposition), None)\n        ):\n            basename = file_name[1].strip(\"'\\\"\")\n        elif path is not None and len(posixpath.basename(path)) > 0:\n            basename = posixpath.basename(path)\n        else:\n            basename = \"dataset_source\"\n        if dst_path is None:\n            dst_path = create_tmp_dir()\n        dst_path = os.path.join(dst_path, basename)\n        with open(dst_path, \"wb\") as f:\n            chunk_size = 1024 * 1024\n            for chunk in resp.iter_content(chunk_size=chunk_size):\n                f.write(chunk)\n        return dst_path\n    def _can_resolve(raw_source: Any) -> bool:\n        \"\"\"\n        :param raw_source: The raw source, e.g. a string like \"http://mysite/mydata.tar.gz\".\n        :return: True if this DatsetSource can resolve the raw source, False otherwise.\n        \"\"\"\n        if not isinstance(raw_source, str):\n            return False\n        try:\n            parsed_source = urlparse(str(raw_source))\n            return parsed_source.scheme in [\"http\", \"https\"]\n        except Exception:\n            return False",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-0520",
        "description": "[{'lang': 'en', 'value': \"A vulnerability in mlflow/mlflow version 8.2.1 allows for remote code execution due to improper neutralization of special elements used in an OS command ('Command Injection') within the `mlflow.data.http_dataset_source.py` module. Specifically, when loading a dataset from a source URL with an HTTP scheme, the filename extracted from the `Content-Disposition` header or the URL path is used to generate the final file path without proper sanitization. This flaw enables an attacker to control the file path fully by utilizing path traversal or absolute path techniques, such as '../../tmp/poc.txt' or '/tmp/poc.txt', leading to arbitrary file write. Exploiting this vulnerability could allow a malicious user to execute commands on the vulnerable machine, potentially gaining access to data and model information. The issue is fixed in version 2.9.0.\"}]",
        "cwe_number": 23
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-231",
      "code": "  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.int32(0), shape=[3, 3, 3, 3], dtype=dtypes.qint32)\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          math_ops.quantize_down_and_shrink_range(input=inputs,\n                                                  input_min=[],\n                                                  input_max=4.0,\n                                                  out_type=dtypes.quint8))",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-35990",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. When `tf.quantization.fake_quant_with_min_max_vars_per_channel_gradient` receives input `min` or `max` of rank other than 1, it gives a `CHECK` fail that can trigger a denial of service attack. We have patched the issue in GitHub commit f3cf67ac5705f4f04721d15e485e192bb319feed. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range.There are no known workarounds for this issue.'}]",
        "cwe_number": 617
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-232",
      "code": "    def __init__(\n        self,\n        special=None,\n        min_freq=0,\n        max_size=None,\n        lower_case=False,\n        delimiter=None,\n        vocab_file=None,\n        pretrained_vocab_file: str = None,\n        never_split=None,\n        unk_token=\"<unk>\",\n        eos_token=\"<eos>\",\n        additional_special_tokens=[\"<formula>\"],\n        language=\"en\",\n        **kwargs,\n    ):\n        logger.error(\n            \"`TransfoXL` was deprecated due to security issues linked to `pickle.load` in `TransfoXLTokenizer`. \"\n            \"See more details on this model's documentation page: \"\n            \"`https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/transfo-xl.md`.\"\n        )\n        requires_backends(self, \"sacremoses\")\n        if special is None:\n            special = []\n        self.counter = Counter()\n        self.special = special\n        self.min_freq = min_freq\n        self.max_size = max_size\n        self.lower_case = lower_case\n        self.delimiter = delimiter\n        self.vocab_file = vocab_file\n        self.punctuation_symbols = '!\"\n        self.punction_without_space_before_pattern = re.compile(rf\"[^\\s][{self.punctuation_symbols}]\")\n        self.punctuation_with_space_around_pattern = self._compile_space_around_punctuation_pattern()\n        self.language = language\n        self.moses_punct_normalizer = sm.MosesPunctNormalizer(language)\n        self.moses_tokenizer = sm.MosesTokenizer(language)\n        self.moses_detokenizer = sm.MosesDetokenizer(language)\n        self.idx2sym = []\n        self.sym2idx = OrderedDict()\n        try:\n            vocab_dict = None\n            if pretrained_vocab_file is not None:\n                with open(pretrained_vocab_file, \"rb\") as f:\n                    vocab_dict = pickle.load(f)\n                if isinstance(vocab_dict, int):\n                    if not is_torch_available():\n                        raise ImportError(\n                            \"Not trying to load dict with PyTorch as you need to install pytorch to load \"\n                            \"from a PyTorch pretrained vocabulary, \"\n                            \"or activate it with environment variables USE_TORCH=1 and USE_TF=0.\"\n                        )\n                    vocab_dict = torch.load(pretrained_vocab_file)\n            if vocab_dict is not None:\n                for key, value in vocab_dict.items():\n                    if key not in self.__dict__ or key in [\"sym2idx\", \"idx2sym\"]:\n                        self.__dict__[key] = value\n            elif vocab_file is not None:\n                self.build_vocab()\n        except Exception as e:\n            raise ValueError(\n                f\"Unable to parse file {pretrained_vocab_file}. Unknown format. \"\n                \"If you tried to load a model saved through TransfoXLTokenizerFast, \"\n                \"please note they are not compatible.\"\n            ) from e\n        if vocab_file is not None:\n            self.build_vocab()\n        super().__init__(\n            special=special,\n            min_freq=min_freq,\n            max_size=max_size,\n            lower_case=lower_case,\n            delimiter=delimiter,\n            vocab_file=vocab_file,\n            pretrained_vocab_file=pretrained_vocab_file,\n            never_split=never_split,\n            unk_token=unk_token,\n            eos_token=eos_token,\n            additional_special_tokens=additional_special_tokens,\n            language=language,\n            **kwargs,\n        )\n        if never_split is None:\n            never_split = self.all_special_tokens\n        self.never_split = never_split\n    def do_lower_case(self):\n        return self.lower_case\n    def _compile_space_around_punctuation_pattern(self):\n        look_ahead_for_special_token = f\"(?=[{self.punctuation_symbols}])\"\n        look_ahead_to_match_all_except_space = r\"(?=[^\\s])\"\n        return re.compile(r\"\" + look_ahead_for_special_token + look_ahead_to_match_all_except_space)\ndef get_lm_corpus(datadir, dataset):\n    fn = os.path.join(datadir, \"cache.pt\")\n    fn_pickle = os.path.join(datadir, \"cache.pkl\")\n    if os.path.exists(fn):\n        logger.info(\"Loading cached dataset...\")\n        corpus = torch.load(fn_pickle)\n    elif os.path.exists(fn):\n        logger.info(\"Loading cached dataset from pickle...\")\n        with open(fn, \"rb\") as fp:\n            corpus = pickle.load(fp)\n    else:\n        logger.info(f\"Producing dataset {dataset}...\")\n        kwargs = {}\n        if dataset in [\"wt103\", \"wt2\"]:\n            kwargs[\"special\"] = [\"<eos>\"]\n            kwargs[\"lower_case\"] = False\n        elif dataset == \"ptb\":\n            kwargs[\"special\"] = [\"<eos>\"]\n            kwargs[\"lower_case\"] = True\n        elif dataset == \"lm1b\":\n            kwargs[\"special\"] = []\n            kwargs[\"lower_case\"] = False\n            kwargs[\"vocab_file\"] = os.path.join(datadir, \"1b_word_vocab.txt\")\n        elif dataset in [\"enwik8\", \"text8\"]:\n            pass\n        corpus = TransfoXLCorpus(datadir, dataset, **kwargs)\n        torch.save(corpus, fn)\n    return corpus\n    def _load_passages(self):\n        logger.info(f\"Loading passages from {self.index_path}\")\n        passages_path = self._resolve_path(self.index_path, self.PASSAGE_FILENAME)\n        with open(passages_path, \"rb\") as passages_file:\n            passages = pickle.load(passages_file)\n        return passages\n    def _deserialize_index(self):\n        logger.info(f\"Loading index from {self.index_path}\")\n        resolved_index_path = self._resolve_path(self.index_path, self.INDEX_FILENAME + \".index.dpr\")\n        self.index = faiss.read_index(resolved_index_path)\n        resolved_meta_path = self._resolve_path(self.index_path, self.INDEX_FILENAME + \".index_meta.dpr\")\n        with open(resolved_meta_path, \"rb\") as metadata_file:\n            self.index_id_to_db_id = pickle.load(metadata_file)\n        assert (\n            len(self.index_id_to_db_id) == self.index.ntotal\n        ), \"Deserialized index_id_to_db_id should match faiss index size\"\n    def is_initialized(self):\n        return self._index_initialized\n    def init_index(self):\n        index = faiss.IndexHNSWFlat(self.vector_size + 1, 512)\n        index.hnsw.efSearch = 128\n        index.hnsw.efConstruction = 200\n        self.index = index\n        self._deserialize_index()\n        self._index_initialized = True\n    def get_doc_dicts(self, doc_ids: np.array):\n        doc_list = []\n        for doc_ids_i in doc_ids:\n            ids = [str(int(doc_id)) for doc_id in doc_ids_i]\n            docs = [self.passages[doc_id] for doc_id in ids]\n            doc_list.append(docs)\n        doc_dicts = []\n        for docs in doc_list:\n            doc_dict = {}\n            doc_dict[\"title\"] = [doc[1] for doc in docs]\n            doc_dict[\"text\"] = [doc[0] for doc in docs]\n            doc_dicts.append(doc_dict)\n        return doc_dicts",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-6730",
        "description": "[{'lang': 'en', 'value': 'Deserialization of Untrusted Data in GitHub repository huggingface/transformers prior to 4.36.'}]",
        "cwe_number": 502
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-233",
      "code": "    def format_help_for_context(self, ctx: commands.Context) -> str:\n        pre_processed = super().format_help_for_context(ctx)\n        return f\"{pre_processed}\\n\\nVersion: {self.__version__}\"\n    async def message(self, ctx, *, message: str):\n        \"\"\"Set the message that is shown at the start of each ticket channel.\\n\\nUse ``{user.mention}`` to mention the person who created the ticket.\"\"\"\n        try:\n            message.format(user=ctx.author)\n            await self.config.guild(ctx.guild).message.set(message)\n            await ctx.send(f\"The message has been set to `{message}`.\")\n        except KeyError:\n            await ctx.send(\n                \"Setting the message failed. Please make sure to only use supported variables in  `\\{\\}`\"\n            )\n    async def create(\n        self,\n        ctx,\n        *,\n        reason: Optional[str] = \"No reason provided.\",\n    ):\n        \"\"\"Create a ticket.\"\"\"\n        if await self._check_settings(ctx):\n            settings = await self.config.guild(ctx.guild).all()\n            if settings[\"use_counter\"]:\n                name = f\"ticket-{settings['current_ticket']}\"\n                await self.config.guild(ctx.guild).current_ticket.set(\n                    settings[\"current_ticket\"] + 1\n                )\n            else:\n                name = f\"{ctx.author.name}-{ctx.author.id}\"\n            found = False\n            for channel in ctx.guild.channels:\n                if channel.name == name.lower():\n                    found = True\n            if not found:\n                if settings[\"modlog\"]:\n                    await modlog.create_case(\n                        ctx.bot,\n                        ctx.guild,\n                        ctx.message.created_at,\n                        action_type=\"ticket_created\",\n                        user=ctx.author,\n                        moderator=ctx.author,\n                        reason=reason,\n                    )\n                overwrite = {\n                    ctx.guild.default_role: discord.PermissionOverwrite(read_messages=False),\n                    ctx.author: discord.PermissionOverwrite(\n                        read_messages=True,\n                        send_messages=True,\n                        embed_links=True,\n                        attach_files=True,\n                    ),\n                    ctx.guild.get_role(settings[\"role\"]): discord.PermissionOverwrite(\n                        read_messages=True,\n                        send_messages=True,\n                        embed_links=True,\n                        attach_files=True,\n                        manage_messages=True,\n                    ),\n                }\n                ticketchannel = await ctx.guild.create_text_channel(\n                    name,\n                    overwrites=overwrite,\n                    category=ctx.guild.get_channel(settings[\"open_category\"]),\n                    topic=reason,\n                )\n                await ticketchannel.send(settings[\"message\"].format(user=ctx.author))\n                embed = discord.Embed(\n                    title=name,\n                    description=reason,\n                    timestamp=datetime.utcnow(),\n                ).set_footer(text=\"Last updated at:\")\n                message = await ctx.guild.get_channel(settings[\"channel\"]).send(embed=embed)\n                async with self.config.guild(ctx.guild).active() as active:\n                    active.append((ticketchannel.id, message.id))\n            else:\n                await ctx.send(\"You already have an open ticket.\")\n        else:\n            await ctx.send(\"Please finish the setup process before creating a ticket.\")",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-29501",
        "description": "[{'lang': 'en', 'value': 'Ticketer is a command based ticket system cog (plugin) for the red discord bot. A vulnerability allowing discord users to expose sensitive information has been found in the Ticketer cog. Please upgrade to version 1.0.1 as soon as possible. As a workaround users may unload the ticketer cog to disable the exploitable code.'}]",
        "cwe_number": 77
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-234",
      "code": "    def is_authenticated(self, user, password):\n        with open(self.filename) as fd:\n            for line in fd:\n                line = line.strip()\n                if line:\n                    login, hash_value = line.split(\":\")\n                    if login == user:\n                        return self.verify(hash_value, password)\n        return False",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2017-8342",
        "description": "[{'lang': 'en', 'value': 'Radicale before 1.1.2 and 2.x before 2.0.0rc2 is prone to timing oracles and simple brute-force attacks when using the htpasswd authentication method.'}]",
        "cwe_number": 362
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-235",
      "code": "async def open_code_folder_in_vs_code(request: OpenCodeFolderInVsCodeRequestModel):\n    \"\"\"\n    Opens code folder.\n    :param request: The HTTP request object.\n    :return: A JSON response with the status of the operation.\n    \"\"\"\n    try:\n        if request.discussion_id:\n            ASCIIColors.info(\"Opening folder:\")\n            root_folder = lollmsElfServer.lollms_paths.personal_outputs_path/\"discussions\"/f\"d_{request.discussion_id}\"\n            root_folder.mkdir(parents=True,exist_ok=True)\n            tmp_file = root_folder/f\"ai_code_{request.message_id}.py\"\n            with open(tmp_file,\"w\") as f:\n                f.write(request.code)\n            if os.path.isdir(root_folder):\n                subprocess.run(['code', root_folder], check=True)\n        elif request.folder_path:\n            ASCIIColors.info(\"Opening folder:\")\n            root_folder = request.folder_path\n            root_folder.mkdir(parents=True,exist_ok=True)\n            if os.path.isdir(root_folder):\n                subprocess.run(['code', root_folder], check=True)\n        return {\"status\": True, \"execution_time\": 0}\n    except Exception as ex:\n        trace_exception(ex)\n        lollmsElfServer.error(ex)\n        return {\"status\":False,\"error\":\"An error occurred during processing.\"}\nclass FilePath(BaseModel):\n    path: Optional[str] = Field(None, max_length=500)\nasync def open_file(file_path: FilePath):\n    \"\"\"\n    Opens code in vs code.\n    :param file_path: The file path object.\n    :return: A JSON response with the status of the operation.\n    \"\"\"\n    try:\n        path = file_path.path\n        if not validate_file_path(path):\n            return {\"status\":False,\"error\":\"Invalid file path\"}\n        path = os.path.realpath(path)\n        subprocess.Popen([\"start\", path])\n        return {\"status\": True, \"execution_time\": 0}\n    except Exception as ex:\n        trace_exception(ex)\n        lollmsElfServer.error(ex)\n        return {\"status\":False,\"error\":str(ex)}\nclass VSCodeData(BaseModel):\nasync def open_code_in_vs_code(vs_code_data: VSCodeData):\n    \"\"\"\n    Opens code in vs code.\n    :param vs_code_data: The data object.\n    :return: A JSON response with the status of the operation.\n    \"\"\"\n    try:\n        discussion_id = vs_code_data.discussion_id\n        message_id = vs_code_data.message_id\n        code = vs_code_data.code\n        ASCIIColors.info(\"Opening folder:\")\n        root_folder = Path(os.path.realpath(lollmsElfServer.lollms_paths.personal_outputs_path/\"discussions\"/f\"d_{discussion_id}\"/f\"{message_id}.py\"))\n        root_folder.mkdir(parents=True,exist_ok=True)\n        tmp_file = root_folder/f\"ai_code_{message_id}.py\"\n        with open(tmp_file,\"w\") as f:\n            f.write(code)\n        subprocess.Popen([\"code\", str(root_folder)])\n        return {\"status\": True, \"execution_time\": 0}\n    except Exception as ex:\n        trace_exception(ex)\n        lollmsElfServer.error(ex)\n        return {\"status\":False,\"error\":str(ex)}\nclass FolderRequest(BaseModel):\n    folder_path: Optional[str] = Field(None, title=\"The folder path\")\nasync def open_code_folder(request: FolderRequest):\n    \"\"\"\n    Opens code folder.\n    :param request: The HTTP request object.\n    :return: A JSON response with the status of the operation.\n    \"\"\"\n    try:\n        if request.discussion_id:\n            discussion_id = request.discussion_id\n            ASCIIColors.info(\"Opening folder:\")\n            root_folder = lollmsElfServer.lollms_paths.personal_outputs_path / \"discussions\" / f\"d_{discussion_id}\"\n            root_folder.mkdir(parents=True, exist_ok=True)\n            if platform.system() == 'Windows':\n                subprocess.run(['start', str(root_folder)], check=True)\n            elif platform.system() == 'Linux':\n                subprocess.run(['xdg-open', str(root_folder)], check=True)\n            elif platform.system() == 'Darwin':\n                subprocess.run(['open', str(root_folder)], check=True)\n            return {\"status\": True, \"execution_time\": 0}\n        elif request.folder_path:\n            folder_path = os.path.realpath(request.folder_path)\n            root_folder = Path(folder_path)\n            is_valid_folder_path = root_folder.is_dir()\n            if not is_valid_folder_path:\n                return {\"status\":False, \"error\":\"Invalid folder path\"}\n            ASCIIColors.info(\"Opening folder:\")\n            root_folder.mkdir(parents=True, exist_ok=True)\n            if platform.system() == 'Windows':\n                subprocess.run(['start', str(root_folder)], check=True)\n            elif platform.system() == 'Linux':\n                subprocess.run(['xdg-open', str(root_folder)], check=True)\n            elif platform.system() == 'Darwin':\n                subprocess.run(['open', str(root_folder)], check=True)\n            return {\"status\": True, \"execution_time\": 0}\n    except Exception as ex:\n        trace_exception(ex)\n        lollmsElfServer.error(ex)\n        return {\"status\": False, \"error\": \"An error occurred while processing the request\"}\ndef start_recording():\n    lollmsElfServer.info(\"Starting audio capture\")\n    try:\n        from lollms.media import AudioRecorder\n        lollmsElfServer.rec_output_folder = lollmsElfServer.lollms_paths.personal_outputs_path/\"audio_rec\"\n        lollmsElfServer.rec_output_folder.mkdir(exist_ok=True, parents=True)\n        lollmsElfServer.summoned = False\n        lollmsElfServer.audio_cap = AudioRecorder(lollmsElfServer.sio,lollmsElfServer.rec_output_folder/\"rt.wav\", callback=lollmsElfServer.audio_callback,lollmsCom=lollmsElfServer, transcribe=True)\n        lollmsElfServer.audio_cap.start_recording()\n    except:\n        lollmsElfServer.InfoMessage(\"Couldn't load media library.\\nYou will not be able to perform any of the media linked operations. please verify the logs and install any required installations\")\ndef stop_recording():\n    lollmsElfServer.info(\"Stopping audio capture\")\n    text = lollmsElfServer.audio_cap.stop_recording()\n    return text",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-1569",
        "description": "[{'lang': 'en', 'value': 'parisneo/lollms-webui is vulnerable to a denial of service (DoS) attack due to uncontrolled resource consumption. Attackers can exploit the `/open_code_in_vs_code` and similar endpoints without authentication by sending repeated HTTP POST requests, leading to the opening of Visual Studio Code or the default folder opener (e.g., File Explorer, xdg-open) multiple times. This can render the host machine unusable by exhausting system resources. The vulnerability is present in the latest version of the software.'}]",
        "cwe_number": 400
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-236",
      "code": "async def largest_content_len(urls: list[str]) -> tuple[str, int]:\n    largest_url = \"\"\n    largest_len = 0\n    async def do(client: AsyncClient, url: str) -> Response:\n        return await client.head(url, headers={\"User-Agent\": _FIREFOX_UA})\n    async with AsyncClient() as client:\n        tasks = [do(client, url) for url in urls]\n        responses: list[Response] = await gather_with_concurrency(10, *tasks, ignore_exceptions=True)\n        for response in responses:\n            len_int = int(response.headers.get(\"Content-Length\", 0))\n            if len_int > largest_len:\n                largest_url = str(response.url)\n                largest_len = len_int\n    return largest_url, largest_len\nclass NotAnImageError(Exception):\n    def _validate_image_url(url: str) -> bool:\n        \"\"\"\n        Validates that the URL is of an allowed source and restricts certain sources to prevent\n        malicious images from being downloaded.\n        \"\"\"\n        invalid_domains = {\"127.0.0.1\", \"localhost\"}\n        for domain in invalid_domains:\n            if domain in url:\n                return False\n        return True\n    async def scrape_image(self, image_url) -> None:\n        self.logger.info(f\"Image URL: {image_url}\")\n        if not self._validate_image_url(image_url):\n            self.logger.error(f\"Invalid image URL: {image_url}\")\n            raise InvalidDomainError(f\"Invalid domain: {image_url}\")\n        if isinstance(image_url, str):\n            pass\n        elif isinstance(image_url, list):\n            image_url, _ = await largest_content_len(image_url)\n        elif isinstance(image_url, dict):\n            for key in image_url:\n                if key == \"url\":\n                    image_url = image_url.get(\"url\")\n        ext = image_url.split(\".\")[-1]\n        if ext not in img.IMAGE_EXTENSIONS:\n            ext = \"jpg\"\n        file_name = f\"{str(self.recipe_id)}.{ext}\"\n        file_path = Recipe.directory_from_id(self.recipe_id).joinpath(\"images\", file_name)\n        async with AsyncClient() as client:\n            try:\n                r = await client.get(image_url, headers={\"User-Agent\": _FIREFOX_UA})\n            except Exception:\n                self.logger.exception(\"Fatal Image Request Exception\")\n                return None\n            if r.status_code != 200:\n                return None\n            content_type = r.headers.get(\"content-type\", \"\")\n            if \"image\" not in content_type:\n                self.logger.error(f\"Content-Type: {content_type} is not an image\")\n                raise NotAnImageError(f\"Content-Type {content_type} is not an image\")\n            self.logger.debug(f\"File Name Suffix {file_path.suffix}\")\n            self.write_image(r.read(), file_path.suffix)\n            file_path.unlink(missing_ok=True)\nasync def safe_scrape_html(url: str) -> str:\n    \"\"\"\n    Scrapes the html from a url but will cancel the request\n    if the request takes longer than 15 seconds. This is used to mitigate\n    DDOS attacks from users providing a url with arbitrary large content.\n    \"\"\"\n    async with AsyncClient() as client:\n        html_bytes = b\"\"\n        async with client.stream(\"GET\", url, timeout=SCRAPER_TIMEOUT, headers={\"User-Agent\": _FIREFOX_UA}) as resp:\n            start_time = time.time()\n            async for chunk in resp.aiter_bytes(chunk_size=1024):\n                html_bytes += chunk\n                if time.time() - start_time > SCRAPER_TIMEOUT:\n                    raise ForceTimeoutException()\n        content = None\n        encoding = resp.encoding\n        if not html_bytes:\n            return \"\"\n        if encoding is None:\n            encoding = resp.apparent_encoding\n        try:\n            content = str(html_bytes, encoding, errors=\"replace\")\n        except (LookupError, TypeError):\n            content = str(html_bytes, errors=\"replace\")\n        return content\nasync def create_from_url(url: str, translator: Translator) -> tuple[Recipe, ScrapedExtras | None]:\n    \"\"\"Main entry point for generating a recipe from a URL. Pass in a URL and\n    a Recipe object will be returned if successful.\n    Args:\n        url (str): a valid string representing a URL\n    Returns:\n        Recipe: Recipe Object\n    \"\"\"\n    scraper = RecipeScraper(translator)\n    new_recipe, extras = await scraper.scrape(url)\n    if not new_recipe:\n        raise HTTPException(status.HTTP_400_BAD_REQUEST, {\"details\": ParserErrors.BAD_RECIPE_DATA.value})\n    new_recipe.id = uuid4()\n    logger = get_logger()\n    logger.debug(f\"Image {new_recipe.image}\")\n    recipe_data_service = RecipeDataService(new_recipe.id)\n    try:\n        await recipe_data_service.scrape_image(new_recipe.image)\n        if new_recipe.name is None:\n            new_recipe.name = \"Untitled\"\n        new_recipe.slug = slugify(new_recipe.name)\n        new_recipe.image = cache.new_key(4)\n    except Exception as e:\n        recipe_data_service.logger.exception(f\"Error Scraping Image: {e}\")\n        new_recipe.image = \"no image\"\n    if new_recipe.name is None or new_recipe.name == \"\":\n        new_recipe.name = f\"No Recipe Name Found - {str(uuid4())}\"\n        new_recipe.slug = slugify(new_recipe.name)\n    return new_recipe, extras",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-31994",
        "description": "[{'lang': 'en', 'value': 'Mealie is a self hosted recipe manager and meal planner. Prior to 1.4.0, an attacker can point the image request to an arbitrarily large file. Mealie will attempt to retrieve this file in whole. If it can be retrieved, it may be stored on the file system in whole (leading to possible disk consumption), however the more likely scenario given resource limitations is that the container will OOM during file retrieval if the target file size is greater than the allocated memory of the container. At best this can be used to force the container to infinitely restart due to OOM (if so configured in `docker-compose.yml), or at worst this can be used to force the Mealie container to crash and remain offline. In the event that the file can be retrieved, the lack of rate limiting on this endpoint also permits an attacker to generate ongoing requests to any target of their choice, potentially contributing to an external-facing DoS attack. This vulnerability is fixed in 1.4.0.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-237",
      "code": "    def _parse_For_range(self):\n        if \"type\" in self.stmt.target._metadata:\n            iter_typ = self.stmt.target._metadata[\"type\"]\n        else:\n            iter_typ = INT256_T\n        arg0 = self.stmt.iter.args[0]\n        num_of_args = len(self.stmt.iter.args)\n        if num_of_args == 1:\n            arg0_val = self._get_range_const_value(arg0)\n            start = IRnode.from_list(0, typ=iter_typ)\n            rounds = arg0_val\n        elif self._check_valid_range_constant(self.stmt.iter.args[1]).is_literal:\n            arg0_val = self._get_range_const_value(arg0)\n            arg1_val = self._get_range_const_value(self.stmt.iter.args[1])\n            start = IRnode.from_list(arg0_val, typ=iter_typ)\n            rounds = IRnode.from_list(arg1_val - arg0_val, typ=iter_typ)\n        else:\n            arg1 = self.stmt.iter.args[1]\n            rounds = self._get_range_const_value(arg1.right)\n            start = Expr.parse_value_expr(arg0, self.context)\n        r = rounds if isinstance(rounds, int) else rounds.value\n        if r < 1:\n            return\n        varname = self.stmt.target.id\n        i = IRnode.from_list(self.context.fresh_varname(\"range_ix\"), typ=UINT256_T)\n        iptr = self.context.new_variable(varname, iter_typ)\n        self.context.forvars[varname] = True\n        loop_body = [\"seq\"]\n        loop_body.append([\"mstore\", iptr, i])\n        loop_body.append(parse_body(self.stmt.body, self.context))\n        ir_node = IRnode.from_list([\"repeat\", i, start, rounds, rounds, loop_body])\n        del self.context.forvars[varname]\n        return ir_node\n    def _parse_For_list(self):\n        with self.context.range_scope():\n            iter_list = Expr(self.stmt.iter, self.context).ir_node\n        target_type = self.stmt.target._metadata[\"type\"]\n        iter_list.typ.value_type = target_type\n        varname = self.stmt.target.id\n        loop_var = IRnode.from_list(\n            self.context.new_variable(varname, target_type), typ=target_type, location=MEMORY\n        )\n        i = IRnode.from_list(self.context.fresh_varname(\"for_list_ix\"), typ=UINT256_T)\n        self.context.forvars[varname] = True\n        ret = [\"seq\"]\n        if isinstance(self.stmt.iter, vy_ast.List):\n            tmp_list = IRnode.from_list(\n                self.context.new_internal_variable(iter_list.typ),\n                typ=iter_list.typ,\n                location=MEMORY,\n            )\n            ret.append(make_setter(tmp_list, iter_list))\n            iter_list = tmp_list\n        e = get_element_ptr(iter_list, i, array_bounds_check=False)\n        body = [\"seq\", make_setter(loop_var, e), parse_body(self.stmt.body, self.context)]\n        repeat_bound = iter_list.typ.count\n        if isinstance(iter_list.typ, DArrayT):\n            array_len = get_dyn_array_count(iter_list)\n        else:\n            array_len = repeat_bound\n        ret.append([\"repeat\", i, 0, array_len, repeat_bound, body])\n        del self.context.forvars[varname]\n        return IRnode.from_list(ret)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-32058",
        "description": "[{'lang': 'en', 'value': 'Vyper is a Pythonic smart contract language for the Ethereum virtual machine. Prior to version 0.3.8, due to missing overflow check for loop variables, by assigning the iterator of a loop to a variable, it is possible to overflow the type of the latter. The issue seems to happen only in loops of type `for i in range(a, a + N)` as in loops of type `for i in range(start, stop)` and `for i in range(stop)`, the compiler is able to raise a `TypeMismatch` when trying to overflow the variable. The problem has been patched in version 0.3.8.'}]",
        "cwe_number": 190
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-238",
      "code": "    def to_dict(self) -> Dict[str, Any]:\n        an_enum_value = self.an_enum_value.value\n        if isinstance(self.a_camel_date_time, datetime):\n            a_camel_date_time = self.a_camel_date_time.isoformat()\n        else:\n            a_camel_date_time = self.a_camel_date_time.isoformat()\n        a_date = self.a_date.isoformat()\n        if self.nested_list_of_enums is None:\n            nested_list_of_enums = None\n        else:\n            nested_list_of_enums = []\n            for nested_list_of_enums_item_data in self.nested_list_of_enums:\n                nested_list_of_enums_item = []\n                for nested_list_of_enums_item_item_data in nested_list_of_enums_item_data:\n                    nested_list_of_enums_item_item = nested_list_of_enums_item_item_data.value\n                    nested_list_of_enums_item.append(nested_list_of_enums_item_item)\n                nested_list_of_enums.append(nested_list_of_enums_item)\n        some_dict = self.some_dict\n        return {\n            \"an_enum_value\": an_enum_value,\n            \"aCamelDateTime\": a_camel_date_time,\n            \"a_date\": a_date,\n            \"nested_list_of_enums\": nested_list_of_enums,\n            \"some_dict\": some_dict,\n        }\n    def from_dict(d: Dict[str, Any]) -> AModel:\n        an_enum_value = AnEnum(d[\"an_enum_value\"])\n        def _parse_a_camel_date_time(data: Dict[str, Any]) -> Union[datetime, date]:\n            a_camel_date_time: Union[datetime, date]\n            try:\n                a_camel_date_time = datetime.fromisoformat(d[\"aCamelDateTime\"])\n                return a_camel_date_time\n            except:\n                pass\n            a_camel_date_time = date.fromisoformat(d[\"aCamelDateTime\"])\n            return a_camel_date_time\n        a_camel_date_time = _parse_a_camel_date_time(d[\"aCamelDateTime\"])\n        a_date = date.fromisoformat(d[\"a_date\"])\n        nested_list_of_enums = []\n        for nested_list_of_enums_item_data in d.get(\"nested_list_of_enums\") or []:\n            nested_list_of_enums_item = []\n            for nested_list_of_enums_item_item_data in nested_list_of_enums_item_data:\n                nested_list_of_enums_item_item = DifferentEnum(nested_list_of_enums_item_item_data)\n                nested_list_of_enums_item.append(nested_list_of_enums_item_item)\n            nested_list_of_enums.append(nested_list_of_enums_item)\n        some_dict = d.get(\"some_dict\")\n        return AModel(\n            an_enum_value=an_enum_value,\n            a_camel_date_time=a_camel_date_time,\n            a_date=a_date,\n            nested_list_of_enums=nested_list_of_enums,\n            some_dict=some_dict,\n        )\nasync def get_user_list(\n    *, client: Client, an_enum_value: List[AnEnum], some_date: Union[date, datetime],\n) -> Union[\n    List[AModel], HTTPValidationError,\n]:\n    \"\"\" Get a list of things  \"\"\"\n    url = \"{}/tests/\".format(client.base_url,)\n    headers: Dict[str, Any] = client.get_headers()\n    json_an_enum_value = []\n    for an_enum_value_item_data in an_enum_value:\n        an_enum_value_item = an_enum_value_item_data.value\n        json_an_enum_value.append(an_enum_value_item)\n    if isinstance(some_date, date):\n        json_some_date = some_date.isoformat()\n    else:\n        json_some_date = some_date.isoformat()\n    params: Dict[str, Any] = {\n        \"an_enum_value\": json_an_enum_value,\n        \"some_date\": json_some_date,\n    }\n    async with httpx.AsyncClient() as _client:\n        response = await _client.get(url=url, headers=headers, params=params,)\n    if response.status_code == 200:\n        return [AModel.from_dict(item) for item in cast(List[Dict[str, Any]], response.json())]\n    if response.status_code == 422:\n        return HTTPValidationError.from_dict(cast(Dict[str, Any], response.json()))\n    else:\n        raise ApiResponseError(response=response)\ndef get_user_list(\n    *, client: Client, an_enum_value: List[AnEnum], some_date: Union[date, datetime],\n) -> Union[\n    List[AModel], HTTPValidationError,\n]:\n    \"\"\" Get a list of things  \"\"\"\n    url = \"{}/tests/\".format(client.base_url)\n    headers: Dict[str, Any] = client.get_headers()\n    json_an_enum_value = []\n    for an_enum_value_item_data in an_enum_value:\n        an_enum_value_item = an_enum_value_item_data.value\n        json_an_enum_value.append(an_enum_value_item)\n    if isinstance(some_date, date):\n        json_some_date = some_date.isoformat()\n    else:\n        json_some_date = some_date.isoformat()\n    params: Dict[str, Any] = {\n        \"an_enum_value\": json_an_enum_value,\n        \"some_date\": json_some_date,\n    }\n    response = httpx.get(url=url, headers=headers, params=params,)\n    if response.status_code == 200:\n        return [AModel.from_dict(item) for item in cast(List[Dict[str, Any]], response.json())]\n    if response.status_code == 422:\n        return HTTPValidationError.from_dict(cast(Dict[str, Any], response.json()))\n    else:\n        raise ApiResponseError(response=response)\n    def from_data(*, data: oai.Operation, path: str, method: str, tag: str) -> Union[Endpoint, ParseError]:\n        \"\"\" Construct an endpoint from the OpenAPI data \"\"\"\n        if data.operationId is None:\n            return ParseError(data=data, detail=\"Path operations with operationId are not yet supported\")\n        endpoint = Endpoint(\n            path=path,\n            method=method,\n            description=data.description,\n            name=data.operationId,\n            requires_security=bool(data.security),\n            tag=tag,\n        )\n        result = Endpoint._add_parameters(endpoint, data)\n        if isinstance(result, ParseError):\n            return result\n        result = Endpoint._add_responses(result, data.responses)\n        if isinstance(result, ParseError):\n            return result\n        result = Endpoint._add_body(result, data)\n        return result\n    def get_type_string(self) -> str:\n        \"\"\" Get a string representation of type that should be used when declaring this property \"\"\"\n        if self.required:\n            return self._type_string\n        return f\"Optional[{self._type_string}]\"\n    def get_imports(self, *, prefix: str) -> Set[str]:\n        \"\"\"\n        Get a set of import strings that should be included when this property is used somewhere\n        Args:\n            prefix: A prefix to put before any relative (local) module names.\n        \"\"\"\n        if not self.required:\n            return {\"from typing import Optional\"}\n        return set()\n    def get_imports(self, *, prefix: str) -> Set[str]:\n        \"\"\"\n        Get a set of import strings that should be included when this property is used somewhere\n        Args:\n            prefix: A prefix to put before any relative (local) module names.\n        \"\"\"\n        imports = super().get_imports(prefix=prefix)\n        imports.update({\"from datetime import datetime\", \"from typing import cast\"})\n        return imports\nclass DateProperty(Property):\n    def values_from_list(values: List[str]) -> Dict[str, str]:\n        \"\"\" Convert a list of values into dict of {name: value} \"\"\"\n        output: Dict[str, str] = {}\n        for i, value in enumerate(values):\n            if value[0].isalpha():\n                key = value.upper()\n            else:\n                key = f\"VALUE_{i}\"\n            if key in output:\n                raise ValueError(f\"Duplicate key {key} in Enum\")\n            output[key] = value\n        return output\nclass RefProperty(Property):\n    def get_imports(self, *, prefix: str) -> Set[str]:\n        \"\"\"\n        Get a set of import strings that should be included when this property is used somewhere\n        Args:\n            prefix: A prefix to put before any relative (local) module names.\n        \"\"\"\n        imports = super().get_imports(prefix=prefix)\n        imports.update(\n            {\n                f\"from {prefix}.{self.reference.module_name} import {self.reference.class_name}\",\n                \"from typing import Dict\",\n                \"from typing import cast\",\n            }\n        )\n        return imports\nclass DictProperty(Property):\n    def __post_init__(self) -> None:\n        super().__post_init__()\n        if self.default is not None:\n            self.default = f\"field(default_factory=lambda: cast({self.get_type_string()}, {self.default}))\"\n    def __init__(self, *, openapi: GeneratorData) -> None:\n        self.openapi: GeneratorData = openapi\n        self.env: Environment = Environment(loader=PackageLoader(__package__), trim_blocks=True, lstrip_blocks=True)\n        self.project_name: str = self.project_name_override or f\"{utils.kebab_case(openapi.title).lower()}-client\"\n        self.project_dir: Path = Path.cwd() / self.project_name\n        self.package_name: str = self.package_name_override or self.project_name.replace(\"-\", \"_\")\n        self.package_dir: Path = self.project_dir / self.package_name\n        self.package_description: str = f\"A client library for accessing {self.openapi.title}\"\n        self.version: str = openapi.version\n        self.env.filters.update(self.TEMPLATE_FILTERS)\n    def build(self) -> Sequence[GeneratorError]:\n        \"\"\" Create the project from templates \"\"\"\n        print(f\"Generating {self.project_name}\")\n        try:\n            self.project_dir.mkdir()\n        except FileExistsError:\n            return [GeneratorError(detail=\"Directory already exists. Delete it or use the update command.\")]\n        self._create_package()\n        self._build_metadata()\n        self._build_models()\n        self._build_api()\n        self._reformat()\n        return self._get_errors()\ndef _sanitize(value: str) -> str:\n    return re.sub(r\"[^\\w _-]+\", \"\", value)\ndef group_title(value: str) -> str:\n    value = re.sub(r\"([A-Z]{2,})([A-Z][a-z]|[ -_]|$)\", lambda m: m.group(1).title() + m.group(2), value.strip())\n    value = re.sub(r\"(^|[ _-])([A-Z])\", lambda m: m.group(1) + m.group(2).lower(), value)\n    return value\ndef snake_case(value: str) -> str:\n    return stringcase.snakecase(group_title(_sanitize(value)))\ndef pascal_case(value: str) -> str:\n    return stringcase.pascalcase(_sanitize(value))\ndef kebab_case(value: str) -> str:\n    return stringcase.spinalcase(group_title(_sanitize(value)))\ndef get_list(an_enum_value: List[AnEnum] = Query(...), some_date: Union[date, datetime] = Query(...)):\n    \"\"\" Get a list of things \"\"\"\n    return\ndef generate_openapi_json():\n    path = Path(__file__).parent / \"openapi.json\"\n    path.write_text(json.dumps(app.openapi(), indent=4))\nif __name__ == \"__main__\":\n    generate_openapi_json()",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2020-15142",
        "description": "[{'lang': 'en', 'value': 'In openapi-python-client before version 0.5.3, clients generated with a maliciously crafted OpenAPI Document can generate arbitrary Python code. Subsequent execution of this malicious client is arbitrary code execution.'}]",
        "cwe_number": 94
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-239",
      "code": "    async def start_verification(self, client_id, username):\n        async with self.lock:\n            await self.db.execute('SELECT code FROM scratchverifier_usage WHERE \\\nclient_id=? AND username=?', (client_id, username))\n            row = await self.db.fetchone()\n        if row is not None:\n            await self.db.execute('UPDATE scratchverifier_usage SET expiry=? \\\nWHERE client_id=? AND username=? AND code=?', (int(time.time()) + VERIFY_EXPIRY,\n                                               client_id, username, row[0]))\n            return row[0]\n        code = sha256(\n            str(client_id).encode()\n            + str(time.time()).encode()\n            + username.encode()\n            + token_bytes()\n        ).hexdigest().translate({ord('0') + i: ord('A') + i for i in range(10)})\n        await self.db.execute('INSERT INTO scratchverifier_usage (client_id, \\\ncode, username, expiry) VALUES (?, ?, ?, ?)', (client_id, code, username,\n                               int(time.time() + VERIFY_EXPIRY)))\n        await self.db.execute('INSERT INTO scratchverifier_logs (client_id, \\\nusername, log_time, log_type) VALUES (?, ?, ?, ?)', (client_id, username,\n                                                     int(time.time()), 1))\n        await self.db.execute('DELETE FROM scratchverifier_usage WHERE \\\nexpiry<=?', (int(time.time()),))\n        return code",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2020-26236",
        "description": "[{'lang': 'en', 'value': \"In ScratchVerifier before commit a603769, an attacker can hijack the verification process to log into someone else's account on any site that uses ScratchVerifier for logins. A possible exploitation would follow these steps: 1. User starts login process. 2. Attacker attempts login for user, and is given the same verification code. 3. User comments code as part of their normal login. 4. Before user can, attacker completes the login process now that the code is commented. 5. User gets a failed login and attacker now has control of the account. Since commit a603769 starting a login twice will generate different verification codes, causing both user and attacker login to fail. For clients that rely on a clone of ScratchVerifier not hosted by the developers, their users may attempt to finish the login process as soon as possible after commenting the code. There is no reliable way for the attacker to know before the user can finish the process that the user has commented the code, so this vulnerability only really affects those who comment the code and then take several seconds before finishing the login.\"}]",
        "cwe_number": 287
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-240",
      "code": "    async def send(self, conn: \"Connection\") -> \"ClientResponse\":\n        if self.method == hdrs.METH_CONNECT:\n            connect_host = self.url.raw_host\n            assert connect_host is not None\n            if helpers.is_ipv6_address(connect_host):\n                connect_host = f\"[{connect_host}]\"\n            path = f\"{connect_host}:{self.url.port}\"\n        elif self.proxy and not self.is_ssl():\n            path = str(self.url)\n        else:\n            path = self.url.raw_path\n            if self.url.raw_query_string:\n                path += \"?\" + self.url.raw_query_string\n        protocol = conn.protocol\n        assert protocol is not None\n        writer = StreamWriter(\n            protocol,\n            self.loop,\n            on_chunk_sent=functools.partial(\n                self._on_chunk_request_sent, self.method, self.url\n            ),\n            on_headers_sent=functools.partial(\n                self._on_headers_request_sent, self.method, self.url\n            ),\n        )\n        if self.compress:\n            writer.enable_compression(self.compress)\n        if self.chunked is not None:\n            writer.enable_chunking()\n        if (\n            self.method in self.POST_METHODS\n            and hdrs.CONTENT_TYPE not in self.skip_auto_headers\n            and hdrs.CONTENT_TYPE not in self.headers\n        ):\n            self.headers[hdrs.CONTENT_TYPE] = \"application/octet-stream\"\n        connection = self.headers.get(hdrs.CONNECTION)\n        if not connection:\n            if self.keep_alive():\n                if self.version == HttpVersion10:\n                    connection = \"keep-alive\"\n            else:\n                if self.version == HttpVersion11:\n                    connection = \"close\"\n        if connection is not None:\n            self.headers[hdrs.CONNECTION] = connection\n        status_line = \"{0} {1} HTTP/{2[0]}.{2[1]}\".format(\n            self.method, path, self.version\n        )\n        await writer.write_headers(status_line, self.headers)\n        self._writer = self.loop.create_task(self.write_bytes(writer, conn))\n        response_class = self.response_class\n        assert response_class is not None\n        self.response = response_class(\n            self.method,\n            self.original_url,\n            writer=self._writer,\n            continue100=self._continue,\n            timer=self._timer,\n            request_info=self.request_info,\n            traces=self._traces,\n            loop=self.loop,\n            session=self._session,\n        )\n        return self.response",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-49081",
        "description": "[{'lang': 'en', 'value': 'aiohttp is an asynchronous HTTP client/server framework for asyncio and Python. Improper validation made it possible for an attacker to modify the HTTP request (e.g. to insert a new header) or create a new HTTP request if the attacker controls the HTTP version. The vulnerability only occurs if the attacker can control the HTTP version of the request. This issue has been patched in version 3.9.0.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-241",
      "code": "    def size(self) -> Optional[int]:\n        \"\"\"Size of the payload.\"\"\"\n        total = 0\n        for part, encoding, te_encoding in self._parts:\n            if encoding or te_encoding or part.size is None:\n                return None\n            total += int(\n                2\n                + len(self._boundary)\n                + 2\n                + part.size\n                + len(part._binary_headers)\n                + 2\n            )\n        total += 2 + len(self._boundary) + 4\n        return total\n    async def write(self, writer: Any, close_boundary: bool = True) -> None:\n        \"\"\"Write body.\"\"\"\n        for part, encoding, te_encoding in self._parts:\n            await writer.write(b\"--\" + self._boundary + b\"\\r\\n\")\n            await writer.write(part._binary_headers)\n            if encoding or te_encoding:\n                w = MultipartPayloadWriter(writer)\n                if encoding:\n                    w.enable_compression(encoding)\n                if te_encoding:\n                    w.enable_encoding(te_encoding)\n                await part.write(w)\n                await w.write_eof()\n            else:\n                await part.write(writer)\n            await writer.write(b\"\\r\\n\")\n        if close_boundary:\n            await writer.write(b\"--\" + self._boundary + b\"--\\r\\n\")\nclass MultipartPayloadWriter:",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-30251",
        "description": "[{'lang': 'en', 'value': 'aiohttp is an asynchronous HTTP client/server framework for asyncio and Python. In affected versions an attacker can send a specially crafted POST (multipart/form-data) request. When the aiohttp server processes it, the server will enter an infinite loop and be unable to process any further requests. An attacker can stop the application from serving requests after sending a single request. This issue has been addressed in version 3.9.4. Users are advised to upgrade. Users unable to upgrade may manually apply a patch to their systems. Please see the linked GHSA for instructions.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-242",
      "code": "    def __call__(self, value, system):\n        if 'request' in system:\n            request = system['request']\n            mime, encoding = mimetypes.guess_type(value['filename'])\n            request.response_content_type = mime\n            if encoding:\n                request.response_encoding = encoding\n            f = os.path.join(self.repository_root,\n                             value['filename'][0].lower(),\n                             value['filename'])\n            if not os.path.exists(f):\n                dir_ = os.path.join(self.repository_root,\n                             value['filename'][0].lower())\n                if not os.path.exists(dir_):\n                    os.makedirs(dir_, 0750)\n                resp = requests.get(value['url'])\n                with open(f, 'wb') as rf:\n                    rf.write(resp.content)\n                return resp.content\n            else:\n                data = ''\n                with open(f, 'rb') as rf:\n                    data = ''\n                    while True:\n                        content = rf.read(2<<16)\n                        if not content:\n                            break\n                        data += content\n                return data\ndef renderer_factory(info):\n    return ReleaseFileRenderer(info.settings['pyshop.repository'])\ndef get_release_file(root, request):\n    session = DBSession()\n    f = ReleaseFile.by_id(session, int(request.matchdict['file_id']))\n    rv = {'id': f.id,\n          'url': f.url,\n          'filename': f.filename,\n          }\n    f.downloads += 1\n    f.release.downloads += 1\n    f.release.package.downloads += 1\n    session.add(f.release.package)\n    session.add(f.release)\n    session.add(f)\n    return rv",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2013-1630",
        "description": "[{'lang': 'en', 'value': 'pyshop before 0.7.1 uses HTTP to retrieve packages from the PyPI repository, and does not perform integrity checks on package contents, which allows man-in-the-middle attackers to execute arbitrary code via a crafted response to a download operation.'}]",
        "cwe_number": 20
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-243",
      "code": "    def _load_from(self, data: bytes) -> None:\n        if data.strip() == b'':\n            data = XMP_EMPTY\n        def basic_parser(xml):\n            return parse(BytesIO(xml))\n        def strip_illegal_bytes_parser(xml):\n            return parse(BytesIO(re_xml_illegal_bytes.sub(b'', xml)))\n        def recovery_parser(xml):\n            parser = XMLParser(recover=True)\n            return parse(BytesIO(xml), parser)\n        def replace_with_empty_xmp(_xml=None):\n            log.warning(\"Error occurred parsing XMP, replacing with empty XMP.\")\n            return basic_parser(XMP_EMPTY)\n        if self.overwrite_invalid_xml:\n            parsers: Iterable[Callable] = [\n                basic_parser,\n                strip_illegal_bytes_parser,\n                recovery_parser,\n                replace_with_empty_xmp,\n            ]\n        else:\n            parsers = [basic_parser]\n        for parser in parsers:\n            try:\n                self._xmp = parser(data)\n            except (XMLSyntaxError if self.overwrite_invalid_xml else NeverRaise) as e:\n                if str(e).startswith(\"Start tag expected, '<' not found\") or str(\n                    e\n                ).startswith(\"Document is empty\"):\n                    self._xmp = replace_with_empty_xmp()\n                    break\n            else:\n                break\n        try:\n            pis = self._xmp.xpath('/processing-instruction()')\n            for pi in pis:\n                etree.strip_tags(self._xmp, pi.tag)\n            self._get_rdf_root()\n        except (Exception if self.overwrite_invalid_xml else NeverRaise) as e:\n            log.warning(\"Error occurred parsing XMP\", exc_info=e)\n            self._xmp = replace_with_empty_xmp()\n        return",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-29421",
        "description": "[{'lang': 'en', 'value': 'models/metadata.py in the pikepdf package 1.3.0 through 2.9.2 for Python allows XXE when parsing XMP metadata entries.'}]",
        "cwe_number": 611
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-244",
      "code": "    def connection_made(self, transport: asyncio.BaseTransport) -> None:\n        self._set_rset_state()\n        self.session = self._create_session()\n        self.session.peer = transport.get_extra_info('peername')\n        self._reset_timeout()\n        seen_starttls = (self._original_transport is not None)\n        if self.transport is not None and seen_starttls:\n            self._reader._transport = transport\n            self._writer._transport = transport\n            self.transport = transport\n            assert self._tls_protocol is not None\n            self.session.ssl = self._tls_protocol._extra\n            hook = self._handle_hooks.get(\"STARTTLS\")\n            if hook is None:\n                self._tls_handshake_okay = True\n            else:\n                self._tls_handshake_okay = hook(\n                    self, self.session, self.envelope)\n        else:\n            super().connection_made(transport)\n            self.transport = transport\n            log.info('Peer: %r', self.session.peer)\n            self._handler_coroutine = self.loop.create_task(\n                self._handle_client())\n    def connection_lost(self, error: Optional[Exception]) -> None:\n        assert self.session is not None\n        log.info('%r connection lost', self.session.peer)\n        assert self._timeout_handle is not None\n        self._timeout_handle.cancel()\n        if self._original_transport is not None:\n            self._original_transport.close()\n        super().connection_lost(error)\n        assert self._handler_coroutine is not None\n        self._handler_coroutine.cancel()\n        self.transport = None",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-34083",
        "description": "[{'lang': 'en', 'value': 'aiosmptd is  a reimplementation of the Python stdlib smtpd.py based on asyncio. Prior to version 1.4.6, servers based on aiosmtpd accept extra unencrypted commands after STARTTLS, treating them as if they came from inside the encrypted connection. This could be exploited by a man-in-the-middle attack. Version 1.4.6 contains a patch for the issue.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-245",
      "code": "def side_effect(old_cmd, command):\n    with tarfile.TarFile(_tar_file(old_cmd.script_parts)[0]) as archive:\n        for file in archive.getnames():\n            try:\n                os.remove(file)\n            except OSError:\n                pass\ndef side_effect(old_cmd, command):\n    with zipfile.ZipFile(_zip_file(old_cmd), 'r') as archive:\n        for file in archive.namelist():\n            try:\n                os.remove(file)\n            except OSError:\n                pass\nrequires_output = False",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-34363",
        "description": "[{'lang': 'en', 'value': 'The thefuck (aka The Fuck) package before 3.31 for Python allows Path Traversal that leads to arbitrary file deletion via the \"undo archive operation\" feature.'}]",
        "cwe_number": 22
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-246",
      "code": "    def get(self, rdtype, domain):\n        t1 = time.time()\n        rdtype = rdtype.upper()\n        current_app.logger.info(\n            'Request from %s - %s %s', request.remote_addr, rdtype, domain)\n        self.valid_args(rdtype, domain)\n        nameservers = current_app.config['RESOLVERS']\n        for nameserver in nameservers:\n            dns_resolver.nameservers = [nameserver]\n            try:\n                answer = dns_resolver.query(\n                    domain, rdtype, raise_on_no_answer=False)\n                break\n            except (NoNameservers, NXDOMAIN):\n                return {'message': \"No nameservers for %s\" % domain}, 404\n            except Timeout as e:\n                if nameserver is nameservers[-1]:\n                    current_app.logger.info(e)\n                    return {'message': 'All nameservers timed out.'}, 503\n                continue\n            except Exception as e:\n                current_app.logger.error(e)\n                return {'message': 'An unexpected error occured.'}, 500\n        t2 = time.time()\n        duration = t2 - t1\n        return parse_query(answer, nameserver, duration)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2015-10011",
        "description": "[{'lang': 'en', 'value': 'A vulnerability classified as problematic has been found in OpenDNS OpenResolve. This affects an unknown part of the file resolverapi/endpoints.py. The manipulation leads to improper output neutralization for logs. The identifier of the patch is 9eba6ba5abd89d0e36a008921eb307fcef8c5311. It is recommended to apply a patch to fix this issue. The identifier VDB-217197 was assigned to this vulnerability.'}]",
        "cwe_number": 116
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-247",
      "code": "    def parse_headers(\n        self, lines: List[bytes]\n    ) -> Tuple[\"CIMultiDictProxy[str]\", RawHeaders]:\n        headers: CIMultiDict[str] = CIMultiDict()\n        raw_headers = []\n        lines_idx = 1\n        line = lines[1]\n        line_count = len(lines)\n        while line:\n            try:\n                bname, bvalue = line.split(b\":\", 1)\n            except ValueError:\n                raise InvalidHeader(line) from None\n            if {bname[0], bname[-1]} & {32, 9}:\n                raise InvalidHeader(line)\n            bvalue = bvalue.lstrip(b\" \\t\")\n            if HDRRE.search(bname):\n                raise InvalidHeader(bname)\n            if len(bname) > self.max_field_size:\n                raise LineTooLong(\n                    \"request header name {}\".format(\n                        bname.decode(\"utf8\", \"backslashreplace\")\n                    ),\n                    str(self.max_field_size),\n                    str(len(bname)),\n                )\n            header_length = len(bvalue)\n            lines_idx += 1\n            line = lines[lines_idx]\n            continuation = line and line[0] in (32, 9)\n            if continuation:\n                bvalue_lst = [bvalue]\n                while continuation:\n                    header_length += len(line)\n                    if header_length > self.max_field_size:\n                        raise LineTooLong(\n                            \"request header field {}\".format(\n                                bname.decode(\"utf8\", \"backslashreplace\")\n                            ),\n                            str(self.max_field_size),\n                            str(header_length),\n                        )\n                    bvalue_lst.append(line)\n                    lines_idx += 1\n                    if lines_idx < line_count:\n                        line = lines[lines_idx]\n                        if line:\n                            continuation = line[0] in (32, 9)\n                    else:\n                        line = b\"\"\n                        break\n                bvalue = b\"\".join(bvalue_lst)\n            else:\n                if header_length > self.max_field_size:\n                    raise LineTooLong(\n                        \"request header field {}\".format(\n                            bname.decode(\"utf8\", \"backslashreplace\")\n                        ),\n                        str(self.max_field_size),\n                        str(header_length),\n                    )\n            bvalue = bvalue.strip(b\" \\t\")\n            name = bname.decode(\"utf-8\", \"surrogateescape\")\n            value = bvalue.decode(\"utf-8\", \"surrogateescape\")\n            if \"\\n\" in value or \"\\r\" in value or \"\\x00\" in value:\n                raise InvalidHeader(bvalue)\n            headers.add(name, value)\n            raw_headers.append((bname, bvalue))\n        return (CIMultiDictProxy(headers), tuple(raw_headers))\n    def feed_data(\n        self,\n        data: bytes,\n        SEP: _SEP = b\"\\r\\n\",\n        EMPTY: bytes = b\"\",\n        CONTENT_LENGTH: istr = hdrs.CONTENT_LENGTH,\n        METH_CONNECT: str = hdrs.METH_CONNECT,\n        SEC_WEBSOCKET_KEY1: istr = hdrs.SEC_WEBSOCKET_KEY1,\n    ) -> Tuple[List[Tuple[_MsgT, StreamReader]], bool, bytes]:\n        messages = []\n        if self._tail:\n            data, self._tail = self._tail + data, b\"\"\n        data_len = len(data)\n        start_pos = 0\n        loop = self.loop\n        while start_pos < data_len:\n            if self._payload_parser is None and not self._upgraded:\n                pos = data.find(SEP, start_pos)\n                if pos == start_pos and not self._lines:\n                    start_pos = pos + len(SEP)\n                    continue\n                if pos >= start_pos:\n                    line = data[start_pos:pos]\n                    if SEP == b\"\\n\":\n                        line = line.rstrip(b\"\\r\")\n                    self._lines.append(line)\n                    start_pos = pos + len(SEP)\n                    if self._lines[-1] == EMPTY:\n                        try:\n                            msg: _MsgT = self.parse_message(self._lines)\n                        finally:\n                            self._lines.clear()\n                        def get_content_length() -> Optional[int]:\n                            length_hdr = msg.headers.get(CONTENT_LENGTH)\n                            if length_hdr is None:\n                                return None\n                            if not length_hdr.strip(\" \\t\").isdecimal():\n                                raise InvalidHeader(CONTENT_LENGTH)\n                            return int(length_hdr)\n                        length = get_content_length()\n                        if SEC_WEBSOCKET_KEY1 in msg.headers:\n                            raise InvalidHeader(SEC_WEBSOCKET_KEY1)\n                        self._upgraded = msg.upgrade\n                        method = getattr(msg, \"method\", self.method)\n                        code = getattr(msg, \"code\", 0)\n                        assert self.protocol is not None\n                        empty_body = status_code_must_be_empty_body(code) or bool(\n                            method and method_must_be_empty_body(method)\n                        )\n                        if not empty_body and (\n                            (length is not None and length > 0)\n                            or msg.chunked\n                            and not msg.upgrade\n                        ):\n                            payload = StreamReader(\n                                self.protocol,\n                                timer=self.timer,\n                                loop=loop,\n                                limit=self._limit,\n                            )\n                            payload_parser = HttpPayloadParser(\n                                payload,\n                                length=length,\n                                chunked=msg.chunked,\n                                method=method,\n                                compression=msg.compression,\n                                code=self.code,\n                                readall=self.readall,\n                                response_with_body=self.response_with_body,\n                                auto_decompress=self._auto_decompress,\n                                lax=self.lax,\n                            )\n                            if not payload_parser.done:\n                                self._payload_parser = payload_parser\n                        elif method == METH_CONNECT:\n                            assert isinstance(msg, RawRequestMessage)\n                            payload = StreamReader(\n                                self.protocol,\n                                timer=self.timer,\n                                loop=loop,\n                                limit=self._limit,\n                            )\n                            self._upgraded = True\n                            self._payload_parser = HttpPayloadParser(\n                                payload,\n                                method=msg.method,\n                                compression=msg.compression,\n                                readall=True,\n                                auto_decompress=self._auto_decompress,\n                                lax=self.lax,\n                            )\n                        elif not empty_body and length is None and self.read_until_eof:\n                            payload = StreamReader(\n                                self.protocol,\n                                timer=self.timer,\n                                loop=loop,\n                                limit=self._limit,\n                            )\n                            payload_parser = HttpPayloadParser(\n                                payload,\n                                length=length,\n                                chunked=msg.chunked,\n                                method=method,\n                                compression=msg.compression,\n                                code=self.code,\n                                readall=True,\n                                response_with_body=self.response_with_body,\n                                auto_decompress=self._auto_decompress,\n                                lax=self.lax,\n                            )\n                            if not payload_parser.done:\n                                self._payload_parser = payload_parser\n                        else:\n                            payload = EMPTY_PAYLOAD\n                        messages.append((msg, payload))\n                else:\n                    self._tail = data[start_pos:]\n                    data = EMPTY\n                    break\n            elif self._payload_parser is None and self._upgraded:\n                assert not self._lines\n                break\n            elif data and start_pos < data_len:\n                assert not self._lines\n                assert self._payload_parser is not None\n                try:\n                    eof, data = self._payload_parser.feed_data(data[start_pos:], SEP)\n                except BaseException as exc:\n                    if self.payload_exception is not None:\n                        self._payload_parser.payload.set_exception(\n                            self.payload_exception(str(exc))\n                        )\n                    else:\n                        self._payload_parser.payload.set_exception(exc)\n                    eof = True\n                    data = b\"\"\n                if eof:\n                    start_pos = 0\n                    data_len = len(data)\n                    self._payload_parser = None\n                    continue\n            else:\n                break\n        if data and start_pos < data_len:\n            data = data[start_pos:]\n        else:\n            data = EMPTY\n        return messages, self._upgraded, data\n    def parse_message(self, lines: List[bytes]) -> RawRequestMessage:\n        line = lines[0].decode(\"utf-8\", \"surrogateescape\")\n        try:\n            method, path, version = line.split(\" \", maxsplit=2)\n        except ValueError:\n            raise BadStatusLine(line) from None\n        if len(path) > self.max_line_size:\n            raise LineTooLong(\n                \"Status line is too long\", str(self.max_line_size), str(len(path))\n            )\n        if not METHRE.fullmatch(method):\n            raise BadStatusLine(method)\n        match = VERSRE.fullmatch(version)\n        if match is None:\n            raise BadStatusLine(line)\n        version_o = HttpVersion(int(match.group(1)), int(match.group(2)))\n        if method == \"CONNECT\":\n            url = URL.build(authority=path, encoded=True)\n        elif path.startswith(\"/\"):\n            path_part, _hash_separator, url_fragment = path.partition(\"\n            path_part, _question_mark_separator, qs_part = path_part.partition(\"?\")\n            url = URL.build(\n                path=path_part,\n                query_string=qs_part,\n                fragment=url_fragment,\n                encoded=True,\n            )\n        elif path == \"*\" and method == \"OPTIONS\":\n            url = URL(path, encoded=True)\n        else:\n            url = URL(path, encoded=True)\n            if url.scheme == \"\":\n                raise InvalidURLError(\n                    path.encode(errors=\"surrogateescape\").decode(\"latin1\")\n                )\n        (\n            headers,\n            raw_headers,\n            close,\n            compression,\n            upgrade,\n            chunked,\n        ) = self.parse_headers(lines)\n        if close is None:\n            if version_o <= HttpVersion10:\n                close = True\n            else:\n                close = False\n        return RawRequestMessage(\n            method,\n            path,\n            version_o,\n            headers,\n            raw_headers,\n            close,\n            compression,\n            upgrade,\n            chunked,\n            url,\n        )\n    def feed_data(\n        self, chunk: bytes, SEP: _SEP = b\"\\r\\n\", CHUNK_EXT: bytes = b\";\"\n    ) -> Tuple[bool, bytes]:\n        if self._type == ParseState.PARSE_LENGTH:\n            required = self._length\n            chunk_len = len(chunk)\n            if required >= chunk_len:\n                self._length = required - chunk_len\n                self.payload.feed_data(chunk, chunk_len)\n                if self._length == 0:\n                    self.payload.feed_eof()\n                    return True, b\"\"\n            else:\n                self._length = 0\n                self.payload.feed_data(chunk[:required], required)\n                self.payload.feed_eof()\n                return True, chunk[required:]\n        elif self._type == ParseState.PARSE_CHUNKED:\n            if self._chunk_tail:\n                chunk = self._chunk_tail + chunk\n                self._chunk_tail = b\"\"\n            while chunk:\n                if self._chunk == ChunkState.PARSE_CHUNKED_SIZE:\n                    pos = chunk.find(SEP)\n                    if pos >= 0:\n                        i = chunk.find(CHUNK_EXT, 0, pos)\n                        if i >= 0:\n                            size_b = chunk[:i]\n                        else:\n                            size_b = chunk[:pos]\n                        if self._lax:\n                            size_b = size_b.strip()\n                        if not re.fullmatch(HEXDIGIT, size_b):\n                            exc = TransferEncodingError(\n                                chunk[:pos].decode(\"ascii\", \"surrogateescape\")\n                            )\n                            self.payload.set_exception(exc)\n                            raise exc\n                        size = int(bytes(size_b), 16)\n                        chunk = chunk[pos + len(SEP) :]\n                        if size == 0:\n                            self._chunk = ChunkState.PARSE_MAYBE_TRAILERS\n                            if self._lax and chunk.startswith(b\"\\r\"):\n                                chunk = chunk[1:]\n                        else:\n                            self._chunk = ChunkState.PARSE_CHUNKED_CHUNK\n                            self._chunk_size = size\n                            self.payload.begin_http_chunk_receiving()\n                    else:\n                        self._chunk_tail = chunk\n                        return False, b\"\"\n                if self._chunk == ChunkState.PARSE_CHUNKED_CHUNK:\n                    required = self._chunk_size\n                    chunk_len = len(chunk)\n                    if required > chunk_len:\n                        self._chunk_size = required - chunk_len\n                        self.payload.feed_data(chunk, chunk_len)\n                        return False, b\"\"\n                    else:\n                        self._chunk_size = 0\n                        self.payload.feed_data(chunk[:required], required)\n                        chunk = chunk[required:]\n                        if self._lax and chunk.startswith(b\"\\r\"):\n                            chunk = chunk[1:]\n                        self._chunk = ChunkState.PARSE_CHUNKED_CHUNK_EOF\n                        self.payload.end_http_chunk_receiving()\n                if self._chunk == ChunkState.PARSE_CHUNKED_CHUNK_EOF:\n                    if chunk[: len(SEP)] == SEP:\n                        chunk = chunk[len(SEP) :]\n                        self._chunk = ChunkState.PARSE_CHUNKED_SIZE\n                    else:\n                        self._chunk_tail = chunk\n                        return False, b\"\"\n                if self._chunk == ChunkState.PARSE_MAYBE_TRAILERS:\n                    head = chunk[: len(SEP)]\n                    if head == SEP:\n                        self.payload.feed_eof()\n                        return True, chunk[len(SEP) :]\n                    if not head:\n                        return False, b\"\"\n                    if head == SEP[:1]:\n                        self._chunk_tail = head\n                        return False, b\"\"\n                    self._chunk = ChunkState.PARSE_TRAILERS\n                if self._chunk == ChunkState.PARSE_TRAILERS:\n                    pos = chunk.find(SEP)\n                    if pos >= 0:\n                        chunk = chunk[pos + len(SEP) :]\n                        self._chunk = ChunkState.PARSE_MAYBE_TRAILERS\n                    else:\n                        self._chunk_tail = chunk\n                        return False, b\"\"\n        elif self._type == ParseState.PARSE_UNTIL_EOF:\n            self.payload.feed_data(chunk, len(chunk))\n        return False, b\"\"",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-23829",
        "description": "[{'lang': 'en', 'value': 'aiohttp is an asynchronous HTTP client/server framework for asyncio and Python. Security-sensitive parts of the Python HTTP parser retained minor differences in allowable character sets, that must trigger error handling to robustly match frame boundaries of proxies in order to protect against injection of additional requests. Additionally, validation could trigger exceptions that were not handled consistently with processing of other malformed input.  Being more lenient than internet standards require could, depending on deployment environment, assist in request smuggling. The unhandled exception could cause excessive resource consumption on the application server and/or its logging facilities. This vulnerability exists due to an incomplete fix for CVE-2023-47627. Version 3.9.2 fixes this vulnerability.'}]",
        "cwe_number": 444
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-248",
      "code": "    def createRequest(self, op, req, outgoing_data):\n        \"\"\"Return a new QNetworkReply object.\n        Args:\n             op: Operation op\n             req: const QNetworkRequest & req\n             outgoing_data: QIODevice * outgoingData\n        Return:\n            A QNetworkReply.\n        \"\"\"\n        proxy_factory = objreg.get('proxy-factory', None)\n        if proxy_factory is not None:\n            proxy_error = proxy_factory.get_error()\n            if proxy_error is not None:\n                return networkreply.ErrorNetworkReply(\n                    req, proxy_error, QNetworkReply.UnknownProxyError,\n                    self)\n        scheme = req.url().scheme()\n        if scheme in self._scheme_handlers:\n            result = self._scheme_handlers[scheme](req)\n            if result is not None:\n                result.setParent(self)\n                return result\n        for header, value in shared.custom_headers(url=req.url()):\n            req.setRawHeader(header, value)\n        host_blocker = objreg.get('host-blocker')\n        if host_blocker.is_blocked(req.url()):\n            log.webview.info(\"Request to {} blocked by host blocker.\".format(\n                req.url().host()))\n            return networkreply.ErrorNetworkReply(\n                req, HOSTBLOCK_ERROR_STRING, QNetworkReply.ContentAccessDenied,\n                self)\n        current_url = QUrl()\n        if self._tab_id is not None:\n            assert self._win_id is not None\n            try:\n                tab = objreg.get('tab', scope='tab', window=self._win_id,\n                                 tab=self._tab_id)\n                current_url = tab.url()\n            except (KeyError, RuntimeError):\n                current_url = QUrl()\n        if 'log-requests' in self._args.debug_flags:\n            operation = debug.qenum_key(QNetworkAccessManager, op)\n            operation = operation.replace('Operation', '').upper()\n            log.webview.debug(\"{} {}, first-party {}\".format(\n                operation,\n                req.url().toDisplayString(),\n                current_url.toDisplayString()))\n        self.set_referer(req, current_url)\n        return super().createRequest(op, req, outgoing_data)\n    def requestStarted(self, job):\n        \"\"\"Handle a request for a qute: scheme.\n        This method must be reimplemented by all custom URL scheme handlers.\n        The request is asynchronous and does not need to be handled right away.\n        Args:\n            job: QWebEngineUrlRequestJob\n        \"\"\"\n        url = job.requestUrl()\n        if url.scheme() in ['chrome-error', 'chrome-extension']:\n            job.fail(QWebEngineUrlRequestJob.UrlInvalid)\n            return\n        assert job.requestMethod() == b'GET'\n        assert url.scheme() == 'qute'\n        log.misc.debug(\"Got request for {}\".format(url.toDisplayString()))\n        try:\n            mimetype, data = qutescheme.data_for_url(url)\n        except qutescheme.NoHandlerFound:\n            log.misc.debug(\"No handler found for {}\".format(\n                url.toDisplayString()))\n            job.fail(QWebEngineUrlRequestJob.UrlNotFound)\n        except qutescheme.QuteSchemeOSError:\n            log.misc.exception(\"OSError while handling qute://* URL\")\n            job.fail(QWebEngineUrlRequestJob.UrlNotFound)\n        except qutescheme.QuteSchemeError:\n            log.misc.exception(\"Error while handling qute://* URL\")\n            job.fail(QWebEngineUrlRequestJob.RequestFailed)\n        except qutescheme.Redirect as e:\n            qtutils.ensure_valid(e.url)\n            job.redirect(e.url)\n        else:\n            log.misc.debug(\"Returning {} data\".format(mimetype))\n            buf = QBuffer(parent=self)\n            buf.open(QIODevice.WriteOnly)\n            buf.write(data)\n            buf.seek(0)\n            buf.close()\n            job.reply(mimetype.encode('ascii'), buf)\n    def interceptRequest(self, info):\n        \"\"\"Handle the given request.\n        Reimplementing this virtual function and setting the interceptor on a\n        profile makes it possible to intercept URL requests. This function is\n        executed on the IO thread, and therefore running long tasks here will\n        block networking.\n        info contains the information about the URL request and will track\n        internally whether its members have been altered.\n        Args:\n            info: QWebEngineUrlRequestInfo &info\n        \"\"\"\n        if 'log-requests' in self._args.debug_flags:\n            resource_type = debug.qenum_key(QWebEngineUrlRequestInfo,\n                                            info.resourceType())\n            navigation_type = debug.qenum_key(QWebEngineUrlRequestInfo,\n                                              info.navigationType())\n            log.webview.debug(\"{} {}, first-party {}, resource {}, \"\n                              \"navigation {}\".format(\n                                  bytes(info.requestMethod()).decode('ascii'),\n                                  info.requestUrl().toDisplayString(),\n                                  info.firstPartyUrl().toDisplayString(),\n                                  resource_type, navigation_type))\n        url = info.requestUrl()\n        if self._host_blocker.is_blocked(url):\n            log.webview.info(\"Request to {} blocked by host blocker.\".format(\n                url.host()))\n            info.block(True)\n        for header, value in shared.custom_headers(url=url):\n            info.setHttpHeader(header, value)\n        user_agent = config.instance.get('content.headers.user_agent', url=url)\n        if user_agent is not None:\n            info.setHttpHeader(b'User-Agent', user_agent.encode('ascii'))\ndef handler(request):\n    \"\"\"Scheme handler for qute:// URLs.\n    Args:\n        request: QNetworkRequest to answer to.\n    Return:\n        A QNetworkReply.\n    \"\"\"\n    try:\n        mimetype, data = qutescheme.data_for_url(request.url())\n    except qutescheme.NoHandlerFound:\n        errorstr = \"No handler found for {}!\".format(\n            request.url().toDisplayString())\n        return networkreply.ErrorNetworkReply(\n            request, errorstr, QNetworkReply.ContentNotFoundError)\n    except qutescheme.QuteSchemeOSError as e:\n        return networkreply.ErrorNetworkReply(\n            request, str(e), QNetworkReply.ContentNotFoundError)\n    except qutescheme.QuteSchemeError as e:\n        return networkreply.ErrorNetworkReply(request, e.errorstring, e.error)\n    except qutescheme.Redirect as e:\n        qtutils.ensure_valid(e.url)\n        return networkreply.RedirectNetworkReply(e.url)\n    return networkreply.FixedDataNetworkReply(request, data, mimetype)\ndef qute_pdfjs(url):\n    \"\"\"Handler for qute://pdfjs. Return the pdf.js viewer.\"\"\"\n    try:\n        data = pdfjs.get_pdfjs_res(url.path())\n    except pdfjs.PDFJSNotFound as e:\n        log.misc.warning(\n            \"pdfjs resource requested but not found: {}\".format(e.path))\n        raise qutescheme.QuteSchemeError(\"Can't find pdfjs resource \"\n                                         \"'{}'\".format(e.path),\n                                         QNetworkReply.ContentNotFoundError)\n    else:\n        mimetype, _encoding = mimetypes.guess_type(url.fileName())\n        assert mimetype is not None, url\n        return mimetype, data\ndef handler(request):\n    \"\"\"Handler for a file:// URL.\n    Args:\n        request: QNetworkRequest to answer to.\n    Return:\n        A QNetworkReply for directories, None for files.\n    \"\"\"\n    path = request.url().toLocalFile()\n    try:\n        if os.path.isdir(path):\n            data = dirbrowser_html(path)\n            return networkreply.FixedDataNetworkReply(\n                request, data, 'text/html')\n        return None\n    except UnicodeEncodeError:\n        return None\ndef qute_bindings(_url):\n    \"\"\"Handler for qute://bindings. View keybindings.\"\"\"\n    bindings = {}\n    defaults = config.val.bindings.default\n    modes = set(defaults.keys()).union(config.val.bindings.commands)\n    modes.remove('normal')\n    modes = ['normal'] + sorted(list(modes))\n    for mode in modes:\n        bindings[mode] = config.key_instance.get_bindings_for(mode)\n    src = jinja.render('bindings.html', title='Bindings',\n                       bindings=bindings)\n    return 'text/html', src\ndef qute_back(url):\n    \"\"\"Handler for qute://back.\n    Simple page to free ram / lazy load a site, goes back on focusing the tab.\n    \"\"\"\n    src = jinja.render(\n        'back.html',\n        title='Suspended: ' + urllib.parse.unquote(url.fragment()))\n    return 'text/html', src",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2018-10895",
        "description": "[{'lang': 'en', 'value': \"qutebrowser before version 1.4.1 is vulnerable to a cross-site request forgery flaw that allows websites to access 'qute://*' URLs. A malicious website could exploit this to load a 'qute://settings/set' URL, which then sets 'editor.command' to a bash script, resulting in arbitrary code execution.\"}]",
        "cwe_number": 352
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-249",
      "code": "    def render_POST(self, request):\n        \"\"\"\n        Register with the Identity Server\n        \"\"\"\n        send_cors(request)\n        args = get_args(request, ('matrix_server_name', 'access_token'))\n        result = yield self.client.get_json(\n            \"matrix://%s/_matrix/federation/v1/openid/userinfo?access_token=%s\" % (\n                args['matrix_server_name'], urllib.parse.quote(args['access_token']),\n            ),\n            1024 * 5,\n        )\n        if 'sub' not in result:\n            raise Exception(\"Invalid response from homeserver\")\n        user_id = result['sub']\n        tok = yield issueToken(self.sydent, user_id)\n        defer.returnValue({\n            \"access_token\": tok,\n            \"token\": tok,\n        })\n    def render_OPTIONS(self, request):\n        send_cors(request)\n        return b''\ndef is_valid_client_secret(client_secret):\n    \"\"\"Validate that a given string matches the client_secret regex defined by the spec\n    :param client_secret: The client_secret to validate\n    :type client_secret: unicode\n    :return: Whether the client_secret is valid\n    :rtype: bool\n    \"\"\"\n    return client_secret_regex.match(client_secret) is not None",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-29431",
        "description": "[{'lang': 'en', 'value': 'Sydent is a reference Matrix identity server. Sydent can be induced to send HTTP GET requests to internal systems, due to lack of parameter validation or IP address blacklisting. It is not possible to exfiltrate data or control request headers, but it might be possible to use the attack to perform an internal port enumeration. This issue has been addressed in in 9e57334, 8936925, 3d531ed, 0f00412. A potential workaround would be to use a firewall to ensure that Sydent cannot reach internal HTTP resources.'}]",
        "cwe_number": 20
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-250",
      "code": "    def __init__(self, bot):\n        self.bot = bot\n        self.config = Config.get_conf(self, identifier=2134287593)\n        default_guild = {\n            'category': None,\n            'closed_category': None,\n            'ticket_role': None,\n            'default_message_ticket_channel': None,\n            'sessions': {}\n        }\n        self.config.register_guild(**default_guild)\n        self.ticket_info_format = '\\n\\n**[{datetime}]** [{author}]\\n{information}'\n    async def create_ticket(self, context):\n        guild = context.guild\n        author = context.author\n        ticket_role = [role for role in guild.roles if await self.config.guild(guild).ticket_role() == role.id]\n        if ticket_role:\n            ticket_role = ticket_role[0]\n        category_channel = await self.config.guild(guild).category()\n        default_message_ticket_channel = await self.config.guild(guild).default_message_ticket_channel()\n        if category_channel and category_channel in [category.id for category in guild.categories]:\n            n1 = 10**10\n            n2 = n1 * 10 - 1\n            ticket_id = int(random.randint(n1, n2))\n            ticket_channel = await guild.create_text_channel('{}-{}'.format(author.display_name, ticket_id),\n                                                             category=self.bot.get_channel(category_channel))\n            await ticket_channel.set_permissions(author, read_messages=True, send_messages=True)\n            await ticket_channel.set_permissions(guild.me, read_messages=True, send_messages=True, manage_channels=True)\n            await ticket_channel.edit(topic=self.ticket_info_format.format(ticket=ticket_id,\n                                      datetime=datetime.utcnow().strftime('%d/%m/%Y %H:%M:%S'),\n                                      author=author.display_name,\n                                      information='Ticket opened'))\n            if default_message_ticket_channel:\n                await ticket_channel.send(default_message_ticket_channel.format(member=author,\n                                                                                channel=ticket_channel,\n                                                                                origin=context.channel,\n                                                                                ticket_role=ticket_role))\n            async with self.config.guild(guild).sessions() as session:\n                    session.update({ticket_channel.id: author.id})\n        else:\n            return 'Naughty! You need to run the setup first.'\n    async def update_ticket(self, context, status):\n        try:\n            await context.message.delete()\n        except discord.Forbidden:\n            pass\n        guild = context.guild\n        channel = context.channel\n        author = context.author\n        sessions = await self.config.guild(guild).sessions()\n        if str(channel.id) in sessions and await self.config.guild(guild).ticket_role() in [role.id for role in author.roles]:\n            ticket_id = str(channel.name).split('-')[1]\n            await channel.edit(topic=channel.topic+self.ticket_info_format.format(\n                                ticket=ticket_id,\n                                datetime=datetime.utcnow().strftime('%d/%m/%Y %H:%M:%S'),\n                                author=author.display_name,\n                                information=status)\n                               )\n    async def close_ticket(self, context):\n        try:\n            await context.message.delete()\n        except discord.Forbidden:\n            pass\n        guild = context.guild\n        channel = context.channel\n        author = context.author\n        sessions = await self.config.guild(guild).sessions()\n        if str(channel.id) not in sessions:\n            return await channel.send(\"Make sure you are doing this within the ticket channel that you want to close.\")\n        if await self.config.guild(guild).ticket_role() not in [role.id for role in author.roles]:\n            return await channel.send(\"You do not have the proper role to manage tickets\")\n        else:\n            member = guild.get_member(sessions[str(channel.id)])\n            ticket_id = str(channel.name).split('-')[1]\n            closed_category = await self.config.guild(guild).closed_category()\n            closed_category = self.bot.get_channel(closed_category)\n            await channel.set_permissions(member, read_messages=True, send_messages=False)\n            await channel.edit(category=closed_category,\n                               topic=channel.topic+self.ticket_info_format.format(\n                                    ticket=ticket_id,\n                                    datetime=datetime.utcnow().strftime('%d/%m/%Y %H:%M:%S'),\n                                    author=author.display_name,\n                                    information='Ticket closed'))\n            async with self.config.guild(guild).sessions() as session:\n                    session.pop(channel.id, None)\n    async def purge_tickets(self, context):\n        try:\n            guild = context.guild\n            closed_channels = [channel for channel in guild.channels if channel.category_id == await self.config.guild(guild).closed_category()]\n            for channel in closed_channels:\n                await channel.delete()\n            return 'All closed tickets removed!'\n        except discord.Forbidden:\n            return 'I need permissions to manage channels.'\n    async def set_default_message_ticket_channel(self, context, message):\n        guild = context.guild\n        await self.config.guild(guild).default_message_ticket_channel.set(message)\n        return 'Your default message has been set.'\n    async def automatic_setup(self, context):\n        guild = context.guild\n        try:\n            overwrites = {\n                guild.default_role: discord.PermissionOverwrite(send_messages=False, read_messages=False),\n            }\n            category_channel = await guild.create_category('Tickets', overwrites=overwrites)\n            closed_category_channel = await guild.create_category('Closed Tickets', overwrites=overwrites)\n            ticket_role = await guild.create_role(name='Ticket')\n            await category_channel.set_permissions(ticket_role, read_messages=True, send_messages=True)\n            await closed_category_channel.set_permissions(ticket_role, read_messages=True, send_messages=True)\n            await self.config.guild(guild).category.set(category_channel.id)\n            await self.config.guild(guild).closed_category.set(closed_category_channel.id)\n            await self.config.guild(guild).ticket_role.set(ticket_role.id)\n            return ':tada: Fabulous! You\\'re all done! Now add the `Ticket` role to anyone who you deem good enough to handle tickets. And if you care, you can change the name of the role and category if you _really_ want to.'\n        except discord.Forbidden:\n            return 'That didn\\'t go well... I need permissions to manage channels and manage roles. :rolling_eyes:'\n    async def ticket(self, context):\n        '''\n        Tickets!\n        '''\n    async def ticket_new(self, context):\n        '''\n        Create a new ticket\n        '''\n        if context.invoked_subcommand is None:\n            message = await self.core.create_ticket(context)\n            if message:\n                await context.send(message)\n    async def ticket_update(self, context, *, status: str):\n        '''\n        Update the status of a ticket\n        '''\n        await self.core.update_ticket(context, status)\n    async def ticket_close(self, context):\n        '''\n        Close a ticket, must be run in the ticket channel you want to close\n        '''\n        await self.core.close_ticket(context)\n    async def ticket_set(self, context):\n        '''\n        Settings\n        '''\n    async def ticket_set_purge(self, context):\n        '''\n        Delete all closed tickets\n        '''\n        message = await self.core.purge_tickets(context)\n        await context.send(message)\n    async def ticket_set_message(self, context, *, message: str):\n        '''\n        Set the default message when a new ticket has been created (markdown safe)\n        '''\n        message = await self.core.set_default_message_ticket_channel(context, message)\n        await context.send(message)\n    async def ticket_setup(self, context):\n        '''\n        Automatic setup, will create two categories for open and closed tickets, and a ticket role for people to be able to manage tickets.\n        '''\n        message = await self.core.automatic_setup(context)\n        await context.send(message)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-29493",
        "description": "[{'lang': 'en', 'value': 'Kennnyshiwa-cogs contains cogs for Red Discordbot. An RCE exploit has been found in the Tickets module of kennnyshiwa-cogs. This exploit allows discord users to craft a message that can reveal sensitive and harmful information. Users can upgrade to version 5a84d60018468e5c0346f7ee74b2b4650a6dade7 to receive a patch or, as a workaround, unload tickets to render the exploit unusable.'}]",
        "cwe_number": 94
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-251",
      "code": "    MODELS_FOLDER_NAME = \"models\"\n    META_DATA_FILE_NAME = \"meta.yaml\"\n    TAGS_FOLDER_NAME = \"tags\"\n    MODEL_VERSION_TAGS_FOLDER_NAME = \"tags\"\n    CREATE_MODEL_VERSION_RETRIES = 3\n    def __init__(self, root_directory=None):\n        \"\"\"\n        Create a new FileStore with the given root directory.\n        \"\"\"\n        super().__init__()\n        self.root_directory = local_file_uri_to_path(root_directory or _default_root_dir())\n        if not exists(self.models_directory):\n            mkdir(self.models_directory)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-1176",
        "description": "[{'lang': 'en', 'value': 'Absolute Path Traversal in GitHub repository mlflow/mlflow prior to 2.2.2.'}]",
        "cwe_number": 36
      },
      "cwe_types": [],
      "severity": "low"
    },
    {
      "id": "ContextAssembler-252",
      "code": "    def from_FunctionDef(\n        cls, node: vy_ast.FunctionDef, is_interface: Optional[bool] = False\n    ) -> \"ContractFunctionT\":\n        \"\"\"\n        Generate a `ContractFunctionT` object from a `FunctionDef` node.\n        Arguments\n        ---------\n        node : FunctionDef\n            Vyper ast node to generate the function definition from.\n        is_interface: bool, optional\n            Boolean indicating if the function definition is part of an interface.\n        Returns\n        -------\n        ContractFunctionT\n        \"\"\"\n        kwargs: Dict[str, Any] = {}\n        if is_interface:\n            if (\n                len(node.body) == 1\n                and isinstance(node.body[0], vy_ast.Expr)\n                and isinstance(node.body[0].value, vy_ast.Name)\n                and StateMutability.is_valid_value(node.body[0].value.id)\n            ):\n                kwargs[\"function_visibility\"] = FunctionVisibility.EXTERNAL\n                kwargs[\"state_mutability\"] = StateMutability(node.body[0].value.id)\n            elif len(node.body) == 1 and node.body[0].get(\"value.id\") in (\"constant\", \"modifying\"):\n                if node.body[0].value.id == \"constant\":\n                    expected = \"view or pure\"\n                else:\n                    expected = \"payable or nonpayable\"\n                raise StructureException(\n                    f\"State mutability should be set to {expected}\", node.body[0]\n                )\n            else:\n                raise StructureException(\n                    \"Body must only contain state mutability label\", node.body[0]\n                )\n        else:\n            for decorator in node.decorator_list:\n                if isinstance(decorator, vy_ast.Call):\n                    if \"nonreentrant\" in kwargs:\n                        raise StructureException(\n                            \"nonreentrant decorator is already set with key: \"\n                            f\"{kwargs['nonreentrant']}\",\n                            node,\n                        )\n                    if decorator.get(\"func.id\") != \"nonreentrant\":\n                        raise StructureException(\"Decorator is not callable\", decorator)\n                    if len(decorator.args) != 1 or not isinstance(decorator.args[0], vy_ast.Str):\n                        raise StructureException(\n                            \"@nonreentrant name must be given as a single string literal\", decorator\n                        )\n                    if node.name == \"__init__\":\n                        msg = \"Nonreentrant decorator disallowed on `__init__`\"\n                        raise FunctionDeclarationException(msg, decorator)\n                    kwargs[\"nonreentrant\"] = decorator.args[0].value\n                elif isinstance(decorator, vy_ast.Name):\n                    if FunctionVisibility.is_valid_value(decorator.id):\n                        if \"function_visibility\" in kwargs:\n                            raise FunctionDeclarationException(\n                                f\"Visibility is already set to: {kwargs['function_visibility']}\",\n                                node,\n                            )\n                        kwargs[\"function_visibility\"] = FunctionVisibility(decorator.id)\n                    elif StateMutability.is_valid_value(decorator.id):\n                        if \"state_mutability\" in kwargs:\n                            raise FunctionDeclarationException(\n                                f\"Mutability is already set to: {kwargs['state_mutability']}\", node\n                            )\n                        kwargs[\"state_mutability\"] = StateMutability(decorator.id)\n                    else:\n                        if decorator.id == \"constant\":\n                            warnings.warn(\n                                \"'@constant' decorator has been removed (see VIP2040). \"\n                                \"Use `@view` instead.\",\n                                DeprecationWarning,\n                            )\n                        raise FunctionDeclarationException(\n                            f\"Unknown decorator: {decorator.id}\", decorator\n                        )\n                else:\n                    raise StructureException(\"Bad decorator syntax\", decorator)\n        if \"function_visibility\" not in kwargs:\n            raise FunctionDeclarationException(\n                f\"Visibility must be set to one of: {', '.join(FunctionVisibility.values())}\", node\n            )\n        if node.name == \"__default__\":\n            if kwargs[\"function_visibility\"] != FunctionVisibility.EXTERNAL:\n                raise FunctionDeclarationException(\n                    \"Default function must be marked as `@external`\", node\n                )\n            if node.args.args:\n                raise FunctionDeclarationException(\n                    \"Default function may not receive any arguments\", node.args.args[0]\n                )\n        if \"state_mutability\" not in kwargs:\n            kwargs[\"state_mutability\"] = StateMutability.NONPAYABLE\n        if kwargs[\"state_mutability\"] == StateMutability.PURE and \"nonreentrant\" in kwargs:\n            raise StructureException(\"Cannot use reentrancy guard on pure functions\", node)\n        if node.name == \"__init__\":\n            if (\n                kwargs[\"state_mutability\"] in (StateMutability.PURE, StateMutability.VIEW)\n                or kwargs[\"function_visibility\"] == FunctionVisibility.INTERNAL\n            ):\n                raise FunctionDeclarationException(\n                    \"Constructor cannot be marked as `@pure`, `@view` or `@internal`\", node\n                )\n            if node.args.defaults:\n                raise FunctionDeclarationException(\n                    \"Constructor may not use default arguments\", node.args.defaults[0]\n                )\n        argnames = set()\n        n_total_args = len(node.args.args)\n        n_positional_args = n_total_args - len(node.args.defaults)\n        positional_args: list[PositionalArg] = []\n        keyword_args: list[KeywordArg] = []\n        for i, arg in enumerate(node.args.args):\n            argname = arg.arg\n            if argname in (\"gas\", \"value\", \"skip_contract_check\", \"default_return_value\"):\n                raise ArgumentException(\n                    f\"Cannot use '{argname}' as a variable name in a function input\", arg\n                )\n            if argname in argnames:\n                raise ArgumentException(f\"Function contains multiple inputs named {argname}\", arg)\n            if arg.annotation is None:\n                raise ArgumentException(f\"Function argument '{argname}' is missing a type\", arg)\n            type_ = type_from_annotation(arg.annotation, DataLocation.CALLDATA)\n            if i < n_positional_args:\n                positional_args.append(PositionalArg(argname, type_, ast_source=arg))\n            else:\n                value = node.args.defaults[i - n_positional_args]\n                if not check_kwargable(value):\n                    raise StateAccessViolation(\n                        \"Value must be literal or environment variable\", value\n                    )\n                validate_expected_type(value, type_)\n                keyword_args.append(KeywordArg(argname, type_, value, ast_source=arg))\n            argnames.add(argname)\n        if node.returns is None:\n            return_type = None\n        elif node.name == \"__init__\":\n            raise FunctionDeclarationException(\n                \"Constructor may not have a return type\", node.returns\n            )\n        elif isinstance(node.returns, (vy_ast.Name, vy_ast.Subscript, vy_ast.Tuple)):\n            return_type = type_from_annotation(node.returns, DataLocation.MEMORY)\n        else:\n            raise InvalidType(\"Function return value must be a type name or tuple\", node.returns)\n        return cls(node.name, positional_args, keyword_args, return_type, **kwargs)\n    def set_reentrancy_key_position(self, position: StorageSlot) -> None:\n        if hasattr(self, \"reentrancy_key_position\"):\n            raise CompilerPanic(\"Position was already assigned\")\n        if self.nonreentrant is None:\n            raise CompilerPanic(f\"No reentrant key {self}\")\n        if position._location != DataLocation.STORAGE:\n            raise CompilerPanic(\"Non-storage reentrant key\")\n        self.reentrancy_key_position = position\n    def __init__(self, message=\"Error Message not found.\", *items):\n        \"\"\"\n        Exception initializer.\n        Arguments\n        ---------\n        message : str\n            Error message to display with the exception.\n        *items : VyperNode | Tuple[str, VyperNode], optional\n            Vyper ast node(s), or tuple of (description, node) indicating where\n            the exception occured. Source annotations are generated in the order\n            the nodes are given.\n            A single tuple of (lineno, col_offset) is also understood to support\n            the old API, but new exceptions should not use this approach.\n        \"\"\"\n        self.message = message\n        self.lineno = None\n        self.col_offset = None\n        if len(items) == 1 and isinstance(items[0], tuple) and isinstance(items[0][0], int):\n            self.lineno, self.col_offset = items[0][:2]\n        else:\n            self.annotations = items\n    def with_annotation(self, *annotations):\n        \"\"\"\n        Creates a copy of this exception with a modified source annotation.\n        Arguments\n        ---------\n        *annotations : VyperNode | Tuple[str, VyperNode]\n            AST node(s), or tuple of (description, node) to use in the annotation.\n        Returns\n        -------\n        A copy of the exception with the new node offset(s) applied.\n        \"\"\"\n        exc = copy.copy(self)\n        exc.annotations = annotations\n        return exc",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-42441",
        "description": "[{'lang': 'en', 'value': 'Vyper is a Pythonic Smart Contract Language for the Ethereum Virtual Machine (EVM). Starting in version 0.2.9 and prior to version 0.3.10, locks of the type `@nonreentrant(\"\")` or `@nonreentrant(\\'\\')` do not produce reentrancy checks at runtime. This issue is fixed in version 0.3.10. As a workaround, ensure the lock name is a non-empty string.'}]",
        "cwe_number": 667
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-253",
      "code": "    def get_all_progress(self, recency=_progress_period_secs * 2):\n        \"\"\"\n        Get progress information for all ongoing operations\n        :param recency: (int) seconds back\n        :return list of progress codes\n        \"\"\"\n        query = \"\"\"\n        SELECT code, array_agg(state) FROM web_progress\n        WHERE create_date > timezone('utc', now()) - INTERVAL '{recency} SECOND'\n              AND recur_depth = 0 {user_id}\n        GROUP BY code\n        \"\"\".format(\n            recency=recency or 0,\n            user_id=not self.is_progress_admin() and \"AND create_uid = {user_id}\"\n                .format(\n                user_id=self.env.user.id,\n            ) or '')\n        self.env.cr.execute(query)\n        result = self.env.cr.fetchall()\n        ret = [{\n            'code': r[0],\n        } for r in result if r[0] and 'cancel' not in r[1] and 'done' not in r[1]]\n        return ret",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-40954",
        "description": "[{'lang': 'en', 'value': 'A SQL injection vulnerability in Grzegorz Marczynski Dynamic Progress Bar (aka web_progress) v. 11.0 through 11.0.2, v12.0 through v12.0.2, v.13.0 through v13.0.2, v.14.0 through v14.0.2.1, v.15.0 through v15.0.2, and v16.0 through v16.0.2.1 allows a remote attacker to gain privileges via the recency parameter in models/web_progress.py component.'}]",
        "cwe_number": 89
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-254",
      "code": "def job_list(request, client_id, project_name):\n    \"\"\"\n    get job list of project from one client\n    :param request: request object\n    :param client_id: client id\n    :param project_name: project name\n    :return: list of jobs\n    \"\"\"\n    if request.method == 'GET':\n        client = Client.objects.get(id=client_id)\n        scrapyd = get_scrapyd(client)\n        try:\n            result = scrapyd.list_jobs(project_name)\n            jobs = []\n            statuses = ['pending', 'running', 'finished']\n            for status in statuses:\n                for job in result.get(status):\n                    job['status'] = status\n                    jobs.append(job)\n            return JsonResponse(jobs)\n        except ConnectionError:\n            return JsonResponse({'message': 'Connect Error'}, status=500)\ndef job_log(request, client_id, project_name, spider_name, job_id):\n    \"\"\"\n    get log of jog\n    :param request: request object\n    :param client_id: client id\n    :param project_name: project name\n    :param spider_name: spider name\n    :param job_id: job id\n    :return: log of job\n    \"\"\"\n    if request.method == 'GET':\n        client = Client.objects.get(id=client_id)\n        url = log_url(client.ip, client.port, project_name, spider_name, job_id)\n        try:\n            response = requests.get(url, timeout=5, headers={\n                'Range': 'bytes=-1000'\n            }, auth=(client.username, client.password) if client.auth else None)\n            encoding = response.apparent_encoding\n            if response.status_code == 404:\n                return JsonResponse({'message': 'Log Not Found'}, status=404)\n            text = response.content.decode(encoding, errors='replace')\n            return HttpResponse(text)\n        except requests.ConnectionError:\n            return JsonResponse({'message': 'Load Log Error'}, status=500)\ndef job_cancel(request, client_id, project_name, job_id):\n    \"\"\"\n    cancel a job\n    :param request: request object\n    :param client_id: client id\n    :param project_name: project name\n    :param job_id: job id\n    :return: json of cancel\n    \"\"\"\n    if request.method == 'GET':\n        client = Client.objects.get(id=client_id)\n        try:\n            scrapyd = get_scrapyd(client)\n            result = scrapyd.cancel(project_name, job_id)\n            return JsonResponse(result)\n        except ConnectionError:\n            return JsonResponse({'message': 'Connect Error'})\ndef del_version(request, client_id, project, version):\n    if request.method == 'GET':\n        client = Client.objects.get(id=client_id)\n        try:\n            scrapyd = get_scrapyd(client)\n            result = scrapyd.delete_version(project=project, version=version)\n            return JsonResponse(result)\n        except ConnectionError:\n            return JsonResponse({'message': 'Connect Error'})\ndef del_project(request, client_id, project):\n    if request.method == 'GET':\n        client = Client.objects.get(id=client_id)\n        try:\n            scrapyd = get_scrapyd(client)\n            result = scrapyd.delete_project(project=project)\n            return JsonResponse(result)\n        except ConnectionError:\n            return JsonResponse({'message': 'Connect Error'})\ndef task_remove(request, task_id):\n    \"\"\"\n    remove task by task_id\n    :param request:\n    :return:\n    \"\"\"\n    if request.method == 'POST':\n        try:\n            task = Task.objects.get(id=task_id)\n            clients = clients_of_task(task)\n            for client in clients:\n                job_id = get_job_id(client, task)\n                DjangoJob.objects.filter(name=job_id).delete()\n            Task.objects.filter(id=task_id).delete()\n            return JsonResponse({'result': '1'})\n        except:\n            return JsonResponse({'result': '0'})\ndef render_html(request):\n    \"\"\"\n    render html with url\n    :param request:\n    :return:\n    \"\"\"\n    if request.method == 'GET':\n        url = request.GET.get('url')\n        url = unquote(base64.b64decode(url).decode('utf-8'))\n        js = request.GET.get('js', 0)\n        script = request.GET.get('script')\n        try:\n            response = requests.get(url, timeout=5)\n            response.encoding = response.apparent_encoding\n            html = process_html(response.text)\n            return HttpResponse(html)\n        except Exception as e:\n            return JsonResponse({'message': e.args}, status=500)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2020-7698",
        "description": "[{'lang': 'en', 'value': 'This affects the package Gerapy from 0 and before 0.9.3. The input being passed to Popen, via the project_configure endpoint, isn\u2019t being sanitized.'}]",
        "cwe_number": 78
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-255",
      "code": "    def load(klass):\n        \"\"\"\n        Insantiates the configuration by attempting to load the\n        configuration from YAML files specified by the CONF_PATH module\n        variable. This should be the main entry point for configuration.\n        \"\"\"\n        config = klass()\n        for path in klass.CONF_PATHS:\n            if os.path.exists(path):\n                with open(path, 'r') as conf:\n                    config.configure(yaml.load(conf))\n        return config",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2017-16763",
        "description": "[{'lang': 'en', 'value': 'An exploitable vulnerability exists in the YAML parsing functionality in config.py in Confire 0.2.0. Due to the user-specific configuration being loaded from \"~/.confire.yaml\" using the yaml.load function, a YAML parser can execute arbitrary Python commands resulting in command execution. An attacker can insert Python into loaded YAML to trigger this vulnerability.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-256",
      "code": "    def authenticate(self, context, credentials=None, ec2Credentials=None):\n        \"\"\"Validate a signed EC2 request and provide a token.\n        Other services (such as Nova) use this **admin** call to determine\n        if a request they signed received is from a valid user.\n        If it is a valid signature, an openstack token that maps\n        to the user/tenant is returned to the caller, along with\n        all the other details returned from a normal token validation\n        call.\n        The returned token is useful for making calls to other\n        OpenStack services within the context of the request.\n        :param context: standard context\n        :param credentials: dict of ec2 signature\n        :param ec2Credentials: DEPRECATED dict of ec2 signature\n        :returns: token: openstack token equivalent to access key along\n                         with the corresponding service catalog and roles\n        \"\"\"\n        if not credentials and ec2Credentials:\n            credentials = ec2Credentials\n        if not 'access' in credentials:\n            raise exception.Unauthorized(message='EC2 signature not supplied.')\n        creds_ref = self._get_credentials(context,\n                                          credentials['access'])\n        self.check_signature(creds_ref, credentials)\n        token_id = uuid.uuid4().hex\n        tenant_ref = self.identity_api.get_tenant(\n            context=context,\n            tenant_id=creds_ref['tenant_id'])\n        user_ref = self.identity_api.get_user(\n            context=context,\n            user_id=creds_ref['user_id'])\n        metadata_ref = self.identity_api.get_metadata(\n            context=context,\n            user_id=user_ref['id'],\n            tenant_id=tenant_ref['id'])\n        catalog_ref = self.catalog_api.get_catalog(\n            context=context,\n            user_id=user_ref['id'],\n            tenant_id=tenant_ref['id'],\n            metadata=metadata_ref)\n        token_ref = self.token_api.create_token(\n            context, token_id, dict(id=token_id,\n                                    user=user_ref,\n                                    tenant=tenant_ref,\n                                    metadata=metadata_ref))\n        roles_ref = []\n        for role_id in metadata_ref.get('roles', []):\n            roles_ref.append(self.identity_api.get_role(context, role_id))\n        token_controller = service.TokenController()\n        return token_controller._format_authenticate(\n            token_ref, roles_ref, catalog_ref)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2012-5571",
        "description": "[{'lang': 'en', 'value': 'OpenStack Keystone Essex (2012.1) and Folsom (2012.2) does not properly handle EC2 tokens when the user role has been removed from a tenant, which allows remote authenticated users to bypass intended authorization restrictions by leveraging a token for the removed user role.'}]",
        "cwe_number": 255
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-257",
      "code": "def time_dataframe():\n    try:\n        from pandas._testing import makeTimeDataFrame\n        return makeTimeDataFrame(), None\n    except ImportError:\n        from pandas.util.testing import makeTimeDataFrame\n        return makeTimeDataFrame(), None",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-21642",
        "description": "[{'lang': 'en', 'value': 'D-Tale is a visualizer for Pandas data structures. Users hosting versions D-Tale prior to 3.9.0 publicly can be vulnerable to server-side request forgery (SSRF), allowing attackers to access files on the server. Users should upgrade to version 3.9.0, where the `Load From the Web` input is turned off by default. The only workaround for versions earlier than 3.9.0 is to only host D-Tale to trusted users.'}]",
        "cwe_number": 918
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-258",
      "code": "def pref_get(key):\n    if get_user() is None:\n        return \"Authentication required\", 401\n    if key in get_preferences():\n        return Response(json.dumps({'key': key, 'value': get_preferences()[key]}))\n    else:\n        return Response(json.dumps({'key': key, 'error': 'novalue'}))\ndef pref_set(key, value):\n    if get_user() is None:\n        return \"Authentication required\", 401\n    get_preferences()[key] = (None if value == 'null' else value)\n    return Response(json.dumps({'key': key, 'success': ''})), 201\nif __name__ == '__main__':\n    app.run(port=5000, host=\"0.0.0.0\")",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2020-36324",
        "description": "[{'lang': 'en', 'value': 'Wikimedia Quarry analytics-quarry-web before 2020-12-15 allows Reflected XSS because app.py does not explicitly set the application/json content type.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-259",
      "code": "def parse_html_description(tree: \"etree.Element\") -> Optional[str]:\n    \"\"\"\n    Calculate a text description based on an HTML document.\n    Grabs any text nodes which are inside the <body/> tag, unless they are within\n    an HTML5 semantic markup tag (<header/>, <nav/>, <aside/>, <footer/>), or\n    if they are within a <script/>, <svg/> or <style/> tag, or if they are within\n    a tag whose content is usually only shown to old browsers\n    (<iframe/>, <video/>, <canvas/>, <picture/>).\n    This is a very very very coarse approximation to a plain text render of the page.\n    Args:\n        tree: The parsed HTML document.\n    Returns:\n        The plain text description, or None if one cannot be generated.\n    \"\"\"\n    from lxml import etree\n    TAGS_TO_REMOVE = (\n        \"header\",\n        \"nav\",\n        \"aside\",\n        \"footer\",\n        \"script\",\n        \"noscript\",\n        \"style\",\n        \"svg\",\n        \"iframe\",\n        \"video\",\n        \"canvas\",\n        \"img\",\n        \"picture\",\n        etree.Comment,\n    )\n    text_nodes = (\n        re.sub(r\"\\s+\", \"\\n\", el).strip()\n        for el in _iterate_over_text(tree.find(\"body\"), *TAGS_TO_REMOVE)\n    )\n    return summarize_paragraphs(text_nodes)\ndef _iterate_over_text(\n    tree: \"etree.Element\", *tags_to_ignore: Union[str, \"etree.Comment\"]\n) -> Generator[str, None, None]:\n    \"\"\"Iterate over the tree returning text nodes in a depth first fashion,\n    skipping text nodes inside certain tags.\n    \"\"\"\n    elements = iter([tree])\n    while True:\n        el = next(elements, None)\n        if el is None:\n            return\n        if isinstance(el, str):\n            yield el\n        elif el.tag not in tags_to_ignore:\n            if el.get(\"role\") in ARIA_ROLES_TO_IGNORE:\n                continue\n            if el.text:\n                yield el.text\n            elements = itertools.chain(\n                itertools.chain.from_iterable(\n                    [child, child.tail] if child.tail else [child]\n                    for child in el.iterchildren()\n                ),\n                elements,\n            )",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-31052",
        "description": "[{'lang': 'en', 'value': \"Synapse is an open source home server implementation for the Matrix chat network. In versions prior to 1.61.1 URL previews of some web pages can exhaust the available stack space for the Synapse process due to unbounded recursion. This is sometimes recoverable and leads to an error for the request causing the problem, but in other cases the Synapse process may crash altogether. It is possible to exploit this maliciously, either by malicious users on the homeserver, or by remote users sending URLs that a local user's client may automatically request a URL preview for. Remote users are not able to exploit this directly, because the URL preview endpoint is authenticated. Deployments with `url_preview_enabled: false` set in configuration are not affected. Deployments with `url_preview_enabled: true` set in configuration **are** affected. Deployments with no configuration value set for `url_preview_enabled` are not affected, because the default is `false`. Administrators of homeservers with URL previews enabled are advised to upgrade to v1.61.1 or higher. Users unable to upgrade should set `url_preview_enabled` to false.\"}]",
        "cwe_number": 674
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-260",
      "code": "    def get_additional_permissions(self):\n        return [\n            {\n                \"key\": \"LIST\",\n                \"name\": \"List plugins\",\n                \"description\": gettext(\"Allows to list installed plugins.\"),\n                \"default_groups\": [READONLY_GROUP, USER_GROUP, ADMIN_GROUP],\n                \"roles\": [\"manage\"],\n            },\n            {\n                \"key\": \"MANAGE\",\n                \"name\": \"Manage plugins\",\n                \"description\": gettext(\n                    \"Allows to enable, disable and uninstall installed plugins.\"\n                ),\n                \"default_groups\": [ADMIN_GROUP],\n                \"roles\": [\"manage\"],\n            },\n            {\n                \"key\": \"INSTALL\",\n                \"name\": \"Install new plugins\",\n                \"description\": gettext(\n                    'Allows to install new plugins. Includes the \"Manage plugins\" permission.'\n                ),\n                \"default_groups\": [ADMIN_GROUP],\n                \"roles\": [\"install\"],\n                \"permissions\": [\"PLUGIN_PLUGINMANAGER_MANAGE\"],\n                \"dangerous\": True,\n            },\n        ]",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-3068",
        "description": "[{'lang': 'en', 'value': 'Improper Privilege Management in GitHub repository octoprint/octoprint prior to 1.8.3.'}]",
        "cwe_number": 269
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-261",
      "code": "    def from_FunctionDef(\n        cls, node: vy_ast.FunctionDef, is_interface: Optional[bool] = False\n    ) -> \"ContractFunctionT\":\n        \"\"\"\n        Generate a `ContractFunctionT` object from a `FunctionDef` node.\n        Arguments\n        ---------\n        node : FunctionDef\n            Vyper ast node to generate the function definition from.\n        is_interface: bool, optional\n            Boolean indicating if the function definition is part of an interface.\n        Returns\n        -------\n        ContractFunctionT\n        \"\"\"\n        kwargs: Dict[str, Any] = {}\n        if is_interface:\n            if (\n                len(node.body) == 1\n                and isinstance(node.body[0], vy_ast.Expr)\n                and isinstance(node.body[0].value, vy_ast.Name)\n                and StateMutability.is_valid_value(node.body[0].value.id)\n            ):\n                kwargs[\"function_visibility\"] = FunctionVisibility.EXTERNAL\n                kwargs[\"state_mutability\"] = StateMutability(node.body[0].value.id)\n            elif len(node.body) == 1 and node.body[0].get(\"value.id\") in (\"constant\", \"modifying\"):\n                if node.body[0].value.id == \"constant\":\n                    expected = \"view or pure\"\n                else:\n                    expected = \"payable or nonpayable\"\n                raise StructureException(\n                    f\"State mutability should be set to {expected}\", node.body[0]\n                )\n            else:\n                raise StructureException(\n                    \"Body must only contain state mutability label\", node.body[0]\n                )\n        else:\n            for decorator in node.decorator_list:\n                if isinstance(decorator, vy_ast.Call):\n                    if \"nonreentrant\" in kwargs:\n                        raise StructureException(\n                            \"nonreentrant decorator is already set with key: \"\n                            f\"{kwargs['nonreentrant']}\",\n                            node,\n                        )\n                    if decorator.get(\"func.id\") != \"nonreentrant\":\n                        raise StructureException(\"Decorator is not callable\", decorator)\n                    if len(decorator.args) != 1 or not isinstance(decorator.args[0], vy_ast.Str):\n                        raise StructureException(\n                            \"@nonreentrant name must be given as a single string literal\", decorator\n                        )\n                    if node.name == \"__init__\":\n                        msg = \"Nonreentrant decorator disallowed on `__init__`\"\n                        raise FunctionDeclarationException(msg, decorator)\n                    kwargs[\"nonreentrant\"] = decorator.args[0].value\n                elif isinstance(decorator, vy_ast.Name):\n                    if FunctionVisibility.is_valid_value(decorator.id):\n                        if \"function_visibility\" in kwargs:\n                            raise FunctionDeclarationException(\n                                f\"Visibility is already set to: {kwargs['function_visibility']}\",\n                                node,\n                            )\n                        kwargs[\"function_visibility\"] = FunctionVisibility(decorator.id)\n                    elif StateMutability.is_valid_value(decorator.id):\n                        if \"state_mutability\" in kwargs:\n                            raise FunctionDeclarationException(\n                                f\"Mutability is already set to: {kwargs['state_mutability']}\", node\n                            )\n                        kwargs[\"state_mutability\"] = StateMutability(decorator.id)\n                    else:\n                        if decorator.id == \"constant\":\n                            warnings.warn(\n                                \"'@constant' decorator has been removed (see VIP2040). \"\n                                \"Use `@view` instead.\",\n                                DeprecationWarning,\n                            )\n                        raise FunctionDeclarationException(\n                            f\"Unknown decorator: {decorator.id}\", decorator\n                        )\n                else:\n                    raise StructureException(\"Bad decorator syntax\", decorator)\n        if \"function_visibility\" not in kwargs:\n            raise FunctionDeclarationException(\n                f\"Visibility must be set to one of: {', '.join(FunctionVisibility.values())}\", node\n            )\n        if node.name == \"__default__\":\n            if kwargs[\"function_visibility\"] != FunctionVisibility.EXTERNAL:\n                raise FunctionDeclarationException(\n                    \"Default function must be marked as `@external`\", node\n                )\n            if node.args.args:\n                raise FunctionDeclarationException(\n                    \"Default function may not receive any arguments\", node.args.args[0]\n                )\n        if \"state_mutability\" not in kwargs:\n            kwargs[\"state_mutability\"] = StateMutability.NONPAYABLE\n        if kwargs[\"state_mutability\"] == StateMutability.PURE and \"nonreentrant\" in kwargs:\n            raise StructureException(\"Cannot use reentrancy guard on pure functions\", node)\n        if node.name == \"__init__\":\n            if (\n                kwargs[\"state_mutability\"] in (StateMutability.PURE, StateMutability.VIEW)\n                or kwargs[\"function_visibility\"] == FunctionVisibility.INTERNAL\n            ):\n                raise FunctionDeclarationException(\n                    \"Constructor cannot be marked as `@pure`, `@view` or `@internal`\", node\n                )\n            if node.args.defaults:\n                raise FunctionDeclarationException(\n                    \"Constructor may not use default arguments\", node.args.defaults[0]\n                )\n        argnames = set()\n        n_total_args = len(node.args.args)\n        n_positional_args = n_total_args - len(node.args.defaults)\n        positional_args: list[PositionalArg] = []\n        keyword_args: list[KeywordArg] = []\n        for i, arg in enumerate(node.args.args):\n            argname = arg.arg\n            if argname in (\"gas\", \"value\", \"skip_contract_check\", \"default_return_value\"):\n                raise ArgumentException(\n                    f\"Cannot use '{argname}' as a variable name in a function input\", arg\n                )\n            if argname in argnames:\n                raise ArgumentException(f\"Function contains multiple inputs named {argname}\", arg)\n            if arg.annotation is None:\n                raise ArgumentException(f\"Function argument '{argname}' is missing a type\", arg)\n            type_ = type_from_annotation(arg.annotation, DataLocation.CALLDATA)\n            if i < n_positional_args:\n                positional_args.append(PositionalArg(argname, type_, ast_source=arg))\n            else:\n                value = node.args.defaults[i - n_positional_args]\n                if not check_kwargable(value):\n                    raise StateAccessViolation(\n                        \"Value must be literal or environment variable\", value\n                    )\n                validate_expected_type(value, type_)\n                keyword_args.append(KeywordArg(argname, type_, value, ast_source=arg))\n            argnames.add(argname)\n        if node.returns is None:\n            return_type = None\n        elif node.name == \"__init__\":\n            raise FunctionDeclarationException(\n                \"Constructor may not have a return type\", node.returns\n            )\n        elif isinstance(node.returns, (vy_ast.Name, vy_ast.Subscript, vy_ast.Tuple)):\n            return_type = type_from_annotation(node.returns, DataLocation.MEMORY)\n        else:\n            raise InvalidType(\"Function return value must be a type name or tuple\", node.returns)\n        return cls(node.name, positional_args, keyword_args, return_type, **kwargs)\n    def __init__(self, message=\"Error Message not found.\", *items):\n        \"\"\"\n        Exception initializer.\n        Arguments\n        ---------\n        message : str\n            Error message to display with the exception.\n        *items : VyperNode | Tuple[str, VyperNode], optional\n            Vyper ast node(s), or tuple of (description, node) indicating where\n            the exception occured. Source annotations are generated in the order\n            the nodes are given.\n            A single tuple of (lineno, col_offset) is also understood to support\n            the old API, but new exceptions should not use this approach.\n        \"\"\"\n        self.message = message\n        self.lineno = None\n        self.col_offset = None\n        if len(items) == 1 and isinstance(items[0], tuple) and isinstance(items[0][0], int):\n            self.lineno, self.col_offset = items[0][:2]\n        else:\n            self.annotations = items\ndef validate_identifier(attr):\n    if not re.match(\"^[_a-zA-Z][a-zA-Z0-9_]*$\", attr):\n        raise StructureException(f\"'{attr}' contains invalid character(s)\")\n    if attr.lower() in RESERVED_KEYWORDS:\n        raise StructureException(f\"'{attr}' is a reserved keyword\")",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-42441",
        "description": "[{'lang': 'en', 'value': 'Vyper is a Pythonic Smart Contract Language for the Ethereum Virtual Machine (EVM). Starting in version 0.2.9 and prior to version 0.3.10, locks of the type `@nonreentrant(\"\")` or `@nonreentrant(\\'\\')` do not produce reentrancy checks at runtime. This issue is fixed in version 0.3.10. As a workaround, ensure the lock name is a non-empty string.'}]",
        "cwe_number": 667
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-262",
      "code": "    def title(self):\n        if base_hasattr(self.context, 'get_full_title'):\n            title = self.context.get_full_title()\n        else:\n            title = self.context.Title()\n        title = title and safe_unicode(title) or u\"\"\n        return title",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-4638",
        "description": "[{'lang': 'en', 'value': 'A vulnerability classified as problematic was found in collective.contact.widget up to 1.12. This vulnerability affects the function title of the file src/collective/contact/widget/widgets.py. The manipulation leads to cross site scripting. The attack can be initiated remotely. The name of the patch is 5da36305ca7ed433782be8901c47387406fcda12. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-216496.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-263",
      "code": "def login():\n    from flask_login import current_user\n    redirect_url = request.args.get(\"redirect\", request.script_root + url_for(\"index\"))\n    permissions = sorted(\n        filter(\n            lambda x: x is not None and isinstance(x, OctoPrintPermission),\n            map(\n                lambda x: getattr(Permissions, x.strip()),\n                request.args.get(\"permissions\", \"\").split(\",\"),\n            ),\n        ),\n        key=lambda x: x.get_name(),\n    )\n    if not permissions:\n        permissions = [Permissions.STATUS, Permissions.SETTINGS_READ]\n    user_id = request.args.get(\"user_id\", \"\")\n    if (not user_id or current_user.get_id() == user_id) and has_permissions(\n        *permissions\n    ):\n        return redirect(redirect_url)\n    render_kwargs = {\n        \"theming\": [],\n        \"redirect_url\": redirect_url,\n        \"permission_names\": map(lambda x: x.get_name(), permissions),\n        \"user_id\": user_id,\n        \"logged_in\": not current_user.is_anonymous,\n    }\n    try:\n        additional_assets = _add_additional_assets(\"octoprint.theming.login\")\n        additional_assets += _add_additional_assets(\"octoprint.plugin.forcelogin.theming\")\n        additional_assets += _add_additional_assets(\"octoprint.plugin.loginui.theming\")\n        render_kwargs.update({\"theming\": additional_assets})\n    except Exception:\n        _logger.exception(\"Error processing theming CSS, ignoring\")\n    return render_template(\"login.jinja2\", **render_kwargs)\ndef recovery():\n    response = require_login_with(permissions=[Permissions.ADMIN])\n    if response:\n        return response\n    render_kwargs = {\"theming\": []}\n    try:\n        additional_assets = _add_additional_assets(\"octoprint.theming.recovery\")\n        render_kwargs.update({\"theming\": additional_assets})\n    except Exception:\n        _logger.exception(\"Error processing theming CSS, ignoring\")\n    try:\n        from octoprint.plugins.backup import MAX_UPLOAD_SIZE\n        from octoprint.util import get_formatted_size\n        render_kwargs.update(\n            {\n                \"plugin_backup_max_upload_size\": MAX_UPLOAD_SIZE,\n                \"plugin_backup_max_upload_size_str\": get_formatted_size(MAX_UPLOAD_SIZE),\n            }\n        )\n    except Exception:\n        _logger.exception(\"Error adding backup upload size info, ignoring\")\n    return render_template(\"recovery.jinja2\", **render_kwargs)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-1430",
        "description": "[{'lang': 'en', 'value': 'Cross-site Scripting (XSS) - DOM in GitHub repository octoprint/octoprint prior to 1.8.0.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-264",
      "code": "        def test_get_object_anonymous(self):\n            \"\"\"\n            GET a single object as an unauthenticated user.\n            \"\"\"\n            url = self._get_detail_url(self._get_queryset().first())\n            if (\n                self.model._meta.app_label,\n                self.model._meta.model_name,\n            ) in settings.EXEMPT_EXCLUDE_MODELS:\n                with testing.disable_warnings(\"django.request\"):\n                    self.assertHttpStatus(self.client.get(url, **self.header), status.HTTP_403_FORBIDDEN)\n            else:\n                response = self.client.get(url, **self.header)\n                self.assertHttpStatus(response, status.HTTP_200_OK)\n        def test_get_object_without_permission(self):\n            \"\"\"\n            GET a single object as an authenticated user without the required permission.\n            \"\"\"\n            url = self._get_detail_url(self._get_queryset().first())\n            with testing.disable_warnings(\"django.request\"):\n                self.assertHttpStatus(self.client.get(url, **self.header), status.HTTP_403_FORBIDDEN)\n        def test_get_object(self):\n            \"\"\"\n            GET a single object as an authenticated user with permission to view the object.\n            \"\"\"\n            self.assertGreaterEqual(\n                self._get_queryset().count(),\n                2,\n                f\"Test requires the creation of at least two {self.model} instances\",\n            )\n            instance1, instance2 = self._get_queryset()[:2]\n            obj_perm = users_models.ObjectPermission(\n                name=\"Test permission\",\n                constraints={\"pk\": instance1.pk},\n                actions=[\"view\"],\n            )\n            obj_perm.save()\n            obj_perm.users.add(self.user)\n            obj_perm.object_types.add(ContentType.objects.get_for_model(self.model))\n            url = self._get_detail_url(instance2)\n            self.assertHttpStatus(self.client.get(url, **self.header), status.HTTP_404_NOT_FOUND)\n            url = self._get_detail_url(instance1)\n            response = self.client.get(url, **self.header)\n            self.assertHttpStatus(response, status.HTTP_200_OK)\n            self.assertIsInstance(response.data, dict)\n            self.assertIn(\"id\", response.data)\n            self.assertEqual(str(response.data[\"id\"]), str(instance1.pk))\n            self.assertIn(\"url\", response.data)\n            self.assertIn(\"display\", response.data)\n            self.assertIn(\"natural_slug\", response.data)\n            self.assertIsInstance(response.data[\"display\"], str)\n            if issubclass(self.model, extras_models.ChangeLoggedModel):\n                self.assertIn(\"created\", response.data)\n                self.assertIn(\"last_updated\", response.data)\n            self.assertNotIn(\"computed_fields\", response.data)\n            self.assertNotIn(\"relationships\", response.data)\n            custom_fields_registry = registry.registry[\"model_features\"][\"custom_fields\"]\n            cf_supported = self.model._meta.model_name in custom_fields_registry.get(self.model._meta.app_label, {})\n            if cf_supported:\n                self.assertIn(\"custom_fields\", response.data)\n                self.assertIsInstance(response.data[\"custom_fields\"], dict)\n            relationships_registry = registry.registry[\"model_features\"][\"relationships\"]\n            rel_supported = self.model._meta.model_name in relationships_registry.get(self.model._meta.app_label, {})\n            if cf_supported or rel_supported:\n                query_params = []\n                if cf_supported:\n                    query_params.append(\"include=computed_fields\")\n                if rel_supported:\n                    query_params.append(\"include=relationships\")\n                query_string = \"&\".join(query_params)\n                url = f\"{url}?{query_string}\"\n                response = self.client.get(url, **self.header)\n                self.assertHttpStatus(response, status.HTTP_200_OK)\n                self.assertIsInstance(response.data, dict)\n                if cf_supported:\n                    self.assertIn(\"computed_fields\", response.data)\n                    self.assertIsInstance(response.data[\"computed_fields\"], dict)\n                else:\n                    self.assertNotIn(\"computed_fields\", response.data)\n                if rel_supported:\n                    self.assertIn(\"relationships\", response.data)\n                    self.assertIsInstance(response.data[\"relationships\"], dict)\n                else:\n                    self.assertNotIn(\"relationships\", response.data)\n        def test_options_object(self):\n            \"\"\"\n            Make an OPTIONS request for a single object.\n            \"\"\"\n            url = self._get_detail_url(self._get_queryset().first())\n            response = self.client.options(url, **self.header)\n            self.assertHttpStatus(response, status.HTTP_200_OK)\n            with self.subTest(\"Assert Detail View Config is generated well\"):\n                serializer = get_serializer_for_model(self._get_queryset().model)\n                advanced_view_schema = response.data[\"view_options\"][\"retrieve\"][\"tabs\"][\"Advanced\"]\n                self.assertEqual(len(advanced_view_schema), 1)\n                self.assertIn(\"Object Details\", advanced_view_schema[0])\n                advanced_tab_fields = advanced_view_schema[0].get(\"Object Details\")[\"fields\"]\n                if detail_view_config := getattr(serializer.Meta, \"detail_view_config\", None):\n                    detail_view_schema = response.data[\"view_options\"][\"retrieve\"][\"tabs\"][\n                        bettertitle(self._get_queryset().model._meta.verbose_name)\n                    ]\n                    self.assertHttpStatus(response, status.HTTP_200_OK)\n                    with self.subTest(\"Assert advanced tab fields should not exist in the detail_view_schema.\"):\n                        if detail_view_config.get(\"include_others\"):\n                            other_fields = detail_view_schema[0][\"Other Fields\"][\"fields\"]\n                            for field in advanced_tab_fields:\n                                self.assertNotIn(field, other_fields)\n                        for col_idx, col in enumerate(detail_view_schema):\n                            for group_title, group in col.items():\n                                if group_title == \"Other Fields\":\n                                    continue\n                                group_fields = group[\"fields\"]\n                                if (\n                                    col_idx < len(detail_view_config[\"layout\"])\n                                    and group_title in detail_view_config[\"layout\"][col_idx]\n                                ):\n                                    fields = detail_view_config[\"layout\"][col_idx][group_title][\"fields\"]\n                                else:\n                                    fields = []\n                                for field in group_fields:\n                                    self.assertNotIn(field, advanced_tab_fields)\n                                for field in fields:\n                                    if field not in advanced_tab_fields:\n                                        self.assertIn(field, group_fields)\n        def test_list_objects_depth_0(self):\n            \"\"\"\n            GET a list of objects using the \"?depth=0\" parameter.\n            \"\"\"\n            depth_fields = self.get_depth_fields()\n            self.add_permissions(f\"{self.model._meta.app_label}.view_{self.model._meta.model_name}\")\n            url = f\"{self._get_list_url()}?depth=0\"\n            response = self.client.get(url, **self.header)\n            self.assertHttpStatus(response, status.HTTP_200_OK)\n            self.assertIsInstance(response.data, dict)\n            self.assertIn(\"results\", response.data)\n            self.assertEqual(len(response.data[\"results\"]), self._get_queryset().count())\n            for response_data in response.data[\"results\"]:\n                for field in depth_fields:\n                    self.assertIn(field, response_data)\n                    if isinstance(response_data[field], list):\n                        for entry in response_data[field]:\n                            self.assertIsInstance(entry, dict)\n                            self.assertTrue(is_uuid(entry[\"id\"]))\n                    else:\n                        if response_data[field] is not None:\n                            self.assertIsInstance(response_data[field], dict)\n                            url = response_data[field][\"url\"]\n                            pk = response_data[field][\"id\"]\n                            object_type = response_data[field][\"object_type\"]\n                            self.assertTrue(is_uuid(url.split(\"/\")[-2]))\n                            self.assertTrue(is_uuid(pk))\n                            with self.subTest(f\"Assert object_type {object_type} is valid\"):\n                                app_label, model_name = object_type.split(\".\")\n                                ContentType.objects.get(app_label=app_label, model=model_name)\n        def test_list_objects_depth_1(self):\n            \"\"\"\n            GET a list of objects using the \"?depth=1\" parameter.\n            \"\"\"\n            depth_fields = self.get_depth_fields()\n            self.add_permissions(f\"{self.model._meta.app_label}.view_{self.model._meta.model_name}\")\n            url = f\"{self._get_list_url()}?depth=1\"\n            response = self.client.get(url, **self.header)\n            self.assertHttpStatus(response, status.HTTP_200_OK)\n            self.assertIsInstance(response.data, dict)\n            self.assertIn(\"results\", response.data)\n            self.assertEqual(len(response.data[\"results\"]), self._get_queryset().count())\n            for response_data in response.data[\"results\"]:\n                for field in depth_fields:\n                    self.assertIn(field, response_data)\n                    if isinstance(response_data[field], list):\n                        for entry in response_data[field]:\n                            self.assertIsInstance(entry, dict)\n                            self.assertTrue(is_uuid(entry[\"id\"]))\n                    else:\n                        if response_data[field] is not None:\n                            self.assertIsInstance(response_data[field], dict)\n                            self.assertTrue(is_uuid(response_data[field][\"id\"]))\n        def test_list_objects_without_permission(self):\n            \"\"\"\n            GET a list of objects as an authenticated user without the required permission.\n            \"\"\"\n            url = self._get_list_url()\n            with testing.disable_warnings(\"django.request\"):\n                self.assertHttpStatus(self.client.get(url, **self.header), status.HTTP_403_FORBIDDEN)\n        def test_list_objects(self):\n            \"\"\"\n            GET a list of objects as an authenticated user with permission to view the objects.\n            \"\"\"\n            self.assertGreaterEqual(\n                self._get_queryset().count(),\n                3,\n                f\"Test requires the creation of at least three {self.model} instances\",\n            )\n            instance1, instance2 = self._get_queryset()[:2]\n            obj_perm = users_models.ObjectPermission(\n                name=\"Test permission\",\n                constraints={\"pk__in\": [instance1.pk, instance2.pk]},\n                actions=[\"view\"],\n            )\n            obj_perm.save()\n            obj_perm.users.add(self.user)\n            obj_perm.object_types.add(ContentType.objects.get_for_model(self.model))\n            response = self.client.get(self._get_list_url(), **self.header)\n            self.assertHttpStatus(response, status.HTTP_200_OK)\n            self.assertIsInstance(response.data, dict)\n            self.assertIn(\"results\", response.data)\n            self.assertEqual(len(response.data[\"results\"]), 2)\n        def test_list_objects_filtered(self):\n            \"\"\"\n            GET a list of objects filtered by ID.\n            \"\"\"\n            self.assertGreaterEqual(\n                self._get_queryset().count(),\n                3,\n                f\"Test requires the creation of at least three {self.model} instances\",\n            )\n            self.add_permissions(f\"{self.model._meta.app_label}.view_{self.model._meta.model_name}\")\n            instance1, instance2 = self._get_queryset()[:2]\n            response = self.client.get(f\"{self._get_list_url()}?id={instance1.pk}&id={instance2.pk}\", **self.header)\n            self.assertHttpStatus(response, status.HTTP_200_OK)\n            self.assertIsInstance(response.data, dict)\n            self.assertIn(\"results\", response.data)\n            self.assertEqual(len(response.data[\"results\"]), 2)\n            for entry in response.data[\"results\"]:\n                self.assertIn(str(entry[\"id\"]), [str(instance1.pk), str(instance2.pk)])\n        def test_list_objects_ascending_ordered(self):\n            if hasattr(self.model, \"name\") and not issubclass(self.model, TreeModel):\n                self.add_permissions(f\"{self.model._meta.app_label}.view_{self.model._meta.model_name}\")\n                response = self.client.get(f\"{self._get_list_url()}?sort=name&limit=3\", **self.header)\n                self.assertHttpStatus(response, status.HTTP_200_OK)\n                result_list = list(map(lambda p: p[\"name\"], response.data[\"results\"]))\n                self.assertEqual(\n                    result_list,\n                    list(self._get_queryset().order_by(\"name\").values_list(\"name\", flat=True)[:3]),\n                    \"API sort not identical to QuerySet.order_by\",\n                )\ndef nested_serializer_factory(relation_info, nested_depth):\n    \"\"\"\n    Return a NestedSerializer representation of a serializer field.\n    This method should only be called in build_nested_field()\n    in which relation_info and nested_depth are already given.\n    \"\"\"\n    nested_serializer_name = f\"Nested{nested_depth}{relation_info.related_model.__name__}\"\n    if nested_serializer_name in NESTED_SERIALIZER_CACHE:\n        field_class = NESTED_SERIALIZER_CACHE[nested_serializer_name]\n        field_kwargs = get_nested_relation_kwargs(relation_info)\n    else:\n        base_serializer_class = get_serializer_for_model(relation_info.related_model)\n        class NautobotNestedSerializer(base_serializer_class):\n            class Meta:\n                model = relation_info.related_model\n                is_nested = True\n                depth = nested_depth - 1\n                if hasattr(base_serializer_class.Meta, \"fields\"):\n                    fields = base_serializer_class.Meta.fields\n                if hasattr(base_serializer_class.Meta, \"exclude\"):\n                    exclude = base_serializer_class.Meta.exclude\n        NautobotNestedSerializer.__name__ = nested_serializer_name\n        NESTED_SERIALIZER_CACHE[nested_serializer_name] = NautobotNestedSerializer\n        field_class = NautobotNestedSerializer\n        field_kwargs = get_nested_relation_kwargs(relation_info)\n    return field_class, field_kwargs",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-46128",
        "description": "[{'lang': 'en', 'value': 'Nautobot is a Network Automation Platform built as a web application atop the Django Python framework with a PostgreSQL or MySQL database. In Nautobot 2.0.x, certain REST API endpoints, in combination with the `?depth=<N>` query parameter, can expose hashed user passwords as stored in the database to any authenticated user with access to these endpoints. The passwords are not exposed in plaintext. This vulnerability has been patched in version 2.0.3.\\n\\n'}]",
        "cwe_number": 312
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-265",
      "code": "    def get(cls, uuid):\n        \"\"\"Return a `Resource` instance of this class identified by\n        the given code or UUID.\n        Only `Resource` classes with specified `member_path` attributes\n        can be directly requested with this method.\n        \"\"\"\n        url = urljoin(recurly.base_uri(), cls.member_path % (uuid,))\n        resp, elem = cls.element_for_url(url)\n        return cls.from_element(elem)\n    def all(cls, **kwargs):\n        \"\"\"Return a `Page` of instances of this `Resource` class from\n        its general collection endpoint.\n        Only `Resource` classes with specified `collection_path`\n        endpoints can be requested with this method. Any provided\n        keyword arguments are passed to the API endpoint as query\n        parameters.\n        \"\"\"\n        url = urljoin(recurly.base_uri(), cls.collection_path)\n        if kwargs:\n            url = '%s?%s' % (url, urlencode(kwargs))\n        return Page.page_for_url(url)\n    def count(cls, **kwargs):\n        \"\"\"Return a count of server side resources given\n        filtering arguments in kwargs.\n        \"\"\"\n        url = urljoin(recurly.base_uri(), cls.collection_path)\n        if kwargs:\n            url = '%s?%s' % (url, urlencode(kwargs))\n        return Page.count_for_url(url)\n    def _create(self):\n        url = urljoin(recurly.base_uri(), self.collection_path)\n        return self.post(url)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2017-0906",
        "description": "[{'lang': 'en', 'value': 'The Recurly Client Python Library before 2.0.5, 2.1.16, 2.2.22, 2.3.1, 2.4.5, 2.5.1, 2.6.2 is vulnerable to a Server-Side Request Forgery vulnerability in the \"Resource.get\" method that could result in compromise of API keys or other critical resources.'}]",
        "cwe_number": 918
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-266",
      "code": "    def format_help_for_context(self, ctx: commands.Context) -> str:\n        pre_processed = super().format_help_for_context(ctx)\n        return f\"{pre_processed}\\n\\nVersion: {self.__version__}\"\n    async def message(self, ctx, *, message: str):\n        \"\"\"Set the message that is shown at the start of each ticket channel.\\n\\nUse ``{user.mention}`` to mention the person who created the ticket.\"\"\"\n        try:\n            message.format(user=ctx.author)\n            await self.config.guild(ctx.guild).message.set(message)\n            await ctx.send(f\"The message has been set to `{message}`.\")\n        except KeyError:\n            await ctx.send(\n                \"Setting the message failed. Please make sure to only use supported variables in  `\\{\\}`\"\n            )\n    async def create(\n        self,\n        ctx,\n        *,\n        reason: Optional[str] = \"No reason provided.\",\n    ):\n        \"\"\"Create a ticket.\"\"\"\n        if await self._check_settings(ctx):\n            settings = await self.config.guild(ctx.guild).all()\n            if settings[\"use_counter\"]:\n                name = f\"ticket-{settings['current_ticket']}\"\n                await self.config.guild(ctx.guild).current_ticket.set(\n                    settings[\"current_ticket\"] + 1\n                )\n            else:\n                name = f\"{ctx.author.name}-{ctx.author.id}\"\n            found = False\n            for channel in ctx.guild.channels:\n                if channel.name == name.lower():\n                    found = True\n            if not found:\n                if settings[\"modlog\"]:\n                    await modlog.create_case(\n                        ctx.bot,\n                        ctx.guild,\n                        ctx.message.created_at,\n                        action_type=\"ticket_created\",\n                        user=ctx.author,\n                        moderator=ctx.author,\n                        reason=reason,\n                    )\n                overwrite = {\n                    ctx.guild.default_role: discord.PermissionOverwrite(read_messages=False),\n                    ctx.author: discord.PermissionOverwrite(\n                        read_messages=True,\n                        send_messages=True,\n                        embed_links=True,\n                        attach_files=True,\n                    ),\n                    ctx.guild.get_role(settings[\"role\"]): discord.PermissionOverwrite(\n                        read_messages=True,\n                        send_messages=True,\n                        embed_links=True,\n                        attach_files=True,\n                        manage_messages=True,\n                    ),\n                }\n                ticketchannel = await ctx.guild.create_text_channel(\n                    name,\n                    overwrites=overwrite,\n                    category=ctx.guild.get_channel(settings[\"open_category\"]),\n                    topic=reason,\n                )\n                await ticketchannel.send(settings[\"message\"].format(user=ctx.author))\n                embed = discord.Embed(\n                    title=name,\n                    description=reason,\n                    timestamp=datetime.utcnow(),\n                ).set_footer(text=\"Last updated at:\")\n                message = await ctx.guild.get_channel(settings[\"channel\"]).send(embed=embed)\n                async with self.config.guild(ctx.guild).active() as active:\n                    active.append((ticketchannel.id, message.id))\n            else:\n                await ctx.send(\"You already have an open ticket.\")\n        else:\n            await ctx.send(\"Please finish the setup process before creating a ticket.\")",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-29501",
        "description": "[{'lang': 'en', 'value': 'Ticketer is a command based ticket system cog (plugin) for the red discord bot. A vulnerability allowing discord users to expose sensitive information has been found in the Ticketer cog. Please upgrade to version 1.0.1 as soon as possible. As a workaround users may unload the ticketer cog to disable the exploitable code.'}]",
        "cwe_number": 77
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-267",
      "code": "    def __init__(\n        self,\n        *,\n        elem_id: str | None = None,\n        elem_classes: list[str] | str | None = None,\n        render: bool = True,\n        visible: bool = True,\n        proxy_url: str | None = None,\n    ):\n        self._id = Context.id\n        Context.id += 1\n        self.visible = visible\n        self.elem_id = elem_id\n        self.elem_classes = (\n            [elem_classes] if isinstance(elem_classes, str) else elem_classes\n        )\n        self.proxy_url = proxy_url\n        self.share_token = secrets.token_urlsafe(32)\n        self.parent: BlockContext | None = None\n        self.is_rendered: bool = False\n        self._constructor_args: list[dict]\n        self.state_session_capacity = 10000\n        self.temp_files: set[str] = set()\n        self.GRADIO_CACHE = str(\n            Path(\n                os.environ.get(\"GRADIO_TEMP_DIR\")\n                or str(Path(tempfile.gettempdir()) / \"gradio\")\n            ).resolve()\n        )\n        if render:\n            self.render()\n    def is_callable(self, fn_index: int = 0) -> bool:\n        \"\"\"Checks if a particular Blocks function is callable (i.e. not stateful or a generator).\"\"\"\n        block_fn = self.fns[fn_index]\n        dependency = self.dependencies[fn_index]\n        if inspect.isasyncgenfunction(block_fn.fn):\n            return False\n        if inspect.isgeneratorfunction(block_fn.fn):\n            return False\n        for input_id in dependency[\"inputs\"]:\n            block = self.blocks[input_id]\n            if getattr(block, \"stateful\", False):\n                return False\n        for output_id in dependency[\"outputs\"]:\n            block = self.blocks[output_id]\n            if getattr(block, \"stateful\", False):\n                return False\n        return True\n    def __call__(self, *inputs, fn_index: int = 0, api_name: str | None = None):\n        \"\"\"\n        Allows Blocks objects to be called as functions. Supply the parameters to the\n        function as positional arguments. To choose which function to call, use the\n        fn_index parameter, which must be a keyword argument.\n        Parameters:\n        *inputs: the parameters to pass to the function\n        fn_index: the index of the function to call (defaults to 0, which for Interfaces, is the default prediction function)\n        api_name: The api_name of the dependency to call. Will take precedence over fn_index.\n        \"\"\"\n        if api_name is not None:\n            inferred_fn_index = next(\n                (\n                    i\n                    for i, d in enumerate(self.dependencies)\n                    if d.get(\"api_name\") == api_name\n                ),\n                None,\n            )\n            if inferred_fn_index is None:\n                raise InvalidApiNameError(\n                    f\"Cannot find a function with api_name {api_name}\"\n                )\n            fn_index = inferred_fn_index\n        if not (self.is_callable(fn_index)):\n            raise ValueError(\n                \"This function is not callable because it is either stateful or is a generator. Please use the .launch() method instead to create an interactive user interface.\"\n            )\n        inputs = list(inputs)\n        processed_inputs = self.serialize_data(fn_index, inputs)\n        batch = self.dependencies[fn_index][\"batch\"]\n        if batch:\n            processed_inputs = [[inp] for inp in processed_inputs]\n        outputs = client_utils.synchronize_async(\n            self.process_api,\n            fn_index=fn_index,\n            inputs=processed_inputs,\n            request=None,\n            state={},\n        )\n        outputs = outputs[\"data\"]\n        if batch:\n            outputs = [out[0] for out in outputs]\n        outputs = self.deserialize_data(fn_index, outputs)\n        processed_outputs = utils.resolve_singleton(outputs)\n        return processed_outputs\n    def validate_inputs(self, fn_index: int, inputs: list[Any]):\n        block_fn = self.fns[fn_index]\n        dependency = self.dependencies[fn_index]\n        dep_inputs = dependency[\"inputs\"]\n        if len(inputs) < len(dep_inputs):\n            name = (\n                f\" ({block_fn.name})\"\n                if block_fn.name and block_fn.name != \"<lambda>\"\n                else \"\"\n            )\n            wanted_args = []\n            received_args = []\n            for input_id in dep_inputs:\n                block = self.blocks[input_id]\n                wanted_args.append(str(block))\n            for inp in inputs:\n                v = f'\"{inp}\"' if isinstance(inp, str) else str(inp)\n                received_args.append(v)\n            wanted = \", \".join(wanted_args)\n            received = \", \".join(received_args)\n            raise ValueError(\n                f\"\"\"An event handler{name} didn't receive enough input values (needed: {len(dep_inputs)}, got: {len(inputs)}).\nCheck if the event handler calls a Javascript function, and make sure its return value is correct.\nWanted inputs:\n    [{wanted}]\nReceived inputs:\n    [{received}]\"\"\"\n            )\n    def preprocess_data(\n        self, fn_index: int, inputs: list[Any], state: SessionState | None\n    ):\n        state = state or SessionState(self)\n        block_fn = self.fns[fn_index]\n        dependency = self.dependencies[fn_index]\n        self.validate_inputs(fn_index, inputs)\n        if block_fn.preprocess:\n            processed_input = []\n            for i, input_id in enumerate(dependency[\"inputs\"]):\n                try:\n                    block = self.blocks[input_id]\n                except KeyError as e:\n                    raise InvalidBlockError(\n                        f\"Input component with id {input_id} used in {dependency['trigger']}() event not found in this gr.Blocks context. You are allowed to nest gr.Blocks contexts, but there must be a gr.Blocks context that contains all components and events.\"\n                    ) from e\n                if not isinstance(block, components.Component):\n                    raise InvalidComponentError(\n                        f\"{block.__class__} Component with id {input_id} not a valid input component.\"\n                    )\n                if getattr(block, \"stateful\", False):\n                    processed_input.append(state[input_id])\n                else:\n                    if input_id in state:\n                        block = state[input_id]\n                    inputs_cached = processing_utils.move_files_to_cache(\n                        inputs[i], block, add_urls=True\n                    )\n                    if getattr(block, \"data_model\", None) and inputs_cached is not None:\n                        if issubclass(block.data_model, GradioModel):\n                            inputs_cached = block.data_model(**inputs_cached)\n                        elif issubclass(block.data_model, GradioRootModel):\n                            inputs_cached = block.data_model(root=inputs_cached)\n                    processed_input.append(block.preprocess(inputs_cached))\n        else:\n            processed_input = inputs\n        return processed_input\n    def validate_outputs(self, fn_index: int, predictions: Any | list[Any]):\n        block_fn = self.fns[fn_index]\n        dependency = self.dependencies[fn_index]\n        dep_outputs = dependency[\"outputs\"]\n        if not isinstance(predictions, (list, tuple)):\n            predictions = [predictions]\n        if len(predictions) < len(dep_outputs):\n            name = (\n                f\" ({block_fn.name})\"\n                if block_fn.name and block_fn.name != \"<lambda>\"\n                else \"\"\n            )\n            wanted_args = []\n            received_args = []\n            for output_id in dep_outputs:\n                block = self.blocks[output_id]\n                wanted_args.append(str(block))\n            for pred in predictions:\n                v = f'\"{pred}\"' if isinstance(pred, str) else str(pred)\n                received_args.append(v)\n            wanted = \", \".join(wanted_args)\n            received = \", \".join(received_args)\n            raise ValueError(\n                f\"\"\"An event handler{name} didn't receive enough output values (needed: {len(dep_outputs)}, received: {len(predictions)}).\nWanted outputs:\n    [{wanted}]\nReceived outputs:\n    [{received}]\"\"\"\n            )\n    async def process_api(\n        self,\n        fn_index: int,\n        inputs: list[Any],\n        state: SessionState | None = None,\n        request: routes.Request | list[routes.Request] | None = None,\n        iterator: AsyncIterator | None = None,\n        session_hash: str | None = None,\n        event_id: str | None = None,\n        event_data: EventData | None = None,\n        in_event_listener: bool = True,\n    ) -> dict[str, Any]:\n        \"\"\"\n        Processes API calls from the frontend. First preprocesses the data,\n        then runs the relevant function, then postprocesses the output.\n        Parameters:\n            fn_index: Index of function to run.\n            inputs: input data received from the frontend\n            state: data stored from stateful components for session (key is input block id)\n            request: the gr.Request object containing information about the network request (e.g. IP address, headers, query parameters, username)\n            iterators: the in-progress iterators for each generator function (key is function index)\n            event_id: id of event that triggered this API call\n            event_data: data associated with the event trigger itself\n        Returns: None\n        \"\"\"\n        block_fn = self.fns[fn_index]\n        batch = self.dependencies[fn_index][\"batch\"]\n        if batch:\n            max_batch_size = self.dependencies[fn_index][\"max_batch_size\"]\n            batch_sizes = [len(inp) for inp in inputs]\n            batch_size = batch_sizes[0]\n            if inspect.isasyncgenfunction(block_fn.fn) or inspect.isgeneratorfunction(\n                block_fn.fn\n            ):\n                raise ValueError(\"Gradio does not support generators in batch mode.\")\n            if not all(x == batch_size for x in batch_sizes):\n                raise ValueError(\n                    f\"All inputs to a batch function must have the same length but instead have sizes: {batch_sizes}.\"\n                )\n            if batch_size > max_batch_size:\n                raise ValueError(\n                    f\"Batch size ({batch_size}) exceeds the max_batch_size for this function ({max_batch_size})\"\n                )\n            inputs = await anyio.to_thread.run_sync(\n                self.run_fn_batch,\n                self.preprocess_data,\n                inputs,\n                fn_index,\n                state,\n                limiter=self.limiter,\n            )\n            result = await self.call_function(\n                fn_index,\n                list(zip(*inputs)),\n                None,\n                request,\n                event_id,\n                event_data,\n                in_event_listener,\n            )\n            preds = result[\"prediction\"]\n            data = await anyio.to_thread.run_sync(\n                self.run_fn_batch,\n                self.postprocess_data,\n                preds,\n                fn_index,\n                state,\n                limiter=self.limiter,\n            )\n            data = list(zip(*data))\n            is_generating, iterator = None, None\n        else:\n            old_iterator = iterator\n            if old_iterator:\n                inputs = []\n            else:\n                inputs = await anyio.to_thread.run_sync(\n                    self.preprocess_data, fn_index, inputs, state, limiter=self.limiter\n                )\n            was_generating = old_iterator is not None\n            result = await self.call_function(\n                fn_index,\n                inputs,\n                old_iterator,\n                request,\n                event_id,\n                event_data,\n                in_event_listener,\n            )\n            data = await anyio.to_thread.run_sync(\n                self.postprocess_data,\n                fn_index,\n                result[\"prediction\"],\n                state,\n                limiter=self.limiter,\n            )\n            is_generating, iterator = result[\"is_generating\"], result[\"iterator\"]\n            if is_generating or was_generating:\n                run = id(old_iterator) if was_generating else id(iterator)\n                data = self.handle_streaming_outputs(\n                    fn_index,\n                    data,\n                    session_hash=session_hash,\n                    run=run,\n                )\n                data = self.handle_streaming_diffs(\n                    fn_index,\n                    data,\n                    session_hash=session_hash,\n                    run=run,\n                    final=not is_generating,\n                )\n        block_fn.total_runtime += result[\"duration\"]\n        block_fn.total_runs += 1\n        return {\n            \"data\": data,\n            \"is_generating\": is_generating,\n            \"iterator\": iterator,\n            \"duration\": result[\"duration\"],\n            \"average_duration\": block_fn.total_runtime / block_fn.total_runs,\n        }\n    def create_limiter(self):\n        self.limiter = (\n            None\n            if self.max_threads == 40\n            else CapacityLimiter(total_tokens=self.max_threads)\n        )\n    def get_config(self):\n        return {\"type\": \"column\"}\n    def get_config_file(self):\n        config = {\n            \"version\": routes.VERSION,\n            \"mode\": self.mode,\n            \"app_id\": self.app_id,\n            \"dev_mode\": self.dev_mode,\n            \"analytics_enabled\": self.analytics_enabled,\n            \"components\": [],\n            \"css\": self.css,\n            \"js\": self.js,\n            \"head\": self.head,\n            \"title\": self.title or \"Gradio\",\n            \"space_id\": self.space_id,\n            \"enable_queue\": True,\n            \"show_error\": getattr(self, \"show_error\", False),\n            \"show_api\": self.show_api,\n            \"is_colab\": utils.colab_check(),\n            \"stylesheets\": self.stylesheets,\n            \"theme\": self.theme.name,\n            \"protocol\": \"sse_v2\",\n            \"body_css\": {\n                \"body_background_fill\": self.theme._get_computed_value(\n                    \"body_background_fill\"\n                ),\n                \"body_text_color\": self.theme._get_computed_value(\"body_text_color\"),\n                \"body_background_fill_dark\": self.theme._get_computed_value(\n                    \"body_background_fill_dark\"\n                ),\n                \"body_text_color_dark\": self.theme._get_computed_value(\n                    \"body_text_color_dark\"\n                ),\n            },\n            \"fill_height\": self.fill_height,\n        }\n        def get_layout(block):\n            if not isinstance(block, BlockContext):\n                return {\"id\": block._id}\n            children_layout = []\n            for child in block.children:\n                children_layout.append(get_layout(child))\n            return {\"id\": block._id, \"children\": children_layout}\n        config[\"layout\"] = get_layout(self)\n        for _id, block in self.blocks.items():\n            props = block.get_config() if hasattr(block, \"get_config\") else {}\n            block_config = {\n                \"id\": _id,\n                \"type\": block.get_block_name(),\n                \"props\": utils.delete_none(props),\n            }\n            block_config[\"skip_api\"] = block.skip_api\n            block_config[\"component_class_id\"] = getattr(\n                block, \"component_class_id\", None\n            )\n            if not block.skip_api:\n                block_config[\"api_info\"] = block.api_info()\n                block_config[\"example_inputs\"] = block.example_inputs()\n            config[\"components\"].append(block_config)\n        config[\"dependencies\"] = self.dependencies\n        return config\ndef move_files_to_cache(\n    data: Any,\n    block: Component,\n    postprocess: bool = False,\n    add_urls=False,\n) -> dict:\n    \"\"\"Move any files in `data` to cache and (optionally), adds URL prefixes (/file=...) needed to access the cached file.\n    Also handles the case where the file is on an external Gradio app (/proxy=...).\n    Runs after .postprocess() and before .preprocess().\n    Args:\n        data: The input or output data for a component. Can be a dictionary or a dataclass\n        block: The component whose data is being processed\n        postprocess: Whether its running from postprocessing\n        root_url: The root URL of the local server, if applicable\n    \"\"\"\n    def _move_to_cache(d: dict):\n        payload = FileData(**d)\n        if payload.url and postprocess:\n            payload.path = payload.url\n        elif not block.proxy_url:\n            temp_file_path = block.move_resource_to_block_cache(payload.path)\n            if temp_file_path is None:\n                raise ValueError(\"Did not determine a file path for the resource.\")\n            payload.path = temp_file_path\n        if add_urls:\n            url_prefix = \"/stream/\" if payload.is_stream else \"/file=\"\n            if block.proxy_url:\n                proxy_url = block.proxy_url.rstrip(\"/\")\n                url = f\"/proxy={proxy_url}{url_prefix}{payload.path}\"\n            elif client_utils.is_http_url_like(payload.path) or payload.path.startswith(\n                f\"{url_prefix}\"\n            ):\n                url = payload.path\n            else:\n                url = f\"{url_prefix}{payload.path}\"\n            payload.url = url\n        return payload.model_dump()\n    if isinstance(data, (GradioRootModel, GradioModel)):\n        data = data.model_dump()\n    return client_utils.traverse(data, _move_to_cache, client_utils.is_file_obj)\ndef add_root_url(data: dict, root_url: str, previous_root_url: str | None) -> dict:\n    def _add_root_url(file_dict: dict):\n        if not client_utils.is_http_url_like(file_dict[\"url\"]):\n            if previous_root_url and file_dict[\"url\"].startswith(previous_root_url):\n                file_dict[\"url\"] = file_dict[\"url\"][len(previous_root_url) :]\n            file_dict[\"url\"] = f'{root_url}{file_dict[\"url\"]}'\n        return file_dict\n    return client_utils.traverse(data, _add_root_url, client_utils.is_file_obj_with_url)\n    def __init__(self, **kwargs):\n        self.tokens = {}\n        self.auth = None\n        self.blocks: gradio.Blocks | None = None\n        self.state_holder = StateHolder()\n        self.iterators: dict[str, AsyncIterator] = {}\n        self.iterators_to_reset: set[str] = set()\n        self.lock = utils.safe_get_lock()\n        self.cookie_id = secrets.token_urlsafe(32)\n        self.queue_token = secrets.token_urlsafe(32)\n        self.startup_events_triggered = False\n        self.uploaded_file_dir = os.environ.get(\"GRADIO_TEMP_DIR\") or str(\n            (Path(tempfile.gettempdir()) / \"gradio\").resolve()\n        )\n        self.change_event: None | threading.Event = None\n        self._asyncio_tasks: list[asyncio.Task] = []\n        kwargs.setdefault(\"docs_url\", None)\n        kwargs.setdefault(\"redoc_url\", None)\n        super().__init__(**kwargs)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-1728",
        "description": "[{'lang': 'en', 'value': 'gradio-app/gradio is vulnerable to a local file inclusion vulnerability due to improper validation of user-supplied input in the UploadButton component. Attackers can exploit this vulnerability to read arbitrary files on the filesystem, such as private SSH keys, by manipulating the file path in the request to the `/queue/join` endpoint. This issue could potentially lead to remote code execution. The vulnerability is present in the handling of file upload paths, allowing attackers to redirect file uploads to unintended locations on the server.'}]",
        "cwe_number": 22
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-268",
      "code": "def get_song_relations(id):\n    top = request.args.get('top')\n    song = Song.query.filter_by(id=id).first()\n    if not song:\n        return route_not_found(song)\n    return make_response(jsonify(song.get_related_songs_json(top)), 200)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2017-20172",
        "description": "[{'lang': 'en', 'value': 'A vulnerability was found in ridhoq soundslike. It has been classified as critical. Affected is the function get_song_relations of the file app/api/songs.py. The manipulation leads to sql injection. The patch is identified as 90bb4fb667d9253d497b619b9adaac83bf0ce0f8. It is recommended to apply a patch to fix this issue. VDB-218490 is the identifier assigned to this vulnerability.'}]",
        "cwe_number": 89
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-269",
      "code": "    def traverse(cls, base, request, path_items):\n        \"\"\"See ``zope.app.pagetemplate.engine``.\"\"\"\n        path_items = list(path_items)\n        path_items.reverse()\n        while path_items:\n            name = path_items.pop()\n            if ITraversable.providedBy(base):\n                base = getattr(base, cls.traverseMethod)(name)\n            else:\n                base = traversePathElement(base, name, path_items,\n                                           request=request)\n        return base",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-32633",
        "description": "[{'lang': 'en', 'value': 'Zope is an open-source web application server. In Zope versions prior to 4.6 and 5.2, users can access untrusted modules indirectly through Python modules that are available for direct use. By default, only users with the Manager role can add or edit Zope Page Templates through the web, but sites that allow untrusted users to add/edit Zope Page Templates through the web are at risk from this vulnerability. The problem has been fixed in Zope 5.2 and 4.6. As a workaround, a site administrator can restrict adding/editing Zope Page Templates through the web using the standard Zope user/role permission mechanisms. Untrusted users should not be assigned the Zope Manager role and adding/editing Zope Page Templates through the web should be restricted to trusted users only.'}]",
        "cwe_number": 22
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-270",
      "code": "  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.int32(0), shape=[3, 3, 3, 3], dtype=dtypes.qint32)\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          math_ops.quantize_down_and_shrink_range(input=inputs,\n                                                  input_min=[],\n                                                  input_max=4.0,\n                                                  out_type=dtypes.quint8))",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-36005",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. When `tf.quantization.fake_quant_with_min_max_vars_gradient` receives input `min` or `max` that is nonscalar, it gives a `CHECK` fail that can trigger a denial of service attack. We have patched the issue in GitHub commit f3cf67ac5705f4f04721d15e485e192bb319feed. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue.'}]",
        "cwe_number": 617
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-271",
      "code": "    enabled_setting = 'REDIRECT_ENABLED'\n    def __init__(self, settings):\n        if not settings.getbool(self.enabled_setting):\n            raise NotConfigured\n        self.max_redirect_times = settings.getint('REDIRECT_MAX_TIMES')\n        self.priority_adjust = settings.getint('REDIRECT_PRIORITY_ADJUST')\n    def from_crawler(cls, crawler):\n        return cls(crawler.settings)\n    def process_response(self, request, response, spider):\n        if (\n            request.meta.get('dont_redirect', False)\n            or response.status in getattr(spider, 'handle_httpstatus_list', [])\n            or response.status in request.meta.get('handle_httpstatus_list', [])\n            or request.meta.get('handle_httpstatus_all', False)\n        ):\n            return response\n        allowed_status = (301, 302, 303, 307, 308)\n        if 'Location' not in response.headers or response.status not in allowed_status:\n            return response\n        location = safe_url_string(response.headers['Location'])\n        if response.headers['Location'].startswith(b'//'):\n            request_scheme = urlparse(request.url).scheme\n            location = request_scheme + '://' + location.lstrip('/')\n        redirected_url = urljoin(request.url, location)\n        if response.status in (301, 307, 308) or request.method == 'HEAD':\n            redirected = request.replace(url=redirected_url)\n            return self._redirect(redirected, request, spider, response.status)\n        redirected = self._redirect_request_using_get(request, redirected_url)\n        return self._redirect(redirected, request, spider, response.status)\nclass MetaRefreshMiddleware(BaseRedirectMiddleware):\n    def __init__(self, settings):\n        super().__init__(settings)\n        self._ignore_tags = settings.getlist('METAREFRESH_IGNORE_TAGS')\n        self._maxdelay = settings.getint('METAREFRESH_MAXDELAY')\n    def process_response(self, request, response, spider):\n        if (\n            request.meta.get('dont_redirect', False)\n            or request.method == 'HEAD'\n            or not isinstance(response, HtmlResponse)\n        ):\n            return response\n        interval, url = get_meta_refresh(response,\n                                         ignore_tags=self._ignore_tags)\n        if url and interval < self._maxdelay:\n            redirected = self._redirect_request_using_get(request, url)\n            return self._redirect(redirected, request, spider, 'meta refresh')\n        return response",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-0577",
        "description": "[{'lang': 'en', 'value': 'Exposure of Sensitive Information to an Unauthorized Actor in GitHub repository scrapy/scrapy prior to 2.6.1.'}]",
        "cwe_number": 863
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-272",
      "code": "    def _lookup(self, name, *args, **kwargs):\n        instance = self._lookup_loader.get(name.lower(), loader=self._loader, templar=self)\n        if instance is not None:\n            wantlist = kwargs.pop('wantlist', False)\n            from ansible.utils.listify import listify_lookup_plugin_terms\n            loop_terms = listify_lookup_plugin_terms(terms=args, templar=self, loader=self._loader, fail_on_undefined=True, convert_bare=False)\n            try:\n                ran = instance.run(loop_terms, variables=self._available_variables, **kwargs)\n            except (AnsibleUndefinedVariable, UndefinedError) as e:\n                raise AnsibleUndefinedVariable(e)\n            except Exception as e:\n                if self._fail_on_lookup_errors:\n                    raise AnsibleError(\"An unhandled exception occurred while running the lookup plugin '%s'. Error was a %s, \"\n                                       \"original message: %s\" % (name, type(e), e))\n                ran = None\n            if ran:\n                if wantlist:\n                    ran = wrap_var(ran)\n                else:\n                    try:\n                        ran = UnsafeProxy(\",\".join(ran))\n                    except TypeError:\n                        if isinstance(ran, list) and len(ran) == 1:\n                            ran = wrap_var(ran[0])\n                        else:\n                            ran = wrap_var(ran)\n            return ran\n        else:\n            raise AnsibleError(\"lookup plugin (%s) not found\" % name)\n    def do_template(self, data, preserve_trailing_newlines=True, escape_backslashes=True, fail_on_undefined=None, overrides=None, disable_lookups=False):\n        data_newlines = _count_newlines_from_end(data)\n        if fail_on_undefined is None:\n            fail_on_undefined = self._fail_on_undefined_errors\n        try:\n            if overrides is None:\n                myenv = self.environment.overlay()\n            else:\n                myenv = self.environment.overlay(overrides)\n            if data.startswith(JINJA2_OVERRIDE):\n                eol = data.find('\\n')\n                line = data[len(JINJA2_OVERRIDE):eol]\n                data = data[eol+1:]\n                for pair in line.split(','):\n                    (key,val) = pair.split(':')\n                    key = key.strip()\n                    setattr(myenv, key, ast.literal_eval(val.strip()))\n            myenv.filters.update(self._get_filters())\n            myenv.tests.update(self._get_tests())\n            if escape_backslashes:\n                data = _escape_backslashes(data, myenv)\n            try:\n                t = myenv.from_string(data)\n            except TemplateSyntaxError as e:\n                raise AnsibleError(\"template error while templating string: %s. String: %s\" % (to_native(e), to_native(data)))\n            except Exception as e:\n                if 'recursion' in to_native(e):\n                    raise AnsibleError(\"recursive loop detected in template string: %s\" % to_native(data))\n                else:\n                    return data\n            if disable_lookups:\n                t.globals['lookup'] = self._fail_lookup\n            else:\n                t.globals['lookup'] = self._lookup\n            t.globals['finalize'] = self._finalize\n            jvars = AnsibleJ2Vars(self, t.globals)\n            new_context = t.new_context(jvars, shared=True)\n            rf = t.root_render_func(new_context)\n            try:\n                res = j2_concat(rf)\n                if new_context.unsafe:\n                    res = wrap_var(res)\n            except TypeError as te:\n                if 'StrictUndefined' in to_native(te):\n                    errmsg  = \"Unable to look up a name or access an attribute in template string (%s).\\n\" % to_native(data)\n                    errmsg += \"Make sure your variable name does not contain invalid characters like '-': %s\" % to_native(te)\n                    raise AnsibleUndefinedVariable(errmsg)\n                else:\n                    display.debug(\"failing because of a type error, template data is: %s\" % to_native(data))\n                    raise AnsibleError(\"Unexpected templating type error occurred on (%s): %s\" % (to_native(data),to_native(te)))\n            if preserve_trailing_newlines:\n                res_newlines = _count_newlines_from_end(res)\n                if data_newlines > res_newlines:\n                    res += self.environment.newline_sequence * (data_newlines - res_newlines)\n            return res\n        except (UndefinedError, AnsibleUndefinedVariable) as e:\n            if fail_on_undefined:\n                raise AnsibleUndefinedVariable(e)\n            else:\n                return data",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2017-7481",
        "description": "[{'lang': 'en', 'value': \"Ansible before versions 2.3.1.0 and 2.4.0.0 fails to properly mark lookup-plugin results as unsafe. If an attacker could control the results of lookup() calls, they could inject Unicode strings to be parsed by the jinja2 templating system, resulting in code execution. By default, the jinja2 templating language is now marked as 'unsafe' and is not evaluated.\"}]",
        "cwe_number": 20
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-273",
      "code": "if __name__ == \"__main__\":\n  test.main()",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-41889",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. If a list of quantized tensors is assigned to an attribute, the pywrap code fails to parse the tensor and returns a `nullptr`, which is not caught. An example can be seen in `tf.compat.v1.extract_volume_patches` by passing in quantized tensors as input `ksizes`. We have patched the issue in GitHub commit e9e95553e5411834d215e6770c81a83a3d0866ce. The fix will be included in TensorFlow 2.11. We will also cherrypick this commit on TensorFlow 2.10.1, 2.9.3, and TensorFlow 2.8.4, as these are also affected and still in supported range.'}]",
        "cwe_number": 476
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-274",
      "code": "def load_jupyter_server_extension(nbapp):\n    \"\"\"create a LanguageServerManager and add handlers\"\"\"\n    nbapp.add_traits(language_server_manager=traitlets.Instance(LanguageServerManager))\n    manager = nbapp.language_server_manager = LanguageServerManager(parent=nbapp)\n    contents = nbapp.contents_manager\n    page_config = nbapp.web_app.settings.setdefault(\"page_config_data\", {})\n    root_uri = \"\"\n    virtual_documents_uri = \"\"\n    if hasattr(contents, \"root_dir\"):\n        root_uri = normalized_uri(contents.root_dir)\n        nbapp.log.debug(\"[lsp] rootUri will be %s\", root_uri)\n        virtual_documents_uri = normalized_uri(\n            Path(contents.root_dir) / manager.virtual_documents_dir\n        )\n        nbapp.log.debug(\"[lsp] virtualDocumentsUri will be %s\", virtual_documents_uri)\n    else:\n        nbapp.log.warn(\n            \"[lsp] %s did not appear to have a root_dir, could not set rootUri\",\n            contents,\n        )\n    page_config.update(rootUri=root_uri, virtualDocumentsUri=virtual_documents_uri)\n    add_handlers(nbapp)\n    nbapp.io_loop.call_later(0, initialize, nbapp, virtual_documents_uri)\n    def change_version(self, new_version: str, dry: bool):\n        changelog = CHANGELOG.read_text(encoding=\"utf-8\")\n        if new_version not in changelog:\n            raise Exception(\n                (\n                    f\"{new_version} is absent in CHANGELOG.md file.\"\n                    f\" Please update the changelog first.\"\n                ).format(new_version=new_version)\n            )\n        for location in self.locations:\n            replace_version(\n                path=location.path,\n                template=location.template,\n                old=self.current_version,\n                new=new_version,\n                dry=dry,\n            )\ndef normalized_uri(root_dir):\n    \"\"\"Attempt to make an LSP rootUri from a ContentsManager root_dir\n    Special care must be taken around windows paths: the canonical form of\n    windows drives and UNC paths is lower case\n    \"\"\"\n    root_uri = pathlib.Path(root_dir).expanduser().resolve().as_uri()\n    root_uri = re.sub(\n        RE_PATH_ANCHOR, lambda m: \"file://{}\".format(m.group(1).lower()), root_uri\n    )\n    return root_uri\n    def initialize(self, *args, **kwargs):\n        super().initialize(*args, **kwargs)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-22415",
        "description": "[{'lang': 'en', 'value': 'jupyter-lsp is a coding assistance tool for JupyterLab (code navigation + hover suggestions + linters + autocompletion + rename) using Language Server Protocol. Installations of jupyter-lsp running in environments without configured file system access control (on the operating system level), and with jupyter-server instances exposed to non-trusted network are vulnerable to unauthorised access and modification of file system beyond the jupyter root directory. This issue has been patched in version 2.2.2 and all users are advised to upgrade. Users unable to upgrade should uninstall jupyter-lsp.'}]",
        "cwe_number": 22
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-275",
      "code": "  def _testDrawBoundingBoxColorCycling(self, img, colors=None):\n    \"\"\"Tests if cycling works appropriately.\n    Args:\n      img: 3-D numpy image on which to draw.\n    \"\"\"\n    color_table = colors\n    if colors is None:\n      color_table = np.asarray([[1, 1, 0, 1], [0, 0, 1, 1], [1, 0, 0, 1],\n                                [0, 1, 0, 1], [0.5, 0, 0.5,\n                                               1], [0.5, 0.5, 0, 1],\n                                [0.5, 0, 0, 1], [0, 0, 0.5, 1], [0, 1, 1, 1],\n                                [1, 0, 1, 1]])\n    assert len(img.shape) == 3\n    depth = img.shape[2]\n    assert depth <= color_table.shape[1]\n    assert depth == 1 or depth == 3 or depth == 4\n    if depth == 1:\n      color_table[:, 0] = 1\n    num_colors = color_table.shape[0]\n    for num_boxes in range(1, num_colors + 2):\n      image = np.copy(img)\n      color = color_table[(num_boxes - 1) % num_colors, 0:depth]\n      test_drawn_image = self._fillBorder(image, color)\n      bboxes = np.asarray([0, 0, 1, 1])\n      bboxes = np.vstack([bboxes for _ in range(num_boxes)])\n      bboxes = math_ops.cast(bboxes, dtypes.float32)\n      bboxes = array_ops.expand_dims(bboxes, 0)\n      image = ops.convert_to_tensor(image)\n      image = image_ops_impl.convert_image_dtype(image, dtypes.float32)\n      image = array_ops.expand_dims(image, 0)\n      image = image_ops.draw_bounding_boxes(image, bboxes, colors=colors)\n      with self.cached_session(use_gpu=False) as sess:\n        op_drawn_image = np.squeeze(sess.run(image), 0)\n        self.assertAllEqual(test_drawn_image, op_drawn_image)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-36001",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. When `DrawBoundingBoxes` receives an input `boxes` that is not of dtype `float`, it gives a `CHECK` fail that can trigger a denial of service attack. We have patched the issue in GitHub commit da0d65cdc1270038e72157ba35bf74b85d9bda11. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue.'}]",
        "cwe_number": 617
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-276",
      "code": "    def create_app(\n        blocks: gradio.Blocks, app_kwargs: Dict[str, Any] | None = None\n    ) -> App:\n        app_kwargs = app_kwargs or {}\n        app_kwargs.setdefault(\"default_response_class\", ORJSONResponse)\n        app = App(**app_kwargs)\n        app.configure_app(blocks)\n        if not wasm_utils.IS_WASM:\n            app.add_middleware(\n                CORSMiddleware,\n                allow_origins=[\"*\"],\n                allow_methods=[\"*\"],\n                allow_headers=[\"*\"],\n            )\n        @app.get(\"/user\")\n        @app.get(\"/user/\")\n        def get_current_user(request: fastapi.Request) -> Optional[str]:\n            token = request.cookies.get(\n                f\"access-token-{app.cookie_id}\"\n            ) or request.cookies.get(f\"access-token-unsecure-{app.cookie_id}\")\n            return app.tokens.get(token)\n        @app.get(\"/login_check\")\n        @app.get(\"/login_check/\")\n        def login_check(user: str = Depends(get_current_user)):\n            if app.auth is None or user is not None:\n                return\n            raise HTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Not authenticated\"\n            )\n        @app.get(\"/token\")\n        @app.get(\"/token/\")\n        def get_token(request: fastapi.Request) -> dict:\n            token = request.cookies.get(f\"access-token-{app.cookie_id}\")\n            return {\"token\": token, \"user\": app.tokens.get(token)}\n        @app.get(\"/app_id\")\n        @app.get(\"/app_id/\")\n        def app_id(request: fastapi.Request) -> dict:\n            return {\"app_id\": app.get_blocks().app_id}\n        @app.get(\"/dev/reload\", dependencies=[Depends(login_check)])\n        async def notify_changes(\n            request: fastapi.Request,\n        ):\n            async def reload_checker(request: fastapi.Request):\n                heartbeat_rate = 15\n                check_rate = 0.05\n                last_heartbeat = time.perf_counter()\n                while True:\n                    if await request.is_disconnected():\n                        return\n                    if app.change_event and app.change_event.is_set():\n                        app.change_event.clear()\n                        yield \"\"\"data: CHANGE\\n\\n\"\"\"\n                    await asyncio.sleep(check_rate)\n                    if time.perf_counter() - last_heartbeat > heartbeat_rate:\n                        yield \"\"\"data: HEARTBEAT\\n\\n\"\"\"\n                        last_heartbeat = time.time()\n            return StreamingResponse(\n                reload_checker(request),\n                media_type=\"text/event-stream\",\n            )\n        @app.post(\"/login\")\n        @app.post(\"/login/\")\n        def login(form_data: OAuth2PasswordRequestForm = Depends()):\n            username, password = form_data.username.strip(), form_data.password\n            if app.auth is None:\n                return RedirectResponse(url=\"/\", status_code=status.HTTP_302_FOUND)\n            if (\n                not callable(app.auth)\n                and username in app.auth\n                and app.auth[username] == password\n            ) or (callable(app.auth) and app.auth.__call__(username, password)):\n                token = secrets.token_urlsafe(16)\n                app.tokens[token] = username\n                response = JSONResponse(content={\"success\": True})\n                response.set_cookie(\n                    key=f\"access-token-{app.cookie_id}\",\n                    value=token,\n                    httponly=True,\n                    samesite=\"none\",\n                    secure=True,\n                )\n                response.set_cookie(\n                    key=f\"access-token-unsecure-{app.cookie_id}\",\n                    value=token,\n                    httponly=True,\n                )\n                return response\n            else:\n                raise HTTPException(status_code=400, detail=\"Incorrect credentials.\")\n        if app.blocks is not None and app.blocks.expects_oauth:\n            attach_oauth(app)\n        @app.head(\"/\", response_class=HTMLResponse)\n        @app.get(\"/\", response_class=HTMLResponse)\n        def main(request: fastapi.Request, user: str = Depends(get_current_user)):\n            mimetypes.add_type(\"application/javascript\", \".js\")\n            blocks = app.get_blocks()\n            root = route_utils.get_root_url(\n                request=request, route_path=\"/\", root_path=app.root_path\n            )\n            if app.auth is None or user is not None:\n                config = app.get_blocks().config\n                config = route_utils.update_root_in_config(config, root)\n            else:\n                config = {\n                    \"auth_required\": True,\n                    \"auth_message\": blocks.auth_message,\n                    \"space_id\": app.get_blocks().space_id,\n                    \"root\": root,\n                }\n            try:\n                template = (\n                    \"frontend/share.html\" if blocks.share else \"frontend/index.html\"\n                )\n                return templates.TemplateResponse(\n                    template,\n                    {\"request\": request, \"config\": config},\n                )\n            except TemplateNotFound as err:\n                if blocks.share:\n                    raise ValueError(\n                        \"Did you install Gradio from source files? Share mode only \"\n                        \"works when Gradio is installed through the pip package.\"\n                    ) from err\n                else:\n                    raise ValueError(\n                        \"Did you install Gradio from source files? You need to build \"\n                        \"the frontend by running /scripts/build_frontend.sh\"\n                    ) from err\n        @app.get(\"/info/\", dependencies=[Depends(login_check)])\n        @app.get(\"/info\", dependencies=[Depends(login_check)])\n        def api_info():\n            return app.get_blocks().get_api_info()\n        @app.get(\"/config/\", dependencies=[Depends(login_check)])\n        @app.get(\"/config\", dependencies=[Depends(login_check)])\n        def get_config(request: fastapi.Request):\n            config = app.get_blocks().config\n            root = route_utils.get_root_url(\n                request=request, route_path=\"/config\", root_path=app.root_path\n            )\n            config = route_utils.update_root_in_config(config, root)\n            return ORJSONResponse(content=config)\n        @app.get(\"/static/{path:path}\")\n        def static_resource(path: str):\n            static_file = safe_join(STATIC_PATH_LIB, path)\n            return FileResponse(static_file)\n        @app.get(\"/custom_component/{id}/{type}/{file_name}\")\n        def custom_component_path(id: str, type: str, file_name: str):\n            config = app.get_blocks().config\n            components = config[\"components\"]\n            location = next(\n                (item for item in components if item[\"component_class_id\"] == id), None\n            )\n            if location is None:\n                raise HTTPException(status_code=404, detail=\"Component not found.\")\n            component_instance = app.get_blocks().get_component(location[\"id\"])\n            module_name = component_instance.__class__.__module__\n            module_path = sys.modules[module_name].__file__\n            if module_path is None or component_instance is None:\n                raise HTTPException(status_code=404, detail=\"Component not found.\")\n            return FileResponse(\n                safe_join(\n                    str(Path(module_path).parent),\n                    f\"{component_instance.__class__.TEMPLATE_DIR}/{type}/{file_name}\",\n                )\n            )\n        @app.get(\"/assets/{path:path}\")\n        def build_resource(path: str):\n            build_file = safe_join(BUILD_PATH_LIB, path)\n            return FileResponse(build_file)\n        @app.get(\"/favicon.ico\")\n        async def favicon():\n            blocks = app.get_blocks()\n            if blocks.favicon_path is None:\n                return static_resource(\"img/logo.svg\")\n            else:\n                return FileResponse(blocks.favicon_path)\n        @app.head(\"/proxy={url_path:path}\", dependencies=[Depends(login_check)])\n        @app.get(\"/proxy={url_path:path}\", dependencies=[Depends(login_check)])\n        async def reverse_proxy(url_path: str):\n            try:\n                rp_req = app.build_proxy_request(url_path)\n            except PermissionError as err:\n                raise HTTPException(status_code=400, detail=str(err)) from err\n            rp_resp = await client.send(rp_req, stream=True)\n            return StreamingResponse(\n                rp_resp.aiter_raw(),\n                status_code=rp_resp.status_code,\n                headers=rp_resp.headers,\n                background=BackgroundTask(rp_resp.aclose),\n            )\n        @app.head(\"/file={path_or_url:path}\", dependencies=[Depends(login_check)])\n        @app.get(\"/file={path_or_url:path}\", dependencies=[Depends(login_check)])\n        async def file(path_or_url: str, request: fastapi.Request):\n            blocks = app.get_blocks()\n            if client_utils.is_http_url_like(path_or_url):\n                return RedirectResponse(\n                    url=path_or_url, status_code=status.HTTP_302_FOUND\n                )\n            abs_path = utils.abspath(path_or_url)\n            in_blocklist = any(\n                utils.is_in_or_equal(abs_path, blocked_path)\n                for blocked_path in blocks.blocked_paths\n            )\n            is_dir = abs_path.is_dir()\n            if in_blocklist or is_dir:\n                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")\n            created_by_app = str(abs_path) in set().union(*blocks.temp_file_sets)\n            in_allowlist = any(\n                utils.is_in_or_equal(abs_path, allowed_path)\n                for allowed_path in blocks.allowed_paths\n            )\n            was_uploaded = utils.is_in_or_equal(abs_path, app.uploaded_file_dir)\n            is_cached_example = utils.is_in_or_equal(\n                abs_path, utils.abspath(utils.get_cache_folder())\n            )\n            if not (\n                created_by_app or in_allowlist or was_uploaded or is_cached_example\n            ):\n                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")\n            if not abs_path.exists():\n                raise HTTPException(404, f\"File not found: {path_or_url}.\")\n            range_val = request.headers.get(\"Range\", \"\").strip()\n            if range_val.startswith(\"bytes=\") and \"-\" in range_val:\n                range_val = range_val[6:]\n                start, end = range_val.split(\"-\")\n                if start.isnumeric() and end.isnumeric():\n                    start = int(start)\n                    end = int(end)\n                    response = ranged_response.RangedFileResponse(\n                        abs_path,\n                        ranged_response.OpenRange(start, end),\n                        dict(request.headers),\n                        stat_result=os.stat(abs_path),\n                    )\n                    return response\n            return FileResponse(abs_path, headers={\"Accept-Ranges\": \"bytes\"})\n        @app.get(\n            \"/stream/{session_hash}/{run}/{component_id}\",\n            dependencies=[Depends(login_check)],\n        )\n        async def stream(\n            session_hash: str,\n            run: int,\n            component_id: int,\n            request: fastapi.Request,\n        ):\n            stream: list = (\n                app.get_blocks()\n                .pending_streams[session_hash]\n                .get(run, {})\n                .get(component_id, None)\n            )\n            if stream is None:\n                raise HTTPException(404, \"Stream not found.\")\n            def stream_wrapper():\n                check_stream_rate = 0.01\n                max_wait_time = 120\n                wait_time = 0\n                while True:\n                    if len(stream) == 0:\n                        if wait_time > max_wait_time:\n                            return\n                        wait_time += check_stream_rate\n                        time.sleep(check_stream_rate)\n                        continue\n                    wait_time = 0\n                    next_stream = stream.pop(0)\n                    if next_stream is None:\n                        return\n                    yield next_stream\n            return StreamingResponse(stream_wrapper())\n        @app.get(\"/file/{path:path}\", dependencies=[Depends(login_check)])\n        async def file_deprecated(path: str, request: fastapi.Request):\n            return await file(path, request)\n        @app.post(\"/reset/\")\n        @app.post(\"/reset\")\n        async def reset_iterator(body: ResetBody):\n            if body.event_id not in app.iterators:\n                return {\"success\": False}\n            async with app.lock:\n                del app.iterators[body.event_id]\n                app.iterators_to_reset.add(body.event_id)\n                await app.get_blocks()._queue.clean_events(event_id=body.event_id)\n            return {\"success\": True}\n        @app.post(\"/run/{api_name}\", dependencies=[Depends(login_check)])\n        @app.post(\"/run/{api_name}/\", dependencies=[Depends(login_check)])\n        @app.post(\"/api/{api_name}\", dependencies=[Depends(login_check)])\n        @app.post(\"/api/{api_name}/\", dependencies=[Depends(login_check)])\n        async def predict(\n            api_name: str,\n            body: PredictBody,\n            request: fastapi.Request,\n            username: str = Depends(get_current_user),\n        ):\n            fn_index_inferred = route_utils.infer_fn_index(\n                app=app, api_name=api_name, body=body\n            )\n            if not app.get_blocks().api_open and app.get_blocks().queue_enabled_for_fn(\n                fn_index_inferred\n            ):\n                raise HTTPException(\n                    detail=\"This API endpoint does not accept direct HTTP POST requests. Please join the queue to use this API.\",\n                    status_code=status.HTTP_404_NOT_FOUND,\n                )\n            gr_request = route_utils.compile_gr_request(\n                app,\n                body,\n                fn_index_inferred=fn_index_inferred,\n                username=username,\n                request=request,\n            )\n            try:\n                output = await route_utils.call_process_api(\n                    app=app,\n                    body=body,\n                    gr_request=gr_request,\n                    fn_index_inferred=fn_index_inferred,\n                )\n            except BaseException as error:\n                show_error = app.get_blocks().show_error or isinstance(error, Error)\n                traceback.print_exc()\n                return JSONResponse(\n                    content={\"error\": str(error) if show_error else None},\n                    status_code=500,\n                )\n            root_path = route_utils.get_root_url(\n                request=request, route_path=f\"/api/{api_name}\", root_path=app.root_path\n            )\n            output = add_root_url(output, root_path, None)\n            return output\n        @app.get(\"/queue/data\", dependencies=[Depends(login_check)])\n        async def queue_data(\n            request: fastapi.Request,\n            session_hash: str,\n        ):\n            blocks = app.get_blocks()\n            root_path = route_utils.get_root_url(\n                request=request, route_path=\"/queue/data\", root_path=app.root_path\n            )\n            async def sse_stream(request: fastapi.Request):\n                try:\n                    last_heartbeat = time.perf_counter()\n                    while True:\n                        if await request.is_disconnected():\n                            await blocks._queue.clean_events(session_hash=session_hash)\n                            return\n                        if (\n                            session_hash\n                            not in blocks._queue.pending_messages_per_session\n                        ):\n                            raise HTTPException(\n                                status_code=status.HTTP_404_NOT_FOUND,\n                                detail=\"Session not found.\",\n                            )\n                        heartbeat_rate = 15\n                        check_rate = 0.05\n                        message = None\n                        try:\n                            messages = blocks._queue.pending_messages_per_session[\n                                session_hash\n                            ]\n                            message = messages.get_nowait()\n                        except EmptyQueue:\n                            await asyncio.sleep(check_rate)\n                            if time.perf_counter() - last_heartbeat > heartbeat_rate:\n                                message = {\n                                    \"msg\": ServerMessage.heartbeat,\n                                }\n                                last_heartbeat = time.perf_counter()\n                        if blocks._queue.stopped:\n                            message = {\n                                \"msg\": \"unexpected_error\",\n                                \"message\": \"Server stopped unexpectedly.\",\n                                \"success\": False,\n                            }\n                        if message:\n                            add_root_url(message, root_path, None)\n                            yield f\"data: {json.dumps(message)}\\n\\n\"\n                            if message[\"msg\"] == ServerMessage.process_completed:\n                                blocks._queue.pending_event_ids_session[\n                                    session_hash\n                                ].remove(message[\"event_id\"])\n                                if message[\"msg\"] == ServerMessage.server_stopped or (\n                                    message[\"msg\"] == ServerMessage.process_completed\n                                    and (\n                                        len(\n                                            blocks._queue.pending_event_ids_session[\n                                                session_hash\n                                            ]\n                                        )\n                                        == 0\n                                    )\n                                ):\n                                    return\n                except BaseException as e:\n                    message = {\n                        \"msg\": \"unexpected_error\",\n                        \"success\": False,\n                        \"message\": str(e),\n                    }\n                    yield f\"data: {json.dumps(message)}\\n\\n\"\n                    if isinstance(e, asyncio.CancelledError):\n                        del blocks._queue.pending_messages_per_session[session_hash]\n                        await blocks._queue.clean_events(session_hash=session_hash)\n                    raise e\n            return StreamingResponse(\n                sse_stream(request),\n                media_type=\"text/event-stream\",\n            )\n        @app.post(\"/queue/join\", dependencies=[Depends(login_check)])\n        async def queue_join(\n            body: PredictBody,\n            request: fastapi.Request,\n            username: str = Depends(get_current_user),\n        ):\n            blocks = app.get_blocks()\n            if blocks._queue.server_app is None:\n                blocks._queue.set_server_app(app)\n            if blocks._queue.stopped:\n                raise HTTPException(\n                    status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n                    detail=\"Queue is stopped.\",\n                )\n            success, event_id = await blocks._queue.push(body, request, username)\n            if not success:\n                status_code = (\n                    status.HTTP_503_SERVICE_UNAVAILABLE\n                    if \"Queue is full.\" in event_id\n                    else status.HTTP_400_BAD_REQUEST\n                )\n                raise HTTPException(status_code=status_code, detail=event_id)\n            return {\"event_id\": event_id}\n        @app.post(\"/component_server\", dependencies=[Depends(login_check)])\n        @app.post(\"/component_server/\", dependencies=[Depends(login_check)])\n        def component_server(body: ComponentServerBody):\n            state = app.state_holder[body.session_hash]\n            component_id = body.component_id\n            block: Block\n            if component_id in state:\n                block = state[component_id]\n            else:\n                block = app.get_blocks().blocks[component_id]\n            fn = getattr(block, body.fn_name, None)\n            if fn is None or not getattr(fn, \"_is_server_fn\", False):\n                raise HTTPException(\n                    status_code=status.HTTP_404_NOT_FOUND,\n                    detail=\"Function not found.\",\n                )\n            return fn(body.data)\n        @app.get(\n            \"/queue/status\",\n            dependencies=[Depends(login_check)],\n            response_model=Estimation,\n        )\n        async def get_queue_status():\n            return app.get_blocks()._queue.get_status()\n        @app.get(\"/upload_progress\")\n        def get_upload_progress(upload_id: str, request: fastapi.Request):\n            async def sse_stream(request: fastapi.Request):\n                last_heartbeat = time.perf_counter()\n                is_done = False\n                while True:\n                    if await request.is_disconnected():\n                        file_upload_statuses.stop_tracking(upload_id)\n                        return\n                    if is_done:\n                        file_upload_statuses.stop_tracking(upload_id)\n                        return\n                    heartbeat_rate = 15\n                    check_rate = 0.05\n                    try:\n                        if file_upload_statuses.is_done(upload_id):\n                            message = {\"msg\": \"done\"}\n                            is_done = True\n                        else:\n                            update = file_upload_statuses.pop(upload_id)\n                            message = {\n                                \"msg\": \"update\",\n                                \"orig_name\": update.filename,\n                                \"chunk_size\": update.chunk_size,\n                            }\n                        yield f\"data: {json.dumps(message)}\\n\\n\"\n                    except FileUploadProgressNotTrackedError:\n                        return\n                    except FileUploadProgressNotQueuedError:\n                        await asyncio.sleep(check_rate)\n                        if time.perf_counter() - last_heartbeat > heartbeat_rate:\n                            message = {\"msg\": \"heartbeat\"}\n                            yield f\"data: {json.dumps(message)}\\n\\n\"\n                            last_heartbeat = time.perf_counter()\n            return StreamingResponse(\n                sse_stream(request),\n                media_type=\"text/event-stream\",\n            )\n        @app.post(\"/upload\", dependencies=[Depends(login_check)])\n        async def upload_file(\n            request: fastapi.Request,\n            bg_tasks: BackgroundTasks,\n            upload_id: Optional[str] = None,\n        ):\n            content_type_header = request.headers.get(\"Content-Type\")\n            content_type: bytes\n            content_type, _ = parse_options_header(content_type_header)\n            if content_type != b\"multipart/form-data\":\n                raise HTTPException(status_code=400, detail=\"Invalid content type.\")\n            try:\n                if upload_id:\n                    file_upload_statuses.track(upload_id)\n                multipart_parser = GradioMultiPartParser(\n                    request.headers,\n                    request.stream(),\n                    max_files=1000,\n                    max_fields=1000,\n                    upload_id=upload_id if upload_id else None,\n                    upload_progress=file_upload_statuses if upload_id else None,\n                )\n                form = await multipart_parser.parse()\n            except MultiPartException as exc:\n                raise HTTPException(status_code=400, detail=exc.message) from exc\n            output_files = []\n            files_to_copy = []\n            locations: list[str] = []\n            for temp_file in form.getlist(\"files\"):\n                assert isinstance(temp_file, GradioUploadFile)\n                if temp_file.filename:\n                    file_name = Path(temp_file.filename).name\n                    name = client_utils.strip_invalid_filename_characters(file_name)\n                else:\n                    name = f\"tmp{secrets.token_hex(5)}\"\n                directory = Path(app.uploaded_file_dir) / temp_file.sha.hexdigest()\n                directory.mkdir(exist_ok=True, parents=True)\n                dest = (directory / name).resolve()\n                temp_file.file.close()\n                try:\n                    os.rename(temp_file.file.name, dest)\n                except OSError:\n                    files_to_copy.append(temp_file.file.name)\n                    locations.append(str(dest))\n                output_files.append(dest)\n            if files_to_copy:\n                bg_tasks.add_task(\n                    move_uploaded_files_to_cache, files_to_copy, locations\n                )\n            return output_files\n        @app.on_event(\"startup\")\n        @app.get(\"/startup-events\")\n        async def startup_events():\n            if not app.startup_events_triggered:\n                app.get_blocks().startup_events()\n                app.startup_events_triggered = True\n                return True\n            return False\n        @app.get(\"/theme.css\", response_class=PlainTextResponse)\n        def theme_css():\n            return PlainTextResponse(app.get_blocks().theme_css, media_type=\"text/css\")\n        @app.get(\"/robots.txt\", response_class=PlainTextResponse)\n        def robots_txt():\n            if app.get_blocks().share:\n                return \"User-agent: *\\nDisallow: /\"\n            else:\n                return \"User-agent: *\\nDisallow: \"\n        return app\ndef update_root_in_config(config: dict, root: str) -> dict:\n    \"\"\"\n    Updates the root \"key\" in the config dictionary to the new root url. If the\n    root url has changed, all of the urls in the config that correspond to component\n    file urls are updated to use the new root url.\n    \"\"\"\n    previous_root = config.get(\"root\", None)\n    if previous_root is None or previous_root != root:\n        config[\"root\"] = root\n        config = processing_utils.add_root_url(config, root, previous_root)\n    return config",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-1729",
        "description": "[{'lang': 'en', 'value': 'A timing attack vulnerability exists in the gradio-app/gradio repository, specifically within the login function in routes.py. The vulnerability arises from the use of a direct comparison operation (`app.auth[username] == password`) to validate user credentials, which can be exploited to guess passwords based on response times. Successful exploitation of this vulnerability could allow an attacker to bypass authentication mechanisms and gain unauthorized access.'}]",
        "cwe_number": 367
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-277",
      "code": "def log_request(handler):\n    \"\"\"log a bit more information about each request than tornado's default\n    - move static file get success to debug-level (reduces noise)\n    - get proxied IP instead of proxy IP\n    - log referer for redirect and failed requests\n    - log user-agent for failed requests\n    \"\"\"\n    status = handler.get_status()\n    request = handler.request\n    try:\n        logger = handler.log\n    except AttributeError:\n        logger = access_log\n    if status < 300 or status == 304:\n        log_method = logger.debug\n    elif status < 400:\n        log_method = logger.info\n    elif status < 500:\n        log_method = logger.warning\n    else:\n        log_method = logger.error\n    request_time = 1000.0 * handler.request.request_time()\n    ns = dict(\n        status=status,\n        method=request.method,\n        ip=request.remote_ip,\n        uri=request.uri,\n        request_time=request_time,\n    )\n    msg = \"{status} {method} {uri} ({ip}) {request_time:.2f}ms\"\n    if status >= 400:\n        ns[\"referer\"] = request.headers.get(\"Referer\", \"None\")\n        msg = msg + \" referer={referer}\"\n    if status >= 500 and status != 502:\n        log_method(json.dumps(dict(request.headers), indent=2))\n    log_method(msg.format(**ns))\n    prometheus_log_method(handler)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-24757",
        "description": "[{'lang': 'en', 'value': 'The Jupyter Server provides the backend (i.e. the core services, APIs, and REST endpoints) for Jupyter web applications. Prior to version 1.15.4, unauthorized actors can access sensitive information from server logs. Anytime a 5xx error is triggered, the auth cookie and other header values are recorded in Jupyter Server logs by default. Considering these logs do not require root access, an attacker can monitor these logs, steal sensitive auth/cookie information, and gain access to the Jupyter server. Jupyter Server version 1.15.4 contains a patch for this issue. There are currently no known workarounds.'}]",
        "cwe_number": 532
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-278",
      "code": "    async def login(cls, username: str, password: str) -> t.Optional[int]:\n        \"\"\"\n        Make sure the user exists and the password is valid. If so, the\n        ``last_login`` value is updated in the database.\n        :returns:\n            The id of the user if a match is found, otherwise ``None``.\n        \"\"\"\n        if len(username) > cls.username.length:\n            logger.warning(\"Excessively long username provided.\")\n            return None\n        if len(password) > cls._max_password_length:\n            logger.warning(\"Excessively long password provided.\")\n            return None\n        response = (\n            await cls.select(cls._meta.primary_key, cls.password)\n            .where(cls.username == username)\n            .first()\n            .run()\n        )\n        if not response:\n            return None\n        stored_password = response[\"password\"]\n        algorithm, iterations_, salt, hashed = cls.split_stored_password(\n            stored_password\n        )\n        iterations = int(iterations_)\n        if cls.hash_password(password, salt, iterations) == stored_password:\n            if iterations != cls._pbkdf2_iteration_count:\n                await cls.update_password(username, password)\n            await cls.update({cls.last_login: datetime.datetime.now()}).where(\n                cls.username == username\n            )\n            return response[\"id\"]\n        else:\n            return None",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-41885",
        "description": "[{'lang': 'en', 'value': 'Piccolo is an ORM and query builder which supports asyncio. In versions 0.120.0 and prior, the implementation of `BaseUser.login` leaks enough information to a malicious user such that they would be able to successfully generate a list of valid users on the platform. As Piccolo on its own does not also enforce strong passwords, these lists of valid accounts are likely to be used in a password spray attack with the outcome being attempted takeover of user accounts on the platform. The impact of this vulnerability is minor as it requires chaining with other attack vectors in order to gain more then simply a list of valid users on the underlying platform. The likelihood of this vulnerability is possible as it requires minimal skills to pull off, especially given the underlying login functionality for Piccolo based sites is open source. This issue has been patched in version 0.121.0.'}]",
        "cwe_number": 203
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-279",
      "code": "    def get_groups(self, env, token):\n        \"\"\"Get groups for the given token.\n        :param env: The current WSGI environment dictionary.\n        :param token: Token to validate and return a group string for.\n        :returns: None if the token is invalid or a string containing a comma\n                  separated list of groups the authenticated user is a member\n                  of. The first group in the list is also considered a unique\n                  identifier for that user.\n        \"\"\"\n        groups = None\n        memcache_client = cache_from_env(env)\n        if memcache_client:\n            memcache_key = '%s/auth/%s' % (self.reseller_prefix, token)\n            cached_auth_data = memcache_client.get(memcache_key)\n            if cached_auth_data:\n                expires, groups = cached_auth_data\n                if expires < time():\n                    groups = None\n        if env.get('HTTP_AUTHORIZATION'):\n            if not self.s3_support:\n                self.logger.warning('S3 support is disabled in swauth.')\n                return None\n            if self.swauth_remote:\n                self.logger.warning('S3-style authorization not supported yet '\n                                 'with swauth_remote mode.')\n                return None\n            try:\n                account = env['HTTP_AUTHORIZATION'].split(' ')[1]\n                account, user, sign = account.split(':')\n            except Exception:\n                self.logger.debug(\n                    'Swauth cannot parse Authorization header value %r' %\n                    env['HTTP_AUTHORIZATION'])\n                return None\n            path = quote('/v1/%s/%s/%s' % (self.auth_account, account, user))\n            resp = self.make_pre_authed_request(\n                env, 'GET', path).get_response(self.app)\n            if resp.status_int // 100 != 2:\n                return None\n            if 'x-object-meta-account-id' in resp.headers:\n                account_id = resp.headers['x-object-meta-account-id']\n            else:\n                path = quote('/v1/%s/%s' % (self.auth_account, account))\n                resp2 = self.make_pre_authed_request(\n                    env, 'HEAD', path).get_response(self.app)\n                if resp2.status_int // 100 != 2:\n                    return None\n                account_id = resp2.headers['x-container-meta-account-id']\n            path = env['PATH_INFO']\n            env['PATH_INFO'] = path.replace(\"%s:%s\" % (account, user),\n                                            account_id, 1)\n            detail = json.loads(resp.body)\n            if detail:\n                creds = detail.get('auth')\n                try:\n                    auth_encoder, creds_dict = \\\n                        swauth.authtypes.validate_creds(creds)\n                except ValueError as e:\n                    self.logger.error('%s' % e.args[0])\n                    return None\n            password = creds_dict['hash']\n            msg = base64.urlsafe_b64decode(unquote(token))\n            if isinstance(password, unicode):\n                password = password.encode('utf-8')\n            if isinstance(msg, unicode):\n                msg = msg.encode('utf-8')\n            s = base64.encodestring(hmac.new(password,\n                                             msg, sha1).digest()).strip()\n            if s != sign:\n                return None\n            groups = [g['name'] for g in detail['groups']]\n            if '.admin' in groups:\n                groups.remove('.admin')\n                groups.append(account_id)\n            groups = ','.join(groups)\n            return groups\n        if not groups:\n            if self.swauth_remote:\n                with Timeout(self.swauth_remote_timeout):\n                    conn = http_connect(self.swauth_remote_parsed.hostname,\n                        self.swauth_remote_parsed.port, 'GET',\n                        '%s/v2/.token/%s' % (self.swauth_remote_parsed.path,\n                                             quote(token)),\n                        ssl=(self.swauth_remote_parsed.scheme == 'https'))\n                    resp = conn.getresponse()\n                    resp.read()\n                    conn.close()\n                if resp.status // 100 != 2:\n                    return None\n                expires_from_now = float(resp.getheader('x-auth-ttl'))\n                groups = resp.getheader('x-auth-groups')\n                if memcache_client:\n                    memcache_client.set(\n                        memcache_key, (time() + expires_from_now, groups),\n                        time=expires_from_now)\n            else:\n                path = quote('/v1/%s/.token_%s/%s' %\n                             (self.auth_account, token[-1], token))\n                resp = self.make_pre_authed_request(\n                    env, 'GET', path).get_response(self.app)\n                if resp.status_int // 100 != 2:\n                    return None\n                detail = json.loads(resp.body)\n                if detail['expires'] < time():\n                    self.make_pre_authed_request(\n                        env, 'DELETE', path).get_response(self.app)\n                    return None\n                groups = [g['name'] for g in detail['groups']]\n                if '.admin' in groups:\n                    groups.remove('.admin')\n                    groups.append(detail['account_id'])\n                groups = ','.join(groups)\n                if memcache_client:\n                    memcache_client.set(\n                        memcache_key,\n                        (detail['expires'], groups),\n                        time=float(detail['expires'] - time()))\n        return groups\n    def handle_delete_user(self, req):\n        \"\"\"Handles the DELETE v2/<account>/<user> call for deleting a user from an\n        account.\n        Can only be called by an account .admin.\n        :param req: The swob.Request to process.\n        :returns: swob.Response, 2xx on success.\n        \"\"\"\n        account = req.path_info_pop()\n        user = req.path_info_pop()\n        if req.path_info or not account or account[0] == '.' or not user or \\\n                user[0] == '.':\n            return HTTPBadRequest(request=req)\n        is_reseller_admin = self.is_user_reseller_admin(req, account, user)\n        if not is_reseller_admin and not req.credentials_valid:\n            return HTTPNotFound(request=req)\n        elif is_reseller_admin and not self.is_super_admin(req):\n            return HTTPForbidden(request=req)\n        if not self.is_account_admin(req, account):\n            return self.denied_response(req)\n        path = quote('/v1/%s/%s/%s' % (self.auth_account, account, user))\n        resp = self.make_pre_authed_request(\n            req.environ, 'HEAD', path).get_response(self.app)\n        if resp.status_int == 404:\n            return HTTPNotFound(request=req)\n        elif resp.status_int // 100 != 2:\n            raise Exception('Could not obtain user details: %s %s' %\n                            (path, resp.status))\n        candidate_token = resp.headers.get('x-object-meta-auth-token')\n        if candidate_token:\n            path = quote('/v1/%s/.token_%s/%s' %\n                (self.auth_account, candidate_token[-1], candidate_token))\n            resp = self.make_pre_authed_request(\n                req.environ, 'DELETE', path).get_response(self.app)\n            if resp.status_int // 100 != 2 and resp.status_int != 404:\n                raise Exception('Could not delete possibly existing token: '\n                                '%s %s' % (path, resp.status))\n        path = quote('/v1/%s/%s/%s' % (self.auth_account, account, user))\n        resp = self.make_pre_authed_request(\n            req.environ, 'DELETE', path).get_response(self.app)\n        if resp.status_int // 100 != 2 and resp.status_int != 404:\n            raise Exception('Could not delete the user object: %s %s' %\n                            (path, resp.status))\n        return HTTPNoContent(request=req)\n    def handle_get_token(self, req):\n        \"\"\"Handles the various `request for token and service end point(s)` calls.\n        There are various formats to support the various auth servers in the\n        past. Examples::\n            GET <auth-prefix>/v1/<act>/auth\n                X-Auth-User: <act>:<usr>  or  X-Storage-User: <usr>\n                X-Auth-Key: <key>         or  X-Storage-Pass: <key>\n            GET <auth-prefix>/auth\n                X-Auth-User: <act>:<usr>  or  X-Storage-User: <act>:<usr>\n                X-Auth-Key: <key>         or  X-Storage-Pass: <key>\n            GET <auth-prefix>/v1.0\n                X-Auth-User: <act>:<usr>  or  X-Storage-User: <act>:<usr>\n                X-Auth-Key: <key>         or  X-Storage-Pass: <key>\n        Values should be url encoded, \"act%3Ausr\" instead of \"act:usr\" for\n        example; however, for backwards compatibility the colon may be included\n        unencoded.\n        On successful authentication, the response will have X-Auth-Token and\n        X-Storage-Token set to the token to use with Swift and X-Storage-URL\n        set to the URL to the default Swift cluster to use.\n        The response body will be set to the account's services JSON object as\n        described here::\n            {\"storage\": {\n                \"default\": \"cluster1\",\n                \"cluster1\": \"<URL to use with Swift>\",\n                \"cluster2\": \"<URL to use with Swift>\"\n             },\n             \"servers\": {\n             },\n            }\n        One can also include an \"X-Auth-New-Token: true\" header to\n        force issuing a new token and revoking any old token, even if\n        it hasn't expired yet.\n        :param req: The swob.Request to process.\n        :returns: swob.Response, 2xx on success with data set as explained\n                  above.\n        \"\"\"\n        try:\n            pathsegs = split_path(req.path_info, minsegs=1, maxsegs=3,\n                                  rest_with_last=True)\n        except ValueError:\n            return HTTPNotFound(request=req)\n        if pathsegs[0] == 'v1' and pathsegs[2] == 'auth':\n            account = pathsegs[1]\n            user = req.headers.get('x-storage-user')\n            if not user:\n                user = unquote(req.headers.get('x-auth-user', ''))\n                if not user or ':' not in user:\n                    return HTTPUnauthorized(request=req)\n                account2, user = user.split(':', 1)\n                if account != account2:\n                    return HTTPUnauthorized(request=req)\n            key = req.headers.get('x-storage-pass')\n            if not key:\n                key = unquote(req.headers.get('x-auth-key', ''))\n        elif pathsegs[0] in ('auth', 'v1.0'):\n            user = unquote(req.headers.get('x-auth-user', ''))\n            if not user:\n                user = req.headers.get('x-storage-user')\n            if not user or ':' not in user:\n                return HTTPUnauthorized(request=req)\n            account, user = user.split(':', 1)\n            key = unquote(req.headers.get('x-auth-key', ''))\n            if not key:\n                key = req.headers.get('x-storage-pass')\n        else:\n            return HTTPBadRequest(request=req)\n        if not all((account, user, key)):\n            return HTTPUnauthorized(request=req)\n        if user == '.super_admin' and self.super_admin_key and \\\n                key == self.super_admin_key:\n            token = self.get_itoken(req.environ)\n            url = '%s/%s.auth' % (self.dsc_url, self.reseller_prefix)\n            return Response(\n                request=req,\n                content_type=CONTENT_TYPE_JSON,\n                body=json.dumps({'storage': {'default': 'local',\n                                             'local': url}}),\n                headers={'x-auth-token': token,\n                         'x-storage-token': token,\n                         'x-storage-url': url})\n        path = quote('/v1/%s/%s/%s' % (self.auth_account, account, user))\n        resp = self.make_pre_authed_request(\n            req.environ, 'GET', path).get_response(self.app)\n        if resp.status_int == 404:\n            return HTTPUnauthorized(request=req)\n        if resp.status_int // 100 != 2:\n            raise Exception('Could not obtain user details: %s %s' %\n                            (path, resp.status))\n        user_detail = json.loads(resp.body)\n        if not self.credentials_match(user_detail, key):\n            return HTTPUnauthorized(request=req)\n        token = None\n        expires = None\n        candidate_token = resp.headers.get('x-object-meta-auth-token')\n        if candidate_token:\n            path = quote('/v1/%s/.token_%s/%s' %\n                (self.auth_account, candidate_token[-1], candidate_token))\n            delete_token = False\n            try:\n                if req.headers.get('x-auth-new-token', 'false').lower() in \\\n                        TRUE_VALUES:\n                    delete_token = True\n                else:\n                    resp = self.make_pre_authed_request(\n                        req.environ, 'GET', path).get_response(self.app)\n                    if resp.status_int // 100 == 2:\n                        token_detail = json.loads(resp.body)\n                        if token_detail['expires'] > time():\n                            token = candidate_token\n                            expires = token_detail['expires']\n                        else:\n                            delete_token = True\n                    elif resp.status_int != 404:\n                        raise Exception(\n                            'Could not detect whether a token already exists: '\n                            '%s %s' % (path, resp.status))\n            finally:\n                if delete_token:\n                    self.make_pre_authed_request(\n                        req.environ, 'DELETE', path).get_response(self.app)\n                    memcache_client = cache_from_env(req.environ)\n                    if memcache_client:\n                        memcache_key = '%s/auth/%s' % (self.reseller_prefix,\n                                                       candidate_token)\n                        memcache_client.delete(memcache_key)\n        if not token:\n            path = quote('/v1/%s/%s' % (self.auth_account, account))\n            resp = self.make_pre_authed_request(\n                req.environ, 'HEAD', path).get_response(self.app)\n            if resp.status_int // 100 != 2:\n                raise Exception('Could not retrieve account id value: '\n                                '%s %s' % (path, resp.status))\n            account_id = \\\n                resp.headers['x-container-meta-account-id']\n            token = '%stk%s' % (self.reseller_prefix, uuid4().hex)\n            path = quote('/v1/%s/.token_%s/%s' %\n                         (self.auth_account, token[-1], token))\n            try:\n                token_life = min(\n                    int(req.headers.get('x-auth-token-lifetime',\n                                        self.token_life)),\n                    self.max_token_life)\n            except ValueError:\n                token_life = self.token_life\n            expires = int(time() + token_life)\n            resp = self.make_pre_authed_request(\n                req.environ, 'PUT', path,\n                json.dumps({'account': account, 'user': user,\n                'account_id': account_id,\n                'groups': user_detail['groups'],\n                'expires': expires})).get_response(self.app)\n            if resp.status_int // 100 != 2:\n                raise Exception('Could not create new token: %s %s' %\n                                (path, resp.status))\n            path = quote('/v1/%s/%s/%s' % (self.auth_account, account, user))\n            resp = self.make_pre_authed_request(\n                req.environ, 'POST', path,\n                headers={'X-Object-Meta-Auth-Token': token}\n            ).get_response(self.app)\n            if resp.status_int // 100 != 2:\n                raise Exception('Could not save new token: %s %s' %\n                                (path, resp.status))\n        path = quote('/v1/%s/%s/.services' % (self.auth_account, account))\n        resp = self.make_pre_authed_request(\n            req.environ, 'GET', path).get_response(self.app)\n        if resp.status_int // 100 != 2:\n            raise Exception('Could not obtain services info: %s %s' %\n                            (path, resp.status))\n        detail = json.loads(resp.body)\n        url = detail['storage'][detail['storage']['default']]\n        return Response(\n            request=req,\n            body=resp.body,\n            content_type=CONTENT_TYPE_JSON,\n            headers={'x-auth-token': token,\n                     'x-storage-token': token,\n                     'x-auth-token-expires': str(int(expires - time())),\n                     'x-storage-url': url})\n    def handle_validate_token(self, req):\n        \"\"\"Handles the GET v2/.token/<token> call for validating a token, usually\n        called by a service like Swift.\n        On a successful validation, X-Auth-TTL will be set for how much longer\n        this token is valid and X-Auth-Groups will contain a comma separated\n        list of groups the user belongs to.\n        The first group listed will be a unique identifier for the user the\n        token represents.\n        .reseller_admin is a special group that indicates the user should be\n        allowed to do anything on any account.\n        :param req: The swob.Request to process.\n        :returns: swob.Response, 2xx on success with data set as explained\n                  above.\n        \"\"\"\n        token = req.path_info_pop()\n        if req.path_info or not token.startswith(self.reseller_prefix):\n            return HTTPBadRequest(request=req)\n        expires = groups = None\n        memcache_client = cache_from_env(req.environ)\n        if memcache_client:\n            memcache_key = '%s/auth/%s' % (self.reseller_prefix, token)\n            cached_auth_data = memcache_client.get(memcache_key)\n            if cached_auth_data:\n                expires, groups = cached_auth_data\n                if expires < time():\n                    groups = None\n        if not groups:\n            path = quote('/v1/%s/.token_%s/%s' %\n                         (self.auth_account, token[-1], token))\n            resp = self.make_pre_authed_request(\n                req.environ, 'GET', path).get_response(self.app)\n            if resp.status_int // 100 != 2:\n                return HTTPNotFound(request=req)\n            detail = json.loads(resp.body)\n            expires = detail['expires']\n            if expires < time():\n                self.make_pre_authed_request(\n                    req.environ, 'DELETE', path).get_response(self.app)\n                return HTTPNotFound(request=req)\n            groups = [g['name'] for g in detail['groups']]\n            if '.admin' in groups:\n                groups.remove('.admin')\n                groups.append(detail['account_id'])\n            groups = ','.join(groups)\n        return HTTPNoContent(headers={'X-Auth-TTL': expires - time(),\n                                      'X-Auth-Groups': groups})",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2017-16613",
        "description": "[{'lang': 'en', 'value': 'An issue was discovered in middleware.py in OpenStack Swauth through 1.2.0 when used with OpenStack Swift through 2.15.1. The Swift object store and proxy server are saving (unhashed) tokens retrieved from the Swauth middleware authentication mechanism to a log file as part of a GET URI. This allows attackers to bypass authentication by inserting a token into an X-Auth-Token header of a new request. NOTE: github.com/openstack/swauth URLs do not mean that Swauth is maintained by an official OpenStack project team.'}]",
        "cwe_number": 287
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-280",
      "code": "    def post(self):\n        \"\"\"This method handles the POST requests to add agents to the Cloud Verifier.\n        Currently, only agents resources are available for POSTing, i.e. /agents. All other POST uri's will return errors.\n        agents requests require a json block sent in the body\n        \"\"\"\n        session = get_session()\n        try:\n            rest_params = web_util.get_restful_params(self.request.uri)\n            if rest_params is None:\n                web_util.echo_json_response(\n                    self, 405, \"Not Implemented: Use /agents/ interface\")\n                return\n            if not web_util.validate_api_version(self, rest_params[\"api_version\"], logger):\n                return\n            if \"agents\" not in rest_params:\n                web_util.echo_json_response(self, 400, \"uri not supported\")\n                logger.warning('POST returning 400 response. uri not supported: %s', self.request.path)\n                return\n            agent_id = rest_params[\"agents\"]\n            if agent_id is not None:\n                if not validators.valid_agent_id(agent_id):\n                    web_util.echo_json_response(self, 400, \"agent_id not not valid\")\n                    logger.error(\"POST received an invalid agent ID: %s\", agent_id)\n                    return\n                content_length = len(self.request.body)\n                if content_length == 0:\n                    web_util.echo_json_response(\n                        self, 400, \"Expected non zero content length\")\n                    logger.warning('POST returning 400 response. Expected non zero content length.')\n                else:\n                    json_body = json.loads(self.request.body)\n                    agent_data = {}\n                    agent_data['v'] = json_body['v']\n                    agent_data['ip'] = json_body['cloudagent_ip']\n                    agent_data['port'] = int(json_body['cloudagent_port'])\n                    agent_data['operational_state'] = states.START\n                    agent_data['public_key'] = \"\"\n                    agent_data['tpm_policy'] = json_body['tpm_policy']\n                    agent_data['meta_data'] = json_body['metadata']\n                    agent_data['allowlist'] = json_body['allowlist']\n                    agent_data['mb_refstate'] = json_body['mb_refstate']\n                    agent_data['ima_sign_verification_keys'] = json_body['ima_sign_verification_keys']\n                    agent_data['revocation_key'] = json_body['revocation_key']\n                    agent_data['accept_tpm_hash_algs'] = json_body['accept_tpm_hash_algs']\n                    agent_data['accept_tpm_encryption_algs'] = json_body['accept_tpm_encryption_algs']\n                    agent_data['accept_tpm_signing_algs'] = json_body['accept_tpm_signing_algs']\n                    agent_data['supported_version'] = json_body['supported_version']\n                    agent_data['hash_alg'] = \"\"\n                    agent_data['enc_alg'] = \"\"\n                    agent_data['sign_alg'] = \"\"\n                    agent_data['agent_id'] = agent_id\n                    agent_data['boottime'] = 0\n                    agent_data['ima_pcrs'] = []\n                    agent_data['pcr10'] = None\n                    agent_data['next_ima_ml_entry'] = 0\n                    agent_data['learned_ima_keyrings'] = {}\n                    agent_data['verifier_id'] = config.get('cloud_verifier', 'cloudverifier_id', fallback=cloud_verifier_common.DEFAULT_VERIFIER_ID)\n                    agent_data['verifier_ip'] = config.get('cloud_verifier', 'cloudverifier_ip')\n                    agent_data['verifier_port'] = config.get('cloud_verifier', 'cloudverifier_port')\n                    registrar_client.init_client_tls('cloud_verifier')\n                    registrar_data = registrar_client.getData(config.get(\"cloud_verifier\", \"registrar_ip\"),\n                                                              config.get(\"cloud_verifier\", \"registrar_port\"), agent_id)\n                    if registrar_data is None:\n                        web_util.echo_json_response(self, 400,\n                                                    f\"Data for agent {agent_id} could not be found in registrar!\")\n                        logger.warning(\"Data for agent %s could not be found in registrar!\", agent_id)\n                        return\n                    agent_data['mtls_cert'] = registrar_data.get('mtls_cert', None)\n                    agent_data['ak_tpm'] = registrar_data['aik_tpm']\n                    if registrar_data.get('mtls_cert', None) is None and agent_data['supported_version'] != \"1.0\":\n                        web_util.echo_json_response(self, 400, \"mTLS certificate for agent is required!\")\n                        return\n                    is_valid, err_msg = cloud_verifier_common.validate_agent_data(agent_data)\n                    if not is_valid:\n                        web_util.echo_json_response(self, 400, err_msg)\n                        logger.warning(err_msg)\n                        return\n                    try:\n                        new_agent_count = session.query(\n                            VerfierMain).filter_by(agent_id=agent_id).count()\n                    except SQLAlchemyError as e:\n                        logger.error('SQLAlchemy Error: %s', e)\n                        raise e\n                    if new_agent_count > 0:\n                        web_util.echo_json_response(\n                            self, 409, f\"Agent of uuid {agent_id} already exists\")\n                        logger.warning(\"Agent of uuid %s already exists\", agent_id)\n                    else:\n                        try:\n                            session.add(VerfierMain(**agent_data))\n                            session.commit()\n                        except SQLAlchemyError as e:\n                            logger.error('SQLAlchemy Error: %s', e)\n                            raise e\n                        for key,val in exclude_db.items():\n                            agent_data[key] = val\n                        agent_mtls_cert_enabled = config.getboolean('cloud_verifier', 'agent_mtls_cert_enabled', fallback=False)\n                        mtls_cert = registrar_data.get('mtls_cert', None)\n                        agent_data['ssl_context'] = None\n                        if agent_mtls_cert_enabled and mtls_cert:\n                            agent_data['ssl_context'] = web_util.generate_agent_mtls_context(mtls_cert, self.mtls_options)\n                        if agent_data['ssl_context'] is None:\n                            logger.warning('Connecting to agent without mTLS: %s', agent_id)\n                        asyncio.ensure_future(\n                            process_agent(agent_data, states.GET_QUOTE))\n                        web_util.echo_json_response(self, 200, \"Success\")\n                        logger.info('POST returning 200 response for adding agent id: %s', agent_id)\n            else:\n                web_util.echo_json_response(self, 400, \"uri not supported\")\n                logger.warning(\"POST returning 400 response. uri not supported\")\n        except Exception as e:\n            web_util.echo_json_response(self, 400, f\"Exception error: {str(e)}\")\n            logger.warning(\"POST returning 400 response. Exception error: %s\", e)\n            logger.exception(e)\n    def do_cv(self):\n        \"\"\" Initiaite v, agent_id and ip and initiate the cloudinit sequence\n        \"\"\"\n        b64_v = base64.b64encode(self.V).decode('utf-8')\n        logger.debug(\"b64_v: %s\", b64_v)\n        data = {\n            'v': b64_v,\n            'cloudagent_ip': self.cv_cloudagent_ip,\n            'cloudagent_port': self.agent_port,\n            'tpm_policy': json.dumps(self.tpm_policy),\n            'allowlist': json.dumps(self.allowlist),\n            'mb_refstate': json.dumps(self.mb_refstate),\n            'ima_sign_verification_keys': json.dumps(self.ima_sign_verification_keys),\n            'metadata': json.dumps(self.metadata),\n            'revocation_key': self.revocation_key,\n            'accept_tpm_hash_algs': self.accept_tpm_hash_algs,\n            'accept_tpm_encryption_algs': self.accept_tpm_encryption_algs,\n            'accept_tpm_signing_algs': self.accept_tpm_signing_algs,\n            'supported_version': self.supported_version,\n        }\n        json_message = json.dumps(data)\n        do_cv = RequestsClient(self.verifier_base_url, self.tls_cv_enabled, ignore_hostname=True)\n        response = do_cv.post(\n            (f'/v{self.api_version}/agents/{self.agent_uuid}'),\n            data=json_message,\n            cert=self.cert,\n            verify=self.verifier_ca_cert\n        )\n        if response.status_code == 503:\n            logger.error(\"Cannot connect to Verifier at %s with Port %s. Connection refused.\", self.verifier_ip, self.verifier_port)\n            sys.exit()\n        elif response.status_code == 504:\n            logger.error(\"Verifier at %s with Port %s timed out.\", self.verifier_ip, self.verifier_port)\n            sys.exit()\n        if response.status_code == 409:\n            logger.error(\"Agent %s already existed at CV. Please use delete or update.\", self.agent_uuid)\n            sys.exit()\n        elif response.status_code != 200:\n            keylime_logging.log_http_response(\n                logger, logging.ERROR, response.json())\n            logger.error(\"POST command response: %s Unexpected response from Cloud Verifier: %s\", response.status_code, response.text)\n            sys.exit()\n    def do_cvstatus(self):\n        \"\"\"Perform operational state look up for agent on the verifier\"\"\"\n        do_cvstatus = RequestsClient(self.verifier_base_url, self.tls_cv_enabled, ignore_hostname=True)\n        response = do_cvstatus.get(\n            (f'/v{self.api_version}/agents/{self.agent_uuid}'),\n            cert=self.cert,\n            verify=self.verifier_ca_cert\n        )\n        if response.status_code == 503:\n            logger.error(\"Cannot connect to Verifier at %s with Port %s. Connection refused.\", self.verifier_ip, self.verifier_port)\n            return response.json()\n        if response.status_code == 504:\n            logger.error(\"Verifier at %s with Port %s timed out.\", self.verifier_ip, self.verifier_port)\n            return response.json()\n        if response.status_code == 404:\n            logger.info(\"Verifier at %s with Port %s does not have agent %s.\",\n                        self.verifier_ip, self.verifier_port, self.agent_uuid)\n            return response.json()\n        if response.status_code == 200:\n            response = response.json()\n            res = response.pop('results')\n            response['results'] = {self.agent_uuid: res}\n            operational_state = states.state_to_str(\n                response['results'][self.agent_uuid]['operational_state'])\n            response['results'][self.agent_uuid]['operational_state'] = operational_state\n            logger.info(\"Agent Info:\\n%s\", json.dumps(response[\"results\"]))\n            return response\n        logger.info(\"Status command response: %s. Unexpected response \"\n                    \"from Cloud Verifier %s on port %s. %s\",\n                    response.status_code,\n                    self.verifier_ip, self.verifier_port, str(response))\n        return response",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-1053",
        "description": "[{'lang': 'en', 'value': 'Keylime does not enforce that the agent registrar data is the same when the tenant uses it for validation of the EK and identity quote and the verifier for validating the integrity quote. This allows an attacker to use one AK, EK pair from a real TPM to pass EK validation and give the verifier an AK of a software TPM. A successful attack breaks the entire chain of trust because a not validated AK is used by the verifier. This issue is worse if the validation happens first and then the agent gets added to the verifier because the timing is easier and the verifier does not validate the regcount entry being equal to 1,'}]",
        "cwe_number": 20
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-281",
      "code": "    def _redirect_safe(self, url, default=None):\n        \"\"\"Redirect if url is on our PATH\n        Full-domain redirects are allowed if they pass our CORS origin checks.\n        Otherwise use default (self.base_url if unspecified).\n        \"\"\"\n        if default is None:\n            default = self.base_url\n        url = url.replace(\"\\\\\", \"%5C\")\n        parsed = urlparse(url)\n        if parsed.netloc or not (parsed.path + \"/\").startswith(self.base_url):\n            allow = False\n            if parsed.netloc:\n                origin = f\"{parsed.scheme}://{parsed.netloc}\"\n                origin = origin.lower()\n                if self.allow_origin:\n                    allow = self.allow_origin == origin\n                elif self.allow_origin_pat:\n                    allow = bool(re.match(self.allow_origin_pat, origin))\n            if not allow:\n                self.log.warning(\"Not allowing login redirect to %r\" % url)\n                url = default\n        self.redirect(url)\n    def get(self):\n        \"\"\"Get the login form.\"\"\"\n        if self.current_user:\n            next_url = self.get_argument(\"next\", default=self.base_url)\n            self._redirect_safe(next_url)\n        else:\n            self._render()\n    def post(self):\n        \"\"\"Post a login.\"\"\"\n        user = self.current_user = self.identity_provider.process_login_form(self)\n        if user is None:\n            self.set_status(401)\n            self._render(message={\"error\": \"Invalid credentials\"})\n            return\n        self.log.info(f\"User {user.username} logged in.\")\n        self.identity_provider.set_login_cookie(self, user)\n        next_url = self.get_argument(\"next\", default=self.base_url)\n        self._redirect_safe(next_url)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-39968",
        "description": "[{'lang': 'en', 'value': 'jupyter-server is the backend for Jupyter web applications. Open Redirect Vulnerability. Maliciously crafted login links to known Jupyter Servers can cause successful login or an already logged-in session to be redirected to arbitrary sites, which should be restricted to Jupyter Server-served URLs. This issue has been addressed in commit `29036259` which is included in release 2.7.2. Users are advised to upgrade. There are no known workarounds for this vulnerability.'}]",
        "cwe_number": 601
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-282",
      "code": "    def __init__(self, path):\n        self.path = path\n    def _write(self, contents: dict):\n        LOGGER.debug(f'Writing to {self.path}')\n        with open(self.path, 'w') as fp:\n            fp.write(json.dumps(contents))",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-32303",
        "description": "[{'lang': 'en', 'value': \"Planet is software that provides satellite data. The secret file stores the user's Planet API authentication information. It should only be accessible by the user, but before version 2.0.1, its permissions allowed the user's group and non-group to read the file as well. This issue was patched in version 2.0.1. As a workaround, set the secret file permissions to only user read/write by hand.\\n\"}]",
        "cwe_number": 732
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-283",
      "code": "def _get_http_response_with_retries(\n    method,\n    url,\n    max_retries,\n    backoff_factor,\n    backoff_jitter,\n    retry_codes,\n    raise_on_status=True,\n    **kwargs,\n):\n    \"\"\"\n    Performs an HTTP request using Python's `requests` module with an automatic retry policy.\n    :param method: a string indicating the method to use, e.g. \"GET\", \"POST\", \"PUT\".\n    :param url: the target URL address for the HTTP request.\n    :param max_retries: Maximum total number of retries.\n    :param backoff_factor: a time factor for exponential backoff. e.g. value 5 means the HTTP\n      request will be retried with interval 5, 10, 20... seconds. A value of 0 turns off the\n      exponential backoff.\n    :param backoff_jitter: A random jitter to add to the backoff interval.\n    :param retry_codes: a list of HTTP response error codes that qualifies for retry.\n    :param raise_on_status: whether to raise an exception, or return a response, if status falls\n      in retry_codes range and retries have been exhausted.\n    :param kwargs: Additional keyword arguments to pass to `requests.Session.request()`\n    :return: requests.Response object.\n    \"\"\"\n    session = _get_request_session(\n        max_retries, backoff_factor, backoff_jitter, retry_codes, raise_on_status\n    )\n    return session.request(method, url, **kwargs)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-6974",
        "description": "[{'lang': 'en', 'value': 'A malicious user could use this issue to access internal HTTP(s) servers and in the worst case (ie: aws instance) it could be abuse to get a remote code execution on the victim machine.'}]",
        "cwe_number": 918
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-284",
      "code": "def allow_all(tag: str, name: str, value: str) -> bool:\n    return True",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2020-26280",
        "description": "[{'lang': 'en', 'value': 'OpenSlides is a free, Web-based presentation and assembly system for managing and projecting agenda, motions, and elections of assemblies. OpenSlides version 3.2, due to unsufficient user input validation and escaping, it is vulnerable to persistant cross-site scripting (XSS). In the web applications users can enter rich text in various places, e.g. for personal notes or in motions. These fields can be used to store arbitrary JavaScript Code that will be executed when other users read the respective text. An attacker could utilize this vulnerability be used to manipulate votes of other users, hijack the moderators session or simply disturb the meeting. The vulnerability was introduced with 6eae497abeab234418dfbd9d299e831eff86ed45 on 16.04.2020, which is first included in the 3.2 release. It has been patched in version 3.3 ( in commit f3809fc8a97ee305d721662a75f788f9e9d21938, merged in master on 20.11.2020).'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-285",
      "code": "    def _expand(self, key_material):\n        output = [b\"\"]\n        counter = 1\n        while (self._algorithm.digest_size // 8) * len(output) < self._length:\n            h = hmac.HMAC(key_material, self._algorithm, backend=self._backend)\n            h.update(output[-1])\n            h.update(self._info)\n            h.update(six.int2byte(counter))\n            output.append(h.finalize())\n            counter += 1\n        return b\"\".join(output)[:self._length]",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2016-9243",
        "description": "[{'lang': 'en', 'value': 'HKDF in cryptography before 1.5.2 returns an empty byte-string if used with a length less than algorithm.digest_size.'}]",
        "cwe_number": 20
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-286",
      "code": "def _is_javascript_scheme(s):\n    if _is_image_dataurl(s):\n        return None\n    return _is_possibly_malicious_scheme(s)",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-43818",
        "description": "[{'lang': 'en', 'value': 'lxml is a library for processing XML and HTML in the Python language. Prior to version 4.6.5, the HTML Cleaner in lxml.html lets certain crafted script content pass through, as well as script content in SVG files embedded using data URIs. Users that employ the HTML cleaner in a security relevant context should upgrade to lxml 4.6.5 to receive a patch. There are no known workarounds available.'}]",
        "cwe_number": 74
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-287",
      "code": "    def pipeline_files_upload(self, base_path, file_handle, case_customer, case_name, is_update):\n        \"\"\"\n        Handle the files for a specific\n        :return:\n        \"\"\"\n        if base_path and Path(base_path).is_dir:\n            file_handle.save(Path(base_path, file_handle.filename))\n            return InterfaceStatus.I2Success(\"Successfully saved file {} to {}\".format(file_handle.filename, base_path))\n        else:\n            return InterfaceStatus.I2Error(\"Directory {} not found. Can't save file\".format(base_path))",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-34060",
        "description": "[{'lang': 'en', 'value': 'IrisEVTXModule is an interface module for Evtx2Splunk and Iris in order to ingest Microsoft EVTX log files. The `iris-evtx-module` is a pipeline plugin of `iris-web` that processes EVTX files through IRIS web application. During the upload of an EVTX through this pipeline, the filename is not safely handled and may cause an Arbitrary File Write. This can lead to a remote code execution (RCE) when combined with a Server Side Template Injection (SSTI). This vulnerability has been patched in version 1.0.0.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-288",
      "code": "try:\n  import yaml\nexcept ImportError:\n  yaml = None\ndef model_from_config(config, custom_objects=None):\n  \"\"\"Instantiates a Keras model from its config.\n  Usage:\n  ```\n  tf.keras.Model().from_config(model.get_config())\n  tf.keras.Sequential().from_config(model.get_config())\n  ```\n  Args:\n      config: Configuration dictionary.\n      custom_objects: Optional dictionary mapping names\n          (strings) to custom classes or functions to be\n          considered during deserialization.\n  Returns:\n      A Keras model instance (uncompiled).\n  Raises:\n      TypeError: if `config` is not a dictionary.\n  \"\"\"\n  if isinstance(config, list):\n    raise TypeError('`model_from_config` expects a dictionary, not a list. '\n                    'Maybe you meant to use '\n                    '`Sequential.from_config(config)`?')\n  from tensorflow.python.keras.layers import deserialize\n  return deserialize(config, custom_objects=custom_objects)\ndef model_from_yaml(yaml_string, custom_objects=None):\n  \"\"\"Parses a yaml model configuration file and returns a model instance.\n  Usage:\n  >>> model = tf.keras.Sequential([\n  ...     tf.keras.layers.Dense(5, input_shape=(3,)),\n  ...     tf.keras.layers.Softmax()])\n  >>> try:\n  ...   import yaml\n  ...   config = model.to_yaml()\n  ...   loaded_model = tf.keras.models.model_from_yaml(config)\n  ... except ImportError:\n  ...   pass\n  Args:\n      yaml_string: YAML string or open file encoding a model configuration.\n      custom_objects: Optional dictionary mapping names\n          (strings) to custom classes or functions to be\n          considered during deserialization.\n  Returns:\n      A Keras model instance (uncompiled).\n  Raises:\n      ImportError: if yaml module is not found.\n  \"\"\"\n  if yaml is None:\n    raise ImportError('Requires yaml module installed (`pip install pyyaml`).')\n  try:\n    config = yaml.unsafe_load(yaml_string)\n  except AttributeError:\n    config = yaml.load(yaml_string)\n  from tensorflow.python.keras.layers import deserialize\n  return deserialize(config, custom_objects=custom_objects)\n  def to_json(self, **kwargs):\n    \"\"\"Returns a JSON string containing the network configuration.\n    To load a network from a JSON save file, use\n    `keras.models.model_from_json(json_string, custom_objects={})`.\n    Args:\n        **kwargs: Additional keyword arguments\n            to be passed to `json.dumps()`.\n    Returns:\n        A JSON string.\n    \"\"\"\n    model_config = self._updated_config()\n    return json.dumps(\n        model_config, default=json_utils.get_json_type, **kwargs)\n  def to_yaml(self, **kwargs):\n    \"\"\"Returns a yaml string containing the network configuration.\n    To load a network from a yaml save file, use\n    `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n    `custom_objects` should be a dictionary mapping\n    the names of custom losses / layers / etc to the corresponding\n    functions / classes.\n    Args:\n        **kwargs: Additional keyword arguments\n            to be passed to `yaml.dump()`.\n    Returns:\n        A YAML string.\n    Raises:\n        ImportError: if yaml module is not found.\n    \"\"\"\n    if yaml is None:\n      raise ImportError(\n          'Requires yaml module installed (`pip install pyyaml`).')\n    return yaml.dump(self._updated_config(), **kwargs)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-37678",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an end-to-end open source platform for machine learning. In affected versions TensorFlow and Keras can be tricked to perform arbitrary code execution when deserializing a Keras model from YAML format. The [implementation](https://github.com/tensorflow/tensorflow/blob/460e000de3a83278fb00b61a16d161b1964f15f4/tensorflow/python/keras/saving/model_config.py#L66-L104) uses `yaml.unsafe_load` which can perform arbitrary code execution on the input. Given that YAML format support requires a significant amount of work, we have removed it for now. We have patched the issue in GitHub commit 23d6383eb6c14084a8fc3bdf164043b974818012. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.'}]",
        "cwe_number": 502
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-289",
      "code": "def execute_cmd(cmd, from_async=False):\n\t\"\"\"execute a request as python module\"\"\"\n\tfor hook in frappe.get_hooks(\"override_whitelisted_methods\", {}).get(cmd, []):\n\t\tcmd = hook\n\t\tbreak\n\tif run_server_script_api(cmd):\n\t\treturn None\n\ttry:\n\t\tmethod = get_attr(cmd)\n\texcept Exception as e:\n\t\tif frappe.local.conf.developer_mode:\n\t\t\traise e\n\t\telse:\n\t\t\tfrappe.respond_as_web_page(title='Invalid Method', html='Method not found',\n\t\t\tindicator_color='red', http_status_code=404)\n\t\treturn\n\tif from_async:\n\t\tmethod = method.queue\n\tis_whitelisted(method)\n\tis_valid_http_method(method)\n\treturn frappe.call(method, **frappe.form_dict)\ndef is_valid_http_method(method):\n\thttp_method = frappe.local.request.method\n\tif http_method not in frappe.allowed_http_methods_for_whitelisted_func[method]:\n\t\tfrappe.throw(_(\"Not permitted\"), frappe.PermissionError)\ndef upload_file():\n\tuser = None\n\tif frappe.session.user == 'Guest':\n\t\tif frappe.get_system_settings('allow_guests_to_upload_files'):\n\t\t\tignore_permissions = True\n\t\telse:\n\t\t\treturn\n\telse:\n\t\tuser = frappe.get_doc(\"User\", frappe.session.user)\n\t\tignore_permissions = False\n\tfiles = frappe.request.files\n\tis_private = frappe.form_dict.is_private\n\tdoctype = frappe.form_dict.doctype\n\tdocname = frappe.form_dict.docname\n\tfieldname = frappe.form_dict.fieldname\n\tfile_url = frappe.form_dict.file_url\n\tfolder = frappe.form_dict.folder or 'Home'\n\tmethod = frappe.form_dict.method\n\tcontent = None\n\tfilename = None\n\tif 'file' in files:\n\t\tfile = files['file']\n\t\tcontent = file.stream.read()\n\t\tfilename = file.filename\n\tfrappe.local.uploaded_file = content\n\tfrappe.local.uploaded_filename = filename\n\tif frappe.session.user == 'Guest' or (user and not user.has_desk_access()):\n\t\timport mimetypes\n\t\tfiletype = mimetypes.guess_type(filename)[0]\n\t\tif filetype not in ALLOWED_MIMETYPES:\n\t\t\tfrappe.throw(_(\"You can only upload JPG, PNG, PDF, or Microsoft documents.\"))\n\tif method:\n\t\tmethod = frappe.get_attr(method)\n\t\tis_whitelisted(method)\n\t\treturn method()\n\telse:\n\t\tret = frappe.get_doc({\n\t\t\t\"doctype\": \"File\",\n\t\t\t\"attached_to_doctype\": doctype,\n\t\t\t\"attached_to_name\": docname,\n\t\t\t\"attached_to_field\": fieldname,\n\t\t\t\"folder\": folder,\n\t\t\t\"file_name\": filename,\n\t\t\t\"file_url\": file_url,\n\t\t\t\"is_private\": cint(is_private),\n\t\t\t\"content\": content\n\t\t})\n\t\tret.save(ignore_permissions=ignore_permissions)\n\t\treturn ret\ndef get_attr(cmd):\n\t\"\"\"get method object from cmd\"\"\"\n\tif '.' in cmd:\n\t\tmethod = frappe.get_attr(cmd)\n\telse:\n\t\tmethod = globals()[cmd]\n\tfrappe.log(\"method:\" + cmd)\n\treturn method\ndef ping():\n\treturn \"pong\"\ndef whitelist(allow_guest=False, xss_safe=False, methods=None):\n\t\"\"\"\n\tDecorator for whitelisting a function and making it accessible via HTTP.\n\tStandard request will be `/api/method/[path.to.method]`\n\t:param allow_guest: Allow non logged-in user to access this method.\n\t:param methods: Allowed http method to access the method.\n\tUse as:\n\t\t@frappe.whitelist()\n\t\tdef myfunc(param1, param2):\n\t\t\tpass\n\t\"\"\"\n\tif not methods:\n\t\tmethods = ['GET', 'POST', 'PUT', 'DELETE']\n\tdef innerfn(fn):\n\t\tglobal whitelisted, guest_methods, xss_safe_methods, allowed_http_methods_for_whitelisted_func\n\t\twhitelisted.append(fn)\n\t\tallowed_http_methods_for_whitelisted_func[fn] = methods\n\t\tif allow_guest:\n\t\t\tguest_methods.append(fn)\n\t\t\tif xss_safe:\n\t\t\t\txss_safe_methods.append(fn)\n\t\treturn fn\n\treturn innerfn\ndef read_only():\n\tdef innfn(fn):\n\t\tdef wrapper_fn(*args, **kwargs):\n\t\t\tif conf.read_from_replica:\n\t\t\t\tconnect_replica()\n\t\t\ttry:\n\t\t\t\tretval = fn(*args, **get_newargs(fn, kwargs))\n\t\t\texcept:\n\t\t\t\traise\n\t\t\tfinally:\n\t\t\t\tif local and hasattr(local, 'primary_db'):\n\t\t\t\t\tlocal.db.close()\n\t\t\t\t\tlocal.db = local.primary_db\n\t\t\treturn retval\n\t\treturn wrapper_fn\n\treturn innfn\ndef validate_and_sanitize_search_inputs(fn, instance, args, kwargs):\n\tkwargs.update(dict(zip(fn.__code__.co_varnames, args)))\n\tsanitize_searchfield(kwargs['searchfield'])\n\tkwargs['start'] = cint(kwargs['start'])\n\tkwargs['page_len'] = cint(kwargs['page_len'])\n\tif kwargs['doctype'] and not frappe.db.exists('DocType', kwargs['doctype']):\n\t\treturn []\n\treturn fn(**kwargs)\n\tdef whitelist(f):\n\t\t\"\"\"Decorator: Whitelist method to be called remotely via REST API.\"\"\"\n\t\tf.whitelisted = True\n\t\treturn f\n\tdef is_whitelisted(self, method):\n\t\tfn = getattr(self, method, None)\n\t\tif not fn:\n\t\t\traise NotFound(\"Method {0} not found\".format(method))\n\t\telif not getattr(fn, \"whitelisted\", False):\n\t\t\traise Forbidden(\"Method {0} not whitelisted\".format(method))",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-23058",
        "description": "[{'lang': 'en', 'value': 'ERPNext in versions v12.0.9-v13.0.3 are affected by a stored XSS vulnerability that allows low privileged users to store malicious scripts in the \u2018username\u2019 field in \u2018my settings\u2019 which can lead to full account takeover.'}]",
        "cwe_number": 79
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-290",
      "code": "def convert_file(filename: str) -> None:\n    \"\"\"\n    Parse a Markdown file and dump the output to stdout.\n    \"\"\"\n    try:\n        with open(filename, \"r\") as fin:\n            rendered = MarkdownIt().render(fin.read())\n            print(rendered, end=\"\")\n    except OSError:\n        sys.stderr.write(f'Cannot open file \"{filename}\".\\n')\n        sys.exit(1)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-26302",
        "description": "[{'lang': 'en', 'value': 'Denial of service could be caused to the command line interface of markdown-it-py, before v2.2.0, if an attacker was allowed to use invalid UTF-8 characters as input.\\n'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-291",
      "code": "def manage_action_containers(\n    request, action, o_type=None, o_id=None, conn=None, **kwargs\n):\n    \"\"\"\n    Handles many different actions on various objects.\n    @param action:      \"addnewcontainer\", (creates a new Project, Dataset,\n                        Screen), \"editname\", \"savename\", \"editdescription\",\n                        \"savedescription\",  (used as GET and POST for in-line\n                        editing),\n                        \"removefromshare\", (tree P/D/I moving etc)\n                        \"delete\", \"deletemany\"      (delete objects)\n                        \"remove\" (remove tag/comment from object)\n    @param o_type:      \"dataset\", \"project\", \"image\", \"screen\", \"plate\",\n                        \"acquisition\", \"well\",\"comment\", \"file\", \"tag\",\n                        \"tagset\",\"share\", \"sharecomment\"\n    \"\"\"\n    template = None\n    manager = None\n    if o_type in (\n        \"dataset\",\n        \"project\",\n        \"image\",\n        \"screen\",\n        \"plate\",\n        \"acquisition\",\n        \"well\",\n        \"comment\",\n        \"file\",\n        \"tag\",\n        \"tagset\",\n    ):\n        kw = {}\n        if o_type is not None and int(o_id) > 0:\n            o_id = int(o_id)\n            kw[str(o_type)] = o_id\n        try:\n            manager = BaseContainer(conn, **kw)\n        except AttributeError as x:\n            return handlerInternalError(request, x)\n    elif o_type in (\"share\", \"sharecomment\", \"chat\"):\n        manager = BaseShare(conn, o_id)\n    else:\n        manager = BaseContainer(conn)\n    form = None\n    if action == \"addnewcontainer\":\n        if not request.method == \"POST\":\n            return JsonResponse(\n                {\"Error\": \"Must use POST to create container\"}, status=405\n            )\n        form = ContainerForm(data=request.POST.copy())\n        if form.is_valid():\n            logger.debug(\"Create new in %s: %s\" % (o_type, str(form.cleaned_data)))\n            name = form.cleaned_data[\"name\"]\n            description = form.cleaned_data[\"description\"]\n            owner = form.cleaned_data[\"owner\"]\n            if o_type == \"project\" and hasattr(manager, o_type) and o_id > 0:\n                oid = manager.createDataset(name, description, owner=owner)\n            elif o_type == \"tagset\" and o_id > 0:\n                oid = manager.createTag(name, description, owner=owner)\n            elif request.POST.get(\"folder_type\") in (\n                \"project\",\n                \"screen\",\n                \"dataset\",\n                \"tag\",\n                \"tagset\",\n            ):\n                folder_type = request.POST.get(\"folder_type\")\n                if folder_type == \"dataset\":\n                    oid = manager.createDataset(\n                        name,\n                        description,\n                        owner=owner,\n                        img_ids=request.POST.getlist(\"image\", None),\n                    )\n                else:\n                    oid = conn.createContainer(\n                        folder_type, name, description, owner=owner\n                    )\n            else:\n                return HttpResponseServerError(\"Object does not exist\")\n            rdict = {\"bad\": \"false\", \"id\": oid}\n            return JsonResponse(rdict)\n        else:\n            d = dict()\n            for e in form.errors.items():\n                d.update({e[0]: unicode(e[1])})\n            rdict = {\"bad\": \"true\", \"errs\": d}\n            return JsonResponse(rdict)\n    elif action == \"add\":\n        template = \"webclient/public/share_form.html\"\n        experimenters = list(conn.getExperimenters())\n        experimenters.sort(key=lambda x: x.getOmeName().lower())\n        if o_type == \"share\":\n            img_ids = request.GET.getlist(\"image\", request.POST.getlist(\"image\"))\n            if request.method == \"GET\" and len(img_ids) == 0:\n                return HttpResponse(\"No images specified\")\n            images_to_share = list(conn.getObjects(\"Image\", img_ids))\n            if request.method == \"POST\":\n                form = BasketShareForm(\n                    initial={\"experimenters\": experimenters, \"images\": images_to_share},\n                    data=request.POST.copy(),\n                )\n                if form.is_valid():\n                    images = form.cleaned_data[\"image\"]\n                    message = form.cleaned_data[\"message\"]\n                    expiration = form.cleaned_data[\"expiration\"]\n                    members = form.cleaned_data[\"members\"]\n                    enable = form.cleaned_data[\"enable\"]\n                    host = \"%s?server=%i\" % (\n                        request.build_absolute_uri(\n                            reverse(\"load_template\", args=[\"public\"])\n                        ),\n                        int(conn.server_id),\n                    )\n                    shareId = manager.createShare(\n                        host, images, message, members, enable, expiration\n                    )\n                    return HttpResponse(\"shareId:%s\" % shareId)\n            else:\n                initial = {\n                    \"experimenters\": experimenters,\n                    \"images\": images_to_share,\n                    \"enable\": True,\n                    \"selected\": request.GET.getlist(\"image\"),\n                }\n                form = BasketShareForm(initial=initial)\n        template = \"webclient/public/share_form.html\"\n        context = {\"manager\": manager, \"form\": form}\n    elif action == \"edit\":\n        if o_id is None:\n            raise Http404(\"No share ID\")\n        if o_type == \"share\" and int(o_id) > 0:\n            template = \"webclient/public/share_form.html\"\n            manager.getMembers(o_id)\n            manager.getComments(o_id)\n            experimenters = list(conn.getExperimenters())\n            experimenters.sort(key=lambda x: x.getOmeName().lower())\n            initial = {\n                \"message\": manager.share.message,\n                \"expiration\": \"\",\n                \"shareMembers\": manager.membersInShare,\n                \"enable\": manager.share.active,\n                \"experimenters\": experimenters,\n            }\n            if manager.share.getExpireDate() is not None:\n                initial[\"expiration\"] = manager.share.getExpireDate().strftime(\n                    \"%Y-%m-%d\"\n                )\n            form = ShareForm(initial=initial)\n            context = {\"manager\": manager, \"form\": form}\n    elif action == \"save\":\n        if not request.method == \"POST\":\n            return HttpResponseRedirect(\n                reverse(\"manage_action_containers\", args=[\"edit\", o_type, o_id])\n            )\n        if o_type == \"share\":\n            experimenters = list(conn.getExperimenters())\n            experimenters.sort(key=lambda x: x.getOmeName().lower())\n            form = ShareForm(\n                initial={\"experimenters\": experimenters}, data=request.POST.copy()\n            )\n            if form.is_valid():\n                logger.debug(\"Update share: %s\" % (str(form.cleaned_data)))\n                message = form.cleaned_data[\"message\"]\n                expiration = form.cleaned_data[\"expiration\"]\n                members = form.cleaned_data[\"members\"]\n                enable = form.cleaned_data[\"enable\"]\n                host = \"%s?server=%i\" % (\n                    request.build_absolute_uri(\n                        reverse(\"load_template\", args=[\"public\"])\n                    ),\n                    int(conn.server_id),\n                )\n                manager.updateShareOrDiscussion(\n                    host, message, members, enable, expiration\n                )\n                r = \"enable\" if enable else \"disable\"\n                return HttpResponse(r)\n            else:\n                template = \"webclient/public/share_form.html\"\n                context = {\"share\": manager, \"form\": form}\n        else:\n            return HttpResponseServerError(\"Object does not exist\")\n    elif action == \"editname\":\n        if hasattr(manager, o_type) and o_id > 0:\n            obj = getattr(manager, o_type)\n            template = \"webclient/ajax_form/container_form_ajax.html\"\n            if o_type == \"tag\":\n                txtValue = obj.textValue\n            else:\n                txtValue = obj.getName()\n            form = ContainerNameForm(initial={\"name\": txtValue})\n            context = {\"manager\": manager, \"form\": form}\n        else:\n            return HttpResponseServerError(\"Object does not exist\")\n    elif action == \"savename\":\n        if not request.method == \"POST\":\n            return HttpResponseRedirect(\n                reverse(\"manage_action_containers\", args=[\"edit\", o_type, o_id])\n            )\n        if hasattr(manager, o_type) and o_id > 0:\n            form = ContainerNameForm(data=request.POST.copy())\n            if form.is_valid():\n                logger.debug(\"Update name form:\" + str(form.cleaned_data))\n                name = form.cleaned_data[\"name\"]\n                rdict = {\"bad\": \"false\", \"o_type\": o_type}\n                manager.updateName(o_type, name)\n                return JsonResponse(rdict)\n            else:\n                d = dict()\n                for e in form.errors.items():\n                    d.update({e[0]: unicode(e[1])})\n                rdict = {\"bad\": \"true\", \"errs\": d}\n                return JsonResponse(rdict)\n        else:\n            return HttpResponseServerError(\"Object does not exist\")\n    elif action == \"editdescription\":\n        if hasattr(manager, o_type) and o_id > 0:\n            obj = getattr(manager, o_type)\n            template = \"webclient/ajax_form/container_form_ajax.html\"\n            form = ContainerDescriptionForm(initial={\"description\": obj.description})\n            context = {\"manager\": manager, \"form\": form}\n        else:\n            return HttpResponseServerError(\"Object does not exist\")\n    elif action == \"savedescription\":\n        if not request.method == \"POST\":\n            return HttpResponseServerError(\n                \"Action '%s' on the '%s' id:%s cannot be complited\"\n                % (action, o_type, o_id)\n            )\n        if hasattr(manager, o_type) and o_id > 0:\n            form = ContainerDescriptionForm(data=request.POST.copy())\n            if form.is_valid():\n                logger.debug(\"Update name form:\" + str(form.cleaned_data))\n                description = form.cleaned_data[\"description\"]\n                manager.updateDescription(o_type, description)\n                rdict = {\"bad\": \"false\"}\n                return JsonResponse(rdict)\n            else:\n                d = dict()\n                for e in form.errors.items():\n                    d.update({e[0]: unicode(e[1])})\n                rdict = {\"bad\": \"true\", \"errs\": d}\n                return JsonResponse(rdict)\n        else:\n            return HttpResponseServerError(\"Object does not exist\")\n    elif action == \"remove\":\n        parents = request.POST[\"parent\"]\n        try:\n            manager.remove(parents.split(\"|\"))\n        except Exception as x:\n            logger.error(traceback.format_exc())\n            rdict = {\"bad\": \"true\", \"errs\": str(x)}\n            return JsonResponse(rdict)\n        rdict = {\"bad\": \"false\"}\n        return JsonResponse(rdict)\n    elif action == \"removefromshare\":\n        image_id = request.POST.get(\"source\")\n        try:\n            manager.removeImage(image_id)\n        except Exception as x:\n            logger.error(traceback.format_exc())\n            rdict = {\"bad\": \"true\", \"errs\": str(x)}\n            return JsonResponse(rdict)\n        rdict = {\"bad\": \"false\"}\n        return JsonResponse(rdict)\n    elif action == \"delete\":\n        child = toBoolean(request.POST.get(\"child\"))\n        anns = toBoolean(request.POST.get(\"anns\"))\n        try:\n            handle = manager.deleteItem(child, anns)\n            request.session[\"callback\"][str(handle)] = {\n                \"job_type\": \"delete\",\n                \"delmany\": False,\n                \"did\": o_id,\n                \"dtype\": o_type,\n                \"status\": \"in progress\",\n                \"error\": 0,\n                \"dreport\": _formatReport(handle),\n                \"start_time\": datetime.datetime.now(),\n            }\n            request.session.modified = True\n        except Exception as x:\n            logger.error(\n                \"Failed to delete: %r\" % {\"did\": o_id, \"dtype\": o_type}, exc_info=True\n            )\n            rdict = {\"bad\": \"true\", \"errs\": str(x)}\n        else:\n            rdict = {\"bad\": \"false\"}\n        return JsonResponse(rdict)\n    elif action == \"deletemany\":\n        object_ids = {\n            \"Image\": request.POST.getlist(\"image\"),\n            \"Dataset\": request.POST.getlist(\"dataset\"),\n            \"Project\": request.POST.getlist(\"project\"),\n            \"Annotation\": request.POST.getlist(\"tag\"),\n            \"Screen\": request.POST.getlist(\"screen\"),\n            \"Plate\": request.POST.getlist(\"plate\"),\n            \"Well\": request.POST.getlist(\"well\"),\n            \"PlateAcquisition\": request.POST.getlist(\"acquisition\"),\n        }\n        child = toBoolean(request.POST.get(\"child\"))\n        anns = toBoolean(request.POST.get(\"anns\"))\n        logger.debug(\n            \"Delete many: child? %s anns? %s object_ids %s\" % (child, anns, object_ids)\n        )\n        try:\n            for key, ids in object_ids.items():\n                if ids is not None and len(ids) > 0:\n                    handle = manager.deleteObjects(key, ids, child, anns)\n                    if key == \"PlateAcquisition\":\n                        key = \"Plate Run\"\n                    dMap = {\n                        \"job_type\": \"delete\",\n                        \"start_time\": datetime.datetime.now(),\n                        \"status\": \"in progress\",\n                        \"error\": 0,\n                        \"dreport\": _formatReport(handle),\n                        \"dtype\": key,\n                    }\n                    if len(ids) > 1:\n                        dMap[\"delmany\"] = len(ids)\n                        dMap[\"did\"] = ids\n                    else:\n                        dMap[\"delmany\"] = False\n                        dMap[\"did\"] = ids[0]\n                    request.session[\"callback\"][str(handle)] = dMap\n            request.session.modified = True\n        except Exception:\n            logger.error(\n                \"Failed to delete: %r\" % {\"did\": ids, \"dtype\": key}, exc_info=True\n            )\n            raise\n        else:\n            rdict = {\"bad\": \"false\"}\n        return JsonResponse(rdict)\n    context[\"template\"] = template\n    return context\ndef _table_query(request, fileid, conn=None, query=None, lazy=False, **kwargs):\n    \"\"\"\n    Query a table specified by fileid\n    Returns a dictionary with query result if successful, error information\n    otherwise\n    @param request:     http request; querystring must contain key 'query'\n                        with query to be executed, or '*' to retrieve all rows.\n                        If query is in the format word-number, e.g. \"Well-7\",\n                        if will be run as (word==number), e.g. \"(Well==7)\".\n                        This is supported to allow more readable query strings.\n    @param fileid:      Numeric identifier of file containing the table\n    @param query:       The table query. If None, use request.GET.get('query')\n                        E.g. '*' to return all rows.\n                        If in the form 'colname-1', query will be (colname==1)\n    @param lazy:        If True, instead of returning a 'rows' list,\n                        'lazy_rows' will be a generator.\n                        Each gen.next() will return a list of row data\n                        AND 'table' returned MUST be closed.\n    @param conn:        L{omero.gateway.BlitzGateway}\n    @param **kwargs:    offset, limit\n    @return:            A dictionary with key 'error' with an error message\n                        or with key 'data' containing a dictionary with keys\n                        'columns' (an array of column names) and 'rows'\n                        (an array of rows, each an array of values)\n    \"\"\"\n    if query is None:\n        query = request.GET.get(\"query\")\n    if not query:\n        return dict(error=\"Must specify query parameter, use * to retrieve all\")\n    col_names = request.GET.getlist(\"col_names\")\n    ctx = conn.createServiceOptsDict()\n    ctx.setOmeroGroup(\"-1\")\n    r = conn.getSharedResources()\n    t = r.openTable(omero.model.OriginalFileI(fileid), ctx)\n    if not t:\n        return dict(error=\"Table %s not found\" % fileid)\n    try:\n        cols = t.getHeaders()\n        col_indices = range(len(cols))\n        if col_names:\n            enumerated_columns = (\n                [(i, j) for (i, j) in enumerate(cols) if j.name in col_names]\n                if col_names\n                else [(i, j) for (i, j) in enumerate(cols)]\n            )\n            cols = []\n            col_indices = []\n            for col_name in col_names:\n                for (i, j) in enumerated_columns:\n                    if col_name == j.name:\n                        col_indices.append(i)\n                        cols.append(j)\n                        break\n        rows = t.getNumberOfRows()\n        offset = kwargs.get(\"offset\", 0)\n        limit = kwargs.get(\"limit\", None)\n        if not offset:\n            offset = int(request.GET.get(\"offset\", 0))\n        if not limit:\n            limit = (\n                int(request.GET.get(\"limit\"))\n                if request.GET.get(\"limit\") is not None\n                else None\n            )\n        range_start = offset\n        range_size = kwargs.get(\"limit\", rows)\n        range_end = min(rows, range_start + range_size)\n        if query == \"*\":\n            hits = range(range_start, range_end)\n            totalCount = rows\n        else:\n            match = re.match(r\"^(\\w+)-(\\d+)\", query)\n            if match:\n                query = \"(%s==%s)\" % (match.group(1), match.group(2))\n            try:\n                logger.info(query)\n                hits = t.getWhereList(query, None, 0, rows, 1)\n                totalCount = len(hits)\n                hits = hits[range_start:range_end]\n            except Exception:\n                return dict(error=\"Error executing query: %s\" % query)\n        def row_generator(table, h):\n            idx = 0\n            batch = 1000\n            while idx < len(h):\n                batch = min(batch, len(h) - idx)\n                res = table.slice(col_indices, h[idx : idx + batch])\n                idx += batch\n                yield [\n                    [col.values[row] for col in res.columns]\n                    for row in range(0, len(res.rowNumbers))\n                ]\n        row_gen = row_generator(t, hits)\n        rsp_data = {\n            \"data\": {\n                \"column_types\": [col.__class__.__name__ for col in cols],\n                \"columns\": [col.name for col in cols],\n            },\n            \"meta\": {\n                \"rowCount\": rows,\n                \"totalCount\": totalCount,\n                \"limit\": limit,\n                \"offset\": offset,\n            },\n        }\n        if not lazy:\n            row_data = []\n            for rows in list(row_gen):\n                row_data.extend(rows)\n            rsp_data[\"data\"][\"rows\"] = row_data\n        else:\n            rsp_data[\"data\"][\"lazy_rows\"] = row_gen\n            rsp_data[\"table\"] = t\n        return rsp_data\n    finally:\n        if not lazy:\n            t.close()\n    def prepare_context(self, request, context, *args, **kwargs):\n        \"\"\"\n        This allows templates to access the current eventContext and user from\n        the L{omero.gateway.BlitzGateway}.\n        E.g. <h1>{{ ome.user.getFullName }}</h1>\n        If these are not required by the template, then they will not need to\n        be loaded by the Blitz Gateway.\n        The results are cached by Blitz Gateway, so repeated calls have no\n        additional cost.\n        We also process some values from settings and add these to the\n        context.\n        \"\"\"\n        if \"conn\" not in kwargs:\n            return\n        conn = kwargs[\"conn\"]\n        context[\"omero\"] = {\n            \"constants\": {\n                \"NSCOMPANIONFILE\": constants.namespaces.NSCOMPANIONFILE,\n                \"ORIGINALMETADATA\": constants.annotation.file.ORIGINALMETADATA,\n                \"NSCLIENTMAPANNOTATION\": constants.metadata.NSCLIENTMAPANNOTATION,\n            }\n        }\n        context.setdefault(\"ome\", {})\n        public_user = omeroweb.decorators.is_public_user(request)\n        if public_user is not None:\n            context[\"ome\"][\"is_public_user\"] = public_user\n        context[\"ome\"][\"eventContext\"] = eventContextMarshal(conn.getEventContext())\n        context[\"ome\"][\"user\"] = conn.getUser\n        context[\"ome\"][\"user_id\"] = request.session.get(\"user_id\", conn.getUserId())\n        context[\"ome\"][\"group_id\"] = request.session.get(\"group_id\", None)\n        context[\"ome\"][\"active_group\"] = request.session.get(\n            \"active_group\", conn.getEventContext().groupId\n        )\n        context[\"global_search_form\"] = GlobalSearchForm()\n        context[\"ome\"][\"can_create\"] = request.session.get(\"can_create\", True)\n        if request.session.get(\"server_settings\"):\n            context[\"ome\"][\"email\"] = request.session.get(\"server_settings\").get(\n                \"email\", False\n            )\n            if request.session.get(\"server_settings\").get(\"ui\"):\n                context.setdefault(\"ui\", {\"tree\": {}})\n                context[\"ui\"][\"orphans\"] = (\n                    request.session.get(\"server_settings\")\n                    .get(\"ui\", {})\n                    .get(\"tree\", {})\n                    .get(\"orphans\")\n                )\n                context[\"ui\"][\"dropdown_menu\"] = (\n                    request.session.get(\"server_settings\")\n                    .get(\"ui\", {})\n                    .get(\"menu\", {})\n                    .get(\"dropdown\")\n                )\n                context[\"ui\"][\"tree\"][\"type_order\"] = (\n                    request.session.get(\"server_settings\")\n                    .get(\"ui\", {})\n                    .get(\"tree\", {})\n                    .get(\"type_order\")\n                )\n        self.load_settings(request, context, conn)\n    def __init__(self, *args, **kwargs):\n        super(BasketShareForm, self).__init__(*args, **kwargs)\n        try:\n            self.fields[\"image\"] = GroupModelMultipleChoiceField(\n                queryset=kwargs[\"initial\"][\"images\"],\n                initial=kwargs[\"initial\"][\"selected\"],\n                widget=forms.SelectMultiple(attrs={\"size\": 10}),\n            )\n        except Exception:\n            self.fields[\"image\"] = GroupModelMultipleChoiceField(\n                queryset=kwargs[\"initial\"][\"images\"],\n                widget=forms.SelectMultiple(attrs={\"size\": 10}),\n            )\n    def prepare_context(self, request, context, *args, **kwargs):\n        \"\"\" Hook for adding additional data to the context dict \"\"\"\n        pass",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-21376",
        "description": "[{'lang': 'en', 'value': 'OMERO.web is open source Django-based software for managing microscopy imaging. OMERO.web before version 5.9.0 loads various information about the current user such as their id, name and the groups they are in, and these are available on the main webclient pages. This represents an information exposure vulnerability. Some additional information being loaded is not used by the webclient and is being removed in this release. This is fixed in version 5.9.0.'}]",
        "cwe_number": 200
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-292",
      "code": "def serve_http(\n    bento_identifier: str | AnyService,\n    working_dir: str | None = None,\n    host: str = Provide[BentoMLContainer.http.host],\n    port: int = Provide[BentoMLContainer.http.port],\n    backlog: int = Provide[BentoMLContainer.api_server_config.backlog],\n    timeout: int | None = None,\n    ssl_certfile: str | None = Provide[BentoMLContainer.ssl.certfile],\n    ssl_keyfile: str | None = Provide[BentoMLContainer.ssl.keyfile],\n    ssl_keyfile_password: str | None = Provide[BentoMLContainer.ssl.keyfile_password],\n    ssl_version: int | None = Provide[BentoMLContainer.ssl.version],\n    ssl_cert_reqs: int | None = Provide[BentoMLContainer.ssl.cert_reqs],\n    ssl_ca_certs: str | None = Provide[BentoMLContainer.ssl.ca_certs],\n    ssl_ciphers: str | None = Provide[BentoMLContainer.ssl.ciphers],\n    bentoml_home: str = Provide[BentoMLContainer.bentoml_home],\n    development_mode: bool = False,\n    reload: bool = False,\n    dependency_map: dict[str, str] | None = None,\n    service_name: str = \"\",\n) -> None:\n    from circus.sockets import CircusSocket\n    from bentoml._internal.log import SERVER_LOGGING_CONFIG\n    from bentoml._internal.utils import reserve_free_port\n    from bentoml._internal.utils.analytics.usage_stats import track_serve\n    from bentoml._internal.utils.circus import create_standalone_arbiter\n    from bentoml.serve import construct_ssl_args\n    from bentoml.serve import create_watcher\n    from bentoml.serve import ensure_prometheus_dir\n    from bentoml.serve import make_reload_plugin\n    from ..loader import import_service\n    from ..loader import normalize_identifier\n    from .allocator import ResourceAllocator\n    prometheus_dir = ensure_prometheus_dir()\n    if isinstance(bento_identifier, Service):\n        svc = bento_identifier\n        bento_identifier = svc.import_string\n        assert (\n            working_dir is None\n        ), \"working_dir should not be set when passing a service in process\"\n        bento_path = pathlib.Path(\".\")\n    else:\n        bento_identifier, bento_path = normalize_identifier(\n            bento_identifier, working_dir\n        )\n        svc = import_service(bento_identifier, bento_path)\n    watchers: list[Watcher] = []\n    sockets: list[CircusSocket] = []\n    allocator = ResourceAllocator()\n    if dependency_map is None:\n        dependency_map = {}\n    if service_name:\n        svc = svc.find_dependent(service_name)\n    num_workers, worker_envs = allocator.get_worker_env(svc)\n    with tempfile.TemporaryDirectory(prefix=\"bentoml-uds-\") as uds_path:\n        if not service_name and not development_mode:\n            with contextlib.ExitStack() as port_stack:\n                for name, dep_svc in svc.all_services().items():\n                    if name == svc.name:\n                        continue\n                    if name in dependency_map:\n                        continue\n                    new_watcher, new_socket, uri = create_dependency_watcher(\n                        bento_identifier,\n                        dep_svc,\n                        uds_path,\n                        port_stack,\n                        backlog,\n                        dependency_map,\n                        allocator,\n                        str(bento_path.absolute()),\n                    )\n                    watchers.append(new_watcher)\n                    sockets.append(new_socket)\n                    dependency_map[name] = uri\n                port_stack.enter_context(reserve_free_port())\n        try:\n            ipaddr = ipaddress.ip_address(host)\n            if ipaddr.version == 4:\n                family = socket.AF_INET\n            elif ipaddr.version == 6:\n                family = socket.AF_INET6\n            else:\n                raise BentoMLConfigException(\n                    f\"Unsupported host IP address version: {ipaddr.version}\"\n                )\n        except ValueError as e:\n            raise BentoMLConfigException(f\"Invalid host IP address: {host}\") from e\n        sockets.append(\n            CircusSocket(\n                name=API_SERVER_NAME,\n                host=host,\n                port=port,\n                family=family,\n                backlog=backlog,\n            )\n        )\n        ssl_args = construct_ssl_args(\n            ssl_certfile=ssl_certfile,\n            ssl_keyfile=ssl_keyfile,\n            ssl_keyfile_password=ssl_keyfile_password,\n            ssl_version=ssl_version,\n            ssl_cert_reqs=ssl_cert_reqs,\n            ssl_ca_certs=ssl_ca_certs,\n            ssl_ciphers=ssl_ciphers,\n        )\n        timeout_args = [\"--timeout\", str(timeout)] if timeout else []\n        server_args = [\n            \"-m\",\n            _SERVICE_WORKER_SCRIPT,\n            bento_identifier,\n            \"--fd\",\n            f\"$(circus.sockets.{API_SERVER_NAME})\",\n            \"--service-name\",\n            svc.name,\n            \"--backlog\",\n            str(backlog),\n            \"--worker-id\",\n            \"$(CIRCUS.WID)\",\n            \"--prometheus-dir\",\n            prometheus_dir,\n            \"--main\",\n            *ssl_args,\n            *timeout_args,\n        ]\n        if worker_envs:\n            server_args.extend([\"--worker-env\", json.dumps(worker_envs)])\n        if development_mode:\n            server_args.append(\"--development-mode\")\n        scheme = \"https\" if BentoMLContainer.ssl.enabled.get() else \"http\"\n        watchers.append(\n            create_watcher(\n                name=\"service\",\n                args=server_args,\n                working_dir=str(bento_path.absolute()),\n                numprocesses=num_workers,\n                close_child_stdin=not development_mode,\n            )\n        )\n        log_host = \"localhost\" if host in [\"0.0.0.0\", \"::\"] else host\n        inject_env = {\"BENTOML_RUNNER_MAP\": json.dumps(dependency_map)}\n        for watcher in watchers:\n            if watcher.env is None:\n                watcher.env = inject_env\n            else:\n                watcher.env.update(inject_env)\n        arbiter_kwargs: dict[str, t.Any] = {\"watchers\": watchers, \"sockets\": sockets}\n        if reload:\n            reload_plugin = make_reload_plugin(str(bento_path.absolute()), bentoml_home)\n            arbiter_kwargs[\"plugins\"] = [reload_plugin]\n        if development_mode:\n            arbiter_kwargs[\"debug\"] = True\n            arbiter_kwargs[\"loggerconfig\"] = SERVER_LOGGING_CONFIG\n            arbiter_kwargs[\"loglevel\"] = \"WARNING\"\n        arbiter = create_standalone_arbiter(**arbiter_kwargs)\n        with track_serve(svc, production=not development_mode):\n            arbiter.start(\n                cb=lambda _: logger.info(\n                    'Starting production %s BentoServer from \"%s\" listening on %s://%s:%d (Press CTRL+C to quit)',\n                    scheme.upper(),\n                    bento_identifier,\n                    scheme,\n                    log_host,\n                    port,\n                ),\n            )\ndef main(\n    bento_identifier: str,\n    service_name: str,\n    fd: int,\n    runner_map: str | None,\n    backlog: int,\n    worker_env: str | None,\n    worker_id: int | None,\n    prometheus_dir: str | None,\n    ssl_certfile: str | None,\n    ssl_keyfile: str | None,\n    ssl_keyfile_password: str | None,\n    ssl_version: int | None,\n    ssl_cert_reqs: int | None,\n    ssl_ca_certs: str | None,\n    ssl_ciphers: str | None,\n    development_mode: bool,\n    timeout: int,\n    is_main: bool = False,\n):\n    \"\"\"\n    Start a HTTP server worker for given service.\n    \"\"\"\n    import psutil\n    import uvicorn\n    if worker_env:\n        env_list: list[dict[str, t.Any]] = json.loads(worker_env)\n        if worker_id is not None:\n            worker_key = worker_id - 1\n            if worker_key >= len(env_list):\n                raise IndexError(\n                    f\"Worker ID {worker_id} is out of range, \"\n                    f\"the maximum worker ID is {len(env_list)}\"\n                )\n            os.environ.update(env_list[worker_key])\n    from _bentoml_impl.loader import import_service\n    from bentoml._internal.container import BentoMLContainer\n    from bentoml._internal.context import component_context\n    from bentoml._internal.log import configure_server_logging\n    from ..server.app import ServiceAppFactory\n    if runner_map:\n        BentoMLContainer.remote_runner_mapping.set(\n            t.cast(t.Dict[str, str], json.loads(runner_map))\n        )\n    service = import_service(bento_identifier)\n    service.inject_config()\n    if service_name and service_name != service.name:\n        service = service.find_dependent(service_name)\n        component_context.component_type = \"service\"\n    else:\n        component_context.component_type = \"entry_service\"\n    if worker_id is not None:\n        component_context.component_index = worker_id\n    configure_server_logging()\n    BentoMLContainer.development_mode.set(development_mode)\n    if prometheus_dir is not None:\n        BentoMLContainer.prometheus_multiproc_dir.set(prometheus_dir)\n    component_context.component_name = service.name\n    app_factory = ServiceAppFactory(service)\n    asgi_app = app_factory(is_main=is_main)\n    uvicorn_extra_options: dict[str, t.Any] = {}\n    if ssl_version is not None:\n        uvicorn_extra_options[\"ssl_version\"] = ssl_version\n    if ssl_cert_reqs is not None:\n        uvicorn_extra_options[\"ssl_cert_reqs\"] = ssl_cert_reqs\n    if ssl_ciphers is not None:\n        uvicorn_extra_options[\"ssl_ciphers\"] = ssl_ciphers\n    if psutil.WINDOWS:\n        uvicorn_extra_options[\"loop\"] = \"asyncio\"\n        import asyncio\n        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n    uvicorn.run(\n        app=asgi_app,\n        fd=fd,\n        backlog=backlog,\n        log_config=None,\n        workers=1,\n        ssl_certfile=ssl_certfile,\n        ssl_keyfile=ssl_keyfile,\n        ssl_keyfile_password=ssl_keyfile_password,\n        ssl_ca_certs=ssl_ca_certs,\n        **uvicorn_extra_options,\n    )\ndef import_service(\n    service_identifier: str,\n    bento_path: pathlib.Path | None = None,\n) -> Service[t.Any]:\n    \"\"\"\n    import a service from a service identifier, which should be normalized by\n    `normalize_identifier` function.\n    \"\"\"\n    from _bentoml_sdk import Service\n    if bento_path is None:\n        bento_path = pathlib.Path(\".\").absolute()\n    if bento_path != pathlib.Path(\".\"):\n        extra_python_path = str(bento_path.absolute())\n        sys.path.insert(0, extra_python_path)\n    else:\n        extra_python_path = None\n    if (\n        bento_path.parent.joinpath(BENTO_YAML_FILENAME).exists()\n        and bento_path.parent.joinpath(\"models\").exists()\n    ):\n        from bentoml._internal.configuration.containers import BentoMLContainer\n        from bentoml._internal.models import ModelStore\n        original_model_store = BentoMLContainer.model_store.get()\n        BentoMLContainer.model_store.set(\n            ModelStore((bento_path.parent.joinpath(\"models\").absolute()))\n        )\n    else:\n        original_model_store = None\n    try:\n        module_name, _, attrs_str = service_identifier.partition(\":\")\n        assert (\n            module_name and attrs_str\n        ), f'Invalid import target \"{service_identifier}\", must format as \"<module>:<attribute>\"'\n        module = importlib.import_module(module_name)\n        root_service_name, _, depend_path = attrs_str.partition(\".\")\n        root_service = getattr(module, root_service_name)\n        assert isinstance(\n            root_service, Service\n        ), f'import target \"{module_name}:{attrs_str}\" is not a bentoml.Service instance'\n        if not depend_path:\n            return root_service\n        else:\n            return root_service.find_dependent(depend_path)\n    except (ImportError, AttributeError, KeyError, AssertionError) as e:\n        sys_path = sys.path.copy()\n        if extra_python_path is not None:\n            sys.path.remove(extra_python_path)\n        if original_model_store is not None:\n            from bentoml._internal.configuration.containers import BentoMLContainer\n            BentoMLContainer.model_store.set(original_model_store)\n        from bentoml.exceptions import ImportServiceError\n        raise ImportServiceError(\n            f'Failed to import service \"{service_identifier}\": {e}, sys.path: {sys_path}, cwd: {pathlib.Path.cwd()}'\n        ) from None\n    def __call__(self, is_main: bool = False) -> Starlette:\n        app = super().__call__()\n        app.add_exception_handler(\n            pydantic.ValidationError, self.handle_validation_error\n        )\n        app.add_exception_handler(BentoMLException, self.handle_bentoml_exception)\n        app.add_exception_handler(Exception, self.handle_uncaught_exception)\n        app.add_route(\"/schema.json\", self.schema_view, name=\"schema\")\n        if is_main:\n            if BentoMLContainer.new_index:\n                assets = Path(__file__).parent / \"assets\"\n                app.mount(\"/assets\", StaticFiles(directory=assets), name=\"assets\")\n            else:\n                from bentoml._internal import server\n                assets = Path(server.__file__).parent / \"static_content\"\n                app.mount(\n                    \"/static_content\",\n                    StaticFiles(directory=assets),\n                    name=\"static_content\",\n                )\n                app.add_route(\"/docs.json\", self.openapi_spec_view, name=\"openapi-spec\")\n            app.add_route(\"/\", self.index_page, name=\"index\")\n        for mount_app, path, name in self.service.mount_apps:\n            app.mount(app=mount_app, path=path, name=name)\n        return app",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2024-2912",
        "description": "[{'lang': 'en', 'value': 'An insecure deserialization vulnerability exists in the BentoML framework, allowing remote code execution (RCE) by sending a specially crafted POST request. By exploiting this vulnerability, attackers can execute arbitrary commands on the server hosting the BentoML application. The vulnerability is triggered when a serialized object, crafted to execute OS commands upon deserialization, is sent to any valid BentoML endpoint. This issue poses a significant security risk, enabling attackers to compromise the server and potentially gain unauthorized access or control.'}]",
        "cwe_number": 1188
      },
      "cwe_types": [],
      "severity": "nan"
    },
    {
      "id": "ContextAssembler-293",
      "code": "    def _check_group_whitelist(self, username, user_id, is_admin, access_token):\n        http_client = AsyncHTTPClient()\n        headers = _api_headers(access_token)\n        if is_admin:\n            for group in map(url_escape, self.gitlab_group_whitelist):\n                url = \"%s/groups/%s/members/%d\" % (GITLAB_API, group, user_id)\n                req = HTTPRequest(url, method=\"GET\", headers=headers)\n                resp = yield http_client.fetch(req, raise_error=False)\n                if resp.code == 200:\n                    return True\n        else:\n            next_page = url_concat(\"%s/groups\" % GITLAB_API,\n                                   dict(all_available=True))\n            while next_page:\n                req = HTTPRequest(next_page, method=\"GET\", headers=headers)\n                resp = yield http_client.fetch(req)\n                resp_json = json.loads(resp.body.decode('utf8', 'replace'))\n                next_page = next_page_from_links(resp)\n                user_groups = set(entry[\"path\"] for entry in resp_json)\n                if len(self.gitlab_group_whitelist & user_groups) > 0:\n                    return True\n            return False",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2018-7206",
        "description": "[{'lang': 'en', 'value': \"An issue was discovered in Project Jupyter JupyterHub OAuthenticator 0.6.x before 0.6.2 and 0.7.x before 0.7.3. When using JupyterHub with GitLab group whitelisting for access control, group membership was not checked correctly, allowing members not in the whitelisted groups to create accounts on the Hub. (Users were not allowed to access other users' accounts, but could create their own accounts on the Hub linked to their GitLab account. GitLab authentication not using gitlab_group_whitelist is unaffected. No other Authenticators are affected.)\"}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-294",
      "code": "def expose(template=None, validators=None, allow_json=None, html=None,\n           format=None, content_type=None, inputform=None, fragment=False,\n           as_format=\"default\", mapping=None, accept_format=None,\n           exclude_from_memory_profiling=False):\n    \"\"\"Exposes a method to the web.\n    By putting the expose decorator on a method, you tell TurboGears that\n    the method should be accessible via URL traversal. Additionally, expose\n    handles the output processing (turning a dictionary into finished\n    output) and is also responsible for ensuring that the request is\n    wrapped in a database transaction.\n    You can apply multiple expose decorators to a method, if\n    you'd like to support multiple output formats. The decorator that's\n    listed first in your code without as_format or accept_format is\n    the default that is chosen when no format is specifically asked for.\n    Any other expose calls that are missing as_format and accept_format\n    will have as_format implicitly set to the whatever comes before\n    the \":\" in the template name (or the whole template name if there\n    is no \":\". For example, <code>expose(\"json\")</code>, if it's not\n    the default expose, will have as_format set to \"json\".\n    When as_format is set, passing the same value in the tg_format\n    parameter in a request will choose the options for that expose\n    decorator. Similarly, accept_format will watch for matching\n    Accept headers. You can also use both. expose(\"json\", as_format=\"json\",\n    accept_format=\"application/json\") will choose JSON output for either\n    case: tg_format=json as a parameter or Accept: application/json as a\n    request header.\n    Passing allow_json=True to an expose decorator\n    is equivalent to adding the decorator just mentioned.\n    Each expose decorator has its own set of options, and each one\n    can choose a different template or even template engine (you can\n    use Kid for HTML output and Cheetah for plain text, for example).\n    See the other expose parameters below to learn about the options\n    you can pass to the template engine.\n    Take a look at the\n    <a href=\"tests/test_expose-source.html\">test_expose.py</a> suite\n    for more examples.\n    @param template \"templateengine:dotted.reference\" reference along the\n            Python path for the template and the template engine. For\n            example, \"kid:foo.bar\" will have Kid render the bar template in\n            the foo package.\n    @keyparam format format for the template engine to output (if the\n            template engine can render different formats. Kid, for example,\n            can render \"html\", \"xml\" or \"xhtml\")\n    @keyparam content_type sets the content-type http header\n    @keyparam allow_json allow the function to be exposed as json\n    @keyparam fragment for template engines (like Kid) that generate\n            DOCTYPE declarations and the like, this is a signal to\n            just generate the immediate template fragment. Use this\n            if you're building up a page from multiple templates or\n            going to put something onto a page with .innerHTML.\n    @keyparam mapping mapping with options that are sent to the template\n            engine\n    @keyparam as_format designates which value of tg_format will choose\n            this expose.\n    @keyparam accept_format which value of an Accept: header will\n            choose this expose.\n    @keyparam html deprecated in favor of template\n    @keyparam validators deprecated. Maps argument names to validator\n            applied to that arg\n    @keyparam inputform deprecated. A form object that generates the\n            input to this method\n    @keyparam exclude_from_memory_profiling allows to exclude individual end points from memory profiling. Can be\n            used for performance or in case profiling generates errors\n    \"\"\"\n    if html:\n        template = html\n    if not template:\n        template = format\n    if format == \"json\" or (format is None and template is None):\n        template = \"json\"\n        allow_json = True\n    if content_type is None:\n        content_type = config.get(\"tg.content_type\", None)\n    if config.get(\"tg.session.automatic_lock\", None):\n        cherrypy.session.acquire_lock()\n    def entangle(func):\n        log.debug(\"Exposing %s\", func)\n        log.debug(\"template: %s, format: %s, allow_json: %s, \"\n            \"content-type: %s\", template, format, allow_json, content_type)\n        if not getattr(func, \"exposed\", False):\n            def expose(func, *args, **kw):\n                accept = request.headers.get('Accept', \"\").lower()\n                accept = tg_util.simplify_http_accept_header(accept)\n                if not hasattr(func, \"_expose\"):\n                    _build_rules(func)\n                if hasattr(request, \"in_transaction\"):\n                    output = func._expose(func, accept, func._allow_json,\n                                *args, **kw)\n                else:\n                    request.in_transaction = True\n                    output = profile_expose_method(_run_with_transaction, accept, args, func, kw,\n                                                   exclude_from_memory_profiling)\n                return output\n            func.exposed = True\n            func._ruleinfo = []\n            allow_json_from_config = config.get(\"tg.allow_json\", False)\n            func._allow_json = allow_json_from_config or template == 'json'\n        else:\n            expose = lambda func, *args, **kw: func(*args, **kw)\n        func._ruleinfo.insert(0, dict(as_format=as_format,\n            accept_format=accept_format, template=template,\n            rulefunc = lambda _func, accept, allow_json, *args, **kw:\n                _execute_func(_func, template, format, content_type,\n                    mapping, fragment, args, kw)))\n        if allow_json:\n            func._allow_json = True\n        if inputform or validators:\n            import warnings\n            warnings.warn(\n                \"Use a separate decorator validate() rather than passing \"\n                \"arguments validators and/or inputform to decorator \"\n                \"expose().\",\n                DeprecationWarning, 2)\n            func = validate(form=inputform, validators=validators)(func)\n        return expose\n    return weak_signature_decorator(entangle)\ndef get_server_name():\n    \"\"\"Return name of the server this application runs on.\n    Respects 'Host' and 'X-Forwarded-Host' header.\n    See the docstring of the 'absolute_url' function for more information.\n    \"\"\"\n    get = config.get\n    h = request.headers\n    host = get('tg.url_domain') or h.get('X-Forwarded-Host', h.get('Host'))\n    if not host:\n        host = '%s:%s' % (get('server.socket_host', 'localhost'),\n            get('server.socket_port', 8080))\n    return host\ndef absolute_url(tgpath='/', params=None, **kw):\n    \"\"\"Return absolute URL (including schema and host to this server).\n    Tries to account for 'Host' header and reverse proxying\n    ('X-Forwarded-Host').\n    The host name is determined this way:\n    * If the config setting 'tg.url_domain' is set and non-null, use this value.\n    * Else, if the 'base_url_filter.use_x_forwarded_host' config setting is\n      True, use the value from the 'Host' or 'X-Forwarded-Host' request header.\n    * Else, if config setting 'base_url_filter.on' is True and\n      'base_url_filter.base_url' is non-null, use its value for the host AND\n      scheme part of the URL.\n    * As a last fallback, use the value of 'server.socket_host' and\n      'server.socket_port' config settings (defaults to 'localhost:8080').\n    The URL scheme ('http' or 'http') used is determined in the following way:\n    * If 'base_url_filter.base_url' is used, use the scheme from this URL.\n    * If there is a 'X-Use-SSL' request header, use 'https'.\n    * Else, if the config setting 'tg.url_scheme' is set, use its value.\n    * Else, use the value of 'cherrypy.request.scheme'.\n    \"\"\"\n    get = config.get\n    use_xfh = get('base_url_filter.use_x_forwarded_host', False)\n    if request.headers.get('X-Use-SSL'):\n        scheme = 'https'\n    else:\n        scheme = get('tg.url_scheme')\n    if not scheme:\n        scheme = request.scheme\n    base_url = '%s://%s' % (scheme, get_server_name())\n    if get('base_url_filter.on', False) and not use_xfh:\n        base_url = get('base_url_filter.base_url').rstrip('/')\n    return '%s%s' % (base_url, url(tgpath, params, **kw))\ndef redirect(redirect_path, redirect_params=None, **kw):\n    \"\"\"Redirect (via cherrypy.HTTPRedirect).\n    Raises the exception instead of returning it, this to allow\n    users to both call it as a function or to raise it as an exception.\n    \"\"\"\n    if not isinstance(redirect_path, basestring):\n        redirect_path = '/'.join(list(redirect_path))\n    if not redirect_path.startswith('/'):\n        path = request.path_info\n        check_app_root()\n        if path.startswith(request.app_root):\n            path = path[len(request.app_root):]\n        redirect_path = urlparse.urljoin(path, redirect_path)\n    raise cherrypy.HTTPRedirect(url(tgpath=redirect_path,\n        tgparams=redirect_params, **kw))\n__all__ = [\n    \"Controller\",\n    \"error_handler\",\n    \"exception_handler\",\n    \"expose\",\n    \"flash\",\n    \"redirect\",\n    \"Root\",\n    \"RootController\",\n    \"url\",\n    \"validate\",\n]",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2019-25101",
        "description": "[{'lang': 'en', 'value': 'A vulnerability classified as critical has been found in OnShift TurboGears 1.0.11.10. This affects an unknown part of the file turbogears/controllers.py of the component HTTP Header Handler. The manipulation leads to http response splitting. It is possible to initiate the attack remotely. Upgrading to version 1.0.11.11 is able to address this issue. The patch is named f68bbaba47f4474e1da553aa51564a73e1d92a84. It is recommended to upgrade the affected component. The associated identifier of this vulnerability is VDB-220059.'}]",
        "cwe_number": 436
      },
      "cwe_types": [],
      "severity": "critical"
    },
    {
      "id": "ContextAssembler-295",
      "code": "    def __init__(\n        self,\n        route_handlers: List[\"ControllerRouterHandler\"],\n        *,\n        after_exception: Optional[\"SingleOrList[AfterExceptionHookHandler]\"] = None,\n        after_request: Optional[\"AfterRequestHookHandler\"] = None,\n        after_response: Optional[\"AfterResponseHookHandler\"] = None,\n        after_shutdown: Optional[\"SingleOrList[LifeSpanHookHandler]\"] = None,\n        after_startup: Optional[\"SingleOrList[LifeSpanHookHandler]\"] = None,\n        allowed_hosts: Optional[Union[List[str], \"AllowedHostsConfig\"]] = None,\n        before_request: Optional[\"BeforeRequestHookHandler\"] = None,\n        before_send: Optional[\"SingleOrList[BeforeMessageSendHookHandler]\"] = None,\n        before_shutdown: Optional[\"SingleOrList[LifeSpanHookHandler]\"] = None,\n        before_startup: Optional[\"SingleOrList[LifeSpanHookHandler]\"] = None,\n        cache_config: CacheConfig = DEFAULT_CACHE_CONFIG,\n        cache_control: Optional[\"CacheControlHeader\"] = None,\n        compression_config: Optional[\"CompressionConfig\"] = None,\n        cors_config: Optional[\"CORSConfig\"] = None,\n        csrf_config: Optional[\"CSRFConfig\"] = None,\n        debug: bool = False,\n        dependencies: Optional[Dict[str, \"Provide\"]] = None,\n        etag: Optional[\"ETag\"] = None,\n        exception_handlers: Optional[\"ExceptionHandlersMap\"] = None,\n        guards: Optional[List[\"Guard\"]] = None,\n        initial_state: Optional[\"InitialStateType\"] = None,\n        logging_config: Union[\"BaseLoggingConfig\", \"EmptyType\", None] = Empty,\n        middleware: Optional[List[\"Middleware\"]] = None,\n        on_app_init: Optional[List[\"OnAppInitHandler\"]] = None,\n        on_shutdown: Optional[List[\"LifeSpanHandler\"]] = None,\n        on_startup: Optional[List[\"LifeSpanHandler\"]] = None,\n        openapi_config: Optional[OpenAPIConfig] = DEFAULT_OPENAPI_CONFIG,\n        opt: Optional[Dict[str, Any]] = None,\n        parameters: Optional[\"ParametersMap\"] = None,\n        plugins: Optional[List[\"PluginProtocol\"]] = None,\n        request_class: Optional[Type[\"Request\"]] = None,\n        response_class: Optional[\"ResponseType\"] = None,\n        response_cookies: Optional[\"ResponseCookies\"] = None,\n        response_headers: Optional[\"ResponseHeadersMap\"] = None,\n        security: Optional[List[\"SecurityRequirement\"]] = None,\n        static_files_config: Optional[Union[\"StaticFilesConfig\", List[\"StaticFilesConfig\"]]] = None,\n        tags: Optional[List[str]] = None,\n        template_config: Optional[\"TemplateConfig\"] = None,\n        type_encoders: Optional[\"TypeEncodersMap\"] = None,\n        websocket_class: Optional[Type[\"WebSocket\"]] = None,\n    ) -> None:\n        \"\"\"Initialize a ``Starlite`` application.\n        Args:\n            after_exception: An application level :class:`exception hook handler <starlite.types.AfterExceptionHookHandler>`\n                or list thereof.This hook is called after an exception occurs. In difference to exception handlers,\n                it is not meant to return a response - only to process the exception (e.g. log it, send it to Sentry etc.).\n            after_request: A sync or async function executed after the route handler function returned and the response\n                object has been resolved. Receives the response object.\n            after_response: A sync or async function called after the response has been awaited. It receives the\n                :class:`Request <starlite.connection.Request>` object and should not return any values.\n            after_shutdown: An application level :class:`life-span hook handler <starlite.types.LifeSpanHookHandler>` or\n                list thereof. This hook is called during the ASGI shutdown, after all callables in the 'on_shutdown'\n                list have been called.\n            after_startup: An application level :class:`life-span hook handler <starlite.types.LifeSpanHookHandler>` or\n                list thereof. This hook is called during the ASGI startup, after all callables in the 'on_startup'\n                list have been called.\n            allowed_hosts: A list of allowed hosts - enables the builtin allowed hosts middleware.\n            before_request: A sync or async function called immediately before calling the route handler.\n                Receives the :class:`Request <starlite.connection.Request>` instance and any non-``None`` return value is\n                used for the response, bypassing the route handler.\n            before_send: An application level :class:`before send hook handler <starlite.types.BeforeMessageSendHookHandler>` or\n                list thereof. This hook is called when the ASGI send function is called.\n            before_shutdown: An application level :class:`life-span hook handler <starlite.types.LifeSpanHookHandler>` or\n                list thereof. This hook is called during the ASGI shutdown, before any callables in the 'on_shutdown'\n                list have been called.\n            before_startup: An application level :class:`life-span hook handler <starlite.types.LifeSpanHookHandler>` or\n                list thereof. This hook is called during the ASGI startup, before any callables in the 'on_startup'\n                list have been called.\n            cache_config: Configures caching behavior of the application.\n            cache_control: A ``cache-control`` header of type\n                :class:`CacheControlHeader <starlite.datastructures.CacheControlHeader>` to add to route handlers of this app.\n                Can be overridden by route handlers.\n            compression_config: Configures compression behaviour of the application, this enabled a builtin or user\n                defined Compression middleware.\n            cors_config: If set this enables the builtin CORS middleware.\n            csrf_config: If set this enables the builtin CSRF middleware.\n            debug: If ``True``, app errors rendered as HTML with a stack trace.\n            dependencies: A string keyed dictionary of dependency :class:`Provider <starlite.datastructures.Provide>` instances.\n            etag: An ``etag`` header of type :class:`ETag <datastructures.ETag>` to add to route handlers of this app.\n                Can be overridden by route handlers.\n            exception_handlers: A dictionary that maps handler functions to status codes and/or exception types.\n            guards: A list of :class:`Guard <starlite.types.Guard>` callables.\n            initial_state: An object from which to initialize the app state.\n            logging_config: A subclass of :class:`BaseLoggingConfig <starlite.config.logging.BaseLoggingConfig>`.\n            middleware: A list of :class:`Middleware <starlite.types.Middleware>`.\n            on_app_init: A sequence of :class:`OnAppInitHandler <starlite.types.OnAppInitHandler>` instances. Handlers receive\n                an instance of :class:`AppConfig <starlite.config.app.AppConfig>` that will have been initially populated with\n                the parameters passed to :class:`Starlite <starlite.app.Starlite>`, and must return an instance of same. If more\n                than one handler is registered they are called in the order they are provided.\n            on_shutdown: A list of :class:`LifeSpanHandler <starlite.types.LifeSpanHandler>` called during\n                application shutdown.\n            on_startup: A list of :class:`LifeSpanHandler <starlite.types.LifeSpanHandler>` called during\n                application startup.\n            openapi_config: Defaults to :attr:`DEFAULT_OPENAPI_CONFIG`\n            opt: A string keyed dictionary of arbitrary values that can be accessed in :class:`Guards <starlite.types.Guard>` or wherever you\n                have access to :class:`Request <starlite.connection.request.Request>` or :class:`ASGI Scope <starlite.types.Scope>`.\n            parameters: A mapping of :class:`Parameter <starlite.params.Parameter>` definitions available to all\n                application paths.\n            plugins: List of plugins.\n            request_class: An optional subclass of :class:`Request <starlite.connection.request.Request>` to use for\n                http connections.\n            response_class: A custom subclass of [starlite.response.Response] to be used as the app's default response.\n            response_cookies: A list of [Cookie](starlite.datastructures.Cookie] instances.\n            response_headers: A string keyed dictionary mapping :class:`ResponseHeader <starlite.datastructures.ResponseHeader>`\n                instances.\n            route_handlers: A required list of route handlers, which can include instances of\n                :class:`Router <starlite.router.Router>`, subclasses of :class:`Controller <starlite.controller.Controller>` or\n                any function decorated by the route handler decorators.\n            security: A list of dictionaries that will be added to the schema of all route handlers in the application.\n                See :class:`SecurityRequirement <pydantic_openapi_schema.v3_1_0.security_requirement.SecurityRequirement>` for details.\n            static_files_config: An instance or list of :class:`StaticFilesConfig <starlite.config.StaticFilesConfig>`\n            tags: A list of string tags that will be appended to the schema of all route handlers under the application.\n            template_config: An instance of :class:`TemplateConfig <starlite.config.TemplateConfig>`\n            type_encoders: A mapping of types to callables that transform them into types supported for serialization.\n            websocket_class: An optional subclass of :class:`WebSocket <starlite.connection.websocket.WebSocket>` to use for\n                websocket connections.\n        \"\"\"\n        self.openapi_schema: Optional[\"OpenAPI\"] = None\n        self.get_logger: \"GetLogger\" = get_logger_placeholder\n        self.logger: Optional[\"Logger\"] = None\n        self.routes: List[Union[\"HTTPRoute\", \"ASGIRoute\", \"WebSocketRoute\"]] = []\n        self.asgi_router = ASGIRouter(app=self)\n        config = AppConfig(\n            after_exception=after_exception or [],\n            after_request=after_request,\n            after_response=after_response,\n            after_shutdown=after_shutdown or [],\n            after_startup=after_startup or [],\n            allowed_hosts=allowed_hosts or [],\n            before_request=before_request,\n            before_send=before_send or [],\n            before_shutdown=before_shutdown or [],\n            before_startup=before_startup or [],\n            cache_config=cache_config,\n            cache_control=cache_control,\n            compression_config=compression_config,\n            cors_config=cors_config,\n            csrf_config=csrf_config,\n            debug=debug,\n            dependencies=dependencies or {},\n            etag=etag,\n            exception_handlers=exception_handlers or {},\n            guards=guards or [],\n            initial_state=initial_state or {},\n            logging_config=logging_config if logging_config is not Empty else LoggingConfig() if debug else None,\n            middleware=middleware or [],\n            on_shutdown=on_shutdown or [],\n            on_startup=on_startup or [],\n            openapi_config=openapi_config,\n            opt=opt or {},\n            parameters=parameters or {},\n            plugins=plugins or [],\n            request_class=request_class,\n            response_class=response_class,\n            response_cookies=response_cookies or [],\n            response_headers=response_headers or {},\n            route_handlers=route_handlers,\n            security=security or [],\n            static_files_config=static_files_config or [],\n            tags=tags or [],\n            template_config=template_config,\n            type_encoders=type_encoders,\n            websocket_class=websocket_class,\n        )\n        for handler in on_app_init or []:\n            config = handler(config)\n        self.allowed_hosts = cast(\"Optional[AllowedHostsConfig]\", config.allowed_hosts)\n        self.after_exception = as_async_callable_list(config.after_exception)\n        self.after_shutdown = as_async_callable_list(config.after_shutdown)\n        self.after_startup = as_async_callable_list(config.after_startup)\n        self.before_send = as_async_callable_list(config.before_send)\n        self.before_shutdown = as_async_callable_list(config.before_shutdown)\n        self.before_startup = as_async_callable_list(config.before_startup)\n        self.cache = config.cache_config.to_cache()\n        self.compression_config = config.compression_config\n        self.cors_config = config.cors_config\n        self.csrf_config = config.csrf_config\n        self.debug = config.debug\n        self.logging_config = config.logging_config\n        self.on_shutdown = config.on_shutdown\n        self.on_startup = config.on_startup\n        self.openapi_config = config.openapi_config\n        self.plugins = config.plugins\n        self.request_class = config.request_class or Request\n        self.state = State(config.initial_state, deep_copy=True)\n        self.static_files_config = config.static_files_config\n        self.template_engine = config.template_config.engine_instance if config.template_config else None\n        self.websocket_class = config.websocket_class or WebSocket\n        super().__init__(\n            after_request=config.after_request,\n            after_response=config.after_response,\n            before_request=config.before_request,\n            cache_control=config.cache_control,\n            dependencies=config.dependencies,\n            etag=config.etag,\n            exception_handlers=config.exception_handlers,\n            guards=config.guards,\n            middleware=config.middleware,\n            opt=config.opt,\n            parameters=config.parameters,\n            path=\"\",\n            response_class=config.response_class,\n            response_cookies=config.response_cookies,\n            response_headers=config.response_headers,\n            route_handlers=[],\n            security=config.security,\n            tags=config.tags,\n            type_encoders=config.type_encoders,\n        )\n        for plugin in self.plugins:\n            plugin.on_app_init(app=self)\n        for route_handler in config.route_handlers:\n            self.register(route_handler)\n        if self.debug and isinstance(self.logging_config, LoggingConfig):\n            self.logging_config.loggers[\"starlite\"][\"level\"] = \"DEBUG\"\n        if self.logging_config:\n            self.get_logger = self.logging_config.configure()\n            self.logger = self.get_logger(\"starlite\")\n        if self.openapi_config:\n            self.openapi_schema = self.openapi_config.to_openapi_schema()\n            self.update_openapi_schema()\n            self.register(self.openapi_config.openapi_controller)\n        for static_config in (\n            self.static_files_config if isinstance(self.static_files_config, list) else [self.static_files_config]\n        ):\n            self.register(static_config.to_static_files_app())\n        self.asgi_handler = self._create_asgi_handler()\n    async def __call__(\n        self,\n        scope: Union[\"Scope\", \"LifeSpanScope\"],\n        receive: Union[\"Receive\", \"LifeSpanReceive\"],\n        send: Union[\"Send\", \"LifeSpanSend\"],\n    ) -> None:\n        \"\"\"Application entry point.\n        Lifespan events (startup / shutdown) are sent to the lifespan handler, otherwise the ASGI handler is used\n        Args:\n            scope: The ASGI connection scope.\n            receive: The ASGI receive function.\n            send: The ASGI send function.\n        Returns:\n            None\n        \"\"\"\n        scope[\"app\"] = self\n        if scope[\"type\"] == \"lifespan\":\n            await self.asgi_router.lifespan(receive=receive, send=send)\n            return\n        scope[\"state\"] = {}\n        await self.asgi_handler(scope, receive, self._wrap_send(send=send, scope=scope))\ndef Body(\n    *,\n    media_type: Union[str, \"RequestEncodingType\"] = RequestEncodingType.JSON,\n    examples: Optional[List[\"Example\"]] = None,\n    external_docs: Optional[\"ExternalDocumentation\"] = None,\n    content_encoding: Optional[str] = None,\n    default: Any = Empty,\n    title: Optional[str] = None,\n    description: Optional[str] = None,\n    const: Optional[bool] = None,\n    gt: Optional[float] = None,\n    ge: Optional[float] = None,\n    lt: Optional[float] = None,\n    le: Optional[float] = None,\n    multiple_of: Optional[float] = None,\n    min_items: Optional[int] = None,\n    max_items: Optional[int] = None,\n    min_length: Optional[int] = None,\n    max_length: Optional[int] = None,\n    regex: Optional[str] = None\n) -> Any:\n    \"\"\"Create an extended request body kwarg definition.\n    Args:\n        media_type: Defaults to RequestEncodingType.JSON.\n        examples: A list of Example models.\n        external_docs: A url pointing at external documentation for the given\n            parameter.\n        content_encoding: The content encoding of the value. Applicable on to string values. See\n            OpenAPI 3.1 for details.\n        default: A default value. If const is true, this value is required.\n        title: String value used in the title section of the OpenAPI schema for the given\n            parameter.\n        description: String value used in the description section of the OpenAPI schema for the\n            given parameter.\n        const: A boolean flag dictating whether this parameter is a constant. If True, the value passed\n            to the parameter must equal its default value. This also causes the OpenAPI const field to be populated with\n            the default value.\n        gt: Constrict value to be greater than a given float or int. Equivalent to\n            exclusiveMinimum in the OpenAPI specification.\n        ge: Constrict value to be greater or equal to a given float or int. Equivalent to\n            minimum in the OpenAPI specification.\n        lt: Constrict value to be less than a given float or int. Equivalent to\n            exclusiveMaximum in the OpenAPI specification.\n        le: Constrict value to be less or equal to a given float or int. Equivalent to maximum\n            in the OpenAPI specification.\n        multiple_of: Constrict value to a multiple of a given float or int. Equivalent to\n            multipleOf in the OpenAPI specification.\n        min_items: Constrict a set or a list to have a minimum number of items. Equivalent to\n            minItems in the OpenAPI specification.\n        max_items: Constrict a set or a list to have a maximum number of items. Equivalent to\n            maxItems in the OpenAPI specification.\n        min_length: Constrict a string or bytes value to have a minimum length. Equivalent to\n            minLength in the OpenAPI specification.\n        max_length: Constrict a string or bytes value to have a maximum length. Equivalent to\n            maxLength in the OpenAPI specification.\n        regex: A string representing a regex against which the given string will be matched.\n            Equivalent to pattern in the OpenAPI specification.\n    \"\"\"\n    return BodyKwarg(\n        media_type=media_type,\n        examples=examples,\n        external_docs=external_docs,\n        content_encoding=content_encoding,\n        default=default,\n        title=title,\n        description=description,\n        const=const,\n        gt=gt,\n        ge=ge,\n        lt=lt,\n        le=le,\n        multiple_of=multiple_of,\n        min_items=min_items,\n        max_items=max_items,\n        min_length=min_length,\n        max_length=max_length,\n        regex=regex,\n    )\nclass DependencyKwarg:\ndef parse_multipart_form(body: bytes, boundary: bytes) -> Dict[str, Any]:\n    \"\"\"Parse multipart form data.\n    Args:\n        body: Body of the request.\n        boundary: Boundary of the multipart message.\n    Returns:\n        A dictionary of parsed results.\n    \"\"\"\n    fields: DefaultDict[str, List[Any]] = defaultdict(list)\n    if body and boundary:\n        form_parts = body.split(boundary)\n        for form_part in form_parts[1:-1]:\n            file_name = None\n            content_type = \"text/plain\"\n            content_charset = \"utf-8\"\n            field_name = None\n            line_index = 2\n            line_end_index = 0\n            headers: List[Tuple[str, str]] = []\n            while line_end_index != -1:\n                line_end_index = form_part.find(b\"\\r\\n\", line_index)\n                form_line = form_part[line_index:line_end_index].decode(\"utf-8\")\n                if not form_line:\n                    break\n                line_index = line_end_index + 2\n                colon_index = form_line.index(\":\")\n                current_idx = colon_index + 2\n                form_header_field = form_line[0:colon_index].lower()\n                form_header_value, form_parameters = parse_content_header(form_line[current_idx:])\n                if form_header_field == \"content-disposition\":\n                    field_name = form_parameters.get(\"name\")\n                    file_name = form_parameters.get(\"filename\")\n                    if file_name is None and (filename_with_asterisk := form_parameters.get(\"filename*\")):\n                        encoding, _, value = decode_rfc2231(filename_with_asterisk)\n                        file_name = unquote(value, encoding=encoding or content_charset)\n                elif form_header_field == \"content-type\":\n                    content_type = form_header_value\n                    content_charset = form_parameters.get(\"charset\", \"utf-8\")\n                headers.append((form_header_field, form_header_value))\n            if field_name:\n                post_data = form_part[line_index:-4].lstrip(b\"\\r\\n\")\n                if file_name:\n                    form_file = UploadFile(\n                        content_type=content_type, filename=file_name, file_data=post_data, headers=dict(headers)\n                    )\n                    fields[field_name].append(form_file)\n                else:\n                    try:\n                        fields[field_name].append(decode_json(post_data))\n                    except SerializationException:\n                        fields[field_name].append(post_data.decode(content_charset))\n    return {k: v if len(v) > 1 else v[0] for k, v in fields.items()}\ndef create_multipart_extractor(\n    signature_field: \"SignatureField\", is_data_optional: bool\n) -> Callable[[\"ASGIConnection[Any, Any, Any]\"], Coroutine[Any, Any, Any]]:\n    \"\"\"Create a multipart form-data extractor.\n    Args:\n        signature_field: A SignatureField instance.\n        is_data_optional: Boolean dictating whether the field is optional.\n    Returns:\n        An extractor function.\n    \"\"\"\n    async def extract_multipart(\n        connection: \"Request[Any, Any]\",\n    ) -> Any:\n        connection.scope[\"_form\"] = form_values = (\n            connection.scope[\"_form\"]\n            if \"_form\" in connection.scope\n            else parse_multipart_form(\n                body=await connection.body(), boundary=connection.content_type[-1].get(\"boundary\", \"\").encode()\n            )\n        )\n        if signature_field.is_non_string_sequence:\n            return list(form_values.values())\n        if signature_field.is_simple_type and signature_field.field_type is UploadFile and form_values:\n            return [v for v in form_values.values() if isinstance(v, UploadFile)][0]\n        return form_values if form_values or not is_data_optional else None\n    return cast(\"Callable[[ASGIConnection[Any, Any, Any]], Coroutine[Any, Any, Any]]\", extract_multipart)\ndef create_url_encoded_data_extractor(\n    is_data_optional: bool,\n) -> Callable[[\"ASGIConnection[Any, Any, Any]\"], Coroutine[Any, Any, Any]]:\n    \"\"\"Create extractor for url encoded form-data.\n    Args:\n        is_data_optional: Boolean dictating whether the field is optional.\n    Returns:\n        An extractor function.\n    \"\"\"\n    async def extract_url_encoded_extractor(\n        connection: \"Request[Any, Any]\",\n    ) -> Any:\n        connection.scope[\"_form\"] = form_values = (\n            connection.scope[\"_form\"]\n            if \"_form\" in connection.scope\n            else parse_url_encoded_form_data(await connection.body())\n        )\n        return form_values if form_values or not is_data_optional else None\n    return cast(\"Callable[[ASGIConnection[Any, Any, Any]], Coroutine[Any, Any, Any]]\", extract_url_encoded_extractor)\n    async def form(self) -> FormMultiDict:\n        \"\"\"Retrieve form data from the request. If the request is either a 'multipart/form-data' or an\n        'application/x-www-form- urlencoded', return a FormMultiDict instance populated with the values sent in the\n        request, otherwise, an empty instance.\n        Returns:\n            A FormMultiDict instance\n        \"\"\"\n        if self._form is Empty:\n            content_type, options = self.content_type\n            if content_type == RequestEncodingType.MULTI_PART:\n                self._form = self.scope[\"_form\"] = form_values = parse_multipart_form(\n                    body=await self.body(), boundary=options.get(\"boundary\", \"\").encode()\n                )\n                return FormMultiDict(form_values)\n            if content_type == RequestEncodingType.URL_ENCODED:\n                self._form = self.scope[\"_form\"] = form_values = parse_url_encoded_form_data(\n                    await self.body(),\n                )\n                return FormMultiDict(form_values)\n            return FormMultiDict()\n        return FormMultiDict(self._form)\n    async def send_push_promise(self, path: str) -> None:\n        \"\"\"Send a push promise.\n        This method requires the `http.response.push` extension to be sent from the ASGI server.\n        Args:\n            path: Path to send the promise to.\n        Returns:\n            None\n        \"\"\"\n        extensions: Dict[str, Dict[Any, Any]] = self.scope.get(\"extensions\") or {}\n        if \"http.response.push\" in extensions:\n            raw_headers = []\n            for name in SERVER_PUSH_HEADERS:\n                for value in self.headers.getall(name, []):\n                    raw_headers.append((name.encode(\"latin-1\"), value.encode(\"latin-1\")))\n            await self.send({\"type\": \"http.response.push\", \"path\": path, \"headers\": raw_headers})",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2023-25578",
        "description": "[{'lang': 'en', 'value': 'Starlite is an Asynchronous Server Gateway Interface (ASGI) framework. Prior to version 1.5.2, the request body parsing in `starlite` allows a potentially unauthenticated attacker to consume a large amount of CPU time and RAM. The multipart body parser processes an unlimited number of file parts and an unlimited number of field parts. This is a remote, potentially unauthenticated Denial of Service vulnerability. This vulnerability affects applications with a request handler that accepts a `Body(media_type=RequestEncodingType.MULTI_PART)`. The large amount of CPU time required for processing requests can block all available worker processes and significantly delay or slow down the processing of legitimate user requests. The large amount of RAM accumulated while processing requests can lead to Out-Of-Memory kills. Complete DoS is achievable by sending many concurrent multipart requests in a loop. Version 1.51.2 contains a patch for this issue.\\n'}]",
        "cwe_number": 770
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-296",
      "code": "    def skip(self, type):\n        if type == TType.STOP:\n            return\n        elif type == TType.BOOL:\n            self.readBool()\n        elif type == TType.BYTE:\n            self.readByte()\n        elif type == TType.I16:\n            self.readI16()\n        elif type == TType.I32:\n            self.readI32()\n        elif type == TType.I64:\n            self.readI64()\n        elif type == TType.DOUBLE:\n            self.readDouble()\n        elif type == TType.FLOAT:\n            self.readFloat()\n        elif type == TType.STRING:\n            self.readString()\n        elif type == TType.STRUCT:\n            name = self.readStructBegin()\n            while True:\n                (name, type, id) = self.readFieldBegin()\n                if type == TType.STOP:\n                    break\n                self.skip(type)\n                self.readFieldEnd()\n            self.readStructEnd()\n        elif type == TType.MAP:\n            (ktype, vtype, size) = self.readMapBegin()\n            for _ in range(size):\n                self.skip(ktype)\n                self.skip(vtype)\n            self.readMapEnd()\n        elif type == TType.SET:\n            (etype, size) = self.readSetBegin()\n            for _ in range(size):\n                self.skip(etype)\n            self.readSetEnd()\n        elif type == TType.LIST:\n            (etype, size) = self.readListBegin()\n            for _ in range(size):\n                self.skip(etype)\n            self.readListEnd()",
      "label": 1,
      "metadata": {
        "CVEFixes-Number": "CVE-2019-3552",
        "description": "[{'lang': 'en', 'value': 'C++ Facebook Thrift servers (using cpp2) would not error upon receiving messages with containers of fields of unknown type. As a result, malicious clients could send short messages which would take a long time for the server to parse, potentially leading to denial of service. This issue affects Facebook Thrift prior to v2019.02.18.00.'}]",
        "cwe_number": 755
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-297",
      "code": "  def test_sparse_bincount_all_count(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    size = 1000\n    n_elems = 4096\n    inp_indices = np.random.randint(0, num_rows, (n_elems,))\n    inp_vals = np.random.randint(0, size, (n_elems,), dtype=dtype)\n    np_out = np.bincount(inp_vals, minlength=size)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_indices,\n                values=inp_vals,\n                dense_shape=[num_rows],\n                size=size,\n                weights=[])))\n  def test_sparse_bincount_all_count_with_weights(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    size = 1000\n    n_elems = 4096\n    inp_indices = np.random.randint(0, num_rows, (n_elems,))\n    inp_vals = np.random.randint(0, size, (n_elems,), dtype=dtype)\n    inp_weight = np.random.random((n_elems,))\n    np_out = np.bincount(inp_vals, minlength=size, weights=inp_weight)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_indices,\n                values=inp_vals,\n                dense_shape=[num_rows],\n                size=size,\n                weights=inp_weight)))\n  def test_sparse_bincount_all_binary(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    size = 10\n    n_elems = 4096\n    inp_indices = np.random.randint(0, num_rows, (n_elems,))\n    inp_vals = np.random.randint(0, size, (n_elems,), dtype=dtype)\n    np_out = np.ones((size,))\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_indices,\n                values=inp_vals,\n                dense_shape=[num_rows],\n                size=size,\n                weights=[],\n                binary_output=True)))\n  def test_sparse_bincount_all_binary_weights(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    size = 10\n    n_elems = 4096\n    inp_indices = np.random.randint(0, num_rows, (n_elems,))\n    inp_vals = np.random.randint(0, size, (n_elems,), dtype=dtype)\n    inp_weight = np.random.random((n_elems,))\n    np_out = np.ones((size,))\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_indices,\n                values=inp_vals,\n                dense_shape=[num_rows],\n                size=size,\n                weights=inp_weight,\n                binary_output=True)))\nclass RaggedBincountOpTest(test_util.TensorFlowTestCase,\n                           parameterized.TestCase):\n  def test_ragged_bincount_count(self, dtype):\n    x = ragged_factory_ops.constant([[], [], [3, 0, 1], [], [5, 0, 4, 4]])\n    expected_output = [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0,\n                                            0], [1, 1, 0, 1, 0, 0],\n                       [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 2, 1]]\n    self.assertAllEqual(\n        expected_output,\n        self.evaluate(\n            gen_math_ops.ragged_bincount(\n                splits=x.row_splits, values=x.values, weights=[], size=6)))",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-35982",
        "description": "[{'lang': 'en', 'value': 'TensorFlow is an open source platform for machine learning. If `SparseBincount` is given inputs for `indices`, `values`, and `dense_shape` that do not make a valid sparse tensor, it results in a segfault that can be used to trigger a denial of service attack. We have patched the issue in GitHub commit 40adbe4dd15b582b0210dfbf40c243a62f5119fa. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue.'}]",
        "cwe_number": null
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-298",
      "code": "def get_parser():\n    parser = configargparse.ArgumentParser(\n        prog='rdiffweb',\n        description='Web interface to browse and restore rdiff-backup repositories.',\n        default_config_files=['/etc/rdiffweb/rdw.conf', '/etc/rdiffweb/rdw.conf.d/*.conf'],\n        add_env_var_help=True,\n        auto_env_var_prefix='RDIFFWEB_',\n        config_file_parser_class=ConfigFileParser,\n        conflict_handler='resolve',\n    )\n    parser.add_argument(\n        '-f', '--config', is_config_file=True, metavar='FILE', help='location of Rdiffweb configuration file'\n    )\n    parser.add(\n        '--database-uri',\n        '--sqlitedb-file',\n        '--sqlitedbfile',\n        metavar='URI',\n        help=\"\"\"Location of the database used for persistence. SQLite and PostgreSQL\n            database are supported officially. To use a SQLite database you may\n            define the location using a file path or a URI.\n            e.g.: /srv/rdiffweb/file.db or sqlite:///srv/rdiffweb/file.db`.\n            To use PostgreSQL server you must provide\n            a URI similar to postgresql://user:pass@10.255.1.34/dbname and you\n            must install required dependencies.\n            By default, Rdiffweb uses a SQLite embedded database located at\n            /etc/rdiffweb/rdw.db.\"\"\",\n        default='/etc/rdiffweb/rdw.db',\n    )\n    parser.add_argument(\n        '-d',\n        '--debug',\n        action='store_true',\n        help='enable rdiffweb debug mode - change the log level to DEBUG, print exception stack trace to the web interface and show SQL query in logs',\n    )\n    parser.add_argument(\n        '--admin-user',\n        '--adminuser',\n        metavar='USERNAME',\n        help='administrator username. The administrator user get created on startup if the database is empty.',\n        default='admin',\n    )\n    parser.add_argument(\n        '--admin-password',\n        metavar='USERNAME',\n        help=\"\"\"administrator encrypted password as SSHA. Read online\n            documentation to know more about how to encrypt your password\n            into SSHA or use http://projects.marsching.org/weave4j/util/genpassword.php\n            When defined, administrator password cannot be updated using the web interface.\n            When undefined, default administrator password is `admin123` and\n            it can be updated using the web interface.\"\"\",\n    )\n    parser.add_argument(\n        '--default-theme',\n        '--defaulttheme',\n        help='define the default theme. Either: default, blue or orange. Define the CSS file to be loaded in the web interface. You may manually edit a CSS file to customize it. The location is similar to `/usr/local/lib/python3.9/dist-packages/rdiffweb/static/`',\n        choices=['default', 'blue', 'orange'],\n        default='default',\n    )\n    parser.add_argument(\n        '--environment',\n        choices=['development', 'production'],\n        help='define the type of environment: development, production. This is used to limit the information shown to the user when an error occur.',\n        default='production',\n    )\n    parser.add_argument(\n        '--email-encryption',\n        '--emailencryption',\n        choices=['none', 'ssl', 'starttls'],\n        help='type of encryption to be used when establishing communication with SMTP server. Default: none',\n        default='none',\n    )\n    parser.add_argument(\n        '--email-host',\n        '--emailhost',\n        metavar='HOST',\n        help='SMTP server used to send email in the form <host>:<port>. If the port is not provided, default to standard port 25 or 465 is used. e.g.: smtp.gmail.com:587',\n    )\n    parser.add_argument(\n        '--email-sender',\n        '--emailsender',\n        metavar='EMAIL',\n        help='email addres used for the `from:` field when sending email.',\n    )\n    parser.add_argument(\n        '--email-notification-time',\n        '--emailnotificationtime',\n        metavar='TIME',\n        help='time when the email notifcation should be sent for inactive backups. e.g.: 22:00 Default value: 23:00',\n        default='23:00',\n    )\n    parser.add_argument(\n        '--email-username',\n        '--emailusername',\n        metavar='USERNAME',\n        help='username used for authentication with the SMTP server.',\n    )\n    parser.add_argument(\n        '--email-password',\n        '--emailpassword',\n        metavar='PASSWORD',\n        help='password used for authentication with the SMTP server.',\n    )\n    parser.add_argument(\n        '--email-send-changed-notification',\n        '--emailsendchangednotification',\n        help='True to send notification when sensitive information get change in user profile.',\n        action='store_true',\n        default=False,\n    )\n    parser.add_argument(\n        '--favicon',\n        help='location of an icon to be used as a favicon displayed in web browser.',\n        default=pkg_resources.resource_filename('rdiffweb', 'static/favicon.ico'),\n    )\n    parser.add_argument(\n        '--footer-name', '--footername', help=argparse.SUPPRESS, default='rdiffweb'\n    )\n    parser.add_argument(\n        '--footer-url', '--footerurl', help=argparse.SUPPRESS, default='https://rdiffweb.org/'\n    )\n    parser.add_argument(\n        '--header-logo',\n        '--headerlogo',\n        help='location of an image (preferably a .png) to be used as a replacement for the rdiffweb logo.',\n    )\n    parser.add_argument(\n        '--header-name',\n        '--headername',\n        help='application name displayed in the title bar and header menu.',\n        default='rdiffweb',\n    )\n    parser.add_argument(\n        '--ldap-add-missing-user',\n        '--addmissinguser',\n        action='store_true',\n        help='enable creation of users from LDAP when the credential are valid.',\n        default=False,\n    )\n    parser.add_argument(\n        '--ldap-add-user-default-role',\n        help='default role used when creating users from LDAP. This parameter is only useful when `--ldap-add-missing-user` is enabled.',\n        default='user',\n        choices=['admin', 'maintainer', 'user'],\n    )\n    parser.add_argument(\n        '--ldap-add-user-default-userroot',\n        help='default user root directory used when creating users from LDAP. LDAP attributes may be used to define the default location. e.g.: `/backups/{uid[0]}/`. This parameter is only useful when `--ldap-add-missing-user` is enabled.',\n        default='',\n    )\n    parser.add_argument(\n        '--ldap-uri',\n        '--ldapuri',\n        help='URL to the LDAP server used to validate user credentials. e.g.: ldap://localhost:389',\n    )\n    parser.add_argument(\n        '--ldap-base-dn',\n        '--ldapbasedn',\n        metavar='DN',\n        help='DN of the branch of the directory where all searches should start from. e.g.: dc=my,dc=domain',\n        default=\"\",\n    )\n    parser.add_argument(\n        '--ldap-scope',\n        '--ldapscope',\n        help='scope of the search. Can be either base, onelevel or subtree',\n        choices=['base', 'onelevel', 'subtree'],\n        default=\"subtree\",\n    )\n    parser.add_argument('--ldap-tls', '--ldaptls', action='store_true', help='enable TLS')\n    parser.add_argument(\n        '--ldap-username-attribute',\n        '--ldapattribute',\n        metavar='ATTRIBUTE',\n        help=\"The attribute to search username. If no attributes are provided, the default is to use `uid`. It's a good idea to choose an attribute that will be unique across all entries in the subtree you will be using.\",\n        default='uid',\n    )\n    parser.add_argument(\n        '--ldap-filter',\n        '--ldapfilter',\n        help=\"search filter to limit LDAP lookup. If not provided, defaults to (objectClass=*), which searches for all objects in the tree.\",\n        default='(objectClass=*)',\n    )\n    parser.add_argument(\n        '--ldap-required-group',\n        '--ldaprequiredgroup',\n        metavar='GROUPNAME',\n        help=\"name of the group of which the user must be a member to access rdiffweb. Should be used with ldap-group-attribute and ldap-group-attribute-is-dn.\",\n    )\n    parser.add_argument(\n        '--ldap-group-attribute',\n        '--ldapgroupattribute',\n        metavar='ATTRIBUTE',\n        help=\"name of the attribute defining the groups of which the user is a member. Should be used with ldap-required-group and ldap-group-attribute-is-dn.\",\n        default='member',\n    )\n    parser.add_argument(\n        '--ldap-group-attribute-is-dn',\n        '--ldapgroupattributeisdn',\n        help=\"True if the content of the attribute `ldap-group-attribute` is a DN.\",\n        action='store_true',\n    )\n    parser.add_argument(\n        '--ldap-bind-dn',\n        '--ldapbinddn',\n        metavar='DN',\n        help=\"optional DN used to bind to the server when searching for entries. If not provided, will use an anonymous bind.\",\n        default=\"\",\n    )\n    parser.add_argument(\n        '--ldap-bind-password',\n        '--ldapbindpassword',\n        metavar='PASSWORD',\n        help=\"password to use in conjunction with LdapBindDn. Note that the bind password is probably sensitive data, and should be properly protected. You should only use the LdapBindDn and LdapBindPassword if you absolutely need them to search the directory.\",\n        default=\"\",\n    )\n    parser.add_argument(\n        '--ldap-version',\n        '--ldapversion',\n        '--ldapprotocolversion',\n        help=\"version of LDAP in use either 2 or 3. Default to 3.\",\n        default=3,\n        type=int,\n        choices=[2, 3],\n    )\n    parser.add_argument(\n        '--ldap-network-timeout',\n        '--ldapnetworktimeout',\n        metavar='SECONDS',\n        help=\"timeout in seconds value used for LDAP connection\",\n        default=100,\n        type=int,\n    )\n    parser.add_argument(\n        '--ldap-timeout',\n        '--ldaptimeout',\n        metavar='SECONDS',\n        help=\"timeout in seconds value used for LDAP request\",\n        default=300,\n        type=int,\n    )\n    parser.add_argument(\n        '--ldap-encoding',\n        '--ldapencoding',\n        metavar='ENCODING',\n        help=\"encoding used by your LDAP server.\",\n        default=\"utf-8\",\n    )\n    parser.add_argument(\n        '--log-access-file', '--logaccessfile', metavar='FILE', help='location of Rdiffweb log access file.'\n    )\n    parser.add_argument(\n        '--log-file',\n        '--logfile',\n        metavar='FILE',\n        help='location of Rdiffweb log file. Print log to the console if not define in config file.',\n    )\n    parser.add_argument(\n        '--log-level',\n        '--loglevel',\n        help='Define the log level.',\n        choices=['ERROR', 'WARN', 'INFO', 'DEBUG'],\n        default='INFO',\n    )\n    parser.add_argument(\n        '--max-depth',\n        '--maxdepth',\n        metavar='DEPTH',\n        help=\"define the maximum folder depthness to search into the user's root directory to find repositories. This is commonly used if you repositories are organised with multiple sub-folder.\",\n        type=int,\n        default=3,\n    )\n    parser.add('--quota-set-cmd', '--quotasetcmd', metavar='COMMAND', help=\"command line to set the user's quota.\")\n    parser.add('--quota-get-cmd', '--quotagetcmd', metavar='COMMAND', help=\"command line to get the user's quota.\")\n    parser.add(\n        '--quota-used-cmd', '--quotausedcmd', metavar='COMMAND', help=\"Command line to get user's quota disk usage.\"\n    )\n    parser.add(\n        '--remove-older-time',\n        '--removeoldertime',\n        metavar='TIME',\n        help=\"Time when to execute the remove older scheduled job. e.g.: 22:30\",\n        default='23:00',\n    )\n    parser.add('--server-host', '--serverhost', metavar='IP', default='127.0.0.1', help='IP address to listen to')\n    parser.add(\n        '--server-port',\n        '--serverport',\n        metavar='PORT',\n        help='port to listen to for HTTP request',\n        default='8080',\n        type=int,\n    )\n    parser.add(\n        '--session-dir',\n        '--sessiondir',\n        metavar='FOLDER',\n        help='location where to store user session information. When undefined, the user sessions are kept in memory.',\n    )\n    parser.add(\n        '--rate-limit',\n        metavar='LIMIT',\n        type=int,\n        default=10,\n        help='maximum number of requests per minute that can be made by an IP address for an unauthenticated connection. When this limit is reached, an HTTP 429 message is returned to the user. This security measure is used to limit brute force attacks on the login page and the RESTful API.',\n    )\n    parser.add(\n        '--ssl-certificate',\n        '--sslcertificate',\n        metavar='CERT',\n        help='location of the SSL Certification to enable HTTPS (not recommended)',\n    )\n    parser.add(\n        '--ssl-private-key',\n        '--sslprivatekey',\n        metavar='KEY',\n        help='location of the SSL Private Key to enable HTTPS (not recommended)',\n    )\n    parser.add(\n        '--tempdir',\n        metavar='FOLDER',\n        help='alternate temporary folder to be used when restoring files. Might be useful if the default location has limited disk space. Default to TEMPDIR environment or `/tmp`.',\n    )\n    parser.add(\n        '--disable-ssh-keys',\n        action='store_true',\n        help='used to hide SSH Key management to avoid users to add or remove SSH Key using the web application',\n        default=False,\n    )\n    parser.add_argument('--version', action='version', version='%(prog)s ' + VERSION)\n    flags = ['--welcome-msg'] + ['--welcome-msg-' + i for i in ['ca', 'en', 'es', 'fr', 'ru']] + ['--welcomemsg']\n    parser.add_argument(\n        *flags,\n        metavar='HTML',\n        help='replace the welcome message displayed in the login page for default locale or for a specific locale',\n        action=LocaleAction\n    )\n    return parser\ndef parse_args(args=None, config_file_contents=None):\n    args = sys.argv[1:] if args is None else args\n    return get_parser().parse_args(args, config_file_contents=config_file_contents)\nclass LocaleAction(argparse.Action):\n    def __init__(self, option_strings, dest, nargs=None, **kwargs):\n        super(LocaleAction, self).__init__(option_strings, dest, **kwargs)\n    def set_password(self, password, old_password=None):\n        \"\"\"\n        Change the user's password. Raise a ValueError if the username or\n        the password are invalid.\n        \"\"\"\n        assert isinstance(password, str)\n        assert old_password is None or isinstance(old_password, str)\n        if not password:\n            raise ValueError(\"password can't be empty\")\n        if self.username == self._store._admin_user and self._store._admin_password:\n            raise ValueError(_(\"can't update admin-password defined in configuration file\"))\n        if old_password and not check_password(old_password, self.hash_password):\n            raise ValueError(_(\"Wrong password\"))\n        logger.info(\"updating user password [%s]\", self.username)\n        self.hash_password = hash_password(password)\n        self._store.bus.publish('user_password_changed', self)\n    def _set_user_root(self, value):\n        \"\"\"\n        Used to take care of updating the user_root.\n        When user_root get update, we also want to update the repository list\n        to reflect the filesystem.\n        \"\"\"\n        self._set_attr('user_root', value)\n        self.refresh_repos()\n    def validate_role(self, field):\n        currentuser = cherrypy.request.currentuser\n        if self.username.data == currentuser.username and self.role.data != currentuser.role:\n            raise ValueError(_('Cannot edit your own role.'))\n    def populate_obj(self, userobj):\n        if self.password.data:\n            userobj.set_password(self.password.data, old_password=None)\n        userobj.role = self.role.data\n        userobj.email = self.email.data or ''\n        userobj.user_root = self.user_root.data\n        if not userobj.valid_user_root():\n            flash(_(\"User's root directory %s is not accessible!\") % userobj.user_root, level='error')\n            logger.warning(\"user's root directory %s is not accessible\" % userobj.user_root)\n        new_quota = self.disk_quota.data or 0\n        old_quota = humanfriendly.parse_size(humanfriendly.format_size(self.disk_quota.object_data or 0, binary=True))\n        if old_quota != new_quota:\n            userobj.disk_quota = new_quota\n            if userobj.disk_quota != new_quota:\n                flash(_(\"Setting user's quota is not supported\"), level='warning')\nclass PrefsGeneralPanelProvider(Controller):\n    panel_id = 'general'\n    panel_name = _('Profile')\n    def _handle_set_password(self, action, form):\n        \"\"\"\n        Called when changing user password.\n        \"\"\"\n        assert self.app.currentuser\n        assert action == 'set_password'\n        assert form\n        if not form.validate():\n            flash(form.error_message, level='error')\n            return\n        try:\n            self.app.currentuser.set_password(form.new.data, old_password=form.current.data)\n            flash(_(\"Password updated successfully.\"), level='success')\n        except ValueError as e:\n            flash(str(e), level='warning')",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2022-3179",
        "description": "[{'lang': 'en', 'value': 'Weak Password Requirements in GitHub repository ikus060/rdiffweb prior to 2.4.2.'}]",
        "cwe_number": 521
      },
      "cwe_types": [],
      "severity": "high"
    },
    {
      "id": "ContextAssembler-299",
      "code": "    def __init__(self, cfg, reactor=twisted.internet.reactor):\n        self.reactor = reactor\n        self.config_file = get_config_file_path()\n        self.cfg = cfg\n        logger.info(\"Starting Sydent server\")\n        self.pidfile = self.cfg.get('general', \"pidfile.path\");\n        self.db = SqliteDatabase(self).db\n        self.server_name = self.cfg.get('general', 'server.name')\n        if self.server_name == '':\n            self.server_name = os.uname()[1]\n            logger.warn((\"You had not specified a server name. I have guessed that this server is called '%s' \"\n                        + \"and saved this in the config file. If this is incorrect, you should edit server.name in \"\n                        + \"the config file.\") % (self.server_name,))\n            self.cfg.set('general', 'server.name', self.server_name)\n            self.save_config()\n        if self.cfg.has_option(\"general\", \"sentry_dsn\"):\n            import sentry_sdk\n            sentry_sdk.init(\n                dsn=self.cfg.get(\"general\", \"sentry_dsn\"),\n            )\n            with sentry_sdk.configure_scope() as scope:\n                scope.set_tag(\"sydent_server_name\", self.server_name)\n        if self.cfg.has_option(\"general\", \"prometheus_port\"):\n            import prometheus_client\n            prometheus_client.start_http_server(\n                port=self.cfg.getint(\"general\", \"prometheus_port\"),\n                addr=self.cfg.get(\"general\", \"prometheus_addr\"),\n            )\n        if self.cfg.has_option(\"general\", \"templates.path\"):\n            root_template_path = self.cfg.get(\"general\", \"templates.path\")\n            if os.path.exists(root_template_path):\n                self.valid_brands = {\n                    p for p in os.listdir(root_template_path) if os.path.isdir(os.path.join(root_template_path, p))\n                }\n            else:\n                self.valid_brands = set()\n        self.enable_v1_associations = parse_cfg_bool(\n            self.cfg.get(\"general\", \"enable_v1_associations\")\n        )\n        self.delete_tokens_on_bind = parse_cfg_bool(\n            self.cfg.get(\"general\", \"delete_tokens_on_bind\")\n        )\n        self.default_web_client_location = self.cfg.get(\n            \"email\", \"email.default_web_client_location\"\n        )\n        self.username_obfuscate_characters = int(self.cfg.get(\n            \"email\", \"email.third_party_invite_username_obfuscate_characters\"\n        ))\n        self.domain_obfuscate_characters = int(self.cfg.get(\n            \"email\", \"email.third_party_invite_domain_obfuscate_characters\"\n        ))\n        hashing_metadata_store = HashingMetadataStore(self)\n        lookup_pepper = hashing_metadata_store.get_lookup_pepper()\n        if not lookup_pepper:\n            lookup_pepper = generateAlphanumericTokenOfLength(5)\n            hashing_metadata_store.store_lookup_pepper(sha256_and_url_safe_base64,\n                                                       lookup_pepper)\n        self.validators = Validators()\n        self.validators.email = EmailValidator(self)\n        self.validators.msisdn = MsisdnValidator(self)\n        self.keyring = Keyring()\n        self.keyring.ed25519 = SydentEd25519(self).signing_key\n        self.keyring.ed25519.alg = 'ed25519'\n        self.sig_verifier = Verifier(self)\n        self.servlets = Servlets()\n        self.servlets.v1 = V1Servlet(self)\n        self.servlets.v2 = V2Servlet(self)\n        self.servlets.emailRequestCode = EmailRequestCodeServlet(self)\n        self.servlets.emailRequestCodeV2 = EmailRequestCodeServlet(self, require_auth=True)\n        self.servlets.emailValidate = EmailValidateCodeServlet(self)\n        self.servlets.emailValidateV2 = EmailValidateCodeServlet(self, require_auth=True)\n        self.servlets.msisdnRequestCode = MsisdnRequestCodeServlet(self)\n        self.servlets.msisdnRequestCodeV2 = MsisdnRequestCodeServlet(self, require_auth=True)\n        self.servlets.msisdnValidate = MsisdnValidateCodeServlet(self)\n        self.servlets.msisdnValidateV2 = MsisdnValidateCodeServlet(self, require_auth=True)\n        self.servlets.lookup = LookupServlet(self)\n        self.servlets.bulk_lookup = BulkLookupServlet(self)\n        self.servlets.hash_details = HashDetailsServlet(self, lookup_pepper)\n        self.servlets.lookup_v2 = LookupV2Servlet(self, lookup_pepper)\n        self.servlets.pubkey_ed25519 = Ed25519Servlet(self)\n        self.servlets.pubkeyIsValid = PubkeyIsValidServlet(self)\n        self.servlets.ephemeralPubkeyIsValid = EphemeralPubkeyIsValidServlet(self)\n        self.servlets.threepidBind = ThreePidBindServlet(self)\n        self.servlets.threepidBindV2 = ThreePidBindServlet(self, require_auth=True)\n        self.servlets.threepidUnbind = ThreePidUnbindServlet(self)\n        self.servlets.replicationPush = ReplicationPushServlet(self)\n        self.servlets.getValidated3pid = GetValidated3pidServlet(self)\n        self.servlets.getValidated3pidV2 = GetValidated3pidServlet(self, require_auth=True)\n        self.servlets.storeInviteServlet = StoreInviteServlet(self)\n        self.servlets.storeInviteServletV2 = StoreInviteServlet(self, require_auth=True)\n        self.servlets.blindlySignStuffServlet = BlindlySignStuffServlet(self)\n        self.servlets.blindlySignStuffServletV2 = BlindlySignStuffServlet(self, require_auth=True)\n        self.servlets.termsServlet = TermsServlet(self)\n        self.servlets.accountServlet = AccountServlet(self)\n        self.servlets.registerServlet = RegisterServlet(self)\n        self.servlets.logoutServlet = LogoutServlet(self)\n        self.threepidBinder = ThreepidBinder(self)\n        self.sslComponents = SslComponents(self)\n        self.clientApiHttpServer = ClientApiHttpServer(self)\n        self.replicationHttpsServer = ReplicationHttpsServer(self)\n        self.replicationHttpsClient = ReplicationHttpsClient(self)\n        self.pusher = Pusher(self)\n        self.cleanupValSession = ThreePidValSessionStore(self)\n        cb = task.LoopingCall(self.cleanupValSession.deleteOldSessions)\n        cb.clock = self.reactor\n        cb.start(10 * 60.0)\n        gc.disable()\n        cb = task.LoopingCall(run_gc)\n        cb.clock = self.reactor\n        cb.start(1.0)\n    def save_config(self):\n        fp = open(self.config_file, 'w')\n        self.cfg.write(fp)\n        fp.close()\n    def run(self):\n        self.clientApiHttpServer.setup()\n        self.replicationHttpsServer.setup()\n        self.pusher.setup()\n        internalport = self.cfg.get('http', 'internalapi.http.port')\n        if internalport:\n            try:\n                interface = self.cfg.get('http', 'internalapi.http.bind_address')\n            except configparser.NoOptionError:\n                interface = '::1'\n            self.internalApiHttpServer = InternalApiHttpServer(self)\n            self.internalApiHttpServer.setup(interface, int(internalport))\n        if self.pidfile:\n            with open(self.pidfile, 'w') as pidfile:\n                pidfile.write(str(os.getpid()) + \"\\n\")\n        self.reactor.run()\n    def ip_from_request(self, request):\n        if (self.cfg.get('http', 'obey_x_forwarded_for') and\n                request.requestHeaders.hasHeader(\"X-Forwarded-For\")):\n            return request.requestHeaders.getRawHeaders(\"X-Forwarded-For\")[0]\n        return request.getClientIP()\n    def __init__(self, sydent):\n        self.sydent = sydent\n        self.agent = Agent(\n            self.sydent.reactor,\n            connectTimeout=15,\n        )\nclass FederationHttpClient(HTTPClient):",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-29431",
        "description": "[{'lang': 'en', 'value': 'Sydent is a reference Matrix identity server. Sydent can be induced to send HTTP GET requests to internal systems, due to lack of parameter validation or IP address blacklisting. It is not possible to exfiltrate data or control request headers, but it might be possible to use the attack to perform an internal port enumeration. This issue has been addressed in in 9e57334, 8936925, 3d531ed, 0f00412. A potential workaround would be to use a firewall to ensure that Sydent cannot reach internal HTTP resources.'}]",
        "cwe_number": 20
      },
      "cwe_types": [],
      "severity": "medium"
    },
    {
      "id": "ContextAssembler-300",
      "code": "def google_remote_app():\n    if \"google\" not in oauth.remote_apps:\n        oauth.remote_app(\n            \"google\",\n            base_url=\"https://www.google.com/accounts/\",\n            authorize_url=\"https://accounts.google.com/o/oauth2/auth?prompt=select_account+consent\",\n            request_token_url=None,\n            request_token_params={\n                \"scope\": \"https://www.googleapis.com/auth/userinfo.email https://www.googleapis.com/auth/userinfo.profile\"\n            },\n            access_token_url=\"https://accounts.google.com/o/oauth2/token\",\n            access_token_method=\"POST\",\n            consumer_key=settings.GOOGLE_CLIENT_ID,\n            consumer_secret=settings.GOOGLE_CLIENT_SECRET,\n        )\n    return oauth.google\ndef get_user_profile(access_token):\n    headers = {\"Authorization\": \"OAuth {}\".format(access_token)}\n    response = requests.get(\n        \"https://www.googleapis.com/oauth2/v1/userinfo\", headers=headers\n    )\n    if response.status_code == 401:\n        logger.warning(\"Failed getting user profile (response code 401).\")\n        return None\n    return response.json()\ndef verify_profile(org, profile):\n    if org.is_public:\n        return True\n    email = profile[\"email\"]\n    domain = email.split(\"@\")[-1]\n    if domain in org.google_apps_domains:\n        return True\n    if org.has_user(email) == 1:\n        return True\n    return False\ndef org_login(org_slug):\n    session[\"org_slug\"] = current_org.slug\n    return redirect(url_for(\".authorize\", next=request.args.get(\"next\", None)))\ndef login():\n    callback = url_for(\".callback\", _external=True)\n    next_path = request.args.get(\n        \"next\", url_for(\"redash.index\", org_slug=session.get(\"org_slug\"))\n    )\n    logger.debug(\"Callback url: %s\", callback)\n    logger.debug(\"Next is: %s\", next_path)\n    return google_remote_app().authorize(callback=callback, state=next_path)\ndef authorized():\n    resp = google_remote_app().authorized_response()\n    access_token = resp[\"access_token\"]\n    if access_token is None:\n        logger.warning(\"Access token missing in call back request.\")\n        flash(\"Validation error. Please retry.\")\n        return redirect(url_for(\"redash.login\"))\n    profile = get_user_profile(access_token)\n    if profile is None:\n        flash(\"Validation error. Please retry.\")\n        return redirect(url_for(\"redash.login\"))\n    if \"org_slug\" in session:\n        org = models.Organization.get_by_slug(session.pop(\"org_slug\"))\n    else:\n        org = current_org\n    if not verify_profile(org, profile):\n        logger.warning(\n            \"User tried to login with unauthorized domain name: %s (org: %s)\",\n            profile[\"email\"],\n            org,\n        )\n        flash(\"Your Google Apps account ({}) isn't allowed.\".format(profile[\"email\"]))\n        return redirect(url_for(\"redash.login\", org_slug=org.slug))\n    picture_url = \"%s?sz=40\" % profile[\"picture\"]\n    user = create_and_login_user(org, profile[\"name\"], profile[\"email\"], picture_url)\n    if user is None:\n        return logout_and_redirect_to_index()\n    unsafe_next_path = request.args.get(\"state\") or url_for(\n        \"redash.index\", org_slug=org.slug\n    )\n    next_path = get_next_path(unsafe_next_path)\n    return redirect(next_path)\ndef init_app(app):\n    from redash.authentication import (\n        google_oauth,\n        saml_auth,\n        remote_user_auth,\n        ldap_auth,\n    )\n    login_manager.init_app(app)\n    login_manager.anonymous_user = models.AnonymousUser\n    login_manager.REMEMBER_COOKIE_DURATION = settings.REMEMBER_COOKIE_DURATION\n    @app.before_request\n    def extend_session():\n        session.permanent = True\n        app.permanent_session_lifetime = timedelta(seconds=settings.SESSION_EXPIRY_TIME)\n    from redash.security import csrf\n    for auth in [google_oauth, saml_auth, remote_user_auth, ldap_auth]:\n        blueprint = auth.blueprint\n        csrf.exempt(blueprint)\n        app.register_blueprint(blueprint)\n    user_logged_in.connect(log_user_logged_in)\n    login_manager.request_loader(request_loader)",
      "label": 0,
      "metadata": {
        "CVEFixes-Number": "CVE-2021-43777",
        "description": "[{'lang': 'en', 'value': 'Redash is a package for data visualization and sharing. In Redash version 10.0 and prior, the implementation of Google Login (via OAuth) incorrectly uses the `state` parameter to pass the next URL to redirect the user to after login. The `state` parameter should be used for a Cross-Site Request Forgery (CSRF) token, not a static and easily predicted value. This vulnerability does not affect users who do not use Google Login for their instance of Redash. A patch in the `master` and `release/10.x.x` branches addresses this by replacing `Flask-Oauthlib` with `Authlib` which automatically provides and validates a CSRF token for the state variable. The new implementation stores the next URL on the user session object. As a workaround, one may disable Google Login to mitigate the vulnerability.'}]",
        "cwe_number": 601
      },
      "cwe_types": [],
      "severity": "medium"
    }
  ]
}