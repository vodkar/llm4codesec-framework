# VulnerabilityDetection Benchmark

This document describes how to use the VulnerabilityDetection benchmark in the llm4codesec framework.

## Overview

The VulnerabilityDetection benchmark is based on the dataset from the VulnerabilityDetection repository, which contains code samples for vulnerability detection tasks. The benchmark supports both binary classification (vulnerable/non-vulnerable) and token-level vulnerability detection.

## Dataset Structure

The VulnerabilityDetection dataset includes:
- **Binary Classification**: Determine if code contains vulnerabilities
- **Token-level Detection**: Identify specific vulnerable tokens/lines in code
- **Multiple Vulnerability Types**: Buffer overflow, SQL injection, XSS, command injection, etc.
- **CWE Mapping**: Common Weakness Enumeration classifications
- **Severity Levels**: Critical, High, Medium, Low

## Configuration

### Dataset Configuration

The benchmark configuration is defined in `src/configs/vulnerabilitydetection_experiments.json`:

```json
{
  "datasets": {
    "vulnerabilitydetection_binary": {
      "name": "VulnerabilityDetection Binary",
      "description": "Binary vulnerability detection dataset",
      "task_type": "binary_classification",
      "data_path": "datasets_processed/vulnerabilitydetection/binary/",
      "prompt_strategies": ["basic_security", "detailed_analysis", "cwe_focused"]
    },
    "vulnerabilitydetection_token": {
      "name": "VulnerabilityDetection Token-level",
      "description": "Token-level vulnerability detection dataset", 
      "task_type": "token_classification",
      "data_path": "datasets_processed/vulnerabilitydetection/token/",
      "prompt_strategies": ["basic_security", "detailed_analysis", "line_by_line"]
    }
  }
}
```

### Prompt Strategies

Available prompt strategies:
- **basic_security**: Simple vulnerability detection prompts
- **detailed_analysis**: In-depth security analysis prompts
- **cwe_focused**: CWE-specific vulnerability detection prompts
- **line_by_line**: Token-level line-by-line analysis prompts

## Usage

### Standalone Runner

Run the VulnerabilityDetection benchmark directly:

```bash
python src/entrypoints/run_vulnerabilitydetection_benchmark.py \
    --dataset vulnerabilitydetection_binary \
    --model qwen2.5-7b \
    --prompt_strategy basic_security \
    --output_dir results/vulnerabilitydetection_experiments
```

#### Command Line Options

- `--dataset`: Dataset to use (`vulnerabilitydetection_binary` or `vulnerabilitydetection_token`)
- `--model`: Model to use for evaluation
- `--prompt_strategy`: Prompting strategy to apply
- `--output_dir`: Directory to save results
- `--experiment_plan`: Run a predefined experiment plan
- `--config_file`: Custom configuration file path
- `--limit`: Limit number of samples to process
- `--resume`: Resume from checkpoint if available

#### Example Commands

```bash
# Binary classification with basic security prompts
python src/entrypoints/run_vulnerabilitydetection_benchmark.py \
    --dataset vulnerabilitydetection_binary \
    --model qwen2.5-7b \
    --prompt_strategy basic_security

# Token-level detection with detailed analysis
python src/entrypoints/run_vulnerabilitydetection_benchmark.py \
    --dataset vulnerabilitydetection_token \
    --model claude-3-haiku \
    --prompt_strategy detailed_analysis \
    --limit 100

# Run experiment plan
python src/entrypoints/run_vulnerabilitydetection_benchmark.py \
    --experiment_plan quick_test
```

### Unified Runner

Use the unified benchmark runner:

```bash
python src/entrypoints/run_unified_benchmark.py \
    --dataset_type vulnerabilitydetection \
    --dataset vulnerabilitydetection_binary \
    --model qwen2.5-7b \
    --prompt_strategy basic_security
```

#### Unified Runner Options

```bash
# Run single experiment
python src/entrypoints/run_unified_benchmark.py \
    --dataset_type vulnerabilitydetection \
    --dataset vulnerabilitydetection_binary \
    --model qwen2.5-7b \
    --prompt_strategy basic_security \
    --output_dir results/vuln_detection_test

# Run experiment plan
python src/entrypoints/run_unified_benchmark.py \
    --dataset_type vulnerabilitydetection \
    --experiment_plan quick_test
```

## Data Processing

### Raw Data Structure

The original VulnerabilityDetection data is located in:
```
benchmarks/VulnerabilityDetection/Code/data/
├── train_data.json      # Training data (line-delimited JSON)
├── test_data.json       # Test data (line-delimited JSON)
└── validation_data.json # Validation data (line-delimited JSON)
```

### Processing Script

Process raw data into the framework format:

```bash
python src/scripts/process_vulnerabilitydetection_data.py \
    --input_dir benchmarks/VulnerabilityDetection/Code/data \
    --output_dir datasets_processed/vulnerabilitydetection \
    --task_type binary
```

#### Processing Options

- `--input_dir`: Directory containing raw VulnerabilityDetection data
- `--output_dir`: Output directory for processed data
- `--task_type`: Task type (`binary` or `token`)
- `--limit`: Limit number of samples to process
- `--vulnerability_types`: Filter by specific vulnerability types

## Experiment Plans

### Available Plans

- **quick_test**: Fast evaluation with limited samples
- **comprehensive**: Full evaluation across all datasets and models
- **model_comparison**: Compare different models on same dataset
- **prompt_analysis**: Analyze different prompting strategies

### Custom Experiment Plans

Define custom experiment plans in the configuration file:

```json
{
  "experiment_plans": {
    "custom_plan": {
      "description": "Custom VulnerabilityDetection experiment",
      "experiments": [
        {
          "dataset": "vulnerabilitydetection_binary",
          "model": "qwen2.5-7b",
          "prompt_strategy": "basic_security"
        }
      ]
    }
  }
}
```

## Results and Analysis

### Output Structure

Results are saved in the specified output directory:
```
results/vulnerabilitydetection_experiment_TIMESTAMP/
├── config.json                    # Experiment configuration
├── results.json                  # Raw results
├── metrics.json                  # Computed metrics
├── model_responses/              # Individual model responses
└── analysis/                     # Analysis reports
```

### Metrics

The benchmark computes various metrics:
- **Binary Classification**: Accuracy, Precision, Recall, F1-score, AUC-ROC
- **Token-level**: Token-level precision/recall, exact match accuracy
- **Vulnerability-specific**: Per-CWE and per-severity metrics
- **Model Performance**: Response time, token usage

## Integration Notes

### Dataset Loader

The VulnerabilityDetection dataset loader (`src/datasets/loaders/vulnerabilitydetection_dataset_loader.py`) supports:
- Multiple task types (binary, token-level)
- CWE and severity mapping
- Flexible data format handling
- Sample limiting and filtering

### Prompt Templates

Prompt templates are dynamically generated based on:
- Task type (binary vs token-level)
- Vulnerability context
- CWE information
- Code complexity

## Troubleshooting

### Common Issues

1. **Data Processing Errors**
   - Ensure raw data files exist in expected location
   - Check file format (should be line-delimited JSON)
   - Verify sufficient disk space for processing

2. **Model Loading Issues**
   - Confirm model is properly configured
   - Check model API credentials if using external models
   - Verify model supports the task type

3. **Memory Issues**
   - Use `--limit` to process fewer samples
   - Process in batches for large datasets
   - Monitor system memory usage

### Performance Tips

- Use `--limit` for quick testing
- Enable resume functionality for long experiments
- Process data once and reuse processed datasets
- Use appropriate batch sizes for your hardware

## See Also

- [CASTLE README](CASTLE_README.md) - Similar binary classification benchmark
- [CVEFixes README](CVEFIXES_README.md) - Another vulnerability benchmark
- [Usage Examples](USAGE_EXAMPLES.md) - More usage examples
- [Docker README](DOCKER_README.md) - Running in Docker environment
