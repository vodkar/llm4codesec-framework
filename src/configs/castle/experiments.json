{
    "experiment_metadata": {
        "name": "CASTLE Benchmark LLM Evaluation",
        "description": "Comprehensive evaluation of LLMs on CASTLE benchmark with multiple prompt strategies",
        "version": "1.0",
        "dataset": "CASTLE-Benchmark v1.2",
        "created_date": "2025-06-09"
    },
    "experiment_plans": {
        "test_problematic_models": {
            "description": "Test Problematic Models",
            "datasets": [
                "binary_all"
            ],
            "models": [
                "llama4-scout-17b-16e",
                "gemma3-27b",
                "deepseek-coder-v2-lite-16b"
            ],
            "prompts": [
                "basic_security"
            ],
            "sample_limit": 10
        },
        "quick_test": {
            "description": "Quick test with limited samples for development",
            "datasets": [
                "binary_all"
            ],
            "models": [
                "qwen3-8b-gguf"
            ],
            "prompts": [
                "basic_security"
            ],
            "sample_limit": 3
        },
        "llama_cpp_evaluation": {
            "description": "llama.cpp backend evaluation using Qwen3-8B GGUF quantizations",
            "datasets": [
                "binary_all"
            ],
            "models": [
                "qwen3-8b-gguf",
                "qwen3-8b-gguf-q8"
            ],
            "prompts": [
                "basic_security",
                "detailed_analysis"
            ],
            "sample_limit": 3
        },
        "multiclass_quick_test": {
            "description": "Quick test for multiclass classification with limited samples",
            "datasets": [
                "multiclass_all"
            ],
            "models": [
                "qwen3-4b",
                "llama3.2-3B",
                "gemma3-4b"
            ],
            "prompts": [
                "multiclass_basic",
                "multiclass_detailed"
            ],
            "sample_limit": 10
        },
        "prompt_comparison": {
            "description": "Compare different prompt strategies on binary classification",
            "datasets": [
                "binary_all"
            ],
            "models": [
                "qwen3-4b"
            ],
            "prompts": [
                "detailed_analysis"
            ],
            "sample_limit": 100
        },
        "multiclass_prompt_comparison": {
            "description": "Compare different prompt strategies on multiclass classification",
            "datasets": [
                "multiclass_all"
            ],
            "models": [
                "qwen3-4b"
            ],
            "prompts": [
                "multiclass_detailed",
                "multiclass_comprehensive"
            ],
            "sample_limit": 100
        },
        "small_models_cwe_specific_analysis": {
            "description": "CWE-specific vulnerability detection",
            "datasets": [
                "cwe_125",
                "cwe_190",
                "cwe_476",
                "cwe_787"
            ],
            "models": [
                "qwen3-4b",
                "qwen3-4b-thinking",
                "qwen3-8b",
                "qwen3-8b-thinking",
                "llama3.2-3B",
                "gemma3-4b",
                "deepseek-r1-distill-qwen2.5-7b"
            ],
            "prompts": [
                "cwe_focused"
            ],
            "sample_limit": 100
        },
        "large_models_cwe_specific_analysis": {
            "description": "CWE-specific vulnerability detection",
            "datasets": [
                "cwe_125",
                "cwe_190",
                "cwe_476",
                "cwe_787"
            ],
            "models": [
                "qwen3-30b",
                "qwen3-30b-thinking",
                "deepseek-coder-v2-lite-16b",
                "deepseek-r1-distill-qwen2.5-32b",
                "wizard-coder-34b",
                "llama4-scout-17b-16e",
                "gemma3-27b"
            ],
            "prompts": [
                "cwe_focused"
            ],
            "sample_limit": 100
        },
        "small_models_binary": {
            "description": "Small models evaluation on binary classification tasks",
            "datasets": [
                "binary_all"
            ],
            "models": [
                "qwen3-4b",
                "qwen3-4b-thinking",
                "qwen3-8b",
                "qwen3-8b-thinking",
                "llama3.2-3B",
                "gemma3-4b",
                "deepseek-r1-distill-qwen2.5-7b"
            ],
            "prompts": [
                "detailed_analysis"
            ],
            "sample_limit": 100
        },
        "small_models_multiclass": {
            "description": "Small models evaluation on multiclass classification",
            "datasets": [
                "multiclass_all"
            ],
            "models": [
                "qwen3-4b",
                "qwen3-4b-thinking",
                "qwen3-8b",
                "qwen3-8b-thinking",
                "llama3.2-3B",
                "gemma3-4b",
                "deepseek-r1-distill-qwen2.5-7b"
            ],
            "prompts": [
                "multiclass_comprehensive"
            ],
            "sample_limit": 100
        },
        "large_models_binary": {
            "description": "Large models evaluation on binary classification tasks",
            "datasets": [
                "binary_all"
            ],
            "models": [
                "qwen3-30b",
                "qwen3-30b-thinking",
                "deepseek-coder-v2-lite-16b",
                "deepseek-r1-distill-qwen2.5-32b",
                "wizard-coder-34b",
                "llama4-scout-17b-16e",
                "gemma3-27b"
            ],
            "prompts": [
                "detailed_analysis"
            ],
            "sample_limit": 100
        },
        "large_models_multiclass": {
            "description": "Large models evaluation on multiclass classification",
            "datasets": [
                "multiclass_all"
            ],
            "models": [
                "qwen3-30b",
                "qwen3-30b-thinking",
                "deepseek-coder-v2-lite-16b",
                "deepseek-r1-distill-qwen2.5-32b",
                "wizard-coder-34b",
                "llama4-scout-17b-16e",
                "gemma3-27b"
            ],
            "prompts": [
                "multiclass_comprehensive"
            ],
            "sample_limit": 100
        }
    },
    "output_settings": {
        "base_output_dir": "results/castle_experiments",
        "create_subdirs": true,
        "save_predictions": true,
        "save_metrics": true,
        "save_detailed_report": true,
        "include_timestamp": true
    },
    "evaluation_settings": {
        "metrics": [
            "accuracy",
            "precision",
            "recall",
            "f1",
            "auc"
        ],
        "calculate_per_cwe": true,
        "create_confusion_matrix": true,
        "analyze_errors": true
    }
}
