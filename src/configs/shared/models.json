{
    "models": {
        "qwen3-4b": {
            "model_identifier": "Qwen/Qwen3-4B",
            "model_type": "qwen-3",
            "max_tokens": 2048,
            "temperature": 0.1,
            "batch_size": 50,
            "use_quantization": true
        },
        "qwen3-4b-thinking": {
            "model_identifier": "Qwen/Qwen3-4B",
            "model_type": "qwen-3",
            "max_tokens": 2048,
            "temperature": 0.1,
            "batch_size": 50,
            "use_quantization": true,
            "is_thinking_enabled": true
        },
        "qwen3-8b": {
            "model_identifier": "Qwen/Qwen3-8B",
            "model_type": "qwen-3",
            "max_tokens": 2048,
            "temperature": 0.1,
            "batch_size": 30,
            "use_quantization": true
        },
        "qwen3-8b-thinking": {
            "model_identifier": "Qwen/Qwen3-8B",
            "model_type": "qwen-3",
            "max_tokens": 2048,
            "temperature": 0.1,
            "batch_size": 30,
            "use_quantization": true,
            "is_thinking_enabled": true
        },
        "qwen3-30b": {
            "model_identifier": "Qwen/Qwen3-30B-A3B",
            "model_type": "qwen-3",
            "batch_size": 2,
            "max_tokens": 2048,
            "temperature": 0.1,
            "use_quantization": true,
            "is_thinking_enabled": false
        },
        "qwen3-30b-thinking": {
            "model_identifier": "Qwen/Qwen3-30B-A3B",
            "model_type": "qwen-3",
            "batch_size": 2,
            "max_tokens": 2048,
            "temperature": 0.1,
            "use_quantization": true,
            "is_thinking_enabled": true
        },
        "deepseek-coder-v2-lite-16b": {
            "model_identifier": "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct",
            "model_type": "deepseek-coder-v2",
            "max_tokens": 2048,
            "temperature": 0.1,
            "batch_size": 2,
            "use_quantization": true
        },
        "deepseek-r1-distill-qwen2.5-7b": {
            "model_identifier": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
            "model_type": "deepseek-r1",
            "max_tokens": 2048,
            "temperature": 0.1,
            "batch_size": 50,
            "use_quantization": true
        },
        "deepseek-r1-distill-qwen2.5-32b": {
            "model_identifier": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
            "model_type": "deepseek-r1",
            "max_tokens": 2048,
            "temperature": 0.1,
            "batch_size": 2,
            "use_quantization": true
        },
        "wizard-coder-34b": {
            "model_identifier": "WizardLMTeam/WizardCoder-33B-V1.1",
            "model_type": "wizardcoder",
            "max_tokens": 2048,
            "temperature": 0.1,
            "batch_size": 2,
            "use_quantization": true
        },
        "llama3.2-3B": {
            "model_identifier": "meta-llama/Llama-3.2-3B-Instruct",
            "model_type": "llama-3",
            "max_tokens": 1024,
            "temperature": 0.1,
            "batch_size": 30,
            "use_quantization": true
        },
        "llama4-scout-17b-16e": {
            "model_identifier": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
            "model_type": "llama-4",
            "max_tokens": 1024,
            "temperature": 0.1,
            "batch_size": 2,
            "use_quantization": true
        },
        "gemma3-4b": {
            "model_identifier": "google/gemma-3-4b-it-qat-q4_0-unquantized",
            "model_type": "gemma-3",
            "max_tokens": 1024,
            "temperature": 0.1,
            "batch_size": 50,
            "use_quantization": true
        },
        "gemma3-27b": {
            "model_identifier": "google/gemma-3-27b-it",
            "model_type": "gemma-3",
            "max_tokens": 1024,
            "temperature": 0.1,
            "batch_size": 2,
            "use_quantization": true
        },
        "qwen3-8b-gguf": {
            "model_identifier": "hf://Qwen/Qwen3-8B-GGUF/Qwen3-8B-Q4_K_M.gguf",
            "model_type": "qwen-3",
            "backend": "llama_cpp",
            "max_tokens": 2048,
            "temperature": 0.6,
            "batch_size": 1,
            "use_quantization": false
        },
        "qwen3-8b-gguf-q8": {
            "model_identifier": "hf://Qwen/Qwen3-8B-GGUF/Qwen3-8B-Q8_0.gguf",
            "model_type": "qwen-3",
            "backend": "llama_cpp",
            "max_tokens": 2048,
            "temperature": 0.6,
            "batch_size": 1,
            "use_quantization": false
        }
    }
}
