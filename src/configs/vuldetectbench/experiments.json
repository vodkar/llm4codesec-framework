{
    "experiment_metadata": {
        "name": "VulDetectBench LLM Evaluation",
        "description": "Comprehensive evaluation of LLMs on VulDetectBench with 5 tasks of increasing difficulty",
        "version": "1.0",
        "dataset": "VulDetectBench v1.0",
        "created_date": "2025-06-17"
    },
    "experiment_plans": {
        "quick_test": {
            "description": "Quick test with limited samples for development",
            "datasets": [
                "task1_vulnerability"
            ],
            "models": [
                "qwen3-4b",
                "llama3.2-3B",
                "gemma3-4b"
            ],
            "prompts": [
                "basic_security",
                "task1_specific"
            ],
            "sample_limit": 10
        },
        "task1_small_models": {
            "description": "Evaluate small models on Task 1 - Vulnerability Existence Detection",
            "datasets": [
                "task1_vulnerability"
            ],
            "models": [
                "qwen3-4b",
                "qwen3-4b-thinking",
                "qwen3-8b",
                "qwen3-8b-thinking",
                "llama3.2-3B",
                "gemma3-4b",
                "deepseek-r1-distill-qwen2.5-7b"
            ],
            "prompts": [
                "task1_specific"
            ],
            "sample_limit": 200
        },
        "task1_large_models": {
            "description": "Evaluate large models on Task 1 - Vulnerability Existence Detection",
            "datasets": [
                "task1_vulnerability"
            ],
            "models": [
                "qwen3-30b",
                "qwen3-30b-thinking",
                "deepseek-coder-v2-lite-16b",
                "deepseek-r1-distill-qwen2.5-32b",
                "wizard-coder-34b",
                "llama4-scout-17b-16e",
                "gemma3-27b"
            ],
            "prompts": [
                "task1_specific"
            ],
            "sample_limit": 100
        },
        "task2_small_models": {
            "description": "Evaluate small models on Task 2 - Vulnerability Type Inference",
            "datasets": [
                "task2_multiclass"
            ],
            "models": [
                "qwen3-4b",
                "qwen3-4b-thinking",
                "qwen3-8b",
                "qwen3-8b-thinking",
                "llama3.2-3B",
                "gemma3-4b",
                "deepseek-r1-distill-qwen2.5-7b"
            ],
            "prompts": [
                "task2_specific"
            ],
            "sample_limit": 200
        },
        "task2_large_models": {
            "description": "Evaluate large models on Task 2 - Vulnerability Type Inference",
            "datasets": [
                "task2_multiclass"
            ],
            "models": [
                "qwen3-30b",
                "qwen3-30b-thinking",
                "deepseek-coder-v2-lite-16b",
                "deepseek-r1-distill-qwen2.5-32b",
                "wizard-coder-34b",
                "llama4-scout-17b-16e",
                "gemma3-27b"
            ],
            "prompts": [
                "task2_specific"
            ],
            "sample_limit": 100
        },
        "task3_small_models": {
            "description": "Evaluate small models on Task 3 - Key Objects Identification",
            "datasets": [
                "task3_objects"
            ],
            "models": [
                "qwen3-4b",
                "qwen3-4b-thinking",
                "qwen3-8b",
                "qwen3-8b-thinking",
                "llama3.2-3B",
                "gemma3-4b",
                "deepseek-r1-distill-qwen2.5-7b"
            ],
            "prompts": [
                "task3_specific"
            ],
            "sample_limit": 200
        },
        "task3_large_models": {
            "description": "Evaluate large models on Task 3 - Key Objects Identification",
            "datasets": [
                "task3_objects"
            ],
            "models": [
                "qwen3-30b",
                "qwen3-30b-thinking",
                "deepseek-coder-v2-lite-16b",
                "deepseek-r1-distill-qwen2.5-32b",
                "wizard-coder-34b",
                "llama4-scout-17b-16e",
                "gemma3-27b"
            ],
            "prompts": [
                "detailed_analysis",
                "step_by_step",
                "task3_specific"
            ],
            "sample_limit": 100
        },
        "task4_small_models": {
            "description": "Evaluate small models on Task 4 - Root Cause Location",
            "datasets": [
                "task4_root_cause"
            ],
            "models": [
                "qwen3-4b",
                "qwen3-4b-thinking",
                "qwen3-8b",
                "qwen3-8b-thinking",
                "llama3.2-3B",
                "gemma3-4b",
                "deepseek-r1-distill-qwen2.5-7b"
            ],
            "prompts": [
                "task4_specific"
            ],
            "sample_limit": 200
        },
        "task4_large_models": {
            "description": "Evaluate large models on Task 4 - Root Cause Location",
            "datasets": [
                "task4_root_cause"
            ],
            "models": [
                "qwen3-30b",
                "qwen3-30b-thinking",
                "deepseek-coder-v2-lite-16b",
                "deepseek-r1-distill-qwen2.5-32b",
                "wizard-coder-34b",
                "llama4-scout-17b-16e",
                "gemma3-27b"
            ],
            "prompts": [
                "task4_specific"
            ],
            "sample_limit": null
        },
        "task5_small_models": {
            "description": "Evaluate small models on Task 5 - Trigger Point Location",
            "datasets": [
                "task5_trigger_point"
            ],
            "models": [
                "qwen3-4b",
                "qwen3-4b-thinking",
                "qwen3-8b",
                "qwen3-8b-thinking",
                "llama3.2-3B",
                "gemma3-4b",
                "deepseek-r1-distill-qwen2.5-7b"
            ],
            "prompts": [
                "task5_specific"
            ],
            "sample_limit": 200
        },
        "task5_large_models": {
            "description": "Evaluate large models on Task 5 - Trigger Point Location",
            "datasets": [
                "task5_trigger_point"
            ],
            "models": [
                "qwen3-30b",
                "qwen3-30b-thinking",
                "deepseek-coder-v2-lite-16b",
                "deepseek-r1-distill-qwen2.5-32b",
                "wizard-coder-34b",
                "llama4-scout-17b-16e",
                "gemma3-27b"
            ],
            "prompts": [
                "task5_specific"
            ],
            "sample_limit": 100
        }
    },
    "output_settings": {
        "base_output_dir": "results/vuldetectbench_experiments",
        "create_subdirs": true,
        "save_predictions": true,
        "save_metrics": true,
        "save_detailed_report": true,
        "include_timestamp": true
    },
    "evaluation_settings": {
        "metrics": [
            "accuracy",
            "precision",
            "recall",
            "f1",
            "token_recall",
            "line_recall"
        ],
        "task_specific_metrics": {
            "task1": [
                "accuracy",
                "f1",
                "precision",
                "recall"
            ],
            "task2": [
                "accuracy",
                "moderate_evaluation_score",
                "strict_evaluation_score"
            ],
            "task3": [
                "token_recall",
                "macro_token_recall",
                "micro_token_recall"
            ],
            "task4": [
                "union_line_recall",
                "line_recall"
            ],
            "task5": [
                "union_line_recall",
                "line_recall"
            ]
        },
        "create_confusion_matrix": true,
        "analyze_errors": true
    }
}
