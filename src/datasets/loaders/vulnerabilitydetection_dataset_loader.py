#!/usr/bin/env python3
"""
VulnerabilityDetection Dataset Loader

This module provides functionality to load and process VulnerabilityDetection datasets
for vulnerability detection benchmarks. VulnerabilityDetection uses token-level vulnerability
classification with LSTM models and provides pre-processed datasets for various vulnerability types.
"""

import json
import logging
from pathlib import Path
from typing import Any, Optional

from benchmark.benchmark_framework import BenchmarkSample


class VulnerabilityDetectionDatasetLoader:
    """Dataset loader for VulnerabilityDetection benchmark format."""

    def __init__(self):
        self.logger = logging.getLogger(__name__)

    def load_dataset(
        self,
        data_path: str,
        vulnerability_type: str = "sql",
        max_samples: Optional[int] = None,
    ) -> list[BenchmarkSample]:
        """
        Load VulnerabilityDetection dataset from JSON file.

        Args:
            data_path: Path to the dataset JSON file
            vulnerability_type: Type of vulnerability (sql, xss, command_injection, etc.)
            max_samples: Maximum number of samples to load

        Returns:
            list of BenchmarkSample objects
        """
        self.logger.info(f"Loading VulnerabilityDetection dataset: {data_path}")

        data_file = Path(data_path)
        if not data_file.exists():
            raise FileNotFoundError(f"Dataset file not found: {data_path}")

        # Try to detect format - structured JSON vs line-delimited JSON
        try:
            with open(data_file, "r", encoding="utf-8") as f:
                first_line = f.readline().strip()
                f.seek(0)

                # If first line starts with { and we can parse the whole file as JSON,
                # it's likely structured JSON with metadata and samples
                if first_line.startswith("{"):
                    try:
                        data = json.load(f)
                        if isinstance(data, dict) and "samples" in data:
                            # Structured format
                            samples = [
                                BenchmarkSample(**sample) for sample in data["samples"]
                            ]
                            self.logger.info(
                                f"Loaded {len(samples)} samples from structured JSON format"
                            )
                            return samples
                    except json.JSONDecodeError:
                        # Fall back to line-delimited format
                        pass

                # Default to line-delimited JSON format
                samples = self._load_raw_dataset(
                    data_file, vulnerability_type, max_samples
                )

        except Exception as e:
            self.logger.error(f"Error loading dataset from {data_path}: {e}")
            raise

        self.logger.info(f"Loaded {len(samples)} samples from {data_path}")
        return samples

    def load_processed_dataset(self, data_file: Path) -> list[BenchmarkSample]:
        """Load processed dataset from JSON file."""
        self.logger.info(f"Loading processed dataset: {data_file}")

        if not data_file.exists():
            raise FileNotFoundError(f"Processed dataset file not found: {data_file}")

        with open(data_file, "r", encoding="utf-8") as f:
            try:
                data = json.load(f)
                if isinstance(data, dict) and "samples" in data:
                    return [BenchmarkSample(**sample) for sample in data["samples"]]
                else:
                    return [BenchmarkSample(**item) for item in data]
            except json.JSONDecodeError as e:
                self.logger.error(f"Error decoding JSON from {data_file}: {e}")
                raise

    def _load_raw_dataset(
        self, data_path: Path, vulnerability_type: str, max_samples: Optional[int]
    ) -> list[BenchmarkSample]:
        """Load raw dataset from JSON file and convert to BenchmarkSample objects."""
        samples: list[BenchmarkSample] = []

        try:
            with open(data_path, "r", encoding="utf-8") as f:
                # VulnerabilityDetection data is stored as one JSON object per line
                for i, line in enumerate(f):
                    if max_samples and i >= max_samples:
                        break

                    line = line.strip()
                    if not line:
                        continue

                    try:
                        item = json.loads(line)
                        vulnerability_samples = (
                            self._convert_vulnerabilitydetection_item_to_samples(
                                item, i, vulnerability_type
                            )
                        )
                        samples.extend(vulnerability_samples)
                    except json.JSONDecodeError as e:
                        self.logger.warning(f"Error parsing line {i + 1}: {e}")
                        continue
                    except Exception as e:
                        self.logger.warning(f"Error processing item {i}: {e}")
                        continue

        except Exception as e:
            self.logger.error(f"Error loading dataset: {e}")
            raise

        return samples

    def _convert_vulnerabilitydetection_item_to_samples(
        self, item: dict[str, Any], line_num: int, vulnerability_type: str
    ) -> list[BenchmarkSample]:
        """Convert a VulnerabilityDetection item to BenchmarkSample objects."""
        samples = []

        # Extract basic information from VulnerabilityDetection format
        code = item.get("code", "").strip()
        vulnerable_tokens = item.get("vulnerable_tokens", [])
        filename = item.get("filename", f"sample_{line_num}")
        commit_hash = item.get("commit_hash", "")
        repo_url = item.get("repo_url", "")

        if not code:
            return samples

        # Create base metadata
        base_metadata = {
            "vulnerability_type": vulnerability_type,
            "filename": filename,
            "commit_hash": commit_hash,
            "repo_url": repo_url,
            "source": "vulnerabilitydetection",
            "line_number": line_num,
            "vulnerable_tokens": vulnerable_tokens,
            "token_level_analysis": True,
        }

        # Determine vulnerability status based on presence of vulnerable tokens
        is_vulnerable = len(vulnerable_tokens) > 0

        # Binary classification sample
        sample = BenchmarkSample(
            id=f"vulnerabilitydetection_{vulnerability_type}_{line_num}",
            code=code,
            label=1 if is_vulnerable else 0,  # Binary: 1 for vulnerable, 0 for safe
            metadata=base_metadata,
            cwe_types=[self._get_cwe_for_vulnerability_type(vulnerability_type)]
            if is_vulnerable
            else [],
            severity=self._get_vulnerability_severity(vulnerability_type)
            if is_vulnerable
            else None,
        )
        samples.append(sample)

        return samples

    def _get_cwe_for_vulnerability_type(self, vulnerability_type: str) -> str:
        """Map vulnerability type to corresponding CWE."""
        cwe_mapping = {
            "sql": "CWE-89",  # SQL Injection
            "xss": "CWE-79",  # Cross-site Scripting
            "command_injection": "CWE-78",  # OS Command Injection
            "xsrf": "CWE-352",  # Cross-Site Request Forgery
            "path_disclosure": "CWE-200",  # Information Exposure
            "open_redirect": "CWE-601",  # URL Redirection to Untrusted Site
            "remote_code_execution": "CWE-94",  # Code Injection
        }
        return cwe_mapping.get(vulnerability_type, "CWE-Other")

    def _get_vulnerability_severity(self, vulnerability_type: str) -> str:
        """Determine severity based on vulnerability type."""
        high_severity = ["sql", "command_injection", "remote_code_execution", "xss"]
        medium_severity = ["xsrf", "open_redirect"]

        if vulnerability_type in high_severity:
            return "high"
        elif vulnerability_type in medium_severity:
            return "medium"
        else:
            return "low"

    def create_dataset_from_vulnerabilitydetection_data(
        self,
        vulnerabilitydetection_data_dir: str,
        output_path: str,
        vulnerability_type: str = "sql",
    ) -> None:
        """
        Create processed dataset from raw VulnerabilityDetection data.

        Args:
            vulnerabilitydetection_data_dir: Directory containing raw VulnerabilityDetection data
            output_path: Path to save the processed dataset
            vulnerability_type: Type of vulnerability to process
        """
        data_dir = Path(vulnerabilitydetection_data_dir)
        if not data_dir.exists():
            raise FileNotFoundError(
                f"VulnerabilityDetection data directory not found: {vulnerabilitydetection_data_dir}"
            )

        # Look for the plain_{vulnerability_type} file
        vuln_file = data_dir / f"plain_{vulnerability_type}"
        if not vuln_file.exists():
            raise FileNotFoundError(f"Vulnerability data file not found: {vuln_file}")

        processed_data: list[BenchmarkSample] = []

        try:
            # Read the VulnerabilityDetection data file (one JSON per line)
            with open(vuln_file, "r", encoding="utf-8") as f:
                for line_num, line in enumerate(f):
                    line = line.strip()
                    if not line:
                        continue

                    try:
                        item = json.loads(line)
                        processed_item = BenchmarkSample(
                            id=f"vulnerabilitydetection_{vulnerability_type}_{line_num}",
                            code=item.get("code", "").strip(),
                            label=1 if item.get("vulnerable_tokens", []) else 0,
                            metadata={
                                "vulnerability_type": vulnerability_type,
                                "filename": item.get("filename", f"sample_{line_num}"),
                                "commit_hash": item.get("commit_hash", ""),
                                "repo_url": item.get("repo_url", ""),
                                "vulnerable_tokens": item.get("vulnerable_tokens", []),
                                "source": "vulnerabilitydetection",
                                "line_number": line_num,
                            },
                            cwe_types=[
                                self._get_cwe_for_vulnerability_type(vulnerability_type)
                            ]
                            if item.get("vulnerable_tokens", [])
                            else [],
                            severity=self._get_vulnerability_severity(
                                vulnerability_type
                            )
                            if item.get("vulnerable_tokens", [])
                            else None,
                        )

                        processed_data.append(processed_item)

                    except Exception as e:
                        self.logger.warning(f"Error processing line {line_num}: {e}")
                        continue

        except Exception as e:
            self.logger.error(f"Error reading VulnerabilityDetection data: {e}")
            raise

        # Save processed data
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        # Create metadata
        dataset_info = {
            "metadata": {
                "vulnerability_type": vulnerability_type,
                "total_samples": len(processed_data),
                "source": "vulnerabilitydetection",
                "description": f"VulnerabilityDetection {vulnerability_type} vulnerability detection dataset",
            },
            "samples": [sample.__dict__ for sample in processed_data],
        }

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(dataset_info, f, indent=2, ensure_ascii=False)

        self.logger.info(
            f"Created {vulnerability_type} dataset with {len(processed_data)} samples: {output_path}"
        )

    def get_dataset_stats(self, data_file: str) -> dict[str, Any]:
        """
        Generate statistics for a VulnerabilityDetection dataset.

        Args:
            data_file: Path to the dataset file

        Returns:
            dictionary containing dataset statistics
        """
        stats = {
            "total_samples": 0,
            "vulnerable_samples": 0,
            "safe_samples": 0,
            "vulnerability_type": "",
            "average_code_length": 0,
            "average_vulnerable_tokens": 0,
            "dataset_name": Path(data_file).stem,
        }

        try:
            data_path = Path(data_file)

            # Handle both JSON and line-delimited JSON formats
            if data_path.suffix == ".json":
                with open(data_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    if isinstance(data, dict) and "samples" in data:
                        items = data["samples"]
                        stats["vulnerability_type"] = data.get("metadata", {}).get(
                            "vulnerability_type", "unknown"
                        )
                    else:
                        items = data
            else:  # Line-delimited JSON format
                items = []
                with open(data_file, "r", encoding="utf-8") as f:
                    for line in f:
                        line = line.strip()
                        if line:
                            items.append(json.loads(line))

            code_lengths = []
            vulnerable_token_counts = []

            for item in items:
                stats["total_samples"] += 1

                # Vulnerability status
                vulnerable_tokens = item.get("vulnerable_tokens", [])
                if vulnerable_tokens:
                    stats["vulnerable_samples"] += 1
                    vulnerable_token_counts.append(len(vulnerable_tokens))
                else:
                    stats["safe_samples"] += 1

                # Code length
                code = item.get("code", "")
                code_lengths.append(len(code))

            # Calculate averages
            if code_lengths:
                stats["average_code_length"] = sum(code_lengths) / len(code_lengths)
            if vulnerable_token_counts:
                stats["average_vulnerable_tokens"] = sum(vulnerable_token_counts) / len(
                    vulnerable_token_counts
                )

        except Exception as e:
            self.logger.error(f"Error generating statistics: {e}")
            return {}

        return stats


class VulnerabilityDetectionDatasetLoaderFramework:
    """Framework-compatible wrapper for VulnerabilityDetection dataset loader."""

    def __init__(self):
        self.loader = VulnerabilityDetectionDatasetLoader()
        self.logger = logging.getLogger(__name__)

    def load_dataset(
        self,
        dataset_path: str,
        task_type: str = "binary_vulnerability",
        max_samples: Optional[int] = None,
        **kwargs,
    ) -> list[BenchmarkSample]:
        """
        Load dataset compatible with benchmark framework.

        Args:
            dataset_path: Path to the dataset file
            task_type: Task type (binary_vulnerability)
            max_samples: Maximum number of samples to load
            **kwargs: Additional parameters including vulnerability_type

        Returns:
            list of BenchmarkSample objects
        """
        # Extract vulnerability type from kwargs or path
        vulnerability_type = kwargs.get("vulnerability_type", "sql")
        if "_" in Path(dataset_path).stem:
            extracted_type = Path(dataset_path).stem.split("_")[-1]
            if extracted_type in [
                "sql",
                "xss",
                "command_injection",
                "xsrf",
                "path_disclosure",
                "open_redirect",
                "remote_code_execution",
            ]:
                vulnerability_type = extracted_type

        return self.loader.load_dataset(
            dataset_path, vulnerability_type=vulnerability_type, max_samples=max_samples
        )

    def load_processed_dataset(
        self,
        dataset_path: Path,
    ) -> list[BenchmarkSample]:
        """
        Load processed dataset for benchmark framework.

        Args:
            dataset_path: Path to the processed dataset file

        Returns:
            list of BenchmarkSample objects
        """
        return self.loader.load_processed_dataset(dataset_path)

    def get_dataset_info(self, data_file: str) -> dict[str, Any]:
        """
        Get information about the dataset.

        Args:
            data_file: Path to the dataset file

        Returns:
            dictionary with dataset information
        """
        return self.loader.get_dataset_stats(data_file)
