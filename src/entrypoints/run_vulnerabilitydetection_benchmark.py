#!/usr/bin/env python3
"""
VulnerabilityDetection Benchmark Runner

A specialized script for running LLM benchmarks on the VulnerabilityDetection dataset with
flexible configuration options for different vulnerability detection tasks.
"""

import argparse
import dataclasses
import json
import logging
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any

from benchmark.benchmark_framework import BenchmarkConfig, ModelType, TaskType
from datasets.loaders.vulnerabilitydetection_dataset_loader import (
    VulnerabilityDetectionDatasetLoaderFramework,
)


class VulnerabilityDetectionBenchmarkRunner:
    """Custom benchmark runner for VulnerabilityDetection datasets."""

    def __init__(self, config: BenchmarkConfig):
        self.config = config
        self.dataset_loader = VulnerabilityDetectionDatasetLoaderFramework()

    def run_benchmark(self, sample_limit: int | None = None) -> dict[str, Any]:
        """Run benchmark with VulnerabilityDetection-specific dataset loading."""
        from pathlib import Path

        from benchmark.benchmark_framework import (
            HuggingFaceLLM,
            MetricsCalculator,
            PromptGenerator,
            ResponseParser,
            TaskType,
        )

        logging.info("Starting VulnerabilityDetection benchmark execution")
        start_time = time.time()

        try:
            # Load dataset using VulnerabilityDetection loader
            logging.info(
                f"Loading VulnerabilityDetection dataset from: {self.config.dataset_path}"
            )
            samples = self.dataset_loader.load_dataset(self.config.dataset_path)

            # Apply sample limit if specified
            if sample_limit and sample_limit < len(samples):
                samples = samples[:sample_limit]
                logging.info(f"Limited to {sample_limit} samples")

            logging.info(f"Loaded {len(samples)} samples")

            # Initialize components
            llm = HuggingFaceLLM(self.config)
            prompt_generator = PromptGenerator()
            response_parser = ResponseParser(self.config.task_type)
            metrics_calculator = MetricsCalculator()

            # Create output directory
            Path(self.config.output_dir).mkdir(parents=True, exist_ok=True)

            # Run predictions
            predictions = []
            system_prompt = (
                self.config.system_prompt_template
                or prompt_generator.get_system_prompt(
                    self.config.task_type, self.config.cwe_type
                )
            )

            # Run predictions using batch optimization
            from benchmark.benchmark_framework import BenchmarkRunner

            predictions = BenchmarkRunner.process_samples_with_batch_optimization(
                samples=samples,
                llm=llm,
                system_prompt=system_prompt,
                prompt_generator=prompt_generator,
                response_parser=response_parser,
                config=self.config,
            )

            # Calculate metrics
            if self.config.task_type in [
                TaskType.BINARY_VULNERABILITY,
                TaskType.BINARY_CWE_SPECIFIC,
            ]:
                metrics = metrics_calculator.calculate_binary_metrics(predictions)
            else:
                metrics = metrics_calculator.calculate_multiclass_metrics(predictions)

            # Generate results
            total_time = time.time() - start_time
            results = {
                "vulnerability_type": getattr(
                    self.config, "vulnerability_type", "unknown"
                ),
                "dataset_path": self.config.dataset_path,
                "model_name": self.config.model_name,
                "accuracy": metrics.get("accuracy", 0.0),
                "metrics": metrics,
                "total_samples": len(samples),
                "total_time": total_time,
                "predictions": [
                    dataclasses.asdict(prediction) for prediction in predictions
                ],
                "status": "success",
            }

            # Clean up
            llm.cleanup()

            logging.info("VulnerabilityDetection benchmark completed successfully")
            return results

        except Exception as e:
            logging.exception(f"VulnerabilityDetection benchmark failed: {e}")
            raise


def setup_logging(verbose: bool = False) -> None:
    """Setup logging configuration."""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )


def load_vulnerabilitydetection_config(config_path: str) -> dict[str, Any]:
    """Load VulnerabilityDetection experiment configuration."""
    with open(config_path, "r") as f:
        return json.load(f)


def create_benchmark_config(
    model_config: dict[str, Any],
    dataset_config: dict[str, Any],
    prompt_config: dict[str, Any],
    output_dir: str,
) -> BenchmarkConfig:
    """Create benchmark configuration from config dictionaries."""

    # Model type mapping
    model_type_map = {
        "LLAMA": ModelType.LLAMA,
        "QWEN": ModelType.QWEN,
        "DEEPSEEK": ModelType.DEEPSEEK,
        "CODEBERT": ModelType.CODEBERT,
        "WIZARD": ModelType.CUSTOM,
        "GEMMA": ModelType.CUSTOM,
    }

    # Task type mapping
    task_type_map = {
        "binary_vulnerability": TaskType.BINARY_VULNERABILITY,
        "binary_cwe_specific": TaskType.BINARY_CWE_SPECIFIC,
        "multiclass_vulnerability": TaskType.MULTICLASS_VULNERABILITY,
    }

    config = BenchmarkConfig(
        model_name=model_config["model_name"],
        model_type=model_type_map[model_config["model_type"]],
        task_type=task_type_map[dataset_config["task_type"]],
        description=f"{prompt_config['name']} - {dataset_config['description']}",
        dataset_path=dataset_config["dataset_path"],
        output_dir=output_dir,
        batch_size=model_config.get("batch_size", 1),
        max_tokens=model_config.get("max_tokens", 512),
        temperature=model_config.get("temperature", 0.1),
        use_quantization=model_config.get("use_quantization", True),
        cwe_type=dataset_config.get("cwe_type"),
        system_prompt_template=prompt_config["system_prompt"],
        user_prompt_template=prompt_config["user_prompt"],
        enable_thinking=prompt_config.get("enable_thinking", False),
    )

    # Add vulnerability type as an attribute
    if "vulnerability_type" in dataset_config:
        setattr(config, "vulnerability_type", dataset_config["vulnerability_type"])

    return config


def run_single_experiment(
    model_key: str,
    dataset_key: str,
    prompt_key: str,
    vulnerabilitydetection_config: dict[str, Any],
    sample_limit: int | None = None,
    output_base_dir: str = "results/vulnerabilitydetection_experiments",
) -> dict[str, Any]:
    """Run a single experiment with the specified configuration."""

    # Get configurations
    model_config = vulnerabilitydetection_config["model_configurations"][model_key]
    dataset_config = vulnerabilitydetection_config["dataset_configurations"][
        dataset_key
    ]
    prompt_config = vulnerabilitydetection_config["prompt_strategies"][prompt_key]

    # Create experiment name
    experiment_name = f"{model_key}_{dataset_key}_{prompt_key}"
    output_dir = Path(output_base_dir) / experiment_name
    output_dir.mkdir(parents=True, exist_ok=True)

    logger = logging.getLogger(__name__)
    logger.info(f"Running experiment: {experiment_name}")
    logger.info(f"Model: {model_config['model_name']}")
    logger.info(f"Dataset: {dataset_config['description']}")
    logger.info(f"Prompt: {prompt_config['name']}")

    # Create benchmark configuration
    config = create_benchmark_config(
        model_config, dataset_config, prompt_config, str(output_dir)
    )

    # Initialize and run benchmark
    runner = VulnerabilityDetectionBenchmarkRunner(config)

    try:
        results = runner.run_benchmark(sample_limit=sample_limit)

        logger.info("Experiment completed successfully")
        logger.info(f"Accuracy: {results.get('accuracy', 0):.4f}")

        return {
            "experiment_name": experiment_name,
            "status": "success",
            "results": results,
            "accuracy": results.get(
                "accuracy", 0.0
            ),  # Include accuracy at top level for unified runner
            "output_dir": str(output_dir),
            "model_config": model_config,
            "dataset_config": dataset_config,
            "prompt_config": prompt_config,
            "sample_limit": sample_limit,
        }

    except Exception as e:
        logger.exception(f"Experiment failed: {e}")
        return {
            "experiment_name": experiment_name,
            "status": "failed",
            "error": str(e),
            "accuracy": 0.0,  # Include 0 accuracy for failed experiments
            "output_dir": str(output_dir),
        }


def run_experiment_plan(
    plan_name: str,
    vulnerabilitydetection_config: dict[str, Any],
    output_base_dir: str = "results/vulnerabilitydetection_experiments",
    sample_limit: int | None = None,
) -> dict[str, Any]:
    """
    Run a complete experiment plan with multiple configurations.

    Args:
        plan_name: Name of the experiment plan to run
        vulnerabilitydetection_config: VulnerabilityDetection experiment configuration
        output_base_dir: Base output directory
        sample_limit: Limit samples for testing

    Returns:
        dict containing all experiment results
    """
    logger = logging.getLogger(__name__)

    if plan_name not in vulnerabilitydetection_config["experiment_plans"]:
        raise ValueError(f"Unknown experiment plan: {plan_name}")

    plan = vulnerabilitydetection_config["experiment_plans"][plan_name]
    logger.info(f"Starting experiment plan: {plan_name}")
    logger.info(f"Description: {plan['description']}")

    # Override sample limit if specified in plan
    plan_sample_limit = plan.get("sample_limit", sample_limit)

    # Create plan-specific output directory
    plan_output_dir = (
        Path(output_base_dir)
        / f"plan_{plan_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    )
    plan_output_dir.mkdir(parents=True, exist_ok=True)

    results = {
        "plan_name": plan_name,
        "description": plan["description"],
        "start_time": datetime.now().isoformat(),
        "experiments": [],
        "summary": {},
        "output_dir": str(plan_output_dir),
    }

    # Calculate total experiments
    total_experiments = (
        len(plan["datasets"]) * len(plan["models"]) * len(plan["prompts"])
    )
    logger.info(f"Total experiments to run: {total_experiments}")

    experiment_count = 0
    successful_experiments = 0
    failed_experiments = 0

    # Run all combinations
    for dataset_key in plan["datasets"]:
        for model_key in plan["models"]:
            for prompt_key in plan["prompts"]:
                experiment_count += 1

                logger.info(
                    f"Experiment {experiment_count}/{total_experiments}: {model_key} + {dataset_key} + {prompt_key}"
                )

                try:
                    # Run single experiment
                    experiment_result = run_single_experiment(
                        model_key=model_key,
                        dataset_key=dataset_key,
                        prompt_key=prompt_key,
                        vulnerabilitydetection_config=vulnerabilitydetection_config,
                        sample_limit=plan_sample_limit,
                        output_base_dir=str(plan_output_dir),
                    )

                    results["experiments"].append(experiment_result)

                    if experiment_result["status"] == "success":
                        successful_experiments += 1
                        logger.info("✓ Experiment completed successfully")
                    else:
                        failed_experiments += 1
                        logger.error(
                            f"✗ Experiment failed: {experiment_result.get('error', 'Unknown error')}"
                        )

                except Exception as e:
                    failed_experiments += 1
                    error_result = {
                        "experiment_name": f"{model_key}_{dataset_key}_{prompt_key}",
                        "status": "failed",
                        "error": str(e),
                    }
                    results["experiments"].append(error_result)
                    logger.exception(f"✗ Experiment exception: {e}")

                # Brief pause between experiments
                time.sleep(2)

    # Complete results
    results["end_time"] = datetime.now().isoformat()
    results["summary"] = {
        "total_experiments": total_experiments,
        "successful": successful_experiments,
        "failed": failed_experiments,
        "success_rate": successful_experiments / total_experiments
        if total_experiments > 0
        else 0,
    }

    # Save plan results
    plan_results_file = plan_output_dir / "experiment_plan_results.json"
    with open(plan_results_file, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    logger.info(
        f"Experiment plan completed: {successful_experiments}/{total_experiments} successful"
    )
    logger.info(f"Plan results saved to: {plan_results_file}")

    return results


def create_experiment_summary(results: dict[str, Any]) -> str:
    """
    Create a human-readable summary of experiment results.

    Args:
        results: Experiment results dictionary

    Returns:
        str: Formatted summary
    """
    summary_lines = [
        f"VulnerabilityDetection Experiment Plan: {results['plan_name']}",
        f"Description: {results['description']}",
        f"Start Time: {results['start_time']}",
        f"End Time: {results['end_time']}",
        "",
        "Summary:",
        f"  Total Experiments: {results['summary']['total_experiments']}",
        f"  Successful: {results['summary']['successful']}",
        f"  Failed: {results['summary']['failed']}",
        f"  Success Rate: {results['summary']['success_rate']:.1%}",
        "",
        "Individual Experiments:",
    ]

    for exp in results["experiments"]:
        status_icon = "✓" if exp["status"] == "success" else "✗"
        line = f"  {status_icon} {exp['experiment_name']}: {exp['status']}"

        if exp["status"] == "success" and "results" in exp:
            accuracy = exp["results"].get("accuracy", "N/A")
            if accuracy != "N/A":
                line += f" (Accuracy: {accuracy:.3f})"
        elif exp["status"] == "failed":
            line += f" ({exp.get('error', 'Unknown error')})"

        summary_lines.append(line)

    return "\n".join(summary_lines)


def validate_datasets_exist(vulnerabilitydetection_config: dict[str, Any]) -> bool:
    """
    Validate that all required dataset files exist.

    Args:
        vulnerabilitydetection_config: VulnerabilityDetection configuration

    Returns:
        bool: True if all datasets exist
    """
    logger = logging.getLogger(__name__)
    missing_datasets = []

    for dataset_key, dataset_config in vulnerabilitydetection_config[
        "dataset_configurations"
    ].items():
        dataset_path = Path(dataset_config["dataset_path"])
        if not dataset_path.exists():
            missing_datasets.append(dataset_path)

    if missing_datasets:
        logger.error("Missing dataset files:")
        for path in missing_datasets:
            logger.error(f"  - {path}")
        logger.error(
            "Run dataset preparation scripts first to create processed datasets"
        )
        return False

    return True


def list_available_configurations(config_path: str) -> None:
    """List all available configurations."""
    config = load_vulnerabilitydetection_config(config_path)

    print("=== Available VulnerabilityDetection Configurations ===\n")

    print("Models:")
    for key, model in config["model_configurations"].items():
        print(f"  - {key}: {model['model_name']}")

    print("\nDatasets:")
    for key, dataset in config["dataset_configurations"].items():
        print(f"  - {key}: {dataset['description']}")

    print("\nPrompt Strategies:")
    for key, prompt in config["prompt_strategies"].items():
        print(f"  - {key}: {prompt['name']}")

    print("\nExperiment Plans:")
    for key, plan in config["experiment_plans"].items():
        print(f"  - {key}: {plan['description']}")
        print(f"    Models: {', '.join(plan['models'])}")
        print(f"    Datasets: {', '.join(plan['datasets'])}")
        print(f"    Prompts: {', '.join(plan['prompts'])}")


def main():
    """Main function."""
    parser = argparse.ArgumentParser(
        description="Run VulnerabilityDetection benchmark experiments",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Run quick test plan
  python run_vulnerabilitydetection_benchmark.py --plan quick_test
  
  # Run specific configuration
  python run_vulnerabilitydetection_benchmark.py --model qwen3-4b --dataset sql_vulnerabilities --prompt basic_security
  
  # List available configurations and plans
  python run_vulnerabilitydetection_benchmark.py --list-configs
  
  # Validate that datasets exist
  python run_vulnerabilitydetection_benchmark.py --validate-datasets
        """,
    )
    parser.add_argument(
        "--config",
        default="src/configs/vulnerabilitydetection_experiments.json",
        help="Path to experiment configuration file",
    )
    parser.add_argument("--model", help="Model to use for the experiment")
    parser.add_argument("--dataset", help="Dataset to use for the experiment")
    parser.add_argument("--prompt", help="Prompt strategy to use")
    parser.add_argument("--plan", help="Experiment plan to run")
    parser.add_argument(
        "--sample-limit", type=int, help="Limit number of samples for testing"
    )
    parser.add_argument(
        "--output-dir",
        default="results/vulnerabilitydetection_experiments",
        help="Output directory for results",
    )
    parser.add_argument(
        "--list-configs", action="store_true", help="List all available configurations"
    )
    parser.add_argument(
        "--validate-datasets",
        action="store_true",
        help="Validate that all required datasets exist",
    )
    parser.add_argument("--verbose", action="store_true", help="Enable verbose logging")

    args = parser.parse_args()

    setup_logging(args.verbose)
    logger = logging.getLogger(__name__)

    # Load configuration
    try:
        config = load_vulnerabilitydetection_config(args.config)
    except FileNotFoundError:
        logger.error(f"Configuration file not found: {args.config}")
        sys.exit(1)
    except json.JSONDecodeError as e:
        logger.error(f"Invalid JSON in configuration file: {e}")
        sys.exit(1)

    # List configurations if requested
    if args.list_configs:
        list_available_configurations(args.config)
        return

    # Validate datasets if requested
    if args.validate_datasets:
        if validate_datasets_exist(config):
            logger.info("All required datasets found")
        else:
            sys.exit(1)
        return

    # Handle experiment plan execution
    if args.plan:
        # Validate plan exists
        if args.plan not in config["experiment_plans"]:
            logger.error(f"Experiment plan '{args.plan}' not found in configuration")
            sys.exit(1)

        # Validate datasets exist
        if not validate_datasets_exist(config):
            sys.exit(1)

        # Run experiment plan
        try:
            results = run_experiment_plan(
                plan_name=args.plan,
                vulnerabilitydetection_config=config,
                output_base_dir=args.output_dir,
                sample_limit=args.sample_limit,
            )

            # Print summary
            summary = create_experiment_summary(results)
            print("\n" + "=" * 80)
            print(summary)
            print("=" * 80)

            if results["summary"]["failed"] > 0:
                logger.warning("Some experiments failed. Check logs for details.")
                sys.exit(1)
            else:
                logger.info("All experiments completed successfully!")

        except Exception as e:
            logger.error(f"Experiment plan execution failed: {e}")
            sys.exit(1)
        return

    # Validate required arguments for single experiment
    if not args.model or not args.dataset or not args.prompt:
        logger.error("Either --plan or (--model, --dataset, and --prompt) are required")
        logger.info("Use --list-configs to see available options")
        sys.exit(1)

    # Validate configuration keys
    if args.model not in config["model_configurations"]:
        logger.error(f"Model '{args.model}' not found in configuration")
        sys.exit(1)
    if args.dataset not in config["dataset_configurations"]:
        logger.error(f"Dataset '{args.dataset}' not found in configuration")
        sys.exit(1)
    if args.prompt not in config["prompt_strategies"]:
        logger.error(f"Prompt strategy '{args.prompt}' not found in configuration")
        sys.exit(1)

    # Run experiment
    try:
        results = run_single_experiment(
            model_key=args.model,
            dataset_key=args.dataset,
            prompt_key=args.prompt,
            vulnerabilitydetection_config=config,
            sample_limit=args.sample_limit,
            output_base_dir=args.output_dir,
        )

        if results["status"] == "success":
            logger.info("Benchmark completed successfully!")
        else:
            logger.error(f"Benchmark failed: {results.get('error', 'Unknown error')}")
            sys.exit(1)

    except Exception as e:
        logger.error(f"Benchmark execution failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
